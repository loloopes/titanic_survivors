{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 292,
      "metadata": {
        "id": "s15ktC_bHLX2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from time import time\n",
        "from scipy.stats import randint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {
        "id": "zTHZvcBpHN8x"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('./train.csv')\n",
        "test = pd.read_csv('./test.csv')\n",
        "test_ids = test['PassengerId']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {
        "id": "0VDK0N1KHV4n"
      },
      "outputs": [],
      "source": [
        "data.head(10)\n",
        "\n",
        "def clean(data):\n",
        "  data = data.drop(['Ticket', 'Name', 'Cabin', 'Embarked', 'PassengerId'], axis=1)\n",
        "\n",
        "  cols = ['SibSp', 'Parch', 'Fare', 'Age']\n",
        "  for col in cols:\n",
        "    data[col].fillna(data[col].median(), inplace=True)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {
        "id": "eLv_0cgqHXbK"
      },
      "outputs": [],
      "source": [
        "data = clean(data)\n",
        "test = clean(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "gmgAXMLTII2G",
        "outputId": "cf9ba8d3-0a12-4907-ee2a-2db8ea17313b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Survived  Pclass     Sex   Age  SibSp  Parch     Fare\n",
              "0         0       3    male  22.0      1      0   7.2500\n",
              "1         1       1  female  38.0      1      0  71.2833\n",
              "2         1       3  female  26.0      0      0   7.9250\n",
              "3         1       1  female  35.0      1      0  53.1000\n",
              "4         0       3    male  35.0      0      0   8.0500"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-13a8fe60-ed3f-4bbc-9d01-09b23b2af902\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13a8fe60-ed3f-4bbc-9d01-09b23b2af902')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-13a8fe60-ed3f-4bbc-9d01-09b23b2af902 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-13a8fe60-ed3f-4bbc-9d01-09b23b2af902');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8a647dc5-357d-4f03-9121-f139fcc75788\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8a647dc5-357d-4f03-9121-f139fcc75788')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8a647dc5-357d-4f03-9121-f139fcc75788 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 296
        }
      ],
      "source": [
        "data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {
        "id": "hcrghqbrIv1Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "ada95232-3a0d-4324-c89e-38ebcee4a4fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pclass     Sex   Age  SibSp  Parch     Fare\n",
              "0       3    male  34.5      0      0   7.8292\n",
              "1       3  female  47.0      1      0   7.0000\n",
              "2       2    male  62.0      0      0   9.6875\n",
              "3       3    male  27.0      0      0   8.6625\n",
              "4       3  female  22.0      1      1  12.2875"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2a3c0712-73a9-4653-a33c-54063cc24a36\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>34.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.8292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>47.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>male</td>\n",
              "      <td>62.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9.6875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.6625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12.2875</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a3c0712-73a9-4653-a33c-54063cc24a36')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2a3c0712-73a9-4653-a33c-54063cc24a36 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2a3c0712-73a9-4653-a33c-54063cc24a36');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4541b530-f5b6-4b76-aac2-9c2dd3121920\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4541b530-f5b6-4b76-aac2-9c2dd3121920')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4541b530-f5b6-4b76-aac2-9c2dd3121920 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 297
        }
      ],
      "source": [
        "test.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "data['Sex_encoded'] = le.fit_transform(data['Sex'])\n",
        "test['Sex_encoded'] = le.fit_transform(test['Sex'])"
      ],
      "metadata": {
        "id": "lxtfh4eMS3Ws"
      },
      "execution_count": 298,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "lGcHe_mET8PP",
        "outputId": "5e2be8d1-62bc-4a5e-9899-1d0f3a4c3877"
      },
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Survived  Pclass     Sex   Age  SibSp  Parch     Fare  Sex_encoded\n",
              "0         0       3    male  22.0      1      0   7.2500            1\n",
              "1         1       1  female  38.0      1      0  71.2833            0\n",
              "2         1       3  female  26.0      0      0   7.9250            0\n",
              "3         1       1  female  35.0      1      0  53.1000            0\n",
              "4         0       3    male  35.0      0      0   8.0500            1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-887f097a-8178-48a8-af02-9d3378bd4d6c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Sex_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-887f097a-8178-48a8-af02-9d3378bd4d6c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-887f097a-8178-48a8-af02-9d3378bd4d6c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-887f097a-8178-48a8-af02-9d3378bd4d6c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-55c0fd4f-3ae0-414e-8fb8-6edff49be410\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-55c0fd4f-3ae0-414e-8fb8-6edff49be410')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-55c0fd4f-3ae0-414e-8fb8-6edff49be410 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 299
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "CUGcx-O9Ugl8",
        "outputId": "91fb44aa-39cd-433e-8834-cd8b1396a273"
      },
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pclass     Sex   Age  SibSp  Parch     Fare  Sex_encoded\n",
              "0       3    male  34.5      0      0   7.8292            1\n",
              "1       3  female  47.0      1      0   7.0000            0\n",
              "2       2    male  62.0      0      0   9.6875            1\n",
              "3       3    male  27.0      0      0   8.6625            1\n",
              "4       3  female  22.0      1      1  12.2875            0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dd932265-c19a-4f60-8e17-083830423529\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Sex_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>34.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.8292</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>47.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>male</td>\n",
              "      <td>62.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9.6875</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.6625</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12.2875</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd932265-c19a-4f60-8e17-083830423529')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dd932265-c19a-4f60-8e17-083830423529 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dd932265-c19a-4f60-8e17-083830423529');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a41a804d-8201-4d58-ab7d-f39f06a23049\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a41a804d-8201-4d58-ab7d-f39f06a23049')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a41a804d-8201-4d58-ab7d-f39f06a23049 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 300
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A3HkUo7Rg-kq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "PDdsR5HtK8aT",
        "outputId": "ff6a56c5-6def-4b4b-be98-a014ebb405b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Survived  Pclass     Sex   Age  SibSp  Parch     Fare  Sex_encoded\n",
              "0         0       3    male  22.0      1      0   7.2500            1\n",
              "1         1       1  female  38.0      1      0  71.2833            0\n",
              "2         1       3  female  26.0      0      0   7.9250            0\n",
              "3         1       1  female  35.0      1      0  53.1000            0\n",
              "4         0       3    male  35.0      0      0   8.0500            1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4386b8dd-f539-4d8a-bd78-80d446e5b789\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Sex_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4386b8dd-f539-4d8a-bd78-80d446e5b789')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4386b8dd-f539-4d8a-bd78-80d446e5b789 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4386b8dd-f539-4d8a-bd78-80d446e5b789');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e275d211-64ee-4ffb-8fc4-aad54842523d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e275d211-64ee-4ffb-8fc4-aad54842523d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e275d211-64ee-4ffb-8fc4-aad54842523d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 301
        }
      ],
      "source": [
        "data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop('Sex', axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "11JJXSYyhCkr",
        "outputId": "4f1c327e-1655-4eff-9a48-e1460f8c479f"
      },
      "execution_count": 302,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Survived  Pclass   Age  SibSp  Parch     Fare  Sex_encoded\n",
              "0           0       3  22.0      1      0   7.2500            1\n",
              "1           1       1  38.0      1      0  71.2833            0\n",
              "2           1       3  26.0      0      0   7.9250            0\n",
              "3           1       1  35.0      1      0  53.1000            0\n",
              "4           0       3  35.0      0      0   8.0500            1\n",
              "..        ...     ...   ...    ...    ...      ...          ...\n",
              "886         0       2  27.0      0      0  13.0000            1\n",
              "887         1       1  19.0      0      0  30.0000            0\n",
              "888         0       3  28.0      1      2  23.4500            0\n",
              "889         1       1  26.0      0      0  30.0000            1\n",
              "890         0       3  32.0      0      0   7.7500            1\n",
              "\n",
              "[891 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-04e9d32a-a9e7-4a35-a2b6-8e4b7a4f41de\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Sex_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>886</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13.0000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>887</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>888</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>28.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>23.4500</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>889</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>890</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.7500</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>891 rows  7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-04e9d32a-a9e7-4a35-a2b6-8e4b7a4f41de')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-04e9d32a-a9e7-4a35-a2b6-8e4b7a4f41de button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-04e9d32a-a9e7-4a35-a2b6-8e4b7a4f41de');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2e6a6220-9290-41df-9330-381046480470\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2e6a6220-9290-41df-9330-381046480470')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2e6a6220-9290-41df-9330-381046480470 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 302
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "t4Chr5UPF0_N",
        "outputId": "01f16b6e-bce9-426b-f02b-631d732719ac"
      },
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pclass     Sex   Age  SibSp  Parch     Fare  Sex_encoded\n",
              "0       3    male  34.5      0      0   7.8292            1\n",
              "1       3  female  47.0      1      0   7.0000            0\n",
              "2       2    male  62.0      0      0   9.6875            1\n",
              "3       3    male  27.0      0      0   8.6625            1\n",
              "4       3  female  22.0      1      1  12.2875            0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f52bae21-d83c-47bb-975e-3df805cbf11b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Sex_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>34.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.8292</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>47.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>male</td>\n",
              "      <td>62.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9.6875</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.6625</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12.2875</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f52bae21-d83c-47bb-975e-3df805cbf11b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f52bae21-d83c-47bb-975e-3df805cbf11b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f52bae21-d83c-47bb-975e-3df805cbf11b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c1821b07-9549-466e-aefb-327b07d62859\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c1821b07-9549-466e-aefb-327b07d62859')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c1821b07-9549-466e-aefb-327b07d62859 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 303
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 312,
      "metadata": {
        "id": "VI0RSX2sLLV8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "fed9282d-f09e-4590-99e5-a1b7b8bbde9a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pclass   Age  SibSp  Parch     Fare  Sex_encoded\n",
              "0       3  22.0      1      0   7.2500            1\n",
              "1       1  38.0      1      0  71.2833            0\n",
              "2       3  26.0      0      0   7.9250            0\n",
              "3       1  35.0      1      0  53.1000            0\n",
              "4       3  35.0      0      0   8.0500            1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6c1e0414-dc46-4107-b823-8f3388f3b973\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Sex_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c1e0414-dc46-4107-b823-8f3388f3b973')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6c1e0414-dc46-4107-b823-8f3388f3b973 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6c1e0414-dc46-4107-b823-8f3388f3b973');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-df443986-99eb-42c4-9257-0c0b9e8e3a5f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-df443986-99eb-42c4-9257-0c0b9e8e3a5f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-df443986-99eb-42c4-9257-0c0b9e8e3a5f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 312
        }
      ],
      "source": [
        "X = data.drop(['Survived', 'Sex'], axis=1)\n",
        "X.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {
        "id": "L7bMXkHLLyTW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eef3204e-31b9-4a7d-f23e-748326d51ccb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0\n",
              "1    1\n",
              "2    1\n",
              "3    1\n",
              "4    0\n",
              "Name: Survived, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 313
        }
      ],
      "source": [
        "y = data['Survived']\n",
        "y.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {
        "id": "l3RFYtKnOTeN"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 315,
      "metadata": {
        "id": "WLS72D_POspp"
      },
      "outputs": [],
      "source": [
        "column_types = X_train.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAgq1qSiTAyu",
        "outputId": "61768430-f266-4036-af7c-866364106972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pclass           int64\n",
            "Age            float64\n",
            "SibSp            int64\n",
            "Parch            int64\n",
            "Fare           float64\n",
            "Sex_encoded      int64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(column_types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 317,
      "metadata": {
        "id": "b9w16pw_TM04"
      },
      "outputs": [],
      "source": [
        "#X_train\n",
        "#X_train.shape\n",
        "#y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 318,
      "metadata": {
        "id": "aWK9TqZXXvg9"
      },
      "outputs": [],
      "source": [
        "nn = MLPClassifier(verbose=True, max_iter=1000, hidden_layer_sizes=(100, 100), solver='sgd')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 319,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "045SNlKPZdnT",
        "outputId": "f0a839d9-4a46-4a3c-b4ef-7b2a43b1f4cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.97096544\n",
            "Iteration 2, loss = 1.00917032\n",
            "Iteration 3, loss = 1.00279464\n",
            "Iteration 4, loss = 0.66885620\n",
            "Iteration 5, loss = 0.67909912\n",
            "Iteration 6, loss = 0.62995661\n",
            "Iteration 7, loss = 0.59916725\n",
            "Iteration 8, loss = 0.60136930\n",
            "Iteration 9, loss = 0.63813766\n",
            "Iteration 10, loss = 0.59491172\n",
            "Iteration 11, loss = 0.59474545\n",
            "Iteration 12, loss = 0.62098108\n",
            "Iteration 13, loss = 0.58453993\n",
            "Iteration 14, loss = 0.58464589\n",
            "Iteration 15, loss = 0.58505723\n",
            "Iteration 16, loss = 0.59848472\n",
            "Iteration 17, loss = 0.58813172\n",
            "Iteration 18, loss = 0.58303213\n",
            "Iteration 19, loss = 0.62378133\n",
            "Iteration 20, loss = 0.67448713\n",
            "Iteration 21, loss = 0.64100736\n",
            "Iteration 22, loss = 0.57704878\n",
            "Iteration 23, loss = 0.59262297\n",
            "Iteration 24, loss = 0.57654514\n",
            "Iteration 25, loss = 0.57608213\n",
            "Iteration 26, loss = 0.61012853\n",
            "Iteration 27, loss = 0.57474881\n",
            "Iteration 28, loss = 0.56729290\n",
            "Iteration 29, loss = 0.57454485\n",
            "Iteration 30, loss = 0.57711413\n",
            "Iteration 31, loss = 0.57697761\n",
            "Iteration 32, loss = 0.56632103\n",
            "Iteration 33, loss = 0.60141395\n",
            "Iteration 34, loss = 0.60111544\n",
            "Iteration 35, loss = 0.56486348\n",
            "Iteration 36, loss = 0.57553133\n",
            "Iteration 37, loss = 0.55767307\n",
            "Iteration 38, loss = 0.56047262\n",
            "Iteration 39, loss = 0.55607974\n",
            "Iteration 40, loss = 0.55961209\n",
            "Iteration 41, loss = 0.56795757\n",
            "Iteration 42, loss = 0.59895981\n",
            "Iteration 43, loss = 0.58583393\n",
            "Iteration 44, loss = 0.56474842\n",
            "Iteration 45, loss = 0.55719900\n",
            "Iteration 46, loss = 0.55597847\n",
            "Iteration 47, loss = 0.55200912\n",
            "Iteration 48, loss = 0.55868883\n",
            "Iteration 49, loss = 0.55290825\n",
            "Iteration 50, loss = 0.57638079\n",
            "Iteration 51, loss = 0.56616110\n",
            "Iteration 52, loss = 0.56865368\n",
            "Iteration 53, loss = 0.56337153\n",
            "Iteration 54, loss = 0.60636239\n",
            "Iteration 55, loss = 0.59161187\n",
            "Iteration 56, loss = 0.56289943\n",
            "Iteration 57, loss = 0.64399687\n",
            "Iteration 58, loss = 0.56789234\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, solver='sgd',\n",
              "              verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, solver=&#x27;sgd&#x27;,\n",
              "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, solver=&#x27;sgd&#x27;,\n",
              "              verbose=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 319
        }
      ],
      "source": [
        "nn.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 320,
      "metadata": {
        "id": "oBivNUtbZmFM"
      },
      "outputs": [],
      "source": [
        "forecast = nn.predict(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShxjPEWNd8e1",
        "outputId": "7ddf5f31-61d4-45ed-c102-4b21433785a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 321
        }
      ],
      "source": [
        "forecast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 322,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJTrREBpeA6Z",
        "outputId": "b0edfd22-aa66-4e94-f8dd-e622f984b7c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7238805970149254"
            ]
          },
          "metadata": {},
          "execution_count": 322
        }
      ],
      "source": [
        "accuracy_score(y_val, forecast)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 323,
      "metadata": {
        "id": "5N-ULfqNeZK2"
      },
      "outputs": [],
      "source": [
        "cnf_matrix = confusion_matrix(y_val, forecast)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnf_table = pd.DataFrame(data=cnf_matrix, index=['Survived', 'Died'], columns=['Survived', 'Died'])"
      ],
      "metadata": {
        "id": "2Yxfg3aNzmMI"
      },
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cnf_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAABHGj-ztDD",
        "outputId": "5cd7cd8c-3a9a-4950-a64a-ce9ca8d11ec3"
      },
      "execution_count": 325,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Survived  Died\n",
            "Survived        77     2\n",
            "Died            35    20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "metadata": {
        "id": "xo2elIG2fhcc"
      },
      "outputs": [],
      "source": [
        "nn_params = {\n",
        "    'activation': ['relu', 'logistic', 'tanh'],\n",
        "    'solver': ['adam', 'sgd'],\n",
        "    'batch_size': [10, 60],\n",
        "    'hidden_layer_sizes': [(25, 25), (50, 50), (100, 100), (200, 200)],\n",
        "    'max_iter': [200, 400]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 327,
      "metadata": {
        "id": "hpj9nihJiz6p"
      },
      "outputs": [],
      "source": [
        "random_search = RandomizedSearchCV(\n",
        "    nn,\n",
        "    param_distributions = nn_params,\n",
        "    cv = 5,\n",
        "    random_state = 0,\n",
        "    n_iter=50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 328,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uHUqXSVjAg9",
        "outputId": "1715eca7-28a4-4273-a2a7-fbebc797eab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.81517694\n",
            "Iteration 2, loss = 0.66504990\n",
            "Iteration 3, loss = 0.60322093\n",
            "Iteration 4, loss = 0.59811873\n",
            "Iteration 5, loss = 0.58515758\n",
            "Iteration 6, loss = 0.60843674\n",
            "Iteration 7, loss = 0.61057117\n",
            "Iteration 8, loss = 0.55572253\n",
            "Iteration 9, loss = 0.56928130\n",
            "Iteration 10, loss = 0.54615755\n",
            "Iteration 11, loss = 0.56777310\n",
            "Iteration 12, loss = 0.58847395\n",
            "Iteration 13, loss = 0.57088635\n",
            "Iteration 14, loss = 0.55462157\n",
            "Iteration 15, loss = 0.60026670\n",
            "Iteration 16, loss = 0.56268552\n",
            "Iteration 17, loss = 0.58976590\n",
            "Iteration 18, loss = 0.50664993\n",
            "Iteration 19, loss = 0.55783731\n",
            "Iteration 20, loss = 0.52546589\n",
            "Iteration 21, loss = 0.52622051\n",
            "Iteration 22, loss = 0.52024773\n",
            "Iteration 23, loss = 0.57162084\n",
            "Iteration 24, loss = 0.48268329\n",
            "Iteration 25, loss = 0.48920952\n",
            "Iteration 26, loss = 0.48056456\n",
            "Iteration 27, loss = 0.49738002\n",
            "Iteration 28, loss = 0.48456380\n",
            "Iteration 29, loss = 0.49721217\n",
            "Iteration 30, loss = 0.47107509\n",
            "Iteration 31, loss = 0.46646641\n",
            "Iteration 32, loss = 0.48012024\n",
            "Iteration 33, loss = 0.55389027\n",
            "Iteration 34, loss = 0.49866970\n",
            "Iteration 35, loss = 0.45891558\n",
            "Iteration 36, loss = 0.47311433\n",
            "Iteration 37, loss = 0.63902378\n",
            "Iteration 38, loss = 0.51192601\n",
            "Iteration 39, loss = 0.48560825\n",
            "Iteration 40, loss = 0.46456672\n",
            "Iteration 41, loss = 0.45681306\n",
            "Iteration 42, loss = 0.47266536\n",
            "Iteration 43, loss = 0.52114981\n",
            "Iteration 44, loss = 0.73968458\n",
            "Iteration 45, loss = 0.53853549\n",
            "Iteration 46, loss = 0.48840340\n",
            "Iteration 47, loss = 0.72482044\n",
            "Iteration 48, loss = 0.59294983\n",
            "Iteration 49, loss = 0.48088725\n",
            "Iteration 50, loss = 0.45978824\n",
            "Iteration 51, loss = 0.48419591\n",
            "Iteration 52, loss = 0.45310507\n",
            "Iteration 53, loss = 0.49970458\n",
            "Iteration 54, loss = 0.49762417\n",
            "Iteration 55, loss = 0.45849049\n",
            "Iteration 56, loss = 0.45888534\n",
            "Iteration 57, loss = 0.46618183\n",
            "Iteration 58, loss = 0.43396293\n",
            "Iteration 59, loss = 0.42898222\n",
            "Iteration 60, loss = 0.44488762\n",
            "Iteration 61, loss = 0.45843866\n",
            "Iteration 62, loss = 0.44003815\n",
            "Iteration 63, loss = 0.45124990\n",
            "Iteration 64, loss = 0.44182271\n",
            "Iteration 65, loss = 0.42501189\n",
            "Iteration 66, loss = 0.44302565\n",
            "Iteration 67, loss = 0.43746918\n",
            "Iteration 68, loss = 0.44823448\n",
            "Iteration 69, loss = 0.43225234\n",
            "Iteration 70, loss = 0.42734012\n",
            "Iteration 71, loss = 0.42915876\n",
            "Iteration 72, loss = 0.43814303\n",
            "Iteration 73, loss = 0.45583927\n",
            "Iteration 74, loss = 0.44643972\n",
            "Iteration 75, loss = 0.44789324\n",
            "Iteration 76, loss = 0.42735410\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.73270464\n",
            "Iteration 2, loss = 0.70890392\n",
            "Iteration 3, loss = 0.60144740\n",
            "Iteration 4, loss = 0.64834728\n",
            "Iteration 5, loss = 0.63792712\n",
            "Iteration 6, loss = 0.57473871\n",
            "Iteration 7, loss = 0.59275970\n",
            "Iteration 8, loss = 0.57003298\n",
            "Iteration 9, loss = 0.56120935\n",
            "Iteration 10, loss = 0.53112334\n",
            "Iteration 11, loss = 0.55084592\n",
            "Iteration 12, loss = 0.51728084\n",
            "Iteration 13, loss = 0.50775354\n",
            "Iteration 14, loss = 0.50828082\n",
            "Iteration 15, loss = 0.51475321\n",
            "Iteration 16, loss = 0.54644842\n",
            "Iteration 17, loss = 0.84365384\n",
            "Iteration 18, loss = 0.68780286\n",
            "Iteration 19, loss = 0.51779529\n",
            "Iteration 20, loss = 0.55263674\n",
            "Iteration 21, loss = 0.51787907\n",
            "Iteration 22, loss = 0.53159266\n",
            "Iteration 23, loss = 0.48517028\n",
            "Iteration 24, loss = 0.52809650\n",
            "Iteration 25, loss = 0.51840994\n",
            "Iteration 26, loss = 0.46717345\n",
            "Iteration 27, loss = 0.47556447\n",
            "Iteration 28, loss = 0.46358833\n",
            "Iteration 29, loss = 0.47687146\n",
            "Iteration 30, loss = 0.47394020\n",
            "Iteration 31, loss = 0.45058576\n",
            "Iteration 32, loss = 0.45189420\n",
            "Iteration 33, loss = 0.48241280\n",
            "Iteration 34, loss = 0.50200479\n",
            "Iteration 35, loss = 0.49509179\n",
            "Iteration 36, loss = 0.47251115\n",
            "Iteration 37, loss = 0.45152298\n",
            "Iteration 38, loss = 0.47030909\n",
            "Iteration 39, loss = 0.49787206\n",
            "Iteration 40, loss = 0.49320126\n",
            "Iteration 41, loss = 0.50608221\n",
            "Iteration 42, loss = 0.47263635\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.72076975\n",
            "Iteration 2, loss = 0.68363433\n",
            "Iteration 3, loss = 0.61887086\n",
            "Iteration 4, loss = 0.60035099\n",
            "Iteration 5, loss = 0.62389465\n",
            "Iteration 6, loss = 0.59050036\n",
            "Iteration 7, loss = 0.61100718\n",
            "Iteration 8, loss = 0.57045555\n",
            "Iteration 9, loss = 0.61105561\n",
            "Iteration 10, loss = 0.59328498\n",
            "Iteration 11, loss = 0.57888700\n",
            "Iteration 12, loss = 0.55312741\n",
            "Iteration 13, loss = 0.55937312\n",
            "Iteration 14, loss = 0.57246345\n",
            "Iteration 15, loss = 0.55153789\n",
            "Iteration 16, loss = 0.55293160\n",
            "Iteration 17, loss = 0.54277861\n",
            "Iteration 18, loss = 0.54994256\n",
            "Iteration 19, loss = 0.53678866\n",
            "Iteration 20, loss = 0.50744350\n",
            "Iteration 21, loss = 0.52683101\n",
            "Iteration 22, loss = 0.54311043\n",
            "Iteration 23, loss = 0.52929843\n",
            "Iteration 24, loss = 0.49078611\n",
            "Iteration 25, loss = 0.49677820\n",
            "Iteration 26, loss = 0.54135426\n",
            "Iteration 27, loss = 0.52786190\n",
            "Iteration 28, loss = 0.50534194\n",
            "Iteration 29, loss = 0.52639917\n",
            "Iteration 30, loss = 0.50957177\n",
            "Iteration 31, loss = 0.49616066\n",
            "Iteration 32, loss = 0.49594493\n",
            "Iteration 33, loss = 0.47588485\n",
            "Iteration 34, loss = 0.50585835\n",
            "Iteration 35, loss = 0.50886128\n",
            "Iteration 36, loss = 0.47386980\n",
            "Iteration 37, loss = 0.49818337\n",
            "Iteration 38, loss = 0.48889684\n",
            "Iteration 39, loss = 0.51295102\n",
            "Iteration 40, loss = 0.50871038\n",
            "Iteration 41, loss = 0.48638491\n",
            "Iteration 42, loss = 0.45185166\n",
            "Iteration 43, loss = 0.45507036\n",
            "Iteration 44, loss = 0.48902011\n",
            "Iteration 45, loss = 0.58487378\n",
            "Iteration 46, loss = 0.47484103\n",
            "Iteration 47, loss = 0.46494160\n",
            "Iteration 48, loss = 0.45530547\n",
            "Iteration 49, loss = 0.45524520\n",
            "Iteration 50, loss = 0.49418217\n",
            "Iteration 51, loss = 0.45978115\n",
            "Iteration 52, loss = 0.46476034\n",
            "Iteration 53, loss = 0.44595908\n",
            "Iteration 54, loss = 0.44553210\n",
            "Iteration 55, loss = 0.44349274\n",
            "Iteration 56, loss = 0.46741446\n",
            "Iteration 57, loss = 0.51405565\n",
            "Iteration 58, loss = 0.46378589\n",
            "Iteration 59, loss = 0.44056774\n",
            "Iteration 60, loss = 0.57590901\n",
            "Iteration 61, loss = 0.53275822\n",
            "Iteration 62, loss = 0.49645245\n",
            "Iteration 63, loss = 0.45734277\n",
            "Iteration 64, loss = 0.43848597\n",
            "Iteration 65, loss = 0.44168172\n",
            "Iteration 66, loss = 0.44393520\n",
            "Iteration 67, loss = 0.46005851\n",
            "Iteration 68, loss = 0.45410941\n",
            "Iteration 69, loss = 0.49276579\n",
            "Iteration 70, loss = 0.45866178\n",
            "Iteration 71, loss = 0.44646321\n",
            "Iteration 72, loss = 0.43908360\n",
            "Iteration 73, loss = 0.44059617\n",
            "Iteration 74, loss = 0.44491101\n",
            "Iteration 75, loss = 0.44726910\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.72786406\n",
            "Iteration 2, loss = 0.60735192\n",
            "Iteration 3, loss = 0.58476168\n",
            "Iteration 4, loss = 0.61726287\n",
            "Iteration 5, loss = 0.71356329\n",
            "Iteration 6, loss = 0.62585306\n",
            "Iteration 7, loss = 0.64632793\n",
            "Iteration 8, loss = 0.63858772\n",
            "Iteration 9, loss = 0.57476384\n",
            "Iteration 10, loss = 0.56021655\n",
            "Iteration 11, loss = 0.57893122\n",
            "Iteration 12, loss = 0.62153689\n",
            "Iteration 13, loss = 0.61043223\n",
            "Iteration 14, loss = 0.61787380\n",
            "Iteration 15, loss = 0.56850557\n",
            "Iteration 16, loss = 0.58476966\n",
            "Iteration 17, loss = 0.53571473\n",
            "Iteration 18, loss = 0.52911249\n",
            "Iteration 19, loss = 0.57913595\n",
            "Iteration 20, loss = 0.53482042\n",
            "Iteration 21, loss = 0.52120888\n",
            "Iteration 22, loss = 0.55328907\n",
            "Iteration 23, loss = 0.64432351\n",
            "Iteration 24, loss = 0.51922292\n",
            "Iteration 25, loss = 0.51860986\n",
            "Iteration 26, loss = 0.50808856\n",
            "Iteration 27, loss = 0.51932808\n",
            "Iteration 28, loss = 0.50076101\n",
            "Iteration 29, loss = 0.48263415\n",
            "Iteration 30, loss = 0.50779508\n",
            "Iteration 31, loss = 0.49029413\n",
            "Iteration 32, loss = 0.49048297\n",
            "Iteration 33, loss = 0.47804307\n",
            "Iteration 34, loss = 0.47998109\n",
            "Iteration 35, loss = 0.52826416\n",
            "Iteration 36, loss = 0.47115171\n",
            "Iteration 37, loss = 0.46463021\n",
            "Iteration 38, loss = 0.50447735\n",
            "Iteration 39, loss = 0.47709422\n",
            "Iteration 40, loss = 0.48365949\n",
            "Iteration 41, loss = 0.48744090\n",
            "Iteration 42, loss = 0.49550671\n",
            "Iteration 43, loss = 0.47446823\n",
            "Iteration 44, loss = 0.46460426\n",
            "Iteration 45, loss = 0.45149570\n",
            "Iteration 46, loss = 0.45920092\n",
            "Iteration 47, loss = 0.45147761\n",
            "Iteration 48, loss = 0.44573440\n",
            "Iteration 49, loss = 0.46607217\n",
            "Iteration 50, loss = 0.56366295\n",
            "Iteration 51, loss = 0.57327043\n",
            "Iteration 52, loss = 0.59590300\n",
            "Iteration 53, loss = 0.46115641\n",
            "Iteration 54, loss = 0.45865402\n",
            "Iteration 55, loss = 0.45635085\n",
            "Iteration 56, loss = 0.47190847\n",
            "Iteration 57, loss = 0.45244452\n",
            "Iteration 58, loss = 0.47877761\n",
            "Iteration 59, loss = 0.44357177\n",
            "Iteration 60, loss = 0.47752544\n",
            "Iteration 61, loss = 0.48840898\n",
            "Iteration 62, loss = 0.52920795\n",
            "Iteration 63, loss = 0.45163891\n",
            "Iteration 64, loss = 0.43662537\n",
            "Iteration 65, loss = 0.45756252\n",
            "Iteration 66, loss = 0.44336464\n",
            "Iteration 67, loss = 0.44543225\n",
            "Iteration 68, loss = 0.45655088\n",
            "Iteration 69, loss = 0.44970279\n",
            "Iteration 70, loss = 0.47691758\n",
            "Iteration 71, loss = 0.43068355\n",
            "Iteration 72, loss = 0.43346153\n",
            "Iteration 73, loss = 0.44003713\n",
            "Iteration 74, loss = 0.43508628\n",
            "Iteration 75, loss = 0.47269211\n",
            "Iteration 76, loss = 0.46705532\n",
            "Iteration 77, loss = 0.46738022\n",
            "Iteration 78, loss = 0.43129160\n",
            "Iteration 79, loss = 0.46979643\n",
            "Iteration 80, loss = 0.44033437\n",
            "Iteration 81, loss = 0.42698353\n",
            "Iteration 82, loss = 0.43527626\n",
            "Iteration 83, loss = 0.42572620\n",
            "Iteration 84, loss = 0.42779218\n",
            "Iteration 85, loss = 0.48986531\n",
            "Iteration 86, loss = 0.47231174\n",
            "Iteration 87, loss = 0.57374024\n",
            "Iteration 88, loss = 0.50939660\n",
            "Iteration 89, loss = 0.48239654\n",
            "Iteration 90, loss = 0.43059916\n",
            "Iteration 91, loss = 0.44655344\n",
            "Iteration 92, loss = 0.42024903\n",
            "Iteration 93, loss = 0.46061989\n",
            "Iteration 94, loss = 0.45368881\n",
            "Iteration 95, loss = 0.44781910\n",
            "Iteration 96, loss = 0.43321244\n",
            "Iteration 97, loss = 0.42772093\n",
            "Iteration 98, loss = 0.42020540\n",
            "Iteration 99, loss = 0.51502667\n",
            "Iteration 100, loss = 0.45237477\n",
            "Iteration 101, loss = 0.43219343\n",
            "Iteration 102, loss = 0.42181302\n",
            "Iteration 103, loss = 0.44167149\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.87852652\n",
            "Iteration 2, loss = 0.67640179\n",
            "Iteration 3, loss = 0.66987571\n",
            "Iteration 4, loss = 0.65955938\n",
            "Iteration 5, loss = 0.64520872\n",
            "Iteration 6, loss = 0.62871876\n",
            "Iteration 7, loss = 0.59892504\n",
            "Iteration 8, loss = 0.62675748\n",
            "Iteration 9, loss = 0.58040447\n",
            "Iteration 10, loss = 0.58061997\n",
            "Iteration 11, loss = 0.56051003\n",
            "Iteration 12, loss = 0.60824571\n",
            "Iteration 13, loss = 0.62778973\n",
            "Iteration 14, loss = 0.65591540\n",
            "Iteration 15, loss = 0.70523930\n",
            "Iteration 16, loss = 0.56707192\n",
            "Iteration 17, loss = 0.57908445\n",
            "Iteration 18, loss = 0.52541151\n",
            "Iteration 19, loss = 0.53511549\n",
            "Iteration 20, loss = 0.56189317\n",
            "Iteration 21, loss = 0.55103799\n",
            "Iteration 22, loss = 0.53846921\n",
            "Iteration 23, loss = 0.58728421\n",
            "Iteration 24, loss = 0.53972053\n",
            "Iteration 25, loss = 0.50923976\n",
            "Iteration 26, loss = 0.49552172\n",
            "Iteration 27, loss = 0.49916989\n",
            "Iteration 28, loss = 0.49712304\n",
            "Iteration 29, loss = 0.53819430\n",
            "Iteration 30, loss = 0.51737605\n",
            "Iteration 31, loss = 0.52288461\n",
            "Iteration 32, loss = 0.50626881\n",
            "Iteration 33, loss = 0.48102659\n",
            "Iteration 34, loss = 0.57566445\n",
            "Iteration 35, loss = 0.56765716\n",
            "Iteration 36, loss = 0.48179526\n",
            "Iteration 37, loss = 0.47676110\n",
            "Iteration 38, loss = 0.48669987\n",
            "Iteration 39, loss = 0.50612236\n",
            "Iteration 40, loss = 0.46901548\n",
            "Iteration 41, loss = 0.47125437\n",
            "Iteration 42, loss = 0.46915663\n",
            "Iteration 43, loss = 0.50115634\n",
            "Iteration 44, loss = 0.49496421\n",
            "Iteration 45, loss = 0.85417246\n",
            "Iteration 46, loss = 0.62801805\n",
            "Iteration 47, loss = 0.51282667\n",
            "Iteration 48, loss = 0.48882646\n",
            "Iteration 49, loss = 0.54065595\n",
            "Iteration 50, loss = 0.51112627\n",
            "Iteration 51, loss = 0.47256766\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.63829327\n",
            "Iteration 2, loss = 0.60795105\n",
            "Iteration 3, loss = 0.60207377\n",
            "Iteration 4, loss = 0.60404605\n",
            "Iteration 5, loss = 0.59623930\n",
            "Iteration 6, loss = 0.59405160\n",
            "Iteration 7, loss = 0.59861498\n",
            "Iteration 8, loss = 0.58310396\n",
            "Iteration 9, loss = 0.58339160\n",
            "Iteration 10, loss = 0.58020522\n",
            "Iteration 11, loss = 0.57644861\n",
            "Iteration 12, loss = 0.57268045\n",
            "Iteration 13, loss = 0.56383866\n",
            "Iteration 14, loss = 0.56356680\n",
            "Iteration 15, loss = 0.56075138\n",
            "Iteration 16, loss = 0.55128186\n",
            "Iteration 17, loss = 0.54595338\n",
            "Iteration 18, loss = 0.54550913\n",
            "Iteration 19, loss = 0.54538978\n",
            "Iteration 20, loss = 0.54544707\n",
            "Iteration 21, loss = 0.52726968\n",
            "Iteration 22, loss = 0.52302610\n",
            "Iteration 23, loss = 0.52519972\n",
            "Iteration 24, loss = 0.52012010\n",
            "Iteration 25, loss = 0.51598408\n",
            "Iteration 26, loss = 0.50483957\n",
            "Iteration 27, loss = 0.50771025\n",
            "Iteration 28, loss = 0.49811460\n",
            "Iteration 29, loss = 0.50778925\n",
            "Iteration 30, loss = 0.50141230\n",
            "Iteration 31, loss = 0.49759632\n",
            "Iteration 32, loss = 0.48858070\n",
            "Iteration 33, loss = 0.49131828\n",
            "Iteration 34, loss = 0.48381529\n",
            "Iteration 35, loss = 0.48401251\n",
            "Iteration 36, loss = 0.47697170\n",
            "Iteration 37, loss = 0.49175016\n",
            "Iteration 38, loss = 0.47214309\n",
            "Iteration 39, loss = 0.47010014\n",
            "Iteration 40, loss = 0.48043494\n",
            "Iteration 41, loss = 0.46216105\n",
            "Iteration 42, loss = 0.46839878\n",
            "Iteration 43, loss = 0.46533642\n",
            "Iteration 44, loss = 0.47004030\n",
            "Iteration 45, loss = 0.46165130\n",
            "Iteration 46, loss = 0.46315768\n",
            "Iteration 47, loss = 0.46673024\n",
            "Iteration 48, loss = 0.46135378\n",
            "Iteration 49, loss = 0.45088226\n",
            "Iteration 50, loss = 0.44954057\n",
            "Iteration 51, loss = 0.45489758\n",
            "Iteration 52, loss = 0.45042581\n",
            "Iteration 53, loss = 0.45384839\n",
            "Iteration 54, loss = 0.45491914\n",
            "Iteration 55, loss = 0.44670174\n",
            "Iteration 56, loss = 0.44695070\n",
            "Iteration 57, loss = 0.44729961\n",
            "Iteration 58, loss = 0.43903915\n",
            "Iteration 59, loss = 0.43576896\n",
            "Iteration 60, loss = 0.43889843\n",
            "Iteration 61, loss = 0.44595403\n",
            "Iteration 62, loss = 0.45762913\n",
            "Iteration 63, loss = 0.45239303\n",
            "Iteration 64, loss = 0.43534335\n",
            "Iteration 65, loss = 0.43555867\n",
            "Iteration 66, loss = 0.43321605\n",
            "Iteration 67, loss = 0.42325883\n",
            "Iteration 68, loss = 0.42028868\n",
            "Iteration 69, loss = 0.44680903\n",
            "Iteration 70, loss = 0.44293669\n",
            "Iteration 71, loss = 0.43061675\n",
            "Iteration 72, loss = 0.42516411\n",
            "Iteration 73, loss = 0.43322986\n",
            "Iteration 74, loss = 0.45904252\n",
            "Iteration 75, loss = 0.43687501\n",
            "Iteration 76, loss = 0.42282062\n",
            "Iteration 77, loss = 0.41577241\n",
            "Iteration 78, loss = 0.41594685\n",
            "Iteration 79, loss = 0.42077831\n",
            "Iteration 80, loss = 0.41922720\n",
            "Iteration 81, loss = 0.41271915\n",
            "Iteration 82, loss = 0.41382776\n",
            "Iteration 83, loss = 0.40950945\n",
            "Iteration 84, loss = 0.41714047\n",
            "Iteration 85, loss = 0.41544222\n",
            "Iteration 86, loss = 0.41849501\n",
            "Iteration 87, loss = 0.40609063\n",
            "Iteration 88, loss = 0.42618432\n",
            "Iteration 89, loss = 0.41922801\n",
            "Iteration 90, loss = 0.40542979\n",
            "Iteration 91, loss = 0.40751128\n",
            "Iteration 92, loss = 0.41320923\n",
            "Iteration 93, loss = 0.40382666\n",
            "Iteration 94, loss = 0.40306711\n",
            "Iteration 95, loss = 0.40386098\n",
            "Iteration 96, loss = 0.39941741\n",
            "Iteration 97, loss = 0.39950830\n",
            "Iteration 98, loss = 0.40827522\n",
            "Iteration 99, loss = 0.40911073\n",
            "Iteration 100, loss = 0.42093224\n",
            "Iteration 101, loss = 0.42872358\n",
            "Iteration 102, loss = 0.40956289\n",
            "Iteration 103, loss = 0.39925176\n",
            "Iteration 104, loss = 0.40522946\n",
            "Iteration 105, loss = 0.39990174\n",
            "Iteration 106, loss = 0.40682384\n",
            "Iteration 107, loss = 0.40698164\n",
            "Iteration 108, loss = 0.40703415\n",
            "Iteration 109, loss = 0.40265821\n",
            "Iteration 110, loss = 0.40330496\n",
            "Iteration 111, loss = 0.40230400\n",
            "Iteration 112, loss = 0.39623491\n",
            "Iteration 113, loss = 0.39144937\n",
            "Iteration 114, loss = 0.39616831\n",
            "Iteration 115, loss = 0.39766711\n",
            "Iteration 116, loss = 0.39633081\n",
            "Iteration 117, loss = 0.38995655\n",
            "Iteration 118, loss = 0.39560767\n",
            "Iteration 119, loss = 0.38914096\n",
            "Iteration 120, loss = 0.38890523\n",
            "Iteration 121, loss = 0.40304544\n",
            "Iteration 122, loss = 0.38682568\n",
            "Iteration 123, loss = 0.39209973\n",
            "Iteration 124, loss = 0.39322949\n",
            "Iteration 125, loss = 0.38684864\n",
            "Iteration 126, loss = 0.40800753\n",
            "Iteration 127, loss = 0.40116783\n",
            "Iteration 128, loss = 0.40811181\n",
            "Iteration 129, loss = 0.39723691\n",
            "Iteration 130, loss = 0.38965903\n",
            "Iteration 131, loss = 0.39750504\n",
            "Iteration 132, loss = 0.39438672\n",
            "Iteration 133, loss = 0.38448079\n",
            "Iteration 134, loss = 0.38714233\n",
            "Iteration 135, loss = 0.38804289\n",
            "Iteration 136, loss = 0.38976712\n",
            "Iteration 137, loss = 0.40750264\n",
            "Iteration 138, loss = 0.39057460\n",
            "Iteration 139, loss = 0.39707586\n",
            "Iteration 140, loss = 0.40908146\n",
            "Iteration 141, loss = 0.39150671\n",
            "Iteration 142, loss = 0.39579749\n",
            "Iteration 143, loss = 0.39988470\n",
            "Iteration 144, loss = 0.38608716\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64793276\n",
            "Iteration 2, loss = 0.62780260\n",
            "Iteration 3, loss = 0.63111327\n",
            "Iteration 4, loss = 0.61342597\n",
            "Iteration 5, loss = 0.59854628\n",
            "Iteration 6, loss = 0.59879137\n",
            "Iteration 7, loss = 0.59440440\n",
            "Iteration 8, loss = 0.59226915\n",
            "Iteration 9, loss = 0.58933578\n",
            "Iteration 10, loss = 0.59139794\n",
            "Iteration 11, loss = 0.58629416\n",
            "Iteration 12, loss = 0.57839997\n",
            "Iteration 13, loss = 0.57424446\n",
            "Iteration 14, loss = 0.56883193\n",
            "Iteration 15, loss = 0.56706582\n",
            "Iteration 16, loss = 0.56329290\n",
            "Iteration 17, loss = 0.55892350\n",
            "Iteration 18, loss = 0.56521502\n",
            "Iteration 19, loss = 0.54818273\n",
            "Iteration 20, loss = 0.54153409\n",
            "Iteration 21, loss = 0.53501784\n",
            "Iteration 22, loss = 0.53349634\n",
            "Iteration 23, loss = 0.52547230\n",
            "Iteration 24, loss = 0.51716469\n",
            "Iteration 25, loss = 0.51248683\n",
            "Iteration 26, loss = 0.51049153\n",
            "Iteration 27, loss = 0.49925824\n",
            "Iteration 28, loss = 0.49599346\n",
            "Iteration 29, loss = 0.48891328\n",
            "Iteration 30, loss = 0.48804346\n",
            "Iteration 31, loss = 0.47863887\n",
            "Iteration 32, loss = 0.47608354\n",
            "Iteration 33, loss = 0.47645241\n",
            "Iteration 34, loss = 0.46475224\n",
            "Iteration 35, loss = 0.46151030\n",
            "Iteration 36, loss = 0.46058519\n",
            "Iteration 37, loss = 0.45387284\n",
            "Iteration 38, loss = 0.45299073\n",
            "Iteration 39, loss = 0.45661481\n",
            "Iteration 40, loss = 0.45785391\n",
            "Iteration 41, loss = 0.45654284\n",
            "Iteration 42, loss = 0.44666680\n",
            "Iteration 43, loss = 0.46216020\n",
            "Iteration 44, loss = 0.44975024\n",
            "Iteration 45, loss = 0.45091912\n",
            "Iteration 46, loss = 0.45519834\n",
            "Iteration 47, loss = 0.44581688\n",
            "Iteration 48, loss = 0.44405037\n",
            "Iteration 49, loss = 0.45101425\n",
            "Iteration 50, loss = 0.44327023\n",
            "Iteration 51, loss = 0.43950706\n",
            "Iteration 52, loss = 0.44875786\n",
            "Iteration 53, loss = 0.44420378\n",
            "Iteration 54, loss = 0.44115793\n",
            "Iteration 55, loss = 0.43161320\n",
            "Iteration 56, loss = 0.42846320\n",
            "Iteration 57, loss = 0.42450009\n",
            "Iteration 58, loss = 0.43285726\n",
            "Iteration 59, loss = 0.44748009\n",
            "Iteration 60, loss = 0.43372875\n",
            "Iteration 61, loss = 0.43566787\n",
            "Iteration 62, loss = 0.41993878\n",
            "Iteration 63, loss = 0.43626663\n",
            "Iteration 64, loss = 0.43852324\n",
            "Iteration 65, loss = 0.42258583\n",
            "Iteration 66, loss = 0.42637644\n",
            "Iteration 67, loss = 0.42149374\n",
            "Iteration 68, loss = 0.42575129\n",
            "Iteration 69, loss = 0.41813974\n",
            "Iteration 70, loss = 0.41532212\n",
            "Iteration 71, loss = 0.41333451\n",
            "Iteration 72, loss = 0.41887304\n",
            "Iteration 73, loss = 0.40976485\n",
            "Iteration 74, loss = 0.40943427\n",
            "Iteration 75, loss = 0.40808993\n",
            "Iteration 76, loss = 0.41585165\n",
            "Iteration 77, loss = 0.41164964\n",
            "Iteration 78, loss = 0.42826857\n",
            "Iteration 79, loss = 0.41188538\n",
            "Iteration 80, loss = 0.41278773\n",
            "Iteration 81, loss = 0.40834409\n",
            "Iteration 82, loss = 0.41417119\n",
            "Iteration 83, loss = 0.40854015\n",
            "Iteration 84, loss = 0.40139460\n",
            "Iteration 85, loss = 0.40601502\n",
            "Iteration 86, loss = 0.40135254\n",
            "Iteration 87, loss = 0.40845193\n",
            "Iteration 88, loss = 0.40676500\n",
            "Iteration 89, loss = 0.40648550\n",
            "Iteration 90, loss = 0.41217205\n",
            "Iteration 91, loss = 0.39724923\n",
            "Iteration 92, loss = 0.39889488\n",
            "Iteration 93, loss = 0.39863384\n",
            "Iteration 94, loss = 0.40184986\n",
            "Iteration 95, loss = 0.39727617\n",
            "Iteration 96, loss = 0.40040055\n",
            "Iteration 97, loss = 0.40337554\n",
            "Iteration 98, loss = 0.39911321\n",
            "Iteration 99, loss = 0.39287162\n",
            "Iteration 100, loss = 0.39807137\n",
            "Iteration 101, loss = 0.39152946\n",
            "Iteration 102, loss = 0.39560584\n",
            "Iteration 103, loss = 0.40027369\n",
            "Iteration 104, loss = 0.39231061\n",
            "Iteration 105, loss = 0.39209699\n",
            "Iteration 106, loss = 0.39020618\n",
            "Iteration 107, loss = 0.38950018\n",
            "Iteration 108, loss = 0.38888099\n",
            "Iteration 109, loss = 0.39551304\n",
            "Iteration 110, loss = 0.39748182\n",
            "Iteration 111, loss = 0.39419925\n",
            "Iteration 112, loss = 0.41495472\n",
            "Iteration 113, loss = 0.40024908\n",
            "Iteration 114, loss = 0.40446164\n",
            "Iteration 115, loss = 0.39371794\n",
            "Iteration 116, loss = 0.38906589\n",
            "Iteration 117, loss = 0.38675068\n",
            "Iteration 118, loss = 0.39303710\n",
            "Iteration 119, loss = 0.39112349\n",
            "Iteration 120, loss = 0.39587123\n",
            "Iteration 121, loss = 0.40159294\n",
            "Iteration 122, loss = 0.38444773\n",
            "Iteration 123, loss = 0.39073805\n",
            "Iteration 124, loss = 0.38975402\n",
            "Iteration 125, loss = 0.38497028\n",
            "Iteration 126, loss = 0.38322804\n",
            "Iteration 127, loss = 0.39215231\n",
            "Iteration 128, loss = 0.38593274\n",
            "Iteration 129, loss = 0.38271993\n",
            "Iteration 130, loss = 0.38206910\n",
            "Iteration 131, loss = 0.41703215\n",
            "Iteration 132, loss = 0.38544851\n",
            "Iteration 133, loss = 0.39067911\n",
            "Iteration 134, loss = 0.38316452\n",
            "Iteration 135, loss = 0.38184606\n",
            "Iteration 136, loss = 0.38486157\n",
            "Iteration 137, loss = 0.37906532\n",
            "Iteration 138, loss = 0.39275530\n",
            "Iteration 139, loss = 0.38115190\n",
            "Iteration 140, loss = 0.38187052\n",
            "Iteration 141, loss = 0.37942260\n",
            "Iteration 142, loss = 0.38007540\n",
            "Iteration 143, loss = 0.37768569\n",
            "Iteration 144, loss = 0.40470099\n",
            "Iteration 145, loss = 0.40361055\n",
            "Iteration 146, loss = 0.39351361\n",
            "Iteration 147, loss = 0.38214192\n",
            "Iteration 148, loss = 0.37410977\n",
            "Iteration 149, loss = 0.38285167\n",
            "Iteration 150, loss = 0.38326624\n",
            "Iteration 151, loss = 0.37561806\n",
            "Iteration 152, loss = 0.38173497\n",
            "Iteration 153, loss = 0.37408453\n",
            "Iteration 154, loss = 0.39463695\n",
            "Iteration 155, loss = 0.39155167\n",
            "Iteration 156, loss = 0.38306770\n",
            "Iteration 157, loss = 0.37312994\n",
            "Iteration 158, loss = 0.38730840\n",
            "Iteration 159, loss = 0.37732340\n",
            "Iteration 160, loss = 0.37830075\n",
            "Iteration 161, loss = 0.38602619\n",
            "Iteration 162, loss = 0.38047163\n",
            "Iteration 163, loss = 0.37740437\n",
            "Iteration 164, loss = 0.38083102\n",
            "Iteration 165, loss = 0.39213844\n",
            "Iteration 166, loss = 0.41056601\n",
            "Iteration 167, loss = 0.38806164\n",
            "Iteration 168, loss = 0.37662835\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64074417\n",
            "Iteration 2, loss = 0.61115766\n",
            "Iteration 3, loss = 0.61426666\n",
            "Iteration 4, loss = 0.60408973\n",
            "Iteration 5, loss = 0.59990362\n",
            "Iteration 6, loss = 0.59858745\n",
            "Iteration 7, loss = 0.59344558\n",
            "Iteration 8, loss = 0.58791066\n",
            "Iteration 9, loss = 0.58479696\n",
            "Iteration 10, loss = 0.58083222\n",
            "Iteration 11, loss = 0.57937970\n",
            "Iteration 12, loss = 0.57270567\n",
            "Iteration 13, loss = 0.56694844\n",
            "Iteration 14, loss = 0.57025634\n",
            "Iteration 15, loss = 0.55737393\n",
            "Iteration 16, loss = 0.54996158\n",
            "Iteration 17, loss = 0.54248283\n",
            "Iteration 18, loss = 0.53912283\n",
            "Iteration 19, loss = 0.53575810\n",
            "Iteration 20, loss = 0.51842010\n",
            "Iteration 21, loss = 0.51002071\n",
            "Iteration 22, loss = 0.50688186\n",
            "Iteration 23, loss = 0.49570565\n",
            "Iteration 24, loss = 0.48881160\n",
            "Iteration 25, loss = 0.47929810\n",
            "Iteration 26, loss = 0.46930603\n",
            "Iteration 27, loss = 0.47316340\n",
            "Iteration 28, loss = 0.47687580\n",
            "Iteration 29, loss = 0.46322472\n",
            "Iteration 30, loss = 0.45442151\n",
            "Iteration 31, loss = 0.46609531\n",
            "Iteration 32, loss = 0.45425734\n",
            "Iteration 33, loss = 0.44919715\n",
            "Iteration 34, loss = 0.45238797\n",
            "Iteration 35, loss = 0.44789256\n",
            "Iteration 36, loss = 0.44068323\n",
            "Iteration 37, loss = 0.44468914\n",
            "Iteration 38, loss = 0.43707721\n",
            "Iteration 39, loss = 0.45212671\n",
            "Iteration 40, loss = 0.43786413\n",
            "Iteration 41, loss = 0.43418453\n",
            "Iteration 42, loss = 0.43526473\n",
            "Iteration 43, loss = 0.46033049\n",
            "Iteration 44, loss = 0.45213291\n",
            "Iteration 45, loss = 0.43716834\n",
            "Iteration 46, loss = 0.44142599\n",
            "Iteration 47, loss = 0.43448593\n",
            "Iteration 48, loss = 0.43055013\n",
            "Iteration 49, loss = 0.43641768\n",
            "Iteration 50, loss = 0.42671751\n",
            "Iteration 51, loss = 0.42539040\n",
            "Iteration 52, loss = 0.42953564\n",
            "Iteration 53, loss = 0.42998038\n",
            "Iteration 54, loss = 0.43290822\n",
            "Iteration 55, loss = 0.43252949\n",
            "Iteration 56, loss = 0.43385675\n",
            "Iteration 57, loss = 0.43390948\n",
            "Iteration 58, loss = 0.42295458\n",
            "Iteration 59, loss = 0.42536345\n",
            "Iteration 60, loss = 0.42217327\n",
            "Iteration 61, loss = 0.42716516\n",
            "Iteration 62, loss = 0.42523868\n",
            "Iteration 63, loss = 0.43760347\n",
            "Iteration 64, loss = 0.45785036\n",
            "Iteration 65, loss = 0.42413499\n",
            "Iteration 66, loss = 0.42503593\n",
            "Iteration 67, loss = 0.42366670\n",
            "Iteration 68, loss = 0.42404240\n",
            "Iteration 69, loss = 0.42508092\n",
            "Iteration 70, loss = 0.42350072\n",
            "Iteration 71, loss = 0.42433585\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62624010\n",
            "Iteration 2, loss = 0.60316175\n",
            "Iteration 3, loss = 0.60058004\n",
            "Iteration 4, loss = 0.59288271\n",
            "Iteration 5, loss = 0.59037797\n",
            "Iteration 6, loss = 0.58909347\n",
            "Iteration 7, loss = 0.58679753\n",
            "Iteration 8, loss = 0.58988212\n",
            "Iteration 9, loss = 0.58269464\n",
            "Iteration 10, loss = 0.58022830\n",
            "Iteration 11, loss = 0.57983276\n",
            "Iteration 12, loss = 0.57507278\n",
            "Iteration 13, loss = 0.57000117\n",
            "Iteration 14, loss = 0.56658337\n",
            "Iteration 15, loss = 0.56606444\n",
            "Iteration 16, loss = 0.55765799\n",
            "Iteration 17, loss = 0.56142312\n",
            "Iteration 18, loss = 0.54889784\n",
            "Iteration 19, loss = 0.55030723\n",
            "Iteration 20, loss = 0.54872288\n",
            "Iteration 21, loss = 0.53798464\n",
            "Iteration 22, loss = 0.53132468\n",
            "Iteration 23, loss = 0.52688121\n",
            "Iteration 24, loss = 0.52338471\n",
            "Iteration 25, loss = 0.51882649\n",
            "Iteration 26, loss = 0.51427035\n",
            "Iteration 27, loss = 0.53001348\n",
            "Iteration 28, loss = 0.51900294\n",
            "Iteration 29, loss = 0.51493598\n",
            "Iteration 30, loss = 0.52032220\n",
            "Iteration 31, loss = 0.51114132\n",
            "Iteration 32, loss = 0.49874205\n",
            "Iteration 33, loss = 0.49884824\n",
            "Iteration 34, loss = 0.48946811\n",
            "Iteration 35, loss = 0.49285022\n",
            "Iteration 36, loss = 0.48156814\n",
            "Iteration 37, loss = 0.47546305\n",
            "Iteration 38, loss = 0.47337666\n",
            "Iteration 39, loss = 0.47187512\n",
            "Iteration 40, loss = 0.48172726\n",
            "Iteration 41, loss = 0.47075762\n",
            "Iteration 42, loss = 0.46822366\n",
            "Iteration 43, loss = 0.47118149\n",
            "Iteration 44, loss = 0.45911049\n",
            "Iteration 45, loss = 0.46037532\n",
            "Iteration 46, loss = 0.46028290\n",
            "Iteration 47, loss = 0.45651965\n",
            "Iteration 48, loss = 0.45391748\n",
            "Iteration 49, loss = 0.45722958\n",
            "Iteration 50, loss = 0.45861366\n",
            "Iteration 51, loss = 0.46583196\n",
            "Iteration 52, loss = 0.46193427\n",
            "Iteration 53, loss = 0.45639171\n",
            "Iteration 54, loss = 0.46455198\n",
            "Iteration 55, loss = 0.46505310\n",
            "Iteration 56, loss = 0.47613996\n",
            "Iteration 57, loss = 0.44647077\n",
            "Iteration 58, loss = 0.44441407\n",
            "Iteration 59, loss = 0.43957188\n",
            "Iteration 60, loss = 0.44444526\n",
            "Iteration 61, loss = 0.44271174\n",
            "Iteration 62, loss = 0.44020198\n",
            "Iteration 63, loss = 0.44303613\n",
            "Iteration 64, loss = 0.44168194\n",
            "Iteration 65, loss = 0.44523198\n",
            "Iteration 66, loss = 0.43619805\n",
            "Iteration 67, loss = 0.43506054\n",
            "Iteration 68, loss = 0.43741133\n",
            "Iteration 69, loss = 0.43225404\n",
            "Iteration 70, loss = 0.42652908\n",
            "Iteration 71, loss = 0.42645818\n",
            "Iteration 72, loss = 0.44689256\n",
            "Iteration 73, loss = 0.43322195\n",
            "Iteration 74, loss = 0.45209751\n",
            "Iteration 75, loss = 0.42902217\n",
            "Iteration 76, loss = 0.42352277\n",
            "Iteration 77, loss = 0.41849004\n",
            "Iteration 78, loss = 0.41929327\n",
            "Iteration 79, loss = 0.41912185\n",
            "Iteration 80, loss = 0.41444046\n",
            "Iteration 81, loss = 0.45249282\n",
            "Iteration 82, loss = 0.42522703\n",
            "Iteration 83, loss = 0.42279902\n",
            "Iteration 84, loss = 0.41506978\n",
            "Iteration 85, loss = 0.42624165\n",
            "Iteration 86, loss = 0.42178973\n",
            "Iteration 87, loss = 0.43176374\n",
            "Iteration 88, loss = 0.42104758\n",
            "Iteration 89, loss = 0.41556251\n",
            "Iteration 90, loss = 0.41571180\n",
            "Iteration 91, loss = 0.41423178\n",
            "Iteration 92, loss = 0.42551046\n",
            "Iteration 93, loss = 0.42261571\n",
            "Iteration 94, loss = 0.41331810\n",
            "Iteration 95, loss = 0.41314815\n",
            "Iteration 96, loss = 0.41260040\n",
            "Iteration 97, loss = 0.40879671\n",
            "Iteration 98, loss = 0.40690800\n",
            "Iteration 99, loss = 0.40308659\n",
            "Iteration 100, loss = 0.40397395\n",
            "Iteration 101, loss = 0.40871331\n",
            "Iteration 102, loss = 0.40603081\n",
            "Iteration 103, loss = 0.39884535\n",
            "Iteration 104, loss = 0.41099615\n",
            "Iteration 105, loss = 0.41331214\n",
            "Iteration 106, loss = 0.40570304\n",
            "Iteration 107, loss = 0.39767763\n",
            "Iteration 108, loss = 0.39829234\n",
            "Iteration 109, loss = 0.39680157\n",
            "Iteration 110, loss = 0.41406508\n",
            "Iteration 111, loss = 0.42206026\n",
            "Iteration 112, loss = 0.39800303\n",
            "Iteration 113, loss = 0.40051447\n",
            "Iteration 114, loss = 0.39621012\n",
            "Iteration 115, loss = 0.41547864\n",
            "Iteration 116, loss = 0.41516336\n",
            "Iteration 117, loss = 0.41159706\n",
            "Iteration 118, loss = 0.40500818\n",
            "Iteration 119, loss = 0.39756660\n",
            "Iteration 120, loss = 0.39439631\n",
            "Iteration 121, loss = 0.39184786\n",
            "Iteration 122, loss = 0.39321995\n",
            "Iteration 123, loss = 0.39546283\n",
            "Iteration 124, loss = 0.39014830\n",
            "Iteration 125, loss = 0.39285711\n",
            "Iteration 126, loss = 0.39561330\n",
            "Iteration 127, loss = 0.39905238\n",
            "Iteration 128, loss = 0.39245360\n",
            "Iteration 129, loss = 0.40529340\n",
            "Iteration 130, loss = 0.39164167\n",
            "Iteration 131, loss = 0.38598896\n",
            "Iteration 132, loss = 0.39368573\n",
            "Iteration 133, loss = 0.39489068\n",
            "Iteration 134, loss = 0.38586605\n",
            "Iteration 135, loss = 0.40340714\n",
            "Iteration 136, loss = 0.39478584\n",
            "Iteration 137, loss = 0.38585415\n",
            "Iteration 138, loss = 0.39002375\n",
            "Iteration 139, loss = 0.39055716\n",
            "Iteration 140, loss = 0.39267768\n",
            "Iteration 141, loss = 0.39002223\n",
            "Iteration 142, loss = 0.39489882\n",
            "Iteration 143, loss = 0.39854097\n",
            "Iteration 144, loss = 0.38881686\n",
            "Iteration 145, loss = 0.38456977\n",
            "Iteration 146, loss = 0.38242822\n",
            "Iteration 147, loss = 0.38774320\n",
            "Iteration 148, loss = 0.39256873\n",
            "Iteration 149, loss = 0.38689954\n",
            "Iteration 150, loss = 0.42581662\n",
            "Iteration 151, loss = 0.40578494\n",
            "Iteration 152, loss = 0.39580809\n",
            "Iteration 153, loss = 0.38518998\n",
            "Iteration 154, loss = 0.38335234\n",
            "Iteration 155, loss = 0.39385882\n",
            "Iteration 156, loss = 0.38327301\n",
            "Iteration 157, loss = 0.38060947\n",
            "Iteration 158, loss = 0.37497544\n",
            "Iteration 159, loss = 0.37479871\n",
            "Iteration 160, loss = 0.37412905\n",
            "Iteration 161, loss = 0.37186505\n",
            "Iteration 162, loss = 0.37557618\n",
            "Iteration 163, loss = 0.37210283\n",
            "Iteration 164, loss = 0.37990435\n",
            "Iteration 165, loss = 0.38620201\n",
            "Iteration 166, loss = 0.37539230\n",
            "Iteration 167, loss = 0.37317386\n",
            "Iteration 168, loss = 0.40163334\n",
            "Iteration 169, loss = 0.38637615\n",
            "Iteration 170, loss = 0.37220970\n",
            "Iteration 171, loss = 0.36940544\n",
            "Iteration 172, loss = 0.37182609\n",
            "Iteration 173, loss = 0.37593814\n",
            "Iteration 174, loss = 0.38835941\n",
            "Iteration 175, loss = 0.37048921\n",
            "Iteration 176, loss = 0.39561706\n",
            "Iteration 177, loss = 0.40205075\n",
            "Iteration 178, loss = 0.38326877\n",
            "Iteration 179, loss = 0.40377838\n",
            "Iteration 180, loss = 0.38707573\n",
            "Iteration 181, loss = 0.37093214\n",
            "Iteration 182, loss = 0.37291361\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62922155\n",
            "Iteration 2, loss = 0.60898941\n",
            "Iteration 3, loss = 0.60112796\n",
            "Iteration 4, loss = 0.59541167\n",
            "Iteration 5, loss = 0.58949832\n",
            "Iteration 6, loss = 0.58378317\n",
            "Iteration 7, loss = 0.58213256\n",
            "Iteration 8, loss = 0.57688970\n",
            "Iteration 9, loss = 0.57560866\n",
            "Iteration 10, loss = 0.57713242\n",
            "Iteration 11, loss = 0.56683738\n",
            "Iteration 12, loss = 0.56382878\n",
            "Iteration 13, loss = 0.56751788\n",
            "Iteration 14, loss = 0.58191001\n",
            "Iteration 15, loss = 0.56129068\n",
            "Iteration 16, loss = 0.55260691\n",
            "Iteration 17, loss = 0.54840145\n",
            "Iteration 18, loss = 0.54967083\n",
            "Iteration 19, loss = 0.54833125\n",
            "Iteration 20, loss = 0.54435615\n",
            "Iteration 21, loss = 0.53336510\n",
            "Iteration 22, loss = 0.53442937\n",
            "Iteration 23, loss = 0.52893030\n",
            "Iteration 24, loss = 0.52145703\n",
            "Iteration 25, loss = 0.52888582\n",
            "Iteration 26, loss = 0.51393222\n",
            "Iteration 27, loss = 0.50924387\n",
            "Iteration 28, loss = 0.51083756\n",
            "Iteration 29, loss = 0.51120296\n",
            "Iteration 30, loss = 0.50136837\n",
            "Iteration 31, loss = 0.50066547\n",
            "Iteration 32, loss = 0.51059938\n",
            "Iteration 33, loss = 0.49945728\n",
            "Iteration 34, loss = 0.49603089\n",
            "Iteration 35, loss = 0.48920162\n",
            "Iteration 36, loss = 0.48919915\n",
            "Iteration 37, loss = 0.48755546\n",
            "Iteration 38, loss = 0.48753132\n",
            "Iteration 39, loss = 0.48357453\n",
            "Iteration 40, loss = 0.48992893\n",
            "Iteration 41, loss = 0.48715561\n",
            "Iteration 42, loss = 0.48581858\n",
            "Iteration 43, loss = 0.48030423\n",
            "Iteration 44, loss = 0.47970626\n",
            "Iteration 45, loss = 0.47430634\n",
            "Iteration 46, loss = 0.47291257\n",
            "Iteration 47, loss = 0.47397620\n",
            "Iteration 48, loss = 0.47296931\n",
            "Iteration 49, loss = 0.47434922\n",
            "Iteration 50, loss = 0.48013104\n",
            "Iteration 51, loss = 0.47060761\n",
            "Iteration 52, loss = 0.46659686\n",
            "Iteration 53, loss = 0.46424282\n",
            "Iteration 54, loss = 0.46253536\n",
            "Iteration 55, loss = 0.47187371\n",
            "Iteration 56, loss = 0.47443818\n",
            "Iteration 57, loss = 0.47143789\n",
            "Iteration 58, loss = 0.45944635\n",
            "Iteration 59, loss = 0.46152470\n",
            "Iteration 60, loss = 0.46512879\n",
            "Iteration 61, loss = 0.45882194\n",
            "Iteration 62, loss = 0.45702912\n",
            "Iteration 63, loss = 0.45777608\n",
            "Iteration 64, loss = 0.45659886\n",
            "Iteration 65, loss = 0.45743116\n",
            "Iteration 66, loss = 0.45488270\n",
            "Iteration 67, loss = 0.45716365\n",
            "Iteration 68, loss = 0.45382089\n",
            "Iteration 69, loss = 0.45290947\n",
            "Iteration 70, loss = 0.44844631\n",
            "Iteration 71, loss = 0.45472695\n",
            "Iteration 72, loss = 0.44587104\n",
            "Iteration 73, loss = 0.45203470\n",
            "Iteration 74, loss = 0.48195682\n",
            "Iteration 75, loss = 0.46182323\n",
            "Iteration 76, loss = 0.44577668\n",
            "Iteration 77, loss = 0.44196714\n",
            "Iteration 78, loss = 0.44070556\n",
            "Iteration 79, loss = 0.44662898\n",
            "Iteration 80, loss = 0.46511284\n",
            "Iteration 81, loss = 0.44924966\n",
            "Iteration 82, loss = 0.43648543\n",
            "Iteration 83, loss = 0.44122878\n",
            "Iteration 84, loss = 0.44122412\n",
            "Iteration 85, loss = 0.43658373\n",
            "Iteration 86, loss = 0.43773625\n",
            "Iteration 87, loss = 0.43399515\n",
            "Iteration 88, loss = 0.43619448\n",
            "Iteration 89, loss = 0.43506099\n",
            "Iteration 90, loss = 0.44700500\n",
            "Iteration 91, loss = 0.44578657\n",
            "Iteration 92, loss = 0.46446123\n",
            "Iteration 93, loss = 0.45596293\n",
            "Iteration 94, loss = 0.43338723\n",
            "Iteration 95, loss = 0.42662568\n",
            "Iteration 96, loss = 0.42752016\n",
            "Iteration 97, loss = 0.43018005\n",
            "Iteration 98, loss = 0.42376837\n",
            "Iteration 99, loss = 0.42875874\n",
            "Iteration 100, loss = 0.41820774\n",
            "Iteration 101, loss = 0.42003898\n",
            "Iteration 102, loss = 0.41640293\n",
            "Iteration 103, loss = 0.41568927\n",
            "Iteration 104, loss = 0.42048305\n",
            "Iteration 105, loss = 0.42717288\n",
            "Iteration 106, loss = 0.42920261\n",
            "Iteration 107, loss = 0.41906733\n",
            "Iteration 108, loss = 0.41579117\n",
            "Iteration 109, loss = 0.41088645\n",
            "Iteration 110, loss = 0.41069648\n",
            "Iteration 111, loss = 0.41929818\n",
            "Iteration 112, loss = 0.42333003\n",
            "Iteration 113, loss = 0.40701034\n",
            "Iteration 114, loss = 0.41052799\n",
            "Iteration 115, loss = 0.43660350\n",
            "Iteration 116, loss = 0.40849131\n",
            "Iteration 117, loss = 0.42007956\n",
            "Iteration 118, loss = 0.41037567\n",
            "Iteration 119, loss = 0.40938693\n",
            "Iteration 120, loss = 0.40150772\n",
            "Iteration 121, loss = 0.40487522\n",
            "Iteration 122, loss = 0.40572267\n",
            "Iteration 123, loss = 0.39987278\n",
            "Iteration 124, loss = 0.39838215\n",
            "Iteration 125, loss = 0.39897913\n",
            "Iteration 126, loss = 0.40185393\n",
            "Iteration 127, loss = 0.39452022\n",
            "Iteration 128, loss = 0.39694410\n",
            "Iteration 129, loss = 0.44651863\n",
            "Iteration 130, loss = 0.42191527\n",
            "Iteration 131, loss = 0.42529236\n",
            "Iteration 132, loss = 0.41508700\n",
            "Iteration 133, loss = 0.40485930\n",
            "Iteration 134, loss = 0.39587126\n",
            "Iteration 135, loss = 0.39064006\n",
            "Iteration 136, loss = 0.39350774\n",
            "Iteration 137, loss = 0.40082500\n",
            "Iteration 138, loss = 0.40901594\n",
            "Iteration 139, loss = 0.41904023\n",
            "Iteration 140, loss = 0.40794853\n",
            "Iteration 141, loss = 0.40410280\n",
            "Iteration 142, loss = 0.38904541\n",
            "Iteration 143, loss = 0.38430114\n",
            "Iteration 144, loss = 0.38812693\n",
            "Iteration 145, loss = 0.39270273\n",
            "Iteration 146, loss = 0.38273612\n",
            "Iteration 147, loss = 0.38610550\n",
            "Iteration 148, loss = 0.39156542\n",
            "Iteration 149, loss = 0.39420940\n",
            "Iteration 150, loss = 0.40577145\n",
            "Iteration 151, loss = 0.40183696\n",
            "Iteration 152, loss = 0.38619276\n",
            "Iteration 153, loss = 0.38202006\n",
            "Iteration 154, loss = 0.38580067\n",
            "Iteration 155, loss = 0.38629701\n",
            "Iteration 156, loss = 0.38016356\n",
            "Iteration 157, loss = 0.38221837\n",
            "Iteration 158, loss = 0.37862290\n",
            "Iteration 159, loss = 0.39034577\n",
            "Iteration 160, loss = 0.38091588\n",
            "Iteration 161, loss = 0.38351386\n",
            "Iteration 162, loss = 0.37948038\n",
            "Iteration 163, loss = 0.37461644\n",
            "Iteration 164, loss = 0.38563368\n",
            "Iteration 165, loss = 0.37401649\n",
            "Iteration 166, loss = 0.37184567\n",
            "Iteration 167, loss = 0.37254012\n",
            "Iteration 168, loss = 0.38132977\n",
            "Iteration 169, loss = 0.39226759\n",
            "Iteration 170, loss = 0.37665877\n",
            "Iteration 171, loss = 0.36691234\n",
            "Iteration 172, loss = 0.38021467\n",
            "Iteration 173, loss = 0.36870778\n",
            "Iteration 174, loss = 0.37172163\n",
            "Iteration 175, loss = 0.36674072\n",
            "Iteration 176, loss = 0.36874329\n",
            "Iteration 177, loss = 0.37123487\n",
            "Iteration 178, loss = 0.37126934\n",
            "Iteration 179, loss = 0.36834617\n",
            "Iteration 180, loss = 0.36304071\n",
            "Iteration 181, loss = 0.36749344\n",
            "Iteration 182, loss = 0.36419179\n",
            "Iteration 183, loss = 0.37287590\n",
            "Iteration 184, loss = 0.36672620\n",
            "Iteration 185, loss = 0.36519244\n",
            "Iteration 186, loss = 0.36589309\n",
            "Iteration 187, loss = 0.36507968\n",
            "Iteration 188, loss = 0.36376843\n",
            "Iteration 189, loss = 0.37679071\n",
            "Iteration 190, loss = 0.37182280\n",
            "Iteration 191, loss = 0.35905696\n",
            "Iteration 192, loss = 0.38076011\n",
            "Iteration 193, loss = 0.36286328\n",
            "Iteration 194, loss = 0.36234309\n",
            "Iteration 195, loss = 0.36452844\n",
            "Iteration 196, loss = 0.36729573\n",
            "Iteration 197, loss = 0.36040884\n",
            "Iteration 198, loss = 0.36318999\n",
            "Iteration 199, loss = 0.38893458\n",
            "Iteration 200, loss = 0.37799962\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.82583423\n",
            "Iteration 2, loss = 0.64246098\n",
            "Iteration 3, loss = 0.62284197\n",
            "Iteration 4, loss = 0.59678001\n",
            "Iteration 5, loss = 0.60764468\n",
            "Iteration 6, loss = 0.60842277\n",
            "Iteration 7, loss = 0.56822371\n",
            "Iteration 8, loss = 0.57756052\n",
            "Iteration 9, loss = 0.56158556\n",
            "Iteration 10, loss = 0.56268139\n",
            "Iteration 11, loss = 0.55076355\n",
            "Iteration 12, loss = 0.56273039\n",
            "Iteration 13, loss = 0.55063961\n",
            "Iteration 14, loss = 0.52033984\n",
            "Iteration 15, loss = 0.52907205\n",
            "Iteration 16, loss = 0.52986566\n",
            "Iteration 17, loss = 0.55587742\n",
            "Iteration 18, loss = 0.51252809\n",
            "Iteration 19, loss = 0.53361323\n",
            "Iteration 20, loss = 0.48377859\n",
            "Iteration 21, loss = 0.49325278\n",
            "Iteration 22, loss = 0.49737552\n",
            "Iteration 23, loss = 0.48792577\n",
            "Iteration 24, loss = 0.48107032\n",
            "Iteration 25, loss = 0.47437262\n",
            "Iteration 26, loss = 0.46889612\n",
            "Iteration 27, loss = 0.47724608\n",
            "Iteration 28, loss = 0.48615992\n",
            "Iteration 29, loss = 0.48192975\n",
            "Iteration 30, loss = 0.48673494\n",
            "Iteration 31, loss = 0.49212145\n",
            "Iteration 32, loss = 0.45344726\n",
            "Iteration 33, loss = 0.47109639\n",
            "Iteration 34, loss = 0.46076919\n",
            "Iteration 35, loss = 0.50379105\n",
            "Iteration 36, loss = 0.51167360\n",
            "Iteration 37, loss = 0.46991346\n",
            "Iteration 38, loss = 0.46394244\n",
            "Iteration 39, loss = 0.45528524\n",
            "Iteration 40, loss = 0.44493745\n",
            "Iteration 41, loss = 0.52305762\n",
            "Iteration 42, loss = 0.48054716\n",
            "Iteration 43, loss = 0.44583334\n",
            "Iteration 44, loss = 0.45818655\n",
            "Iteration 45, loss = 0.44000920\n",
            "Iteration 46, loss = 0.45035432\n",
            "Iteration 47, loss = 0.44475813\n",
            "Iteration 48, loss = 0.50219280\n",
            "Iteration 49, loss = 0.45559548\n",
            "Iteration 50, loss = 0.45652049\n",
            "Iteration 51, loss = 0.44138593\n",
            "Iteration 52, loss = 0.44938538\n",
            "Iteration 53, loss = 0.44381288\n",
            "Iteration 54, loss = 0.45679183\n",
            "Iteration 55, loss = 0.43708716\n",
            "Iteration 56, loss = 0.45264139\n",
            "Iteration 57, loss = 0.43060188\n",
            "Iteration 58, loss = 0.42448734\n",
            "Iteration 59, loss = 0.49062258\n",
            "Iteration 60, loss = 0.48780384\n",
            "Iteration 61, loss = 0.46529457\n",
            "Iteration 62, loss = 0.45239046\n",
            "Iteration 63, loss = 0.43202894\n",
            "Iteration 64, loss = 0.42977574\n",
            "Iteration 65, loss = 0.42797561\n",
            "Iteration 66, loss = 0.42460865\n",
            "Iteration 67, loss = 0.49637144\n",
            "Iteration 68, loss = 0.46787579\n",
            "Iteration 69, loss = 0.43259546\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.97412118\n",
            "Iteration 2, loss = 0.71264822\n",
            "Iteration 3, loss = 0.61746812\n",
            "Iteration 4, loss = 0.63556687\n",
            "Iteration 5, loss = 0.59596338\n",
            "Iteration 6, loss = 0.57505034\n",
            "Iteration 7, loss = 0.64482941\n",
            "Iteration 8, loss = 0.56738285\n",
            "Iteration 9, loss = 0.71702641\n",
            "Iteration 10, loss = 0.56981484\n",
            "Iteration 11, loss = 0.54062540\n",
            "Iteration 12, loss = 0.55237836\n",
            "Iteration 13, loss = 0.53059494\n",
            "Iteration 14, loss = 0.53892122\n",
            "Iteration 15, loss = 0.49270690\n",
            "Iteration 16, loss = 0.53031173\n",
            "Iteration 17, loss = 0.52539436\n",
            "Iteration 18, loss = 0.52003190\n",
            "Iteration 19, loss = 0.49299415\n",
            "Iteration 20, loss = 0.50723291\n",
            "Iteration 21, loss = 0.49234251\n",
            "Iteration 22, loss = 0.47939344\n",
            "Iteration 23, loss = 0.48828013\n",
            "Iteration 24, loss = 0.49721753\n",
            "Iteration 25, loss = 0.46868970\n",
            "Iteration 26, loss = 0.49377594\n",
            "Iteration 27, loss = 0.49353357\n",
            "Iteration 28, loss = 0.47896504\n",
            "Iteration 29, loss = 0.47043365\n",
            "Iteration 30, loss = 0.47840032\n",
            "Iteration 31, loss = 0.46242505\n",
            "Iteration 32, loss = 0.45273292\n",
            "Iteration 33, loss = 0.45164636\n",
            "Iteration 34, loss = 0.57375233\n",
            "Iteration 35, loss = 0.48233310\n",
            "Iteration 36, loss = 0.50463548\n",
            "Iteration 37, loss = 0.45136509\n",
            "Iteration 38, loss = 0.47104451\n",
            "Iteration 39, loss = 0.49763356\n",
            "Iteration 40, loss = 0.47254984\n",
            "Iteration 41, loss = 0.46964471\n",
            "Iteration 42, loss = 0.50017850\n",
            "Iteration 43, loss = 0.43934280\n",
            "Iteration 44, loss = 0.50004798\n",
            "Iteration 45, loss = 0.43551295\n",
            "Iteration 46, loss = 0.44520899\n",
            "Iteration 47, loss = 0.42323902\n",
            "Iteration 48, loss = 0.44070748\n",
            "Iteration 49, loss = 0.45747670\n",
            "Iteration 50, loss = 0.44627566\n",
            "Iteration 51, loss = 0.44005883\n",
            "Iteration 52, loss = 0.44561153\n",
            "Iteration 53, loss = 0.43296243\n",
            "Iteration 54, loss = 0.46020883\n",
            "Iteration 55, loss = 0.57924345\n",
            "Iteration 56, loss = 0.43491852\n",
            "Iteration 57, loss = 0.43743097\n",
            "Iteration 58, loss = 0.43106521\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.97427165\n",
            "Iteration 2, loss = 0.67644768\n",
            "Iteration 3, loss = 0.66440433\n",
            "Iteration 4, loss = 0.66552258\n",
            "Iteration 5, loss = 0.64548818\n",
            "Iteration 6, loss = 0.59907851\n",
            "Iteration 7, loss = 0.60764441\n",
            "Iteration 8, loss = 0.57918795\n",
            "Iteration 9, loss = 0.56339839\n",
            "Iteration 10, loss = 0.55674003\n",
            "Iteration 11, loss = 0.56068434\n",
            "Iteration 12, loss = 0.54763901\n",
            "Iteration 13, loss = 0.54280086\n",
            "Iteration 14, loss = 0.53209716\n",
            "Iteration 15, loss = 0.53465311\n",
            "Iteration 16, loss = 0.52987662\n",
            "Iteration 17, loss = 0.49749297\n",
            "Iteration 18, loss = 0.49424501\n",
            "Iteration 19, loss = 0.53624136\n",
            "Iteration 20, loss = 0.49962190\n",
            "Iteration 21, loss = 0.48290389\n",
            "Iteration 22, loss = 0.51207670\n",
            "Iteration 23, loss = 0.61778491\n",
            "Iteration 24, loss = 0.48717603\n",
            "Iteration 25, loss = 0.48269702\n",
            "Iteration 26, loss = 0.48213058\n",
            "Iteration 27, loss = 0.46521768\n",
            "Iteration 28, loss = 0.55446768\n",
            "Iteration 29, loss = 0.52026764\n",
            "Iteration 30, loss = 0.48192957\n",
            "Iteration 31, loss = 0.44963136\n",
            "Iteration 32, loss = 0.49115498\n",
            "Iteration 33, loss = 0.46005493\n",
            "Iteration 34, loss = 0.48183330\n",
            "Iteration 35, loss = 0.52966847\n",
            "Iteration 36, loss = 0.49459222\n",
            "Iteration 37, loss = 0.45107953\n",
            "Iteration 38, loss = 0.46503540\n",
            "Iteration 39, loss = 0.48149827\n",
            "Iteration 40, loss = 0.49269734\n",
            "Iteration 41, loss = 0.45506042\n",
            "Iteration 42, loss = 0.46619305\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.52257349\n",
            "Iteration 2, loss = 0.64186291\n",
            "Iteration 3, loss = 0.59949151\n",
            "Iteration 4, loss = 0.59285101\n",
            "Iteration 5, loss = 0.57231769\n",
            "Iteration 6, loss = 0.57172195\n",
            "Iteration 7, loss = 0.56153817\n",
            "Iteration 8, loss = 0.61636998\n",
            "Iteration 9, loss = 0.55042948\n",
            "Iteration 10, loss = 0.53358686\n",
            "Iteration 11, loss = 0.52021863\n",
            "Iteration 12, loss = 0.54906568\n",
            "Iteration 13, loss = 0.52692120\n",
            "Iteration 14, loss = 0.52929521\n",
            "Iteration 15, loss = 0.50439444\n",
            "Iteration 16, loss = 0.53811168\n",
            "Iteration 17, loss = 0.53913826\n",
            "Iteration 18, loss = 0.48715424\n",
            "Iteration 19, loss = 0.48644806\n",
            "Iteration 20, loss = 0.48185230\n",
            "Iteration 21, loss = 0.47971270\n",
            "Iteration 22, loss = 0.47153001\n",
            "Iteration 23, loss = 0.49631973\n",
            "Iteration 24, loss = 0.47160011\n",
            "Iteration 25, loss = 0.50927900\n",
            "Iteration 26, loss = 0.47019685\n",
            "Iteration 27, loss = 0.47017119\n",
            "Iteration 28, loss = 0.45997041\n",
            "Iteration 29, loss = 0.45876866\n",
            "Iteration 30, loss = 0.49235892\n",
            "Iteration 31, loss = 0.47307272\n",
            "Iteration 32, loss = 0.46758868\n",
            "Iteration 33, loss = 0.45868944\n",
            "Iteration 34, loss = 0.44108374\n",
            "Iteration 35, loss = 0.44762761\n",
            "Iteration 36, loss = 0.45625147\n",
            "Iteration 37, loss = 0.43796078\n",
            "Iteration 38, loss = 0.44123012\n",
            "Iteration 39, loss = 0.45024764\n",
            "Iteration 40, loss = 0.44552442\n",
            "Iteration 41, loss = 0.44433688\n",
            "Iteration 42, loss = 0.49845117\n",
            "Iteration 43, loss = 0.49232545\n",
            "Iteration 44, loss = 0.44500063\n",
            "Iteration 45, loss = 0.44158704\n",
            "Iteration 46, loss = 0.45581218\n",
            "Iteration 47, loss = 0.43363720\n",
            "Iteration 48, loss = 0.46847351\n",
            "Iteration 49, loss = 0.44043705\n",
            "Iteration 50, loss = 0.44574701\n",
            "Iteration 51, loss = 0.44688446\n",
            "Iteration 52, loss = 0.42805361\n",
            "Iteration 53, loss = 0.44219268\n",
            "Iteration 54, loss = 0.44369975\n",
            "Iteration 55, loss = 0.44351131\n",
            "Iteration 56, loss = 0.44443381\n",
            "Iteration 57, loss = 0.45159944\n",
            "Iteration 58, loss = 0.46439672\n",
            "Iteration 59, loss = 0.42562917\n",
            "Iteration 60, loss = 0.43027108\n",
            "Iteration 61, loss = 0.43336904\n",
            "Iteration 62, loss = 0.42527247\n",
            "Iteration 63, loss = 0.43310443\n",
            "Iteration 64, loss = 0.42253353\n",
            "Iteration 65, loss = 0.42550957\n",
            "Iteration 66, loss = 0.51090954\n",
            "Iteration 67, loss = 0.44074165\n",
            "Iteration 68, loss = 0.42209114\n",
            "Iteration 69, loss = 0.43069790\n",
            "Iteration 70, loss = 0.42760281\n",
            "Iteration 71, loss = 0.41897987\n",
            "Iteration 72, loss = 0.41545801\n",
            "Iteration 73, loss = 0.42253702\n",
            "Iteration 74, loss = 0.43663449\n",
            "Iteration 75, loss = 0.42268890\n",
            "Iteration 76, loss = 0.41803137\n",
            "Iteration 77, loss = 0.44764770\n",
            "Iteration 78, loss = 0.41689837\n",
            "Iteration 79, loss = 0.41499835\n",
            "Iteration 80, loss = 0.42050217\n",
            "Iteration 81, loss = 0.41844895\n",
            "Iteration 82, loss = 0.42333094\n",
            "Iteration 83, loss = 0.41196451\n",
            "Iteration 84, loss = 0.44808162\n",
            "Iteration 85, loss = 0.49755544\n",
            "Iteration 86, loss = 0.44468994\n",
            "Iteration 87, loss = 0.42067059\n",
            "Iteration 88, loss = 0.42573662\n",
            "Iteration 89, loss = 0.41637387\n",
            "Iteration 90, loss = 0.42516866\n",
            "Iteration 91, loss = 0.41309481\n",
            "Iteration 92, loss = 0.44729011\n",
            "Iteration 93, loss = 0.49886837\n",
            "Iteration 94, loss = 0.41744867\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.21104402\n",
            "Iteration 2, loss = 0.64198559\n",
            "Iteration 3, loss = 0.62966358\n",
            "Iteration 4, loss = 0.61839559\n",
            "Iteration 5, loss = 0.61281556\n",
            "Iteration 6, loss = 0.60879923\n",
            "Iteration 7, loss = 0.60099974\n",
            "Iteration 8, loss = 0.58638039\n",
            "Iteration 9, loss = 0.58990707\n",
            "Iteration 10, loss = 0.59307817\n",
            "Iteration 11, loss = 0.58097983\n",
            "Iteration 12, loss = 0.58384282\n",
            "Iteration 13, loss = 0.55660650\n",
            "Iteration 14, loss = 0.56912414\n",
            "Iteration 15, loss = 0.57095309\n",
            "Iteration 16, loss = 0.57556637\n",
            "Iteration 17, loss = 0.53610492\n",
            "Iteration 18, loss = 0.54682770\n",
            "Iteration 19, loss = 0.54580969\n",
            "Iteration 20, loss = 0.56636634\n",
            "Iteration 21, loss = 0.53472297\n",
            "Iteration 22, loss = 0.50935827\n",
            "Iteration 23, loss = 0.51402950\n",
            "Iteration 24, loss = 0.52005221\n",
            "Iteration 25, loss = 0.53432463\n",
            "Iteration 26, loss = 0.51244755\n",
            "Iteration 27, loss = 0.49722675\n",
            "Iteration 28, loss = 0.48661280\n",
            "Iteration 29, loss = 0.48705923\n",
            "Iteration 30, loss = 0.51508185\n",
            "Iteration 31, loss = 0.47003768\n",
            "Iteration 32, loss = 0.48064854\n",
            "Iteration 33, loss = 0.49190824\n",
            "Iteration 34, loss = 0.49055151\n",
            "Iteration 35, loss = 0.47778461\n",
            "Iteration 36, loss = 0.47861972\n",
            "Iteration 37, loss = 0.50990985\n",
            "Iteration 38, loss = 0.47260407\n",
            "Iteration 39, loss = 0.45874290\n",
            "Iteration 40, loss = 0.45297534\n",
            "Iteration 41, loss = 0.49737328\n",
            "Iteration 42, loss = 0.47423614\n",
            "Iteration 43, loss = 0.48077728\n",
            "Iteration 44, loss = 0.46616408\n",
            "Iteration 45, loss = 0.45834495\n",
            "Iteration 46, loss = 0.44629975\n",
            "Iteration 47, loss = 0.45535283\n",
            "Iteration 48, loss = 0.44342041\n",
            "Iteration 49, loss = 0.45783163\n",
            "Iteration 50, loss = 0.46047061\n",
            "Iteration 51, loss = 0.44858666\n",
            "Iteration 52, loss = 0.44082912\n",
            "Iteration 53, loss = 0.45588595\n",
            "Iteration 54, loss = 0.47842990\n",
            "Iteration 55, loss = 0.43696344\n",
            "Iteration 56, loss = 0.47362911\n",
            "Iteration 57, loss = 0.45352882\n",
            "Iteration 58, loss = 0.44064645\n",
            "Iteration 59, loss = 0.45523778\n",
            "Iteration 60, loss = 0.44324449\n",
            "Iteration 61, loss = 0.43165073\n",
            "Iteration 62, loss = 0.45261433\n",
            "Iteration 63, loss = 0.43562250\n",
            "Iteration 64, loss = 0.46147441\n",
            "Iteration 65, loss = 0.43593027\n",
            "Iteration 66, loss = 0.43276492\n",
            "Iteration 67, loss = 0.44880649\n",
            "Iteration 68, loss = 0.43390065\n",
            "Iteration 69, loss = 0.44054134\n",
            "Iteration 70, loss = 0.44066075\n",
            "Iteration 71, loss = 0.45079704\n",
            "Iteration 72, loss = 0.43364582\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.72209580\n",
            "Iteration 2, loss = 0.69431482\n",
            "Iteration 3, loss = 0.67622383\n",
            "Iteration 4, loss = 0.66647918\n",
            "Iteration 5, loss = 0.66234197\n",
            "Iteration 6, loss = 0.66056947\n",
            "Iteration 7, loss = 0.66002779\n",
            "Iteration 8, loss = 0.65952152\n",
            "Iteration 9, loss = 0.65887248\n",
            "Iteration 10, loss = 0.65848586\n",
            "Iteration 11, loss = 0.65807113\n",
            "Iteration 12, loss = 0.65783556\n",
            "Iteration 13, loss = 0.65734862\n",
            "Iteration 14, loss = 0.65661891\n",
            "Iteration 15, loss = 0.65659704\n",
            "Iteration 16, loss = 0.65629590\n",
            "Iteration 17, loss = 0.65578943\n",
            "Iteration 18, loss = 0.65560540\n",
            "Iteration 19, loss = 0.65528560\n",
            "Iteration 20, loss = 0.65485282\n",
            "Iteration 21, loss = 0.65449613\n",
            "Iteration 22, loss = 0.65406746\n",
            "Iteration 23, loss = 0.65389518\n",
            "Iteration 24, loss = 0.65352851\n",
            "Iteration 25, loss = 0.65309220\n",
            "Iteration 26, loss = 0.65294774\n",
            "Iteration 27, loss = 0.65235835\n",
            "Iteration 28, loss = 0.65225452\n",
            "Iteration 29, loss = 0.65199648\n",
            "Iteration 30, loss = 0.65182204\n",
            "Iteration 31, loss = 0.65168592\n",
            "Iteration 32, loss = 0.65119514\n",
            "Iteration 33, loss = 0.65076805\n",
            "Iteration 34, loss = 0.65054616\n",
            "Iteration 35, loss = 0.65064057\n",
            "Iteration 36, loss = 0.65006670\n",
            "Iteration 37, loss = 0.64967644\n",
            "Iteration 38, loss = 0.64942282\n",
            "Iteration 39, loss = 0.64916638\n",
            "Iteration 40, loss = 0.64890258\n",
            "Iteration 41, loss = 0.64868953\n",
            "Iteration 42, loss = 0.64853642\n",
            "Iteration 43, loss = 0.64819137\n",
            "Iteration 44, loss = 0.64768048\n",
            "Iteration 45, loss = 0.64736657\n",
            "Iteration 46, loss = 0.64677308\n",
            "Iteration 47, loss = 0.64718688\n",
            "Iteration 48, loss = 0.64648971\n",
            "Iteration 49, loss = 0.64623974\n",
            "Iteration 50, loss = 0.64579565\n",
            "Iteration 51, loss = 0.64545904\n",
            "Iteration 52, loss = 0.64518455\n",
            "Iteration 53, loss = 0.64510662\n",
            "Iteration 54, loss = 0.64456565\n",
            "Iteration 55, loss = 0.64424027\n",
            "Iteration 56, loss = 0.64399650\n",
            "Iteration 57, loss = 0.64340460\n",
            "Iteration 58, loss = 0.64344335\n",
            "Iteration 59, loss = 0.64332939\n",
            "Iteration 60, loss = 0.64302985\n",
            "Iteration 61, loss = 0.64255079\n",
            "Iteration 62, loss = 0.64224296\n",
            "Iteration 63, loss = 0.64222429\n",
            "Iteration 64, loss = 0.64164189\n",
            "Iteration 65, loss = 0.64152596\n",
            "Iteration 66, loss = 0.64123008\n",
            "Iteration 67, loss = 0.64094470\n",
            "Iteration 68, loss = 0.64068932\n",
            "Iteration 69, loss = 0.64033913\n",
            "Iteration 70, loss = 0.63989194\n",
            "Iteration 71, loss = 0.63959491\n",
            "Iteration 72, loss = 0.63948196\n",
            "Iteration 73, loss = 0.63903283\n",
            "Iteration 74, loss = 0.63857025\n",
            "Iteration 75, loss = 0.63841807\n",
            "Iteration 76, loss = 0.63800399\n",
            "Iteration 77, loss = 0.63773914\n",
            "Iteration 78, loss = 0.63738667\n",
            "Iteration 79, loss = 0.63720469\n",
            "Iteration 80, loss = 0.63690769\n",
            "Iteration 81, loss = 0.63654802\n",
            "Iteration 82, loss = 0.63619620\n",
            "Iteration 83, loss = 0.63591803\n",
            "Iteration 84, loss = 0.63580309\n",
            "Iteration 85, loss = 0.63534769\n",
            "Iteration 86, loss = 0.63525060\n",
            "Iteration 87, loss = 0.63489724\n",
            "Iteration 88, loss = 0.63421386\n",
            "Iteration 89, loss = 0.63403620\n",
            "Iteration 90, loss = 0.63361184\n",
            "Iteration 91, loss = 0.63325058\n",
            "Iteration 92, loss = 0.63289814\n",
            "Iteration 93, loss = 0.63271966\n",
            "Iteration 94, loss = 0.63236194\n",
            "Iteration 95, loss = 0.63224589\n",
            "Iteration 96, loss = 0.63199285\n",
            "Iteration 97, loss = 0.63132123\n",
            "Iteration 98, loss = 0.63096083\n",
            "Iteration 99, loss = 0.63133599\n",
            "Iteration 100, loss = 0.63056100\n",
            "Iteration 101, loss = 0.63004683\n",
            "Iteration 102, loss = 0.62969179\n",
            "Iteration 103, loss = 0.62971756\n",
            "Iteration 104, loss = 0.62920953\n",
            "Iteration 105, loss = 0.62885363\n",
            "Iteration 106, loss = 0.62844942\n",
            "Iteration 107, loss = 0.62820431\n",
            "Iteration 108, loss = 0.62822183\n",
            "Iteration 109, loss = 0.62779574\n",
            "Iteration 110, loss = 0.62762344\n",
            "Iteration 111, loss = 0.62643810\n",
            "Iteration 112, loss = 0.62712739\n",
            "Iteration 113, loss = 0.62741912\n",
            "Iteration 114, loss = 0.62709061\n",
            "Iteration 115, loss = 0.62663335\n",
            "Iteration 116, loss = 0.62600546\n",
            "Iteration 117, loss = 0.62557150\n",
            "Iteration 118, loss = 0.62475369\n",
            "Iteration 119, loss = 0.62453231\n",
            "Iteration 120, loss = 0.62424824\n",
            "Iteration 121, loss = 0.62406676\n",
            "Iteration 122, loss = 0.62365332\n",
            "Iteration 123, loss = 0.62357211\n",
            "Iteration 124, loss = 0.62323504\n",
            "Iteration 125, loss = 0.62302130\n",
            "Iteration 126, loss = 0.62299733\n",
            "Iteration 127, loss = 0.62233753\n",
            "Iteration 128, loss = 0.62208565\n",
            "Iteration 129, loss = 0.62190649\n",
            "Iteration 130, loss = 0.62181189\n",
            "Iteration 131, loss = 0.62169021\n",
            "Iteration 132, loss = 0.62177654\n",
            "Iteration 133, loss = 0.62109395\n",
            "Iteration 134, loss = 0.62092481\n",
            "Iteration 135, loss = 0.62071178\n",
            "Iteration 136, loss = 0.62046728\n",
            "Iteration 137, loss = 0.62029143\n",
            "Iteration 138, loss = 0.62011627\n",
            "Iteration 139, loss = 0.61968840\n",
            "Iteration 140, loss = 0.61949966\n",
            "Iteration 141, loss = 0.61956290\n",
            "Iteration 142, loss = 0.61936893\n",
            "Iteration 143, loss = 0.61878002\n",
            "Iteration 144, loss = 0.61867229\n",
            "Iteration 145, loss = 0.61917574\n",
            "Iteration 146, loss = 0.61846023\n",
            "Iteration 147, loss = 0.61792661\n",
            "Iteration 148, loss = 0.61767065\n",
            "Iteration 149, loss = 0.61773177\n",
            "Iteration 150, loss = 0.61743402\n",
            "Iteration 151, loss = 0.61718697\n",
            "Iteration 152, loss = 0.61700410\n",
            "Iteration 153, loss = 0.61695504\n",
            "Iteration 154, loss = 0.61669393\n",
            "Iteration 155, loss = 0.61642968\n",
            "Iteration 156, loss = 0.61623753\n",
            "Iteration 157, loss = 0.61600393\n",
            "Iteration 158, loss = 0.61579705\n",
            "Iteration 159, loss = 0.61585604\n",
            "Iteration 160, loss = 0.61541297\n",
            "Iteration 161, loss = 0.61541450\n",
            "Iteration 162, loss = 0.61512204\n",
            "Iteration 163, loss = 0.61527332\n",
            "Iteration 164, loss = 0.61506802\n",
            "Iteration 165, loss = 0.61504997\n",
            "Iteration 166, loss = 0.61471791\n",
            "Iteration 167, loss = 0.61437551\n",
            "Iteration 168, loss = 0.61422998\n",
            "Iteration 169, loss = 0.61431693\n",
            "Iteration 170, loss = 0.61390358\n",
            "Iteration 171, loss = 0.61368100\n",
            "Iteration 172, loss = 0.61363589\n",
            "Iteration 173, loss = 0.61350628\n",
            "Iteration 174, loss = 0.61308524\n",
            "Iteration 175, loss = 0.61342417\n",
            "Iteration 176, loss = 0.61339232\n",
            "Iteration 177, loss = 0.61297076\n",
            "Iteration 178, loss = 0.61273898\n",
            "Iteration 179, loss = 0.61247470\n",
            "Iteration 180, loss = 0.61286965\n",
            "Iteration 181, loss = 0.61298231\n",
            "Iteration 182, loss = 0.61255332\n",
            "Iteration 183, loss = 0.61238795\n",
            "Iteration 184, loss = 0.61248023\n",
            "Iteration 185, loss = 0.61190576\n",
            "Iteration 186, loss = 0.61172244\n",
            "Iteration 187, loss = 0.61154593\n",
            "Iteration 188, loss = 0.61186834\n",
            "Iteration 189, loss = 0.61153784\n",
            "Iteration 190, loss = 0.61142744\n",
            "Iteration 191, loss = 0.61170425\n",
            "Iteration 192, loss = 0.61105933\n",
            "Iteration 193, loss = 0.61097712\n",
            "Iteration 194, loss = 0.61080756\n",
            "Iteration 195, loss = 0.61070595\n",
            "Iteration 196, loss = 0.61060716\n",
            "Iteration 197, loss = 0.61046207\n",
            "Iteration 198, loss = 0.61038266\n",
            "Iteration 199, loss = 0.61029612\n",
            "Iteration 200, loss = 0.61016134\n",
            "Iteration 201, loss = 0.61008422\n",
            "Iteration 202, loss = 0.60991494\n",
            "Iteration 203, loss = 0.60974888\n",
            "Iteration 204, loss = 0.60966304\n",
            "Iteration 205, loss = 0.60965549\n",
            "Iteration 206, loss = 0.60950203\n",
            "Iteration 207, loss = 0.60957837\n",
            "Iteration 208, loss = 0.60956312\n",
            "Iteration 209, loss = 0.60944016\n",
            "Iteration 210, loss = 0.60895255\n",
            "Iteration 211, loss = 0.60896264\n",
            "Iteration 212, loss = 0.60899151\n",
            "Iteration 213, loss = 0.60898798\n",
            "Iteration 214, loss = 0.60894291\n",
            "Iteration 215, loss = 0.60899643\n",
            "Iteration 216, loss = 0.60859592\n",
            "Iteration 217, loss = 0.60874492\n",
            "Iteration 218, loss = 0.60853784\n",
            "Iteration 219, loss = 0.60845090\n",
            "Iteration 220, loss = 0.60850580\n",
            "Iteration 221, loss = 0.60872301\n",
            "Iteration 222, loss = 0.60841425\n",
            "Iteration 223, loss = 0.60844367\n",
            "Iteration 224, loss = 0.60834929\n",
            "Iteration 225, loss = 0.60827822\n",
            "Iteration 226, loss = 0.60794856\n",
            "Iteration 227, loss = 0.60806252\n",
            "Iteration 228, loss = 0.60845147\n",
            "Iteration 229, loss = 0.60840607\n",
            "Iteration 230, loss = 0.60816049\n",
            "Iteration 231, loss = 0.60800526\n",
            "Iteration 232, loss = 0.60762772\n",
            "Iteration 233, loss = 0.60760255\n",
            "Iteration 234, loss = 0.60757098\n",
            "Iteration 235, loss = 0.60767885\n",
            "Iteration 236, loss = 0.60772286\n",
            "Iteration 237, loss = 0.60757014\n",
            "Iteration 238, loss = 0.60730396\n",
            "Iteration 239, loss = 0.60722593\n",
            "Iteration 240, loss = 0.60715271\n",
            "Iteration 241, loss = 0.60700993\n",
            "Iteration 242, loss = 0.60687011\n",
            "Iteration 243, loss = 0.60689783\n",
            "Iteration 244, loss = 0.60675186\n",
            "Iteration 245, loss = 0.60687723\n",
            "Iteration 246, loss = 0.60705918\n",
            "Iteration 247, loss = 0.60694300\n",
            "Iteration 248, loss = 0.60685627\n",
            "Iteration 249, loss = 0.60676682\n",
            "Iteration 250, loss = 0.60682628\n",
            "Iteration 251, loss = 0.60690879\n",
            "Iteration 252, loss = 0.60655458\n",
            "Iteration 253, loss = 0.60657226\n",
            "Iteration 254, loss = 0.60647370\n",
            "Iteration 255, loss = 0.60641378\n",
            "Iteration 256, loss = 0.60660278\n",
            "Iteration 257, loss = 0.60682166\n",
            "Iteration 258, loss = 0.60645844\n",
            "Iteration 259, loss = 0.60647413\n",
            "Iteration 260, loss = 0.60640938\n",
            "Iteration 261, loss = 0.60632691\n",
            "Iteration 262, loss = 0.60647726\n",
            "Iteration 263, loss = 0.60606961\n",
            "Iteration 264, loss = 0.60604110\n",
            "Iteration 265, loss = 0.60634692\n",
            "Iteration 266, loss = 0.60597077\n",
            "Iteration 267, loss = 0.60595490\n",
            "Iteration 268, loss = 0.60608072\n",
            "Iteration 269, loss = 0.60599924\n",
            "Iteration 270, loss = 0.60609780\n",
            "Iteration 271, loss = 0.60629262\n",
            "Iteration 272, loss = 0.60607456\n",
            "Iteration 273, loss = 0.60626788\n",
            "Iteration 274, loss = 0.60580735\n",
            "Iteration 275, loss = 0.60580702\n",
            "Iteration 276, loss = 0.60633649\n",
            "Iteration 277, loss = 0.60626416\n",
            "Iteration 278, loss = 0.60614553\n",
            "Iteration 279, loss = 0.60560661\n",
            "Iteration 280, loss = 0.60552391\n",
            "Iteration 281, loss = 0.60542972\n",
            "Iteration 282, loss = 0.60535131\n",
            "Iteration 283, loss = 0.60538835\n",
            "Iteration 284, loss = 0.60527695\n",
            "Iteration 285, loss = 0.60509446\n",
            "Iteration 286, loss = 0.60532869\n",
            "Iteration 287, loss = 0.60558189\n",
            "Iteration 288, loss = 0.60531055\n",
            "Iteration 289, loss = 0.60516848\n",
            "Iteration 290, loss = 0.60500669\n",
            "Iteration 291, loss = 0.60481007\n",
            "Iteration 292, loss = 0.60506805\n",
            "Iteration 293, loss = 0.60497502\n",
            "Iteration 294, loss = 0.60503785\n",
            "Iteration 295, loss = 0.60497519\n",
            "Iteration 296, loss = 0.60472229\n",
            "Iteration 297, loss = 0.60492287\n",
            "Iteration 298, loss = 0.60466821\n",
            "Iteration 299, loss = 0.60456196\n",
            "Iteration 300, loss = 0.60462597\n",
            "Iteration 301, loss = 0.60462636\n",
            "Iteration 302, loss = 0.60474047\n",
            "Iteration 303, loss = 0.60455157\n",
            "Iteration 304, loss = 0.60458963\n",
            "Iteration 305, loss = 0.60455420\n",
            "Iteration 306, loss = 0.60457318\n",
            "Iteration 307, loss = 0.60473050\n",
            "Iteration 308, loss = 0.60486276\n",
            "Iteration 309, loss = 0.60420722\n",
            "Iteration 310, loss = 0.60452764\n",
            "Iteration 311, loss = 0.60464590\n",
            "Iteration 312, loss = 0.60469359\n",
            "Iteration 313, loss = 0.60428201\n",
            "Iteration 314, loss = 0.60416578\n",
            "Iteration 315, loss = 0.60512818\n",
            "Iteration 316, loss = 0.60408513\n",
            "Iteration 317, loss = 0.60436712\n",
            "Iteration 318, loss = 0.60398521\n",
            "Iteration 319, loss = 0.60397272\n",
            "Iteration 320, loss = 0.60388841\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68732681\n",
            "Iteration 2, loss = 0.67559961\n",
            "Iteration 3, loss = 0.66789397\n",
            "Iteration 4, loss = 0.66484092\n",
            "Iteration 5, loss = 0.66328240\n",
            "Iteration 6, loss = 0.66239225\n",
            "Iteration 7, loss = 0.66209214\n",
            "Iteration 8, loss = 0.66142455\n",
            "Iteration 9, loss = 0.66087489\n",
            "Iteration 10, loss = 0.66058250\n",
            "Iteration 11, loss = 0.66012727\n",
            "Iteration 12, loss = 0.65939114\n",
            "Iteration 13, loss = 0.65925714\n",
            "Iteration 14, loss = 0.65833163\n",
            "Iteration 15, loss = 0.65783378\n",
            "Iteration 16, loss = 0.65734256\n",
            "Iteration 17, loss = 0.65694220\n",
            "Iteration 18, loss = 0.65641553\n",
            "Iteration 19, loss = 0.65611994\n",
            "Iteration 20, loss = 0.65556388\n",
            "Iteration 21, loss = 0.65512909\n",
            "Iteration 22, loss = 0.65470155\n",
            "Iteration 23, loss = 0.65443269\n",
            "Iteration 24, loss = 0.65388229\n",
            "Iteration 25, loss = 0.65344087\n",
            "Iteration 26, loss = 0.65346484\n",
            "Iteration 27, loss = 0.65286458\n",
            "Iteration 28, loss = 0.65217733\n",
            "Iteration 29, loss = 0.65165332\n",
            "Iteration 30, loss = 0.65109434\n",
            "Iteration 31, loss = 0.65058095\n",
            "Iteration 32, loss = 0.65022848\n",
            "Iteration 33, loss = 0.65026672\n",
            "Iteration 34, loss = 0.64952789\n",
            "Iteration 35, loss = 0.64881683\n",
            "Iteration 36, loss = 0.64856394\n",
            "Iteration 37, loss = 0.64809620\n",
            "Iteration 38, loss = 0.64759092\n",
            "Iteration 39, loss = 0.64696605\n",
            "Iteration 40, loss = 0.64662419\n",
            "Iteration 41, loss = 0.64614280\n",
            "Iteration 42, loss = 0.64615640\n",
            "Iteration 43, loss = 0.64524764\n",
            "Iteration 44, loss = 0.64463872\n",
            "Iteration 45, loss = 0.64404318\n",
            "Iteration 46, loss = 0.64396702\n",
            "Iteration 47, loss = 0.64366806\n",
            "Iteration 48, loss = 0.64312763\n",
            "Iteration 49, loss = 0.64274004\n",
            "Iteration 50, loss = 0.64204986\n",
            "Iteration 51, loss = 0.64214203\n",
            "Iteration 52, loss = 0.64195914\n",
            "Iteration 53, loss = 0.64181612\n",
            "Iteration 54, loss = 0.64122436\n",
            "Iteration 55, loss = 0.64049931\n",
            "Iteration 56, loss = 0.63996971\n",
            "Iteration 57, loss = 0.63893463\n",
            "Iteration 58, loss = 0.63859284\n",
            "Iteration 59, loss = 0.63830748\n",
            "Iteration 60, loss = 0.63803270\n",
            "Iteration 61, loss = 0.63743121\n",
            "Iteration 62, loss = 0.63700728\n",
            "Iteration 63, loss = 0.63687803\n",
            "Iteration 64, loss = 0.63619695\n",
            "Iteration 65, loss = 0.63564376\n",
            "Iteration 66, loss = 0.63554602\n",
            "Iteration 67, loss = 0.63473247\n",
            "Iteration 68, loss = 0.63469198\n",
            "Iteration 69, loss = 0.63408394\n",
            "Iteration 70, loss = 0.63363378\n",
            "Iteration 71, loss = 0.63321913\n",
            "Iteration 72, loss = 0.63282272\n",
            "Iteration 73, loss = 0.63256808\n",
            "Iteration 74, loss = 0.63211532\n",
            "Iteration 75, loss = 0.63156663\n",
            "Iteration 76, loss = 0.63139527\n",
            "Iteration 77, loss = 0.63116965\n",
            "Iteration 78, loss = 0.63095404\n",
            "Iteration 79, loss = 0.63057096\n",
            "Iteration 80, loss = 0.62946530\n",
            "Iteration 81, loss = 0.62914655\n",
            "Iteration 82, loss = 0.62887219\n",
            "Iteration 83, loss = 0.62834578\n",
            "Iteration 84, loss = 0.62773495\n",
            "Iteration 85, loss = 0.62719877\n",
            "Iteration 86, loss = 0.62682657\n",
            "Iteration 87, loss = 0.62642931\n",
            "Iteration 88, loss = 0.62601683\n",
            "Iteration 89, loss = 0.62610899\n",
            "Iteration 90, loss = 0.62558082\n",
            "Iteration 91, loss = 0.62496123\n",
            "Iteration 92, loss = 0.62440788\n",
            "Iteration 93, loss = 0.62441257\n",
            "Iteration 94, loss = 0.62402974\n",
            "Iteration 95, loss = 0.62347510\n",
            "Iteration 96, loss = 0.62341005\n",
            "Iteration 97, loss = 0.62280915\n",
            "Iteration 98, loss = 0.62237455\n",
            "Iteration 99, loss = 0.62199040\n",
            "Iteration 100, loss = 0.62189695\n",
            "Iteration 101, loss = 0.62167976\n",
            "Iteration 102, loss = 0.62117286\n",
            "Iteration 103, loss = 0.62077308\n",
            "Iteration 104, loss = 0.62063454\n",
            "Iteration 105, loss = 0.62104533\n",
            "Iteration 106, loss = 0.62045998\n",
            "Iteration 107, loss = 0.62008576\n",
            "Iteration 108, loss = 0.61938386\n",
            "Iteration 109, loss = 0.61929816\n",
            "Iteration 110, loss = 0.61898325\n",
            "Iteration 111, loss = 0.61846402\n",
            "Iteration 112, loss = 0.61811597\n",
            "Iteration 113, loss = 0.61803656\n",
            "Iteration 114, loss = 0.61763518\n",
            "Iteration 115, loss = 0.61728345\n",
            "Iteration 116, loss = 0.61709940\n",
            "Iteration 117, loss = 0.61712618\n",
            "Iteration 118, loss = 0.61646211\n",
            "Iteration 119, loss = 0.61618677\n",
            "Iteration 120, loss = 0.61600289\n",
            "Iteration 121, loss = 0.61592231\n",
            "Iteration 122, loss = 0.61581466\n",
            "Iteration 123, loss = 0.61523839\n",
            "Iteration 124, loss = 0.61525394\n",
            "Iteration 125, loss = 0.61485996\n",
            "Iteration 126, loss = 0.61490735\n",
            "Iteration 127, loss = 0.61429173\n",
            "Iteration 128, loss = 0.61396540\n",
            "Iteration 129, loss = 0.61374399\n",
            "Iteration 130, loss = 0.61365760\n",
            "Iteration 131, loss = 0.61342035\n",
            "Iteration 132, loss = 0.61309180\n",
            "Iteration 133, loss = 0.61287237\n",
            "Iteration 134, loss = 0.61272419\n",
            "Iteration 135, loss = 0.61250281\n",
            "Iteration 136, loss = 0.61219399\n",
            "Iteration 137, loss = 0.61283998\n",
            "Iteration 138, loss = 0.61294770\n",
            "Iteration 139, loss = 0.61213985\n",
            "Iteration 140, loss = 0.61138270\n",
            "Iteration 141, loss = 0.61116934\n",
            "Iteration 142, loss = 0.61095420\n",
            "Iteration 143, loss = 0.61098636\n",
            "Iteration 144, loss = 0.61091865\n",
            "Iteration 145, loss = 0.61021364\n",
            "Iteration 146, loss = 0.60998798\n",
            "Iteration 147, loss = 0.60992322\n",
            "Iteration 148, loss = 0.60970581\n",
            "Iteration 149, loss = 0.60976456\n",
            "Iteration 150, loss = 0.60968079\n",
            "Iteration 151, loss = 0.60925044\n",
            "Iteration 152, loss = 0.60886101\n",
            "Iteration 153, loss = 0.60910184\n",
            "Iteration 154, loss = 0.60870647\n",
            "Iteration 155, loss = 0.60845068\n",
            "Iteration 156, loss = 0.60830297\n",
            "Iteration 157, loss = 0.60818613\n",
            "Iteration 158, loss = 0.60804343\n",
            "Iteration 159, loss = 0.60791419\n",
            "Iteration 160, loss = 0.60786588\n",
            "Iteration 161, loss = 0.60758548\n",
            "Iteration 162, loss = 0.60760973\n",
            "Iteration 163, loss = 0.60735101\n",
            "Iteration 164, loss = 0.60722143\n",
            "Iteration 165, loss = 0.60728014\n",
            "Iteration 166, loss = 0.60737680\n",
            "Iteration 167, loss = 0.60704219\n",
            "Iteration 168, loss = 0.60684500\n",
            "Iteration 169, loss = 0.60672325\n",
            "Iteration 170, loss = 0.60657671\n",
            "Iteration 171, loss = 0.60653225\n",
            "Iteration 172, loss = 0.60632021\n",
            "Iteration 173, loss = 0.60625410\n",
            "Iteration 174, loss = 0.60614697\n",
            "Iteration 175, loss = 0.60593414\n",
            "Iteration 176, loss = 0.60609688\n",
            "Iteration 177, loss = 0.60589073\n",
            "Iteration 178, loss = 0.60594817\n",
            "Iteration 179, loss = 0.60578464\n",
            "Iteration 180, loss = 0.60543311\n",
            "Iteration 181, loss = 0.60559469\n",
            "Iteration 182, loss = 0.60548851\n",
            "Iteration 183, loss = 0.60548351\n",
            "Iteration 184, loss = 0.60529302\n",
            "Iteration 185, loss = 0.60509579\n",
            "Iteration 186, loss = 0.60527896\n",
            "Iteration 187, loss = 0.60533370\n",
            "Iteration 188, loss = 0.60499258\n",
            "Iteration 189, loss = 0.60482024\n",
            "Iteration 190, loss = 0.60487447\n",
            "Iteration 191, loss = 0.60468941\n",
            "Iteration 192, loss = 0.60472701\n",
            "Iteration 193, loss = 0.60485747\n",
            "Iteration 194, loss = 0.60483092\n",
            "Iteration 195, loss = 0.60467299\n",
            "Iteration 196, loss = 0.60447616\n",
            "Iteration 197, loss = 0.60440992\n",
            "Iteration 198, loss = 0.60439226\n",
            "Iteration 199, loss = 0.60450153\n",
            "Iteration 200, loss = 0.60459774\n",
            "Iteration 201, loss = 0.60430344\n",
            "Iteration 202, loss = 0.60421519\n",
            "Iteration 203, loss = 0.60408709\n",
            "Iteration 204, loss = 0.60402277\n",
            "Iteration 205, loss = 0.60393879\n",
            "Iteration 206, loss = 0.60400759\n",
            "Iteration 207, loss = 0.60392844\n",
            "Iteration 208, loss = 0.60417225\n",
            "Iteration 209, loss = 0.60374367\n",
            "Iteration 210, loss = 0.60377657\n",
            "Iteration 211, loss = 0.60362697\n",
            "Iteration 212, loss = 0.60341530\n",
            "Iteration 213, loss = 0.60345324\n",
            "Iteration 214, loss = 0.60357237\n",
            "Iteration 215, loss = 0.60356706\n",
            "Iteration 216, loss = 0.60398515\n",
            "Iteration 217, loss = 0.60348458\n",
            "Iteration 218, loss = 0.60328423\n",
            "Iteration 219, loss = 0.60315756\n",
            "Iteration 220, loss = 0.60291090\n",
            "Iteration 221, loss = 0.60305952\n",
            "Iteration 222, loss = 0.60339501\n",
            "Iteration 223, loss = 0.60309068\n",
            "Iteration 224, loss = 0.60316176\n",
            "Iteration 225, loss = 0.60289358\n",
            "Iteration 226, loss = 0.60295527\n",
            "Iteration 227, loss = 0.60283014\n",
            "Iteration 228, loss = 0.60267143\n",
            "Iteration 229, loss = 0.60275268\n",
            "Iteration 230, loss = 0.60265217\n",
            "Iteration 231, loss = 0.60255672\n",
            "Iteration 232, loss = 0.60249802\n",
            "Iteration 233, loss = 0.60247956\n",
            "Iteration 234, loss = 0.60244565\n",
            "Iteration 235, loss = 0.60226785\n",
            "Iteration 236, loss = 0.60237701\n",
            "Iteration 237, loss = 0.60239743\n",
            "Iteration 238, loss = 0.60247200\n",
            "Iteration 239, loss = 0.60249524\n",
            "Iteration 240, loss = 0.60235565\n",
            "Iteration 241, loss = 0.60227793\n",
            "Iteration 242, loss = 0.60210600\n",
            "Iteration 243, loss = 0.60192631\n",
            "Iteration 244, loss = 0.60207930\n",
            "Iteration 245, loss = 0.60198727\n",
            "Iteration 246, loss = 0.60179465\n",
            "Iteration 247, loss = 0.60183184\n",
            "Iteration 248, loss = 0.60185162\n",
            "Iteration 249, loss = 0.60211767\n",
            "Iteration 250, loss = 0.60173613\n",
            "Iteration 251, loss = 0.60204300\n",
            "Iteration 252, loss = 0.60175342\n",
            "Iteration 253, loss = 0.60184353\n",
            "Iteration 254, loss = 0.60177014\n",
            "Iteration 255, loss = 0.60163220\n",
            "Iteration 256, loss = 0.60203751\n",
            "Iteration 257, loss = 0.60219032\n",
            "Iteration 258, loss = 0.60182622\n",
            "Iteration 259, loss = 0.60170524\n",
            "Iteration 260, loss = 0.60161767\n",
            "Iteration 261, loss = 0.60132958\n",
            "Iteration 262, loss = 0.60129117\n",
            "Iteration 263, loss = 0.60108423\n",
            "Iteration 264, loss = 0.60123878\n",
            "Iteration 265, loss = 0.60107038\n",
            "Iteration 266, loss = 0.60135645\n",
            "Iteration 267, loss = 0.60124393\n",
            "Iteration 268, loss = 0.60099447\n",
            "Iteration 269, loss = 0.60102189\n",
            "Iteration 270, loss = 0.60098347\n",
            "Iteration 271, loss = 0.60092789\n",
            "Iteration 272, loss = 0.60097222\n",
            "Iteration 273, loss = 0.60090507\n",
            "Iteration 274, loss = 0.60097518\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67168632\n",
            "Iteration 2, loss = 0.66660403\n",
            "Iteration 3, loss = 0.66302611\n",
            "Iteration 4, loss = 0.65993634\n",
            "Iteration 5, loss = 0.65916207\n",
            "Iteration 6, loss = 0.65826280\n",
            "Iteration 7, loss = 0.65767226\n",
            "Iteration 8, loss = 0.65718599\n",
            "Iteration 9, loss = 0.65686401\n",
            "Iteration 10, loss = 0.65635076\n",
            "Iteration 11, loss = 0.65604869\n",
            "Iteration 12, loss = 0.65567964\n",
            "Iteration 13, loss = 0.65531683\n",
            "Iteration 14, loss = 0.65502932\n",
            "Iteration 15, loss = 0.65466273\n",
            "Iteration 16, loss = 0.65458027\n",
            "Iteration 17, loss = 0.65394399\n",
            "Iteration 18, loss = 0.65361980\n",
            "Iteration 19, loss = 0.65326915\n",
            "Iteration 20, loss = 0.65272007\n",
            "Iteration 21, loss = 0.65247663\n",
            "Iteration 22, loss = 0.65242283\n",
            "Iteration 23, loss = 0.65183366\n",
            "Iteration 24, loss = 0.65177752\n",
            "Iteration 25, loss = 0.65099847\n",
            "Iteration 26, loss = 0.65070889\n",
            "Iteration 27, loss = 0.65034744\n",
            "Iteration 28, loss = 0.65003185\n",
            "Iteration 29, loss = 0.65001316\n",
            "Iteration 30, loss = 0.64943962\n",
            "Iteration 31, loss = 0.64914925\n",
            "Iteration 32, loss = 0.64894298\n",
            "Iteration 33, loss = 0.64825903\n",
            "Iteration 34, loss = 0.64791075\n",
            "Iteration 35, loss = 0.64815896\n",
            "Iteration 36, loss = 0.64757220\n",
            "Iteration 37, loss = 0.64716886\n",
            "Iteration 38, loss = 0.64671670\n",
            "Iteration 39, loss = 0.64646088\n",
            "Iteration 40, loss = 0.64612417\n",
            "Iteration 41, loss = 0.64602041\n",
            "Iteration 42, loss = 0.64563840\n",
            "Iteration 43, loss = 0.64523611\n",
            "Iteration 44, loss = 0.64483769\n",
            "Iteration 45, loss = 0.64477956\n",
            "Iteration 46, loss = 0.64438853\n",
            "Iteration 47, loss = 0.64426532\n",
            "Iteration 48, loss = 0.64417845\n",
            "Iteration 49, loss = 0.64339875\n",
            "Iteration 50, loss = 0.64309597\n",
            "Iteration 51, loss = 0.64286874\n",
            "Iteration 52, loss = 0.64231525\n",
            "Iteration 53, loss = 0.64205451\n",
            "Iteration 54, loss = 0.64192801\n",
            "Iteration 55, loss = 0.64165453\n",
            "Iteration 56, loss = 0.64218379\n",
            "Iteration 57, loss = 0.64157046\n",
            "Iteration 58, loss = 0.64088680\n",
            "Iteration 59, loss = 0.63989459\n",
            "Iteration 60, loss = 0.63989293\n",
            "Iteration 61, loss = 0.63953703\n",
            "Iteration 62, loss = 0.63924077\n",
            "Iteration 63, loss = 0.63894585\n",
            "Iteration 64, loss = 0.63886070\n",
            "Iteration 65, loss = 0.63848432\n",
            "Iteration 66, loss = 0.63806762\n",
            "Iteration 67, loss = 0.63788514\n",
            "Iteration 68, loss = 0.63752703\n",
            "Iteration 69, loss = 0.63722088\n",
            "Iteration 70, loss = 0.63685068\n",
            "Iteration 71, loss = 0.63663305\n",
            "Iteration 72, loss = 0.63628692\n",
            "Iteration 73, loss = 0.63611658\n",
            "Iteration 74, loss = 0.63588954\n",
            "Iteration 75, loss = 0.63569633\n",
            "Iteration 76, loss = 0.63552541\n",
            "Iteration 77, loss = 0.63515608\n",
            "Iteration 78, loss = 0.63480722\n",
            "Iteration 79, loss = 0.63440399\n",
            "Iteration 80, loss = 0.63409945\n",
            "Iteration 81, loss = 0.63390456\n",
            "Iteration 82, loss = 0.63359567\n",
            "Iteration 83, loss = 0.63340501\n",
            "Iteration 84, loss = 0.63325631\n",
            "Iteration 85, loss = 0.63306693\n",
            "Iteration 86, loss = 0.63283073\n",
            "Iteration 87, loss = 0.63248201\n",
            "Iteration 88, loss = 0.63236584\n",
            "Iteration 89, loss = 0.63194340\n",
            "Iteration 90, loss = 0.63172372\n",
            "Iteration 91, loss = 0.63151931\n",
            "Iteration 92, loss = 0.63120843\n",
            "Iteration 93, loss = 0.63110973\n",
            "Iteration 94, loss = 0.63059411\n",
            "Iteration 95, loss = 0.63054696\n",
            "Iteration 96, loss = 0.63015553\n",
            "Iteration 97, loss = 0.62988587\n",
            "Iteration 98, loss = 0.62977410\n",
            "Iteration 99, loss = 0.62937181\n",
            "Iteration 100, loss = 0.62923128\n",
            "Iteration 101, loss = 0.62920262\n",
            "Iteration 102, loss = 0.62880287\n",
            "Iteration 103, loss = 0.62862830\n",
            "Iteration 104, loss = 0.62899973\n",
            "Iteration 105, loss = 0.62934390\n",
            "Iteration 106, loss = 0.62886389\n",
            "Iteration 107, loss = 0.62843582\n",
            "Iteration 108, loss = 0.62789613\n",
            "Iteration 109, loss = 0.62757953\n",
            "Iteration 110, loss = 0.62723321\n",
            "Iteration 111, loss = 0.62661744\n",
            "Iteration 112, loss = 0.62690816\n",
            "Iteration 113, loss = 0.62660898\n",
            "Iteration 114, loss = 0.62627633\n",
            "Iteration 115, loss = 0.62614261\n",
            "Iteration 116, loss = 0.62586832\n",
            "Iteration 117, loss = 0.62553576\n",
            "Iteration 118, loss = 0.62534260\n",
            "Iteration 119, loss = 0.62515337\n",
            "Iteration 120, loss = 0.62537458\n",
            "Iteration 121, loss = 0.62452938\n",
            "Iteration 122, loss = 0.62483115\n",
            "Iteration 123, loss = 0.62470140\n",
            "Iteration 124, loss = 0.62399744\n",
            "Iteration 125, loss = 0.62401892\n",
            "Iteration 126, loss = 0.62392387\n",
            "Iteration 127, loss = 0.62375468\n",
            "Iteration 128, loss = 0.62358535\n",
            "Iteration 129, loss = 0.62347254\n",
            "Iteration 130, loss = 0.62327133\n",
            "Iteration 131, loss = 0.62316449\n",
            "Iteration 132, loss = 0.62299872\n",
            "Iteration 133, loss = 0.62290889\n",
            "Iteration 134, loss = 0.62278331\n",
            "Iteration 135, loss = 0.62318014\n",
            "Iteration 136, loss = 0.62285994\n",
            "Iteration 137, loss = 0.62256962\n",
            "Iteration 138, loss = 0.62242631\n",
            "Iteration 139, loss = 0.62230042\n",
            "Iteration 140, loss = 0.62202589\n",
            "Iteration 141, loss = 0.62195179\n",
            "Iteration 142, loss = 0.62247275\n",
            "Iteration 143, loss = 0.62181944\n",
            "Iteration 144, loss = 0.62161744\n",
            "Iteration 145, loss = 0.62169074\n",
            "Iteration 146, loss = 0.62143166\n",
            "Iteration 147, loss = 0.62136258\n",
            "Iteration 148, loss = 0.62121349\n",
            "Iteration 149, loss = 0.62138094\n",
            "Iteration 150, loss = 0.62135274\n",
            "Iteration 151, loss = 0.62077010\n",
            "Iteration 152, loss = 0.62053094\n",
            "Iteration 153, loss = 0.62049779\n",
            "Iteration 154, loss = 0.62041440\n",
            "Iteration 155, loss = 0.62042889\n",
            "Iteration 156, loss = 0.62010685\n",
            "Iteration 157, loss = 0.61985305\n",
            "Iteration 158, loss = 0.61986435\n",
            "Iteration 159, loss = 0.62002065\n",
            "Iteration 160, loss = 0.61992112\n",
            "Iteration 161, loss = 0.61987505\n",
            "Iteration 162, loss = 0.61942679\n",
            "Iteration 163, loss = 0.61944061\n",
            "Iteration 164, loss = 0.61933202\n",
            "Iteration 165, loss = 0.61921074\n",
            "Iteration 166, loss = 0.61938863\n",
            "Iteration 167, loss = 0.61895762\n",
            "Iteration 168, loss = 0.61895941\n",
            "Iteration 169, loss = 0.61877929\n",
            "Iteration 170, loss = 0.61881713\n",
            "Iteration 171, loss = 0.61891677\n",
            "Iteration 172, loss = 0.61842526\n",
            "Iteration 173, loss = 0.61874120\n",
            "Iteration 174, loss = 0.61854526\n",
            "Iteration 175, loss = 0.61835104\n",
            "Iteration 176, loss = 0.61836855\n",
            "Iteration 177, loss = 0.61824826\n",
            "Iteration 178, loss = 0.61825900\n",
            "Iteration 179, loss = 0.61811786\n",
            "Iteration 180, loss = 0.61811720\n",
            "Iteration 181, loss = 0.61807381\n",
            "Iteration 182, loss = 0.61780560\n",
            "Iteration 183, loss = 0.61766238\n",
            "Iteration 184, loss = 0.61778733\n",
            "Iteration 185, loss = 0.61769815\n",
            "Iteration 186, loss = 0.61829647\n",
            "Iteration 187, loss = 0.61763246\n",
            "Iteration 188, loss = 0.61747176\n",
            "Iteration 189, loss = 0.61741269\n",
            "Iteration 190, loss = 0.61723874\n",
            "Iteration 191, loss = 0.61737398\n",
            "Iteration 192, loss = 0.61716470\n",
            "Iteration 193, loss = 0.61720554\n",
            "Iteration 194, loss = 0.61714451\n",
            "Iteration 195, loss = 0.61706676\n",
            "Iteration 196, loss = 0.61690868\n",
            "Iteration 197, loss = 0.61711047\n",
            "Iteration 198, loss = 0.61696269\n",
            "Iteration 199, loss = 0.61709951\n",
            "Iteration 200, loss = 0.61719994\n",
            "Iteration 201, loss = 0.61697201\n",
            "Iteration 202, loss = 0.61681035\n",
            "Iteration 203, loss = 0.61644392\n",
            "Iteration 204, loss = 0.61660969\n",
            "Iteration 205, loss = 0.61641471\n",
            "Iteration 206, loss = 0.61637269\n",
            "Iteration 207, loss = 0.61647436\n",
            "Iteration 208, loss = 0.61625288\n",
            "Iteration 209, loss = 0.61630255\n",
            "Iteration 210, loss = 0.61620297\n",
            "Iteration 211, loss = 0.61623528\n",
            "Iteration 212, loss = 0.61646845\n",
            "Iteration 213, loss = 0.61635791\n",
            "Iteration 214, loss = 0.61622320\n",
            "Iteration 215, loss = 0.61611538\n",
            "Iteration 216, loss = 0.61591171\n",
            "Iteration 217, loss = 0.61602554\n",
            "Iteration 218, loss = 0.61614724\n",
            "Iteration 219, loss = 0.61594337\n",
            "Iteration 220, loss = 0.61578553\n",
            "Iteration 221, loss = 0.61580559\n",
            "Iteration 222, loss = 0.61588746\n",
            "Iteration 223, loss = 0.61580552\n",
            "Iteration 224, loss = 0.61567558\n",
            "Iteration 225, loss = 0.61572030\n",
            "Iteration 226, loss = 0.61551219\n",
            "Iteration 227, loss = 0.61543888\n",
            "Iteration 228, loss = 0.61605946\n",
            "Iteration 229, loss = 0.61570758\n",
            "Iteration 230, loss = 0.61553709\n",
            "Iteration 231, loss = 0.61533260\n",
            "Iteration 232, loss = 0.61524971\n",
            "Iteration 233, loss = 0.61527294\n",
            "Iteration 234, loss = 0.61548794\n",
            "Iteration 235, loss = 0.61532453\n",
            "Iteration 236, loss = 0.61534609\n",
            "Iteration 237, loss = 0.61533233\n",
            "Iteration 238, loss = 0.61523475\n",
            "Iteration 239, loss = 0.61483559\n",
            "Iteration 240, loss = 0.61498001\n",
            "Iteration 241, loss = 0.61494806\n",
            "Iteration 242, loss = 0.61500593\n",
            "Iteration 243, loss = 0.61512569\n",
            "Iteration 244, loss = 0.61484305\n",
            "Iteration 245, loss = 0.61477599\n",
            "Iteration 246, loss = 0.61467517\n",
            "Iteration 247, loss = 0.61480708\n",
            "Iteration 248, loss = 0.61480385\n",
            "Iteration 249, loss = 0.61487022\n",
            "Iteration 250, loss = 0.61478708\n",
            "Iteration 251, loss = 0.61461504\n",
            "Iteration 252, loss = 0.61456027\n",
            "Iteration 253, loss = 0.61465205\n",
            "Iteration 254, loss = 0.61444861\n",
            "Iteration 255, loss = 0.61461150\n",
            "Iteration 256, loss = 0.61431035\n",
            "Iteration 257, loss = 0.61453347\n",
            "Iteration 258, loss = 0.61475353\n",
            "Iteration 259, loss = 0.61439646\n",
            "Iteration 260, loss = 0.61429973\n",
            "Iteration 261, loss = 0.61492929\n",
            "Iteration 262, loss = 0.61440180\n",
            "Iteration 263, loss = 0.61419209\n",
            "Iteration 264, loss = 0.61450801\n",
            "Iteration 265, loss = 0.61471121\n",
            "Iteration 266, loss = 0.61420467\n",
            "Iteration 267, loss = 0.61414664\n",
            "Iteration 268, loss = 0.61431409\n",
            "Iteration 269, loss = 0.61450292\n",
            "Iteration 270, loss = 0.61392204\n",
            "Iteration 271, loss = 0.61404347\n",
            "Iteration 272, loss = 0.61391987\n",
            "Iteration 273, loss = 0.61372744\n",
            "Iteration 274, loss = 0.61380217\n",
            "Iteration 275, loss = 0.61406150\n",
            "Iteration 276, loss = 0.61397148\n",
            "Iteration 277, loss = 0.61377192\n",
            "Iteration 278, loss = 0.61381632\n",
            "Iteration 279, loss = 0.61374728\n",
            "Iteration 280, loss = 0.61372231\n",
            "Iteration 281, loss = 0.61373275\n",
            "Iteration 282, loss = 0.61371511\n",
            "Iteration 283, loss = 0.61395361\n",
            "Iteration 284, loss = 0.61377698\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66226933\n",
            "Iteration 2, loss = 0.66165894\n",
            "Iteration 3, loss = 0.66107966\n",
            "Iteration 4, loss = 0.66058584\n",
            "Iteration 5, loss = 0.65937823\n",
            "Iteration 6, loss = 0.65910210\n",
            "Iteration 7, loss = 0.65862000\n",
            "Iteration 8, loss = 0.65789711\n",
            "Iteration 9, loss = 0.65751553\n",
            "Iteration 10, loss = 0.65671184\n",
            "Iteration 11, loss = 0.65665131\n",
            "Iteration 12, loss = 0.65599993\n",
            "Iteration 13, loss = 0.65491974\n",
            "Iteration 14, loss = 0.65441474\n",
            "Iteration 15, loss = 0.65393653\n",
            "Iteration 16, loss = 0.65338320\n",
            "Iteration 17, loss = 0.65268495\n",
            "Iteration 18, loss = 0.65217265\n",
            "Iteration 19, loss = 0.65146142\n",
            "Iteration 20, loss = 0.65109297\n",
            "Iteration 21, loss = 0.65062313\n",
            "Iteration 22, loss = 0.65016322\n",
            "Iteration 23, loss = 0.65008432\n",
            "Iteration 24, loss = 0.64952519\n",
            "Iteration 25, loss = 0.64842308\n",
            "Iteration 26, loss = 0.64794770\n",
            "Iteration 27, loss = 0.64754569\n",
            "Iteration 28, loss = 0.64693488\n",
            "Iteration 29, loss = 0.64643267\n",
            "Iteration 30, loss = 0.64585345\n",
            "Iteration 31, loss = 0.64558099\n",
            "Iteration 32, loss = 0.64510017\n",
            "Iteration 33, loss = 0.64460388\n",
            "Iteration 34, loss = 0.64433048\n",
            "Iteration 35, loss = 0.64330351\n",
            "Iteration 36, loss = 0.64231969\n",
            "Iteration 37, loss = 0.64175371\n",
            "Iteration 38, loss = 0.64071793\n",
            "Iteration 39, loss = 0.64066813\n",
            "Iteration 40, loss = 0.63996108\n",
            "Iteration 41, loss = 0.63923667\n",
            "Iteration 42, loss = 0.63875614\n",
            "Iteration 43, loss = 0.63811325\n",
            "Iteration 44, loss = 0.63777620\n",
            "Iteration 45, loss = 0.63714586\n",
            "Iteration 46, loss = 0.63657894\n",
            "Iteration 47, loss = 0.63662942\n",
            "Iteration 48, loss = 0.63564784\n",
            "Iteration 49, loss = 0.63511092\n",
            "Iteration 50, loss = 0.63527604\n",
            "Iteration 51, loss = 0.63461216\n",
            "Iteration 52, loss = 0.63396712\n",
            "Iteration 53, loss = 0.63348656\n",
            "Iteration 54, loss = 0.63313982\n",
            "Iteration 55, loss = 0.63236143\n",
            "Iteration 56, loss = 0.63238328\n",
            "Iteration 57, loss = 0.63150082\n",
            "Iteration 58, loss = 0.63114678\n",
            "Iteration 59, loss = 0.63072081\n",
            "Iteration 60, loss = 0.63053805\n",
            "Iteration 61, loss = 0.63006284\n",
            "Iteration 62, loss = 0.62952406\n",
            "Iteration 63, loss = 0.62923716\n",
            "Iteration 64, loss = 0.62872926\n",
            "Iteration 65, loss = 0.62815567\n",
            "Iteration 66, loss = 0.62761788\n",
            "Iteration 67, loss = 0.62756972\n",
            "Iteration 68, loss = 0.62724799\n",
            "Iteration 69, loss = 0.62707319\n",
            "Iteration 70, loss = 0.62707272\n",
            "Iteration 71, loss = 0.62686856\n",
            "Iteration 72, loss = 0.62597971\n",
            "Iteration 73, loss = 0.62541843\n",
            "Iteration 74, loss = 0.62484774\n",
            "Iteration 75, loss = 0.62447891\n",
            "Iteration 76, loss = 0.62409265\n",
            "Iteration 77, loss = 0.62375663\n",
            "Iteration 78, loss = 0.62403875\n",
            "Iteration 79, loss = 0.62335657\n",
            "Iteration 80, loss = 0.62295743\n",
            "Iteration 81, loss = 0.62251158\n",
            "Iteration 82, loss = 0.62230850\n",
            "Iteration 83, loss = 0.62179676\n",
            "Iteration 84, loss = 0.62137485\n",
            "Iteration 85, loss = 0.62097402\n",
            "Iteration 86, loss = 0.62073470\n",
            "Iteration 87, loss = 0.62069649\n",
            "Iteration 88, loss = 0.62029492\n",
            "Iteration 89, loss = 0.61985285\n",
            "Iteration 90, loss = 0.61955443\n",
            "Iteration 91, loss = 0.61911508\n",
            "Iteration 92, loss = 0.61956129\n",
            "Iteration 93, loss = 0.61917224\n",
            "Iteration 94, loss = 0.61863658\n",
            "Iteration 95, loss = 0.61809459\n",
            "Iteration 96, loss = 0.61804923\n",
            "Iteration 97, loss = 0.61781633\n",
            "Iteration 98, loss = 0.61790384\n",
            "Iteration 99, loss = 0.61769329\n",
            "Iteration 100, loss = 0.61740790\n",
            "Iteration 101, loss = 0.61671659\n",
            "Iteration 102, loss = 0.61669305\n",
            "Iteration 103, loss = 0.61582821\n",
            "Iteration 104, loss = 0.61550947\n",
            "Iteration 105, loss = 0.61522523\n",
            "Iteration 106, loss = 0.61505789\n",
            "Iteration 107, loss = 0.61500753\n",
            "Iteration 108, loss = 0.61462800\n",
            "Iteration 109, loss = 0.61443673\n",
            "Iteration 110, loss = 0.61421848\n",
            "Iteration 111, loss = 0.61413098\n",
            "Iteration 112, loss = 0.61376748\n",
            "Iteration 113, loss = 0.61340832\n",
            "Iteration 114, loss = 0.61335982\n",
            "Iteration 115, loss = 0.61243166\n",
            "Iteration 116, loss = 0.61241015\n",
            "Iteration 117, loss = 0.61230400\n",
            "Iteration 118, loss = 0.61184433\n",
            "Iteration 119, loss = 0.61164676\n",
            "Iteration 120, loss = 0.61151504\n",
            "Iteration 121, loss = 0.61116424\n",
            "Iteration 122, loss = 0.61131901\n",
            "Iteration 123, loss = 0.61109439\n",
            "Iteration 124, loss = 0.61066809\n",
            "Iteration 125, loss = 0.61048349\n",
            "Iteration 126, loss = 0.61038655\n",
            "Iteration 127, loss = 0.61016369\n",
            "Iteration 128, loss = 0.61051487\n",
            "Iteration 129, loss = 0.61017489\n",
            "Iteration 130, loss = 0.60962058\n",
            "Iteration 131, loss = 0.60979318\n",
            "Iteration 132, loss = 0.60914967\n",
            "Iteration 133, loss = 0.60952743\n",
            "Iteration 134, loss = 0.60907713\n",
            "Iteration 135, loss = 0.60868736\n",
            "Iteration 136, loss = 0.60858712\n",
            "Iteration 137, loss = 0.60842247\n",
            "Iteration 138, loss = 0.60826320\n",
            "Iteration 139, loss = 0.60813384\n",
            "Iteration 140, loss = 0.60813642\n",
            "Iteration 141, loss = 0.60798435\n",
            "Iteration 142, loss = 0.60771109\n",
            "Iteration 143, loss = 0.60741447\n",
            "Iteration 144, loss = 0.60743640\n",
            "Iteration 145, loss = 0.60724182\n",
            "Iteration 146, loss = 0.60696459\n",
            "Iteration 147, loss = 0.60727413\n",
            "Iteration 148, loss = 0.60704107\n",
            "Iteration 149, loss = 0.60661819\n",
            "Iteration 150, loss = 0.60680096\n",
            "Iteration 151, loss = 0.60662848\n",
            "Iteration 152, loss = 0.60635127\n",
            "Iteration 153, loss = 0.60609030\n",
            "Iteration 154, loss = 0.60601605\n",
            "Iteration 155, loss = 0.60594121\n",
            "Iteration 156, loss = 0.60609249\n",
            "Iteration 157, loss = 0.60586093\n",
            "Iteration 158, loss = 0.60572112\n",
            "Iteration 159, loss = 0.60548769\n",
            "Iteration 160, loss = 0.60560622\n",
            "Iteration 161, loss = 0.60555059\n",
            "Iteration 162, loss = 0.60524396\n",
            "Iteration 163, loss = 0.60516907\n",
            "Iteration 164, loss = 0.60494935\n",
            "Iteration 165, loss = 0.60483724\n",
            "Iteration 166, loss = 0.60468586\n",
            "Iteration 167, loss = 0.60461687\n",
            "Iteration 168, loss = 0.60449101\n",
            "Iteration 169, loss = 0.60451597\n",
            "Iteration 170, loss = 0.60464367\n",
            "Iteration 171, loss = 0.60423764\n",
            "Iteration 172, loss = 0.60385455\n",
            "Iteration 173, loss = 0.60420804\n",
            "Iteration 174, loss = 0.60402622\n",
            "Iteration 175, loss = 0.60424233\n",
            "Iteration 176, loss = 0.60370106\n",
            "Iteration 177, loss = 0.60355972\n",
            "Iteration 178, loss = 0.60356149\n",
            "Iteration 179, loss = 0.60341870\n",
            "Iteration 180, loss = 0.60318526\n",
            "Iteration 181, loss = 0.60309007\n",
            "Iteration 182, loss = 0.60302315\n",
            "Iteration 183, loss = 0.60308637\n",
            "Iteration 184, loss = 0.60296823\n",
            "Iteration 185, loss = 0.60302658\n",
            "Iteration 186, loss = 0.60297335\n",
            "Iteration 187, loss = 0.60252950\n",
            "Iteration 188, loss = 0.60254706\n",
            "Iteration 189, loss = 0.60246225\n",
            "Iteration 190, loss = 0.60237435\n",
            "Iteration 191, loss = 0.60238054\n",
            "Iteration 192, loss = 0.60228469\n",
            "Iteration 193, loss = 0.60210209\n",
            "Iteration 194, loss = 0.60236829\n",
            "Iteration 195, loss = 0.60239760\n",
            "Iteration 196, loss = 0.60223010\n",
            "Iteration 197, loss = 0.60221086\n",
            "Iteration 198, loss = 0.60224168\n",
            "Iteration 199, loss = 0.60194424\n",
            "Iteration 200, loss = 0.60172276\n",
            "Iteration 201, loss = 0.60154150\n",
            "Iteration 202, loss = 0.60164336\n",
            "Iteration 203, loss = 0.60159400\n",
            "Iteration 204, loss = 0.60154018\n",
            "Iteration 205, loss = 0.60168191\n",
            "Iteration 206, loss = 0.60122683\n",
            "Iteration 207, loss = 0.60132691\n",
            "Iteration 208, loss = 0.60129376\n",
            "Iteration 209, loss = 0.60128119\n",
            "Iteration 210, loss = 0.60137722\n",
            "Iteration 211, loss = 0.60120649\n",
            "Iteration 212, loss = 0.60126254\n",
            "Iteration 213, loss = 0.60127743\n",
            "Iteration 214, loss = 0.60135790\n",
            "Iteration 215, loss = 0.60110496\n",
            "Iteration 216, loss = 0.60096521\n",
            "Iteration 217, loss = 0.60061535\n",
            "Iteration 218, loss = 0.60101401\n",
            "Iteration 219, loss = 0.60097270\n",
            "Iteration 220, loss = 0.60091564\n",
            "Iteration 221, loss = 0.60067196\n",
            "Iteration 222, loss = 0.60075929\n",
            "Iteration 223, loss = 0.60070223\n",
            "Iteration 224, loss = 0.60046242\n",
            "Iteration 225, loss = 0.60034620\n",
            "Iteration 226, loss = 0.60031835\n",
            "Iteration 227, loss = 0.60021217\n",
            "Iteration 228, loss = 0.60019961\n",
            "Iteration 229, loss = 0.60009149\n",
            "Iteration 230, loss = 0.60012756\n",
            "Iteration 231, loss = 0.60003244\n",
            "Iteration 232, loss = 0.59999790\n",
            "Iteration 233, loss = 0.59988266\n",
            "Iteration 234, loss = 0.59989200\n",
            "Iteration 235, loss = 0.59983465\n",
            "Iteration 236, loss = 0.59971197\n",
            "Iteration 237, loss = 0.59971658\n",
            "Iteration 238, loss = 0.59951249\n",
            "Iteration 239, loss = 0.59945605\n",
            "Iteration 240, loss = 0.59930065\n",
            "Iteration 241, loss = 0.59932466\n",
            "Iteration 242, loss = 0.59926466\n",
            "Iteration 243, loss = 0.59935433\n",
            "Iteration 244, loss = 0.59927365\n",
            "Iteration 245, loss = 0.59912437\n",
            "Iteration 246, loss = 0.59917333\n",
            "Iteration 247, loss = 0.59897384\n",
            "Iteration 248, loss = 0.59888598\n",
            "Iteration 249, loss = 0.59899633\n",
            "Iteration 250, loss = 0.59881499\n",
            "Iteration 251, loss = 0.59912431\n",
            "Iteration 252, loss = 0.59911129\n",
            "Iteration 253, loss = 0.59872742\n",
            "Iteration 254, loss = 0.59874496\n",
            "Iteration 255, loss = 0.59870445\n",
            "Iteration 256, loss = 0.59851092\n",
            "Iteration 257, loss = 0.59851595\n",
            "Iteration 258, loss = 0.59858850\n",
            "Iteration 259, loss = 0.59840145\n",
            "Iteration 260, loss = 0.59826849\n",
            "Iteration 261, loss = 0.59848774\n",
            "Iteration 262, loss = 0.59852187\n",
            "Iteration 263, loss = 0.59803211\n",
            "Iteration 264, loss = 0.59830191\n",
            "Iteration 265, loss = 0.59835322\n",
            "Iteration 266, loss = 0.59818321\n",
            "Iteration 267, loss = 0.59803944\n",
            "Iteration 268, loss = 0.59803271\n",
            "Iteration 269, loss = 0.59800742\n",
            "Iteration 270, loss = 0.59801385\n",
            "Iteration 271, loss = 0.59785445\n",
            "Iteration 272, loss = 0.59773782\n",
            "Iteration 273, loss = 0.59788257\n",
            "Iteration 274, loss = 0.59781943\n",
            "Iteration 275, loss = 0.59778518\n",
            "Iteration 276, loss = 0.59779689\n",
            "Iteration 277, loss = 0.59766416\n",
            "Iteration 278, loss = 0.59744564\n",
            "Iteration 279, loss = 0.59779685\n",
            "Iteration 280, loss = 0.59744012\n",
            "Iteration 281, loss = 0.59746150\n",
            "Iteration 282, loss = 0.59739358\n",
            "Iteration 283, loss = 0.59739923\n",
            "Iteration 284, loss = 0.59736798\n",
            "Iteration 285, loss = 0.59731117\n",
            "Iteration 286, loss = 0.59716154\n",
            "Iteration 287, loss = 0.59739442\n",
            "Iteration 288, loss = 0.59716906\n",
            "Iteration 289, loss = 0.59751479\n",
            "Iteration 290, loss = 0.59713701\n",
            "Iteration 291, loss = 0.59724145\n",
            "Iteration 292, loss = 0.59697190\n",
            "Iteration 293, loss = 0.59702810\n",
            "Iteration 294, loss = 0.59714814\n",
            "Iteration 295, loss = 0.59729976\n",
            "Iteration 296, loss = 0.59732014\n",
            "Iteration 297, loss = 0.59690481\n",
            "Iteration 298, loss = 0.59688485\n",
            "Iteration 299, loss = 0.59672490\n",
            "Iteration 300, loss = 0.59645921\n",
            "Iteration 301, loss = 0.59640376\n",
            "Iteration 302, loss = 0.59676021\n",
            "Iteration 303, loss = 0.59690097\n",
            "Iteration 304, loss = 0.59693970\n",
            "Iteration 305, loss = 0.59729327\n",
            "Iteration 306, loss = 0.59670156\n",
            "Iteration 307, loss = 0.59655289\n",
            "Iteration 308, loss = 0.59635772\n",
            "Iteration 309, loss = 0.59627138\n",
            "Iteration 310, loss = 0.59625446\n",
            "Iteration 311, loss = 0.59642683\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70301655\n",
            "Iteration 2, loss = 0.68580586\n",
            "Iteration 3, loss = 0.67297397\n",
            "Iteration 4, loss = 0.66486804\n",
            "Iteration 5, loss = 0.66253879\n",
            "Iteration 6, loss = 0.66169537\n",
            "Iteration 7, loss = 0.66166679\n",
            "Iteration 8, loss = 0.66141109\n",
            "Iteration 9, loss = 0.66079640\n",
            "Iteration 10, loss = 0.65996164\n",
            "Iteration 11, loss = 0.65957200\n",
            "Iteration 12, loss = 0.65922691\n",
            "Iteration 13, loss = 0.65907306\n",
            "Iteration 14, loss = 0.65868068\n",
            "Iteration 15, loss = 0.65840891\n",
            "Iteration 16, loss = 0.65816367\n",
            "Iteration 17, loss = 0.65791884\n",
            "Iteration 18, loss = 0.65759132\n",
            "Iteration 19, loss = 0.65729763\n",
            "Iteration 20, loss = 0.65714891\n",
            "Iteration 21, loss = 0.65657792\n",
            "Iteration 22, loss = 0.65643173\n",
            "Iteration 23, loss = 0.65624074\n",
            "Iteration 24, loss = 0.65595890\n",
            "Iteration 25, loss = 0.65546492\n",
            "Iteration 26, loss = 0.65534518\n",
            "Iteration 27, loss = 0.65508508\n",
            "Iteration 28, loss = 0.65469934\n",
            "Iteration 29, loss = 0.65451746\n",
            "Iteration 30, loss = 0.65422202\n",
            "Iteration 31, loss = 0.65398994\n",
            "Iteration 32, loss = 0.65373159\n",
            "Iteration 33, loss = 0.65335643\n",
            "Iteration 34, loss = 0.65316342\n",
            "Iteration 35, loss = 0.65285085\n",
            "Iteration 36, loss = 0.65250626\n",
            "Iteration 37, loss = 0.65228521\n",
            "Iteration 38, loss = 0.65198506\n",
            "Iteration 39, loss = 0.65170746\n",
            "Iteration 40, loss = 0.65148879\n",
            "Iteration 41, loss = 0.65118320\n",
            "Iteration 42, loss = 0.65094973\n",
            "Iteration 43, loss = 0.65069609\n",
            "Iteration 44, loss = 0.65037372\n",
            "Iteration 45, loss = 0.64999720\n",
            "Iteration 46, loss = 0.64982765\n",
            "Iteration 47, loss = 0.64954507\n",
            "Iteration 48, loss = 0.64923514\n",
            "Iteration 49, loss = 0.64908970\n",
            "Iteration 50, loss = 0.64860636\n",
            "Iteration 51, loss = 0.64841622\n",
            "Iteration 52, loss = 0.64794652\n",
            "Iteration 53, loss = 0.64766194\n",
            "Iteration 54, loss = 0.64743515\n",
            "Iteration 55, loss = 0.64705263\n",
            "Iteration 56, loss = 0.64675087\n",
            "Iteration 57, loss = 0.64664947\n",
            "Iteration 58, loss = 0.64643815\n",
            "Iteration 59, loss = 0.64603263\n",
            "Iteration 60, loss = 0.64574228\n",
            "Iteration 61, loss = 0.64535157\n",
            "Iteration 62, loss = 0.64528726\n",
            "Iteration 63, loss = 0.64509422\n",
            "Iteration 64, loss = 0.64480797\n",
            "Iteration 65, loss = 0.64415463\n",
            "Iteration 66, loss = 0.64388003\n",
            "Iteration 67, loss = 0.64358258\n",
            "Iteration 68, loss = 0.64334051\n",
            "Iteration 69, loss = 0.64288954\n",
            "Iteration 70, loss = 0.64262367\n",
            "Iteration 71, loss = 0.64221223\n",
            "Iteration 72, loss = 0.64195192\n",
            "Iteration 73, loss = 0.64173343\n",
            "Iteration 74, loss = 0.64155576\n",
            "Iteration 75, loss = 0.64123687\n",
            "Iteration 76, loss = 0.64097344\n",
            "Iteration 77, loss = 0.64047073\n",
            "Iteration 78, loss = 0.64015930\n",
            "Iteration 79, loss = 0.63982586\n",
            "Iteration 80, loss = 0.63959030\n",
            "Iteration 81, loss = 0.63932016\n",
            "Iteration 82, loss = 0.63885541\n",
            "Iteration 83, loss = 0.63853833\n",
            "Iteration 84, loss = 0.63811899\n",
            "Iteration 85, loss = 0.63784374\n",
            "Iteration 86, loss = 0.63768157\n",
            "Iteration 87, loss = 0.63735895\n",
            "Iteration 88, loss = 0.63709427\n",
            "Iteration 89, loss = 0.63679507\n",
            "Iteration 90, loss = 0.63618949\n",
            "Iteration 91, loss = 0.63596928\n",
            "Iteration 92, loss = 0.63587121\n",
            "Iteration 93, loss = 0.63536201\n",
            "Iteration 94, loss = 0.63505874\n",
            "Iteration 95, loss = 0.63474643\n",
            "Iteration 96, loss = 0.63437491\n",
            "Iteration 97, loss = 0.63405111\n",
            "Iteration 98, loss = 0.63374624\n",
            "Iteration 99, loss = 0.63343470\n",
            "Iteration 100, loss = 0.63312391\n",
            "Iteration 101, loss = 0.63295173\n",
            "Iteration 102, loss = 0.63249320\n",
            "Iteration 103, loss = 0.63226179\n",
            "Iteration 104, loss = 0.63186636\n",
            "Iteration 105, loss = 0.63163357\n",
            "Iteration 106, loss = 0.63124376\n",
            "Iteration 107, loss = 0.63095438\n",
            "Iteration 108, loss = 0.63075701\n",
            "Iteration 109, loss = 0.63051501\n",
            "Iteration 110, loss = 0.63028411\n",
            "Iteration 111, loss = 0.63037198\n",
            "Iteration 112, loss = 0.62931428\n",
            "Iteration 113, loss = 0.62905315\n",
            "Iteration 114, loss = 0.62884508\n",
            "Iteration 115, loss = 0.62850064\n",
            "Iteration 116, loss = 0.62829031\n",
            "Iteration 117, loss = 0.62801412\n",
            "Iteration 118, loss = 0.62784396\n",
            "Iteration 119, loss = 0.62747240\n",
            "Iteration 120, loss = 0.62713601\n",
            "Iteration 121, loss = 0.62689473\n",
            "Iteration 122, loss = 0.62676936\n",
            "Iteration 123, loss = 0.62637682\n",
            "Iteration 124, loss = 0.62621714\n",
            "Iteration 125, loss = 0.62596071\n",
            "Iteration 126, loss = 0.62546506\n",
            "Iteration 127, loss = 0.62507739\n",
            "Iteration 128, loss = 0.62476558\n",
            "Iteration 129, loss = 0.62467325\n",
            "Iteration 130, loss = 0.62447015\n",
            "Iteration 131, loss = 0.62421746\n",
            "Iteration 132, loss = 0.62380993\n",
            "Iteration 133, loss = 0.62373145\n",
            "Iteration 134, loss = 0.62341520\n",
            "Iteration 135, loss = 0.62337238\n",
            "Iteration 136, loss = 0.62310243\n",
            "Iteration 137, loss = 0.62274885\n",
            "Iteration 138, loss = 0.62265751\n",
            "Iteration 139, loss = 0.62217352\n",
            "Iteration 140, loss = 0.62204089\n",
            "Iteration 141, loss = 0.62192103\n",
            "Iteration 142, loss = 0.62179878\n",
            "Iteration 143, loss = 0.62184254\n",
            "Iteration 144, loss = 0.62168079\n",
            "Iteration 145, loss = 0.62101667\n",
            "Iteration 146, loss = 0.62081584\n",
            "Iteration 147, loss = 0.62050102\n",
            "Iteration 148, loss = 0.62016422\n",
            "Iteration 149, loss = 0.62011831\n",
            "Iteration 150, loss = 0.62020990\n",
            "Iteration 151, loss = 0.61987822\n",
            "Iteration 152, loss = 0.61959326\n",
            "Iteration 153, loss = 0.61930007\n",
            "Iteration 154, loss = 0.61882328\n",
            "Iteration 155, loss = 0.61868495\n",
            "Iteration 156, loss = 0.61844876\n",
            "Iteration 157, loss = 0.61822222\n",
            "Iteration 158, loss = 0.61798002\n",
            "Iteration 159, loss = 0.61781439\n",
            "Iteration 160, loss = 0.61754672\n",
            "Iteration 161, loss = 0.61744420\n",
            "Iteration 162, loss = 0.61704387\n",
            "Iteration 163, loss = 0.61700970\n",
            "Iteration 164, loss = 0.61685443\n",
            "Iteration 165, loss = 0.61671252\n",
            "Iteration 166, loss = 0.61658253\n",
            "Iteration 167, loss = 0.61624660\n",
            "Iteration 168, loss = 0.61616767\n",
            "Iteration 169, loss = 0.61602264\n",
            "Iteration 170, loss = 0.61571631\n",
            "Iteration 171, loss = 0.61559230\n",
            "Iteration 172, loss = 0.61537015\n",
            "Iteration 173, loss = 0.61532554\n",
            "Iteration 174, loss = 0.61518611\n",
            "Iteration 175, loss = 0.61499781\n",
            "Iteration 176, loss = 0.61477491\n",
            "Iteration 177, loss = 0.61460848\n",
            "Iteration 178, loss = 0.61453195\n",
            "Iteration 179, loss = 0.61433084\n",
            "Iteration 180, loss = 0.61442646\n",
            "Iteration 181, loss = 0.61422472\n",
            "Iteration 182, loss = 0.61390814\n",
            "Iteration 183, loss = 0.61380766\n",
            "Iteration 184, loss = 0.61392158\n",
            "Iteration 185, loss = 0.61369347\n",
            "Iteration 186, loss = 0.61338432\n",
            "Iteration 187, loss = 0.61338366\n",
            "Iteration 188, loss = 0.61319500\n",
            "Iteration 189, loss = 0.61309176\n",
            "Iteration 190, loss = 0.61298356\n",
            "Iteration 191, loss = 0.61292466\n",
            "Iteration 192, loss = 0.61291578\n",
            "Iteration 193, loss = 0.61288678\n",
            "Iteration 194, loss = 0.61283853\n",
            "Iteration 195, loss = 0.61253405\n",
            "Iteration 196, loss = 0.61289775\n",
            "Iteration 197, loss = 0.61251221\n",
            "Iteration 198, loss = 0.61244195\n",
            "Iteration 199, loss = 0.61220992\n",
            "Iteration 200, loss = 0.61211763\n",
            "Iteration 201, loss = 0.61218062\n",
            "Iteration 202, loss = 0.61228152\n",
            "Iteration 203, loss = 0.61195458\n",
            "Iteration 204, loss = 0.61178224\n",
            "Iteration 205, loss = 0.61167477\n",
            "Iteration 206, loss = 0.61163187\n",
            "Iteration 207, loss = 0.61151135\n",
            "Iteration 208, loss = 0.61145548\n",
            "Iteration 209, loss = 0.61144383\n",
            "Iteration 210, loss = 0.61120426\n",
            "Iteration 211, loss = 0.61116462\n",
            "Iteration 212, loss = 0.61108249\n",
            "Iteration 213, loss = 0.61138221\n",
            "Iteration 214, loss = 0.61093696\n",
            "Iteration 215, loss = 0.61090519\n",
            "Iteration 216, loss = 0.61083410\n",
            "Iteration 217, loss = 0.61065848\n",
            "Iteration 218, loss = 0.61068531\n",
            "Iteration 219, loss = 0.61048235\n",
            "Iteration 220, loss = 0.61038145\n",
            "Iteration 221, loss = 0.61064605\n",
            "Iteration 222, loss = 0.61027016\n",
            "Iteration 223, loss = 0.61019950\n",
            "Iteration 224, loss = 0.61015393\n",
            "Iteration 225, loss = 0.61013454\n",
            "Iteration 226, loss = 0.61010614\n",
            "Iteration 227, loss = 0.61027583\n",
            "Iteration 228, loss = 0.61011414\n",
            "Iteration 229, loss = 0.60989923\n",
            "Iteration 230, loss = 0.60981920\n",
            "Iteration 231, loss = 0.61018312\n",
            "Iteration 232, loss = 0.60978867\n",
            "Iteration 233, loss = 0.60966153\n",
            "Iteration 234, loss = 0.60962958\n",
            "Iteration 235, loss = 0.60952136\n",
            "Iteration 236, loss = 0.60958162\n",
            "Iteration 237, loss = 0.60941415\n",
            "Iteration 238, loss = 0.60940678\n",
            "Iteration 239, loss = 0.60935333\n",
            "Iteration 240, loss = 0.60930506\n",
            "Iteration 241, loss = 0.60920817\n",
            "Iteration 242, loss = 0.60918234\n",
            "Iteration 243, loss = 0.60925967\n",
            "Iteration 244, loss = 0.60903475\n",
            "Iteration 245, loss = 0.60901766\n",
            "Iteration 246, loss = 0.60888939\n",
            "Iteration 247, loss = 0.60894498\n",
            "Iteration 248, loss = 0.60905999\n",
            "Iteration 249, loss = 0.60928594\n",
            "Iteration 250, loss = 0.60879705\n",
            "Iteration 251, loss = 0.60941080\n",
            "Iteration 252, loss = 0.60867880\n",
            "Iteration 253, loss = 0.60880618\n",
            "Iteration 254, loss = 0.60864392\n",
            "Iteration 255, loss = 0.60852188\n",
            "Iteration 256, loss = 0.60854689\n",
            "Iteration 257, loss = 0.60847793\n",
            "Iteration 258, loss = 0.60838409\n",
            "Iteration 259, loss = 0.60839571\n",
            "Iteration 260, loss = 0.60840345\n",
            "Iteration 261, loss = 0.60828834\n",
            "Iteration 262, loss = 0.60827960\n",
            "Iteration 263, loss = 0.60817034\n",
            "Iteration 264, loss = 0.60812662\n",
            "Iteration 265, loss = 0.60812963\n",
            "Iteration 266, loss = 0.60817122\n",
            "Iteration 267, loss = 0.60817144\n",
            "Iteration 268, loss = 0.60794864\n",
            "Iteration 269, loss = 0.60790273\n",
            "Iteration 270, loss = 0.60788643\n",
            "Iteration 271, loss = 0.60795181\n",
            "Iteration 272, loss = 0.60776812\n",
            "Iteration 273, loss = 0.60772563\n",
            "Iteration 274, loss = 0.60774403\n",
            "Iteration 275, loss = 0.60793071\n",
            "Iteration 276, loss = 0.60795295\n",
            "Iteration 277, loss = 0.60782447\n",
            "Iteration 278, loss = 0.60760240\n",
            "Iteration 279, loss = 0.60740812\n",
            "Iteration 280, loss = 0.60729276\n",
            "Iteration 281, loss = 0.60752481\n",
            "Iteration 282, loss = 0.60756129\n",
            "Iteration 283, loss = 0.60763388\n",
            "Iteration 284, loss = 0.60740484\n",
            "Iteration 285, loss = 0.60704776\n",
            "Iteration 286, loss = 0.60712019\n",
            "Iteration 287, loss = 0.60716439\n",
            "Iteration 288, loss = 0.60720616\n",
            "Iteration 289, loss = 0.60703853\n",
            "Iteration 290, loss = 0.60703046\n",
            "Iteration 291, loss = 0.60702840\n",
            "Iteration 292, loss = 0.60698016\n",
            "Iteration 293, loss = 0.60718284\n",
            "Iteration 294, loss = 0.60709389\n",
            "Iteration 295, loss = 0.60685026\n",
            "Iteration 296, loss = 0.60673172\n",
            "Iteration 297, loss = 0.60686844\n",
            "Iteration 298, loss = 0.60689377\n",
            "Iteration 299, loss = 0.60681139\n",
            "Iteration 300, loss = 0.60673814\n",
            "Iteration 301, loss = 0.60685734\n",
            "Iteration 302, loss = 0.60668320\n",
            "Iteration 303, loss = 0.60667476\n",
            "Iteration 304, loss = 0.60670458\n",
            "Iteration 305, loss = 0.60683101\n",
            "Iteration 306, loss = 0.60652248\n",
            "Iteration 307, loss = 0.60651533\n",
            "Iteration 308, loss = 0.60657020\n",
            "Iteration 309, loss = 0.60684340\n",
            "Iteration 310, loss = 0.60655957\n",
            "Iteration 311, loss = 0.60667931\n",
            "Iteration 312, loss = 0.60649668\n",
            "Iteration 313, loss = 0.60656321\n",
            "Iteration 314, loss = 0.60649008\n",
            "Iteration 315, loss = 0.60614727\n",
            "Iteration 316, loss = 0.60620706\n",
            "Iteration 317, loss = 0.60621715\n",
            "Iteration 318, loss = 0.60611945\n",
            "Iteration 319, loss = 0.60603964\n",
            "Iteration 320, loss = 0.60599881\n",
            "Iteration 321, loss = 0.60600696\n",
            "Iteration 322, loss = 0.60622985\n",
            "Iteration 323, loss = 0.60595407\n",
            "Iteration 324, loss = 0.60624119\n",
            "Iteration 325, loss = 0.60608222\n",
            "Iteration 326, loss = 0.60601797\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66176760\n",
            "Iteration 2, loss = 0.60716973\n",
            "Iteration 3, loss = 0.60507900\n",
            "Iteration 4, loss = 0.59117438\n",
            "Iteration 5, loss = 0.57923561\n",
            "Iteration 6, loss = 0.57176833\n",
            "Iteration 7, loss = 0.55528521\n",
            "Iteration 8, loss = 0.53896269\n",
            "Iteration 9, loss = 0.51527260\n",
            "Iteration 10, loss = 0.52477889\n",
            "Iteration 11, loss = 0.49552072\n",
            "Iteration 12, loss = 0.49455985\n",
            "Iteration 13, loss = 0.47809874\n",
            "Iteration 14, loss = 0.47070005\n",
            "Iteration 15, loss = 0.46133619\n",
            "Iteration 16, loss = 0.47469214\n",
            "Iteration 17, loss = 0.46224303\n",
            "Iteration 18, loss = 0.45636975\n",
            "Iteration 19, loss = 0.45784862\n",
            "Iteration 20, loss = 0.44736420\n",
            "Iteration 21, loss = 0.43751168\n",
            "Iteration 22, loss = 0.43667809\n",
            "Iteration 23, loss = 0.43529120\n",
            "Iteration 24, loss = 0.43927048\n",
            "Iteration 25, loss = 0.43995810\n",
            "Iteration 26, loss = 0.43197245\n",
            "Iteration 27, loss = 0.43424257\n",
            "Iteration 28, loss = 0.42198997\n",
            "Iteration 29, loss = 0.42252246\n",
            "Iteration 30, loss = 0.42544796\n",
            "Iteration 31, loss = 0.42846645\n",
            "Iteration 32, loss = 0.42418562\n",
            "Iteration 33, loss = 0.42227764\n",
            "Iteration 34, loss = 0.41762059\n",
            "Iteration 35, loss = 0.42457198\n",
            "Iteration 36, loss = 0.42894948\n",
            "Iteration 37, loss = 0.42237011\n",
            "Iteration 38, loss = 0.41137505\n",
            "Iteration 39, loss = 0.41008524\n",
            "Iteration 40, loss = 0.41812528\n",
            "Iteration 41, loss = 0.41603056\n",
            "Iteration 42, loss = 0.41077577\n",
            "Iteration 43, loss = 0.40869592\n",
            "Iteration 44, loss = 0.42792654\n",
            "Iteration 45, loss = 0.40077370\n",
            "Iteration 46, loss = 0.40454967\n",
            "Iteration 47, loss = 0.41468465\n",
            "Iteration 48, loss = 0.39896336\n",
            "Iteration 49, loss = 0.40230770\n",
            "Iteration 50, loss = 0.39795829\n",
            "Iteration 51, loss = 0.40383824\n",
            "Iteration 52, loss = 0.40175642\n",
            "Iteration 53, loss = 0.41092688\n",
            "Iteration 54, loss = 0.40094742\n",
            "Iteration 55, loss = 0.39922358\n",
            "Iteration 56, loss = 0.39700751\n",
            "Iteration 57, loss = 0.41154535\n",
            "Iteration 58, loss = 0.39789302\n",
            "Iteration 59, loss = 0.39630149\n",
            "Iteration 60, loss = 0.39360377\n",
            "Iteration 61, loss = 0.40670169\n",
            "Iteration 62, loss = 0.38493416\n",
            "Iteration 63, loss = 0.39246960\n",
            "Iteration 64, loss = 0.39816077\n",
            "Iteration 65, loss = 0.39827794\n",
            "Iteration 66, loss = 0.38942303\n",
            "Iteration 67, loss = 0.38772851\n",
            "Iteration 68, loss = 0.38540024\n",
            "Iteration 69, loss = 0.39089736\n",
            "Iteration 70, loss = 0.39437720\n",
            "Iteration 71, loss = 0.39454867\n",
            "Iteration 72, loss = 0.38097434\n",
            "Iteration 73, loss = 0.38540249\n",
            "Iteration 74, loss = 0.40944956\n",
            "Iteration 75, loss = 0.38676608\n",
            "Iteration 76, loss = 0.38405815\n",
            "Iteration 77, loss = 0.37737871\n",
            "Iteration 78, loss = 0.38325151\n",
            "Iteration 79, loss = 0.39538603\n",
            "Iteration 80, loss = 0.38090449\n",
            "Iteration 81, loss = 0.38023906\n",
            "Iteration 82, loss = 0.38796719\n",
            "Iteration 83, loss = 0.38967006\n",
            "Iteration 84, loss = 0.37623391\n",
            "Iteration 85, loss = 0.37916596\n",
            "Iteration 86, loss = 0.37693311\n",
            "Iteration 87, loss = 0.37937445\n",
            "Iteration 88, loss = 0.37890317\n",
            "Iteration 89, loss = 0.37984830\n",
            "Iteration 90, loss = 0.38224561\n",
            "Iteration 91, loss = 0.36793902\n",
            "Iteration 92, loss = 0.37585789\n",
            "Iteration 93, loss = 0.37228833\n",
            "Iteration 94, loss = 0.37113674\n",
            "Iteration 95, loss = 0.37551297\n",
            "Iteration 96, loss = 0.36842426\n",
            "Iteration 97, loss = 0.37480064\n",
            "Iteration 98, loss = 0.37178722\n",
            "Iteration 99, loss = 0.38440240\n",
            "Iteration 100, loss = 0.36888069\n",
            "Iteration 101, loss = 0.37819946\n",
            "Iteration 102, loss = 0.37010869\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61974905\n",
            "Iteration 2, loss = 0.58942803\n",
            "Iteration 3, loss = 0.58745942\n",
            "Iteration 4, loss = 0.56967656\n",
            "Iteration 5, loss = 0.54183556\n",
            "Iteration 6, loss = 0.51476965\n",
            "Iteration 7, loss = 0.50287903\n",
            "Iteration 8, loss = 0.47711250\n",
            "Iteration 9, loss = 0.46618603\n",
            "Iteration 10, loss = 0.47777914\n",
            "Iteration 11, loss = 0.47055233\n",
            "Iteration 12, loss = 0.45784733\n",
            "Iteration 13, loss = 0.45440173\n",
            "Iteration 14, loss = 0.45694968\n",
            "Iteration 15, loss = 0.43271301\n",
            "Iteration 16, loss = 0.43549546\n",
            "Iteration 17, loss = 0.43530140\n",
            "Iteration 18, loss = 0.43421935\n",
            "Iteration 19, loss = 0.45072713\n",
            "Iteration 20, loss = 0.43599473\n",
            "Iteration 21, loss = 0.43394092\n",
            "Iteration 22, loss = 0.42261865\n",
            "Iteration 23, loss = 0.42165577\n",
            "Iteration 24, loss = 0.41650634\n",
            "Iteration 25, loss = 0.42875959\n",
            "Iteration 26, loss = 0.41997090\n",
            "Iteration 27, loss = 0.42213161\n",
            "Iteration 28, loss = 0.42557595\n",
            "Iteration 29, loss = 0.42247988\n",
            "Iteration 30, loss = 0.42412945\n",
            "Iteration 31, loss = 0.41068781\n",
            "Iteration 32, loss = 0.41993247\n",
            "Iteration 33, loss = 0.41696084\n",
            "Iteration 34, loss = 0.40995974\n",
            "Iteration 35, loss = 0.41145994\n",
            "Iteration 36, loss = 0.41940082\n",
            "Iteration 37, loss = 0.41453337\n",
            "Iteration 38, loss = 0.40624430\n",
            "Iteration 39, loss = 0.40787602\n",
            "Iteration 40, loss = 0.40961981\n",
            "Iteration 41, loss = 0.40291729\n",
            "Iteration 42, loss = 0.41590192\n",
            "Iteration 43, loss = 0.39907523\n",
            "Iteration 44, loss = 0.40102914\n",
            "Iteration 45, loss = 0.40970634\n",
            "Iteration 46, loss = 0.39580603\n",
            "Iteration 47, loss = 0.39591118\n",
            "Iteration 48, loss = 0.40049491\n",
            "Iteration 49, loss = 0.41412491\n",
            "Iteration 50, loss = 0.40498440\n",
            "Iteration 51, loss = 0.39954375\n",
            "Iteration 52, loss = 0.39676977\n",
            "Iteration 53, loss = 0.38432698\n",
            "Iteration 54, loss = 0.39073472\n",
            "Iteration 55, loss = 0.40357771\n",
            "Iteration 56, loss = 0.39244645\n",
            "Iteration 57, loss = 0.39065593\n",
            "Iteration 58, loss = 0.40092488\n",
            "Iteration 59, loss = 0.41194020\n",
            "Iteration 60, loss = 0.39774120\n",
            "Iteration 61, loss = 0.40061888\n",
            "Iteration 62, loss = 0.40540424\n",
            "Iteration 63, loss = 0.41203309\n",
            "Iteration 64, loss = 0.38864516\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.63616397\n",
            "Iteration 2, loss = 0.62095452\n",
            "Iteration 3, loss = 0.60828244\n",
            "Iteration 4, loss = 0.60204633\n",
            "Iteration 5, loss = 0.56785683\n",
            "Iteration 6, loss = 0.55000470\n",
            "Iteration 7, loss = 0.52441926\n",
            "Iteration 8, loss = 0.51702931\n",
            "Iteration 9, loss = 0.50127616\n",
            "Iteration 10, loss = 0.48218719\n",
            "Iteration 11, loss = 0.48559712\n",
            "Iteration 12, loss = 0.47523625\n",
            "Iteration 13, loss = 0.47514345\n",
            "Iteration 14, loss = 0.46741397\n",
            "Iteration 15, loss = 0.47354735\n",
            "Iteration 16, loss = 0.45893889\n",
            "Iteration 17, loss = 0.47505442\n",
            "Iteration 18, loss = 0.46848861\n",
            "Iteration 19, loss = 0.47550317\n",
            "Iteration 20, loss = 0.46178606\n",
            "Iteration 21, loss = 0.45364878\n",
            "Iteration 22, loss = 0.45370049\n",
            "Iteration 23, loss = 0.44712581\n",
            "Iteration 24, loss = 0.44952081\n",
            "Iteration 25, loss = 0.45341587\n",
            "Iteration 26, loss = 0.44001110\n",
            "Iteration 27, loss = 0.44485036\n",
            "Iteration 28, loss = 0.44999408\n",
            "Iteration 29, loss = 0.44754833\n",
            "Iteration 30, loss = 0.44991094\n",
            "Iteration 31, loss = 0.44252859\n",
            "Iteration 32, loss = 0.43378340\n",
            "Iteration 33, loss = 0.43961131\n",
            "Iteration 34, loss = 0.44614791\n",
            "Iteration 35, loss = 0.44943213\n",
            "Iteration 36, loss = 0.43662367\n",
            "Iteration 37, loss = 0.43145480\n",
            "Iteration 38, loss = 0.43811349\n",
            "Iteration 39, loss = 0.44414746\n",
            "Iteration 40, loss = 0.42218509\n",
            "Iteration 41, loss = 0.43018171\n",
            "Iteration 42, loss = 0.42783238\n",
            "Iteration 43, loss = 0.43237174\n",
            "Iteration 44, loss = 0.43347117\n",
            "Iteration 45, loss = 0.42950845\n",
            "Iteration 46, loss = 0.43662210\n",
            "Iteration 47, loss = 0.43297457\n",
            "Iteration 48, loss = 0.43717734\n",
            "Iteration 49, loss = 0.42491701\n",
            "Iteration 50, loss = 0.42856246\n",
            "Iteration 51, loss = 0.43572724\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61885081\n",
            "Iteration 2, loss = 0.61233207\n",
            "Iteration 3, loss = 0.59168970\n",
            "Iteration 4, loss = 0.57828890\n",
            "Iteration 5, loss = 0.57932525\n",
            "Iteration 6, loss = 0.56170867\n",
            "Iteration 7, loss = 0.54671444\n",
            "Iteration 8, loss = 0.53400026\n",
            "Iteration 9, loss = 0.52621582\n",
            "Iteration 10, loss = 0.51888522\n",
            "Iteration 11, loss = 0.49928143\n",
            "Iteration 12, loss = 0.49808095\n",
            "Iteration 13, loss = 0.48113170\n",
            "Iteration 14, loss = 0.47199222\n",
            "Iteration 15, loss = 0.47175146\n",
            "Iteration 16, loss = 0.45884866\n",
            "Iteration 17, loss = 0.45895904\n",
            "Iteration 18, loss = 0.44925026\n",
            "Iteration 19, loss = 0.45376148\n",
            "Iteration 20, loss = 0.46310694\n",
            "Iteration 21, loss = 0.43575218\n",
            "Iteration 22, loss = 0.44245489\n",
            "Iteration 23, loss = 0.45234690\n",
            "Iteration 24, loss = 0.43100483\n",
            "Iteration 25, loss = 0.42876817\n",
            "Iteration 26, loss = 0.42806764\n",
            "Iteration 27, loss = 0.42697054\n",
            "Iteration 28, loss = 0.42172735\n",
            "Iteration 29, loss = 0.42451759\n",
            "Iteration 30, loss = 0.41518499\n",
            "Iteration 31, loss = 0.42578681\n",
            "Iteration 32, loss = 0.41365603\n",
            "Iteration 33, loss = 0.41182278\n",
            "Iteration 34, loss = 0.41000342\n",
            "Iteration 35, loss = 0.41072988\n",
            "Iteration 36, loss = 0.40995637\n",
            "Iteration 37, loss = 0.42131706\n",
            "Iteration 38, loss = 0.40646419\n",
            "Iteration 39, loss = 0.42507890\n",
            "Iteration 40, loss = 0.41045576\n",
            "Iteration 41, loss = 0.40047545\n",
            "Iteration 42, loss = 0.40744147\n",
            "Iteration 43, loss = 0.40883664\n",
            "Iteration 44, loss = 0.39831302\n",
            "Iteration 45, loss = 0.39554323\n",
            "Iteration 46, loss = 0.39174088\n",
            "Iteration 47, loss = 0.39427678\n",
            "Iteration 48, loss = 0.39749857\n",
            "Iteration 49, loss = 0.40024179\n",
            "Iteration 50, loss = 0.40487709\n",
            "Iteration 51, loss = 0.39077125\n",
            "Iteration 52, loss = 0.39008702\n",
            "Iteration 53, loss = 0.38642798\n",
            "Iteration 54, loss = 0.40400660\n",
            "Iteration 55, loss = 0.40262503\n",
            "Iteration 56, loss = 0.38937994\n",
            "Iteration 57, loss = 0.39364352\n",
            "Iteration 58, loss = 0.38060440\n",
            "Iteration 59, loss = 0.39328463\n",
            "Iteration 60, loss = 0.38690968\n",
            "Iteration 61, loss = 0.38039225\n",
            "Iteration 62, loss = 0.38048296\n",
            "Iteration 63, loss = 0.38078102\n",
            "Iteration 64, loss = 0.37946214\n",
            "Iteration 65, loss = 0.38309433\n",
            "Iteration 66, loss = 0.37160519\n",
            "Iteration 67, loss = 0.37733468\n",
            "Iteration 68, loss = 0.38498158\n",
            "Iteration 69, loss = 0.37209073\n",
            "Iteration 70, loss = 0.38028044\n",
            "Iteration 71, loss = 0.37687221\n",
            "Iteration 72, loss = 0.39803099\n",
            "Iteration 73, loss = 0.37793927\n",
            "Iteration 74, loss = 0.37626693\n",
            "Iteration 75, loss = 0.38376725\n",
            "Iteration 76, loss = 0.39004831\n",
            "Iteration 77, loss = 0.37147596\n",
            "Iteration 78, loss = 0.36807296\n",
            "Iteration 79, loss = 0.37375670\n",
            "Iteration 80, loss = 0.39403922\n",
            "Iteration 81, loss = 0.38039921\n",
            "Iteration 82, loss = 0.37238789\n",
            "Iteration 83, loss = 0.38441905\n",
            "Iteration 84, loss = 0.36888985\n",
            "Iteration 85, loss = 0.35956886\n",
            "Iteration 86, loss = 0.37426562\n",
            "Iteration 87, loss = 0.36053435\n",
            "Iteration 88, loss = 0.36087289\n",
            "Iteration 89, loss = 0.36216734\n",
            "Iteration 90, loss = 0.37538312\n",
            "Iteration 91, loss = 0.36978243\n",
            "Iteration 92, loss = 0.37386329\n",
            "Iteration 93, loss = 0.37137419\n",
            "Iteration 94, loss = 0.36296003\n",
            "Iteration 95, loss = 0.35811715\n",
            "Iteration 96, loss = 0.36156094\n",
            "Iteration 97, loss = 0.36365479\n",
            "Iteration 98, loss = 0.35891308\n",
            "Iteration 99, loss = 0.35471779\n",
            "Iteration 100, loss = 0.35688677\n",
            "Iteration 101, loss = 0.36743113\n",
            "Iteration 102, loss = 0.35330208\n",
            "Iteration 103, loss = 0.36300598\n",
            "Iteration 104, loss = 0.36057663\n",
            "Iteration 105, loss = 0.35205581\n",
            "Iteration 106, loss = 0.35564432\n",
            "Iteration 107, loss = 0.36333963\n",
            "Iteration 108, loss = 0.36148122\n",
            "Iteration 109, loss = 0.35801540\n",
            "Iteration 110, loss = 0.36866489\n",
            "Iteration 111, loss = 0.35477630\n",
            "Iteration 112, loss = 0.35512790\n",
            "Iteration 113, loss = 0.35379907\n",
            "Iteration 114, loss = 0.35461880\n",
            "Iteration 115, loss = 0.35884946\n",
            "Iteration 116, loss = 0.35340363\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61584851\n",
            "Iteration 2, loss = 0.59378353\n",
            "Iteration 3, loss = 0.57494008\n",
            "Iteration 4, loss = 0.56701813\n",
            "Iteration 5, loss = 0.54522003\n",
            "Iteration 6, loss = 0.53331589\n",
            "Iteration 7, loss = 0.51744186\n",
            "Iteration 8, loss = 0.50276536\n",
            "Iteration 9, loss = 0.48966260\n",
            "Iteration 10, loss = 0.49017122\n",
            "Iteration 11, loss = 0.49512039\n",
            "Iteration 12, loss = 0.48482433\n",
            "Iteration 13, loss = 0.46986622\n",
            "Iteration 14, loss = 0.47763536\n",
            "Iteration 15, loss = 0.48442525\n",
            "Iteration 16, loss = 0.46768398\n",
            "Iteration 17, loss = 0.45816779\n",
            "Iteration 18, loss = 0.47790308\n",
            "Iteration 19, loss = 0.47044125\n",
            "Iteration 20, loss = 0.45374350\n",
            "Iteration 21, loss = 0.46374247\n",
            "Iteration 22, loss = 0.44960443\n",
            "Iteration 23, loss = 0.45329288\n",
            "Iteration 24, loss = 0.45792666\n",
            "Iteration 25, loss = 0.44588356\n",
            "Iteration 26, loss = 0.43681797\n",
            "Iteration 27, loss = 0.45184515\n",
            "Iteration 28, loss = 0.43758639\n",
            "Iteration 29, loss = 0.44333713\n",
            "Iteration 30, loss = 0.42867607\n",
            "Iteration 31, loss = 0.43995005\n",
            "Iteration 32, loss = 0.43006584\n",
            "Iteration 33, loss = 0.43010148\n",
            "Iteration 34, loss = 0.42726870\n",
            "Iteration 35, loss = 0.43471330\n",
            "Iteration 36, loss = 0.41936483\n",
            "Iteration 37, loss = 0.42119528\n",
            "Iteration 38, loss = 0.43400077\n",
            "Iteration 39, loss = 0.42495989\n",
            "Iteration 40, loss = 0.41111085\n",
            "Iteration 41, loss = 0.41504190\n",
            "Iteration 42, loss = 0.40405900\n",
            "Iteration 43, loss = 0.41244538\n",
            "Iteration 44, loss = 0.40881088\n",
            "Iteration 45, loss = 0.40335521\n",
            "Iteration 46, loss = 0.40887628\n",
            "Iteration 47, loss = 0.40935094\n",
            "Iteration 48, loss = 0.40353648\n",
            "Iteration 49, loss = 0.40553468\n",
            "Iteration 50, loss = 0.39818844\n",
            "Iteration 51, loss = 0.39508894\n",
            "Iteration 52, loss = 0.39872343\n",
            "Iteration 53, loss = 0.39986060\n",
            "Iteration 54, loss = 0.39699665\n",
            "Iteration 55, loss = 0.40978212\n",
            "Iteration 56, loss = 0.39297813\n",
            "Iteration 57, loss = 0.40012470\n",
            "Iteration 58, loss = 0.38922138\n",
            "Iteration 59, loss = 0.38500533\n",
            "Iteration 60, loss = 0.39213046\n",
            "Iteration 61, loss = 0.39387889\n",
            "Iteration 62, loss = 0.38359594\n",
            "Iteration 63, loss = 0.38254157\n",
            "Iteration 64, loss = 0.38614221\n",
            "Iteration 65, loss = 0.39021235\n",
            "Iteration 66, loss = 0.38327435\n",
            "Iteration 67, loss = 0.38673145\n",
            "Iteration 68, loss = 0.37271251\n",
            "Iteration 69, loss = 0.40502892\n",
            "Iteration 70, loss = 0.38304561\n",
            "Iteration 71, loss = 0.38641909\n",
            "Iteration 72, loss = 0.39238962\n",
            "Iteration 73, loss = 0.39762248\n",
            "Iteration 74, loss = 0.37866092\n",
            "Iteration 75, loss = 0.37236370\n",
            "Iteration 76, loss = 0.37758215\n",
            "Iteration 77, loss = 0.37879637\n",
            "Iteration 78, loss = 0.37390978\n",
            "Iteration 79, loss = 0.37973582\n",
            "Iteration 80, loss = 0.38024205\n",
            "Iteration 81, loss = 0.38147820\n",
            "Iteration 82, loss = 0.37606214\n",
            "Iteration 83, loss = 0.38870093\n",
            "Iteration 84, loss = 0.37163971\n",
            "Iteration 85, loss = 0.37000709\n",
            "Iteration 86, loss = 0.37748369\n",
            "Iteration 87, loss = 0.36024947\n",
            "Iteration 88, loss = 0.37716400\n",
            "Iteration 89, loss = 0.37625395\n",
            "Iteration 90, loss = 0.37113551\n",
            "Iteration 91, loss = 0.37774950\n",
            "Iteration 92, loss = 0.36525963\n",
            "Iteration 93, loss = 0.37410002\n",
            "Iteration 94, loss = 0.37051614\n",
            "Iteration 95, loss = 0.35757606\n",
            "Iteration 96, loss = 0.36970479\n",
            "Iteration 97, loss = 0.37418690\n",
            "Iteration 98, loss = 0.36719465\n",
            "Iteration 99, loss = 0.36362343\n",
            "Iteration 100, loss = 0.36711901\n",
            "Iteration 101, loss = 0.35989596\n",
            "Iteration 102, loss = 0.36640808\n",
            "Iteration 103, loss = 0.36266692\n",
            "Iteration 104, loss = 0.35543984\n",
            "Iteration 105, loss = 0.35676757\n",
            "Iteration 106, loss = 0.36869728\n",
            "Iteration 107, loss = 0.34788290\n",
            "Iteration 108, loss = 0.36575979\n",
            "Iteration 109, loss = 0.35587553\n",
            "Iteration 110, loss = 0.35662268\n",
            "Iteration 111, loss = 0.36240262\n",
            "Iteration 112, loss = 0.35245290\n",
            "Iteration 113, loss = 0.35567510\n",
            "Iteration 114, loss = 0.35733802\n",
            "Iteration 115, loss = 0.35858651\n",
            "Iteration 116, loss = 0.36487421\n",
            "Iteration 117, loss = 0.38224013\n",
            "Iteration 118, loss = 0.36677568\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67689404\n",
            "Iteration 2, loss = 0.66384683\n",
            "Iteration 3, loss = 0.64903527\n",
            "Iteration 4, loss = 0.63764875\n",
            "Iteration 5, loss = 0.62937809\n",
            "Iteration 6, loss = 0.62318429\n",
            "Iteration 7, loss = 0.61863674\n",
            "Iteration 8, loss = 0.61537322\n",
            "Iteration 9, loss = 0.61307486\n",
            "Iteration 10, loss = 0.61170557\n",
            "Iteration 11, loss = 0.61069167\n",
            "Iteration 12, loss = 0.60998789\n",
            "Iteration 13, loss = 0.60920152\n",
            "Iteration 14, loss = 0.60850700\n",
            "Iteration 15, loss = 0.60647953\n",
            "Iteration 16, loss = 0.60409833\n",
            "Iteration 17, loss = 0.60315395\n",
            "Iteration 18, loss = 0.60308436\n",
            "Iteration 19, loss = 0.60265609\n",
            "Iteration 20, loss = 0.60179647\n",
            "Iteration 21, loss = 0.60070050\n",
            "Iteration 22, loss = 0.60044898\n",
            "Iteration 23, loss = 0.59961691\n",
            "Iteration 24, loss = 0.59919247\n",
            "Iteration 25, loss = 0.59872203\n",
            "Iteration 26, loss = 0.59794171\n",
            "Iteration 27, loss = 0.59729071\n",
            "Iteration 28, loss = 0.59726673\n",
            "Iteration 29, loss = 0.59772038\n",
            "Iteration 30, loss = 0.59644346\n",
            "Iteration 31, loss = 0.59585562\n",
            "Iteration 32, loss = 0.59670218\n",
            "Iteration 33, loss = 0.59511168\n",
            "Iteration 34, loss = 0.59507267\n",
            "Iteration 35, loss = 0.59368610\n",
            "Iteration 36, loss = 0.59350897\n",
            "Iteration 37, loss = 0.59321574\n",
            "Iteration 38, loss = 0.59464527\n",
            "Iteration 39, loss = 0.59187022\n",
            "Iteration 40, loss = 0.59309059\n",
            "Iteration 41, loss = 0.59123828\n",
            "Iteration 42, loss = 0.59029969\n",
            "Iteration 43, loss = 0.58990939\n",
            "Iteration 44, loss = 0.58969089\n",
            "Iteration 45, loss = 0.58985530\n",
            "Iteration 46, loss = 0.58968174\n",
            "Iteration 47, loss = 0.58873013\n",
            "Iteration 48, loss = 0.58850637\n",
            "Iteration 49, loss = 0.58847215\n",
            "Iteration 50, loss = 0.58921937\n",
            "Iteration 51, loss = 0.58784009\n",
            "Iteration 52, loss = 0.58644909\n",
            "Iteration 53, loss = 0.58640243\n",
            "Iteration 54, loss = 0.58485301\n",
            "Iteration 55, loss = 0.58567577\n",
            "Iteration 56, loss = 0.58488931\n",
            "Iteration 57, loss = 0.58440659\n",
            "Iteration 58, loss = 0.58401110\n",
            "Iteration 59, loss = 0.58261213\n",
            "Iteration 60, loss = 0.58228638\n",
            "Iteration 61, loss = 0.58187984\n",
            "Iteration 62, loss = 0.58177960\n",
            "Iteration 63, loss = 0.58209268\n",
            "Iteration 64, loss = 0.58059475\n",
            "Iteration 65, loss = 0.57927338\n",
            "Iteration 66, loss = 0.58272449\n",
            "Iteration 67, loss = 0.57921346\n",
            "Iteration 68, loss = 0.58147923\n",
            "Iteration 69, loss = 0.57960150\n",
            "Iteration 70, loss = 0.57768266\n",
            "Iteration 71, loss = 0.57713677\n",
            "Iteration 72, loss = 0.57653578\n",
            "Iteration 73, loss = 0.57685737\n",
            "Iteration 74, loss = 0.57507113\n",
            "Iteration 75, loss = 0.57642668\n",
            "Iteration 76, loss = 0.57488589\n",
            "Iteration 77, loss = 0.57625775\n",
            "Iteration 78, loss = 0.57427902\n",
            "Iteration 79, loss = 0.57213056\n",
            "Iteration 80, loss = 0.57243173\n",
            "Iteration 81, loss = 0.57178435\n",
            "Iteration 82, loss = 0.57814191\n",
            "Iteration 83, loss = 0.57487000\n",
            "Iteration 84, loss = 0.57109966\n",
            "Iteration 85, loss = 0.57451303\n",
            "Iteration 86, loss = 0.57564389\n",
            "Iteration 87, loss = 0.57085506\n",
            "Iteration 88, loss = 0.56765174\n",
            "Iteration 89, loss = 0.56883580\n",
            "Iteration 90, loss = 0.56738849\n",
            "Iteration 91, loss = 0.56564416\n",
            "Iteration 92, loss = 0.56547473\n",
            "Iteration 93, loss = 0.56837091\n",
            "Iteration 94, loss = 0.56356138\n",
            "Iteration 95, loss = 0.56094412\n",
            "Iteration 96, loss = 0.56327717\n",
            "Iteration 97, loss = 0.56195926\n",
            "Iteration 98, loss = 0.55841086\n",
            "Iteration 99, loss = 0.56448133\n",
            "Iteration 100, loss = 0.55805417\n",
            "Iteration 101, loss = 0.55304015\n",
            "Iteration 102, loss = 0.55382218\n",
            "Iteration 103, loss = 0.55377418\n",
            "Iteration 104, loss = 0.55231433\n",
            "Iteration 105, loss = 0.54985170\n",
            "Iteration 106, loss = 0.54875492\n",
            "Iteration 107, loss = 0.57152541\n",
            "Iteration 108, loss = 0.54991708\n",
            "Iteration 109, loss = 0.55545071\n",
            "Iteration 110, loss = 0.54812486\n",
            "Iteration 111, loss = 0.54762663\n",
            "Iteration 112, loss = 0.55436551\n",
            "Iteration 113, loss = 0.54384969\n",
            "Iteration 114, loss = 0.54761933\n",
            "Iteration 115, loss = 0.54307387\n",
            "Iteration 116, loss = 0.54921890\n",
            "Iteration 117, loss = 0.54204939\n",
            "Iteration 118, loss = 0.54471779\n",
            "Iteration 119, loss = 0.54780117\n",
            "Iteration 120, loss = 0.53888829\n",
            "Iteration 121, loss = 0.54811137\n",
            "Iteration 122, loss = 0.53991240\n",
            "Iteration 123, loss = 0.53367510\n",
            "Iteration 124, loss = 0.53796849\n",
            "Iteration 125, loss = 0.53485780\n",
            "Iteration 126, loss = 0.53268067\n",
            "Iteration 127, loss = 0.53096930\n",
            "Iteration 128, loss = 0.53179724\n",
            "Iteration 129, loss = 0.53652675\n",
            "Iteration 130, loss = 0.53614404\n",
            "Iteration 131, loss = 0.53132505\n",
            "Iteration 132, loss = 0.53117606\n",
            "Iteration 133, loss = 0.52966464\n",
            "Iteration 134, loss = 0.52283572\n",
            "Iteration 135, loss = 0.53024213\n",
            "Iteration 136, loss = 0.52522718\n",
            "Iteration 137, loss = 0.52861474\n",
            "Iteration 138, loss = 0.53332292\n",
            "Iteration 139, loss = 0.52185213\n",
            "Iteration 140, loss = 0.52014104\n",
            "Iteration 141, loss = 0.52055823\n",
            "Iteration 142, loss = 0.53334092\n",
            "Iteration 143, loss = 0.51956919\n",
            "Iteration 144, loss = 0.51623524\n",
            "Iteration 145, loss = 0.51338982\n",
            "Iteration 146, loss = 0.51595913\n",
            "Iteration 147, loss = 0.51862317\n",
            "Iteration 148, loss = 0.53131062\n",
            "Iteration 149, loss = 0.52504054\n",
            "Iteration 150, loss = 0.52310010\n",
            "Iteration 151, loss = 0.51204034\n",
            "Iteration 152, loss = 0.54425523\n",
            "Iteration 153, loss = 0.51601251\n",
            "Iteration 154, loss = 0.53815558\n",
            "Iteration 155, loss = 0.55637446\n",
            "Iteration 156, loss = 0.51595105\n",
            "Iteration 157, loss = 0.53607747\n",
            "Iteration 158, loss = 0.51729187\n",
            "Iteration 159, loss = 0.50929456\n",
            "Iteration 160, loss = 0.51531303\n",
            "Iteration 161, loss = 0.51311110\n",
            "Iteration 162, loss = 0.50313817\n",
            "Iteration 163, loss = 0.50440004\n",
            "Iteration 164, loss = 0.50489027\n",
            "Iteration 165, loss = 0.50408320\n",
            "Iteration 166, loss = 0.50588545\n",
            "Iteration 167, loss = 0.49835247\n",
            "Iteration 168, loss = 0.49965651\n",
            "Iteration 169, loss = 0.49806684\n",
            "Iteration 170, loss = 0.51009664\n",
            "Iteration 171, loss = 0.49461664\n",
            "Iteration 172, loss = 0.50088983\n",
            "Iteration 173, loss = 0.50058165\n",
            "Iteration 174, loss = 0.49990010\n",
            "Iteration 175, loss = 0.50157245\n",
            "Iteration 176, loss = 0.50196423\n",
            "Iteration 177, loss = 0.53793384\n",
            "Iteration 178, loss = 0.49438595\n",
            "Iteration 179, loss = 0.49606781\n",
            "Iteration 180, loss = 0.49049136\n",
            "Iteration 181, loss = 0.49652369\n",
            "Iteration 182, loss = 0.54222411\n",
            "Iteration 183, loss = 0.49954249\n",
            "Iteration 184, loss = 0.49916106\n",
            "Iteration 185, loss = 0.48679314\n",
            "Iteration 186, loss = 0.51987120\n",
            "Iteration 187, loss = 0.49447405\n",
            "Iteration 188, loss = 0.51541353\n",
            "Iteration 189, loss = 0.52779826\n",
            "Iteration 190, loss = 0.48686325\n",
            "Iteration 191, loss = 0.49546426\n",
            "Iteration 192, loss = 0.49552313\n",
            "Iteration 193, loss = 0.48946887\n",
            "Iteration 194, loss = 0.50567159\n",
            "Iteration 195, loss = 0.49299393\n",
            "Iteration 196, loss = 0.49224499\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62512718\n",
            "Iteration 2, loss = 0.62147732\n",
            "Iteration 3, loss = 0.61875501\n",
            "Iteration 4, loss = 0.61695074\n",
            "Iteration 5, loss = 0.61547409\n",
            "Iteration 6, loss = 0.61606871\n",
            "Iteration 7, loss = 0.61309728\n",
            "Iteration 8, loss = 0.61426629\n",
            "Iteration 9, loss = 0.61185550\n",
            "Iteration 10, loss = 0.61143596\n",
            "Iteration 11, loss = 0.61098911\n",
            "Iteration 12, loss = 0.60999055\n",
            "Iteration 13, loss = 0.60929221\n",
            "Iteration 14, loss = 0.60831123\n",
            "Iteration 15, loss = 0.60731485\n",
            "Iteration 16, loss = 0.60734632\n",
            "Iteration 17, loss = 0.60692801\n",
            "Iteration 18, loss = 0.60658224\n",
            "Iteration 19, loss = 0.60657073\n",
            "Iteration 20, loss = 0.60607496\n",
            "Iteration 21, loss = 0.60573985\n",
            "Iteration 22, loss = 0.60566390\n",
            "Iteration 23, loss = 0.60529925\n",
            "Iteration 24, loss = 0.60544445\n",
            "Iteration 25, loss = 0.60472557\n",
            "Iteration 26, loss = 0.60485488\n",
            "Iteration 27, loss = 0.60467392\n",
            "Iteration 28, loss = 0.60426239\n",
            "Iteration 29, loss = 0.60435581\n",
            "Iteration 30, loss = 0.60511407\n",
            "Iteration 31, loss = 0.60443910\n",
            "Iteration 32, loss = 0.60343232\n",
            "Iteration 33, loss = 0.60320591\n",
            "Iteration 34, loss = 0.60356100\n",
            "Iteration 35, loss = 0.60376491\n",
            "Iteration 36, loss = 0.60325846\n",
            "Iteration 37, loss = 0.60394906\n",
            "Iteration 38, loss = 0.60283898\n",
            "Iteration 39, loss = 0.60275018\n",
            "Iteration 40, loss = 0.60233009\n",
            "Iteration 41, loss = 0.60207709\n",
            "Iteration 42, loss = 0.60276037\n",
            "Iteration 43, loss = 0.60152386\n",
            "Iteration 44, loss = 0.60395095\n",
            "Iteration 45, loss = 0.60280986\n",
            "Iteration 46, loss = 0.60157256\n",
            "Iteration 47, loss = 0.60099387\n",
            "Iteration 48, loss = 0.60093139\n",
            "Iteration 49, loss = 0.60209949\n",
            "Iteration 50, loss = 0.60236978\n",
            "Iteration 51, loss = 0.60128351\n",
            "Iteration 52, loss = 0.60119487\n",
            "Iteration 53, loss = 0.60022216\n",
            "Iteration 54, loss = 0.60073337\n",
            "Iteration 55, loss = 0.60017978\n",
            "Iteration 56, loss = 0.59998324\n",
            "Iteration 57, loss = 0.60016226\n",
            "Iteration 58, loss = 0.59960809\n",
            "Iteration 59, loss = 0.59939419\n",
            "Iteration 60, loss = 0.59968340\n",
            "Iteration 61, loss = 0.59974353\n",
            "Iteration 62, loss = 0.59933012\n",
            "Iteration 63, loss = 0.60067137\n",
            "Iteration 64, loss = 0.59823427\n",
            "Iteration 65, loss = 0.59872848\n",
            "Iteration 66, loss = 0.59893834\n",
            "Iteration 67, loss = 0.59834363\n",
            "Iteration 68, loss = 0.59813628\n",
            "Iteration 69, loss = 0.59810698\n",
            "Iteration 70, loss = 0.59809575\n",
            "Iteration 71, loss = 0.59776954\n",
            "Iteration 72, loss = 0.59775297\n",
            "Iteration 73, loss = 0.59776887\n",
            "Iteration 74, loss = 0.59759356\n",
            "Iteration 75, loss = 0.59712172\n",
            "Iteration 76, loss = 0.59757160\n",
            "Iteration 77, loss = 0.59709730\n",
            "Iteration 78, loss = 0.59682481\n",
            "Iteration 79, loss = 0.59672226\n",
            "Iteration 80, loss = 0.59716523\n",
            "Iteration 81, loss = 0.59695009\n",
            "Iteration 82, loss = 0.59629982\n",
            "Iteration 83, loss = 0.59690245\n",
            "Iteration 84, loss = 0.59695633\n",
            "Iteration 85, loss = 0.59609482\n",
            "Iteration 86, loss = 0.59568932\n",
            "Iteration 87, loss = 0.59651886\n",
            "Iteration 88, loss = 0.59629416\n",
            "Iteration 89, loss = 0.59621389\n",
            "Iteration 90, loss = 0.59556855\n",
            "Iteration 91, loss = 0.59602384\n",
            "Iteration 92, loss = 0.59547817\n",
            "Iteration 93, loss = 0.59653096\n",
            "Iteration 94, loss = 0.59606205\n",
            "Iteration 95, loss = 0.59491710\n",
            "Iteration 96, loss = 0.59494461\n",
            "Iteration 97, loss = 0.59429016\n",
            "Iteration 98, loss = 0.59395466\n",
            "Iteration 99, loss = 0.59433640\n",
            "Iteration 100, loss = 0.59485268\n",
            "Iteration 101, loss = 0.59440329\n",
            "Iteration 102, loss = 0.59453435\n",
            "Iteration 103, loss = 0.59289553\n",
            "Iteration 104, loss = 0.59273030\n",
            "Iteration 105, loss = 0.59447703\n",
            "Iteration 106, loss = 0.59432783\n",
            "Iteration 107, loss = 0.59609745\n",
            "Iteration 108, loss = 0.59396945\n",
            "Iteration 109, loss = 0.59296954\n",
            "Iteration 110, loss = 0.59233281\n",
            "Iteration 111, loss = 0.59253949\n",
            "Iteration 112, loss = 0.59264861\n",
            "Iteration 113, loss = 0.59121837\n",
            "Iteration 114, loss = 0.59060552\n",
            "Iteration 115, loss = 0.59062192\n",
            "Iteration 116, loss = 0.59030707\n",
            "Iteration 117, loss = 0.59247698\n",
            "Iteration 118, loss = 0.59088709\n",
            "Iteration 119, loss = 0.58971210\n",
            "Iteration 120, loss = 0.59022912\n",
            "Iteration 121, loss = 0.58930158\n",
            "Iteration 122, loss = 0.58983799\n",
            "Iteration 123, loss = 0.58863266\n",
            "Iteration 124, loss = 0.58890775\n",
            "Iteration 125, loss = 0.58891188\n",
            "Iteration 126, loss = 0.58894537\n",
            "Iteration 127, loss = 0.58798897\n",
            "Iteration 128, loss = 0.58813779\n",
            "Iteration 129, loss = 0.58731964\n",
            "Iteration 130, loss = 0.58980137\n",
            "Iteration 131, loss = 0.59486425\n",
            "Iteration 132, loss = 0.58633121\n",
            "Iteration 133, loss = 0.58753409\n",
            "Iteration 134, loss = 0.58668472\n",
            "Iteration 135, loss = 0.58616977\n",
            "Iteration 136, loss = 0.58631457\n",
            "Iteration 137, loss = 0.58590386\n",
            "Iteration 138, loss = 0.58509178\n",
            "Iteration 139, loss = 0.58632208\n",
            "Iteration 140, loss = 0.58575562\n",
            "Iteration 141, loss = 0.58680371\n",
            "Iteration 142, loss = 0.58546381\n",
            "Iteration 143, loss = 0.58524394\n",
            "Iteration 144, loss = 0.58384030\n",
            "Iteration 145, loss = 0.58505864\n",
            "Iteration 146, loss = 0.58406600\n",
            "Iteration 147, loss = 0.58458825\n",
            "Iteration 148, loss = 0.58445055\n",
            "Iteration 149, loss = 0.58424434\n",
            "Iteration 150, loss = 0.58497258\n",
            "Iteration 151, loss = 0.58138122\n",
            "Iteration 152, loss = 0.58827000\n",
            "Iteration 153, loss = 0.58450148\n",
            "Iteration 154, loss = 0.58177837\n",
            "Iteration 155, loss = 0.58286179\n",
            "Iteration 156, loss = 0.58099649\n",
            "Iteration 157, loss = 0.58123005\n",
            "Iteration 158, loss = 0.58223140\n",
            "Iteration 159, loss = 0.58096219\n",
            "Iteration 160, loss = 0.58282383\n",
            "Iteration 161, loss = 0.58206084\n",
            "Iteration 162, loss = 0.58011341\n",
            "Iteration 163, loss = 0.58101299\n",
            "Iteration 164, loss = 0.57979534\n",
            "Iteration 165, loss = 0.57927404\n",
            "Iteration 166, loss = 0.57985050\n",
            "Iteration 167, loss = 0.57972295\n",
            "Iteration 168, loss = 0.57898242\n",
            "Iteration 169, loss = 0.57875803\n",
            "Iteration 170, loss = 0.57996703\n",
            "Iteration 171, loss = 0.57876508\n",
            "Iteration 172, loss = 0.57837681\n",
            "Iteration 173, loss = 0.58040446\n",
            "Iteration 174, loss = 0.58058674\n",
            "Iteration 175, loss = 0.57863974\n",
            "Iteration 176, loss = 0.57777069\n",
            "Iteration 177, loss = 0.57663139\n",
            "Iteration 178, loss = 0.57696046\n",
            "Iteration 179, loss = 0.57702634\n",
            "Iteration 180, loss = 0.57576725\n",
            "Iteration 181, loss = 0.57689984\n",
            "Iteration 182, loss = 0.57792088\n",
            "Iteration 183, loss = 0.57675834\n",
            "Iteration 184, loss = 0.57482385\n",
            "Iteration 185, loss = 0.57501824\n",
            "Iteration 186, loss = 0.57577548\n",
            "Iteration 187, loss = 0.57488020\n",
            "Iteration 188, loss = 0.57596020\n",
            "Iteration 189, loss = 0.57610691\n",
            "Iteration 190, loss = 0.57448202\n",
            "Iteration 191, loss = 0.57502426\n",
            "Iteration 192, loss = 0.57357781\n",
            "Iteration 193, loss = 0.57280734\n",
            "Iteration 194, loss = 0.57250246\n",
            "Iteration 195, loss = 0.57285185\n",
            "Iteration 196, loss = 0.57225155\n",
            "Iteration 197, loss = 0.57213911\n",
            "Iteration 198, loss = 0.57701661\n",
            "Iteration 199, loss = 0.57332502\n",
            "Iteration 200, loss = 0.57180614\n",
            "Iteration 1, loss = 0.68036449\n",
            "Iteration 2, loss = 0.63534528\n",
            "Iteration 3, loss = 0.61936279\n",
            "Iteration 4, loss = 0.62060996\n",
            "Iteration 5, loss = 0.61699762\n",
            "Iteration 6, loss = 0.61490539\n",
            "Iteration 7, loss = 0.61316991\n",
            "Iteration 8, loss = 0.61225754\n",
            "Iteration 9, loss = 0.61056193\n",
            "Iteration 10, loss = 0.61088730\n",
            "Iteration 11, loss = 0.60988965\n",
            "Iteration 12, loss = 0.60923253\n",
            "Iteration 13, loss = 0.60951480\n",
            "Iteration 14, loss = 0.60871696\n",
            "Iteration 15, loss = 0.60731661\n",
            "Iteration 16, loss = 0.60611978\n",
            "Iteration 17, loss = 0.60698515\n",
            "Iteration 18, loss = 0.60628017\n",
            "Iteration 19, loss = 0.60593357\n",
            "Iteration 20, loss = 0.60552335\n",
            "Iteration 21, loss = 0.60444433\n",
            "Iteration 22, loss = 0.60403446\n",
            "Iteration 23, loss = 0.60387703\n",
            "Iteration 24, loss = 0.60255997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 25, loss = 0.60282806\n",
            "Iteration 26, loss = 0.60239968\n",
            "Iteration 27, loss = 0.59986931\n",
            "Iteration 28, loss = 0.59868595\n",
            "Iteration 29, loss = 0.59891677\n",
            "Iteration 30, loss = 0.60002210\n",
            "Iteration 31, loss = 0.59887759\n",
            "Iteration 32, loss = 0.59783990\n",
            "Iteration 33, loss = 0.59741569\n",
            "Iteration 34, loss = 0.59812388\n",
            "Iteration 35, loss = 0.59682151\n",
            "Iteration 36, loss = 0.59674782\n",
            "Iteration 37, loss = 0.59650046\n",
            "Iteration 38, loss = 0.59666260\n",
            "Iteration 39, loss = 0.59581476\n",
            "Iteration 40, loss = 0.59586425\n",
            "Iteration 41, loss = 0.59471164\n",
            "Iteration 42, loss = 0.59453342\n",
            "Iteration 43, loss = 0.59569081\n",
            "Iteration 44, loss = 0.59482157\n",
            "Iteration 45, loss = 0.59387829\n",
            "Iteration 46, loss = 0.59358403\n",
            "Iteration 47, loss = 0.59329830\n",
            "Iteration 48, loss = 0.59368714\n",
            "Iteration 49, loss = 0.59334413\n",
            "Iteration 50, loss = 0.59386281\n",
            "Iteration 51, loss = 0.59271341\n",
            "Iteration 52, loss = 0.59275606\n",
            "Iteration 53, loss = 0.59285574\n",
            "Iteration 54, loss = 0.59187585\n",
            "Iteration 55, loss = 0.59164198\n",
            "Iteration 56, loss = 0.59104994\n",
            "Iteration 57, loss = 0.59250176\n",
            "Iteration 58, loss = 0.59168905\n",
            "Iteration 59, loss = 0.59115014\n",
            "Iteration 60, loss = 0.59075705\n",
            "Iteration 61, loss = 0.59098452\n",
            "Iteration 62, loss = 0.59055254\n",
            "Iteration 63, loss = 0.58962092\n",
            "Iteration 64, loss = 0.58897960\n",
            "Iteration 65, loss = 0.58902612\n",
            "Iteration 66, loss = 0.58945135\n",
            "Iteration 67, loss = 0.59019899\n",
            "Iteration 68, loss = 0.59019542\n",
            "Iteration 69, loss = 0.58837732\n",
            "Iteration 70, loss = 0.58782234\n",
            "Iteration 71, loss = 0.58777229\n",
            "Iteration 72, loss = 0.58755655\n",
            "Iteration 73, loss = 0.58810562\n",
            "Iteration 74, loss = 0.58741608\n",
            "Iteration 75, loss = 0.58827307\n",
            "Iteration 76, loss = 0.58888978\n",
            "Iteration 77, loss = 0.58551179\n",
            "Iteration 78, loss = 0.58815834\n",
            "Iteration 79, loss = 0.58677508\n",
            "Iteration 80, loss = 0.58991997\n",
            "Iteration 81, loss = 0.58729458\n",
            "Iteration 82, loss = 0.58679521\n",
            "Iteration 83, loss = 0.58613325\n",
            "Iteration 84, loss = 0.58545710\n",
            "Iteration 85, loss = 0.58486790\n",
            "Iteration 86, loss = 0.58684044\n",
            "Iteration 87, loss = 0.58523679\n",
            "Iteration 88, loss = 0.58516199\n",
            "Iteration 89, loss = 0.58423134\n",
            "Iteration 90, loss = 0.58354460\n",
            "Iteration 91, loss = 0.58368119\n",
            "Iteration 92, loss = 0.58301607\n",
            "Iteration 93, loss = 0.58209416\n",
            "Iteration 94, loss = 0.58605685\n",
            "Iteration 95, loss = 0.58079937\n",
            "Iteration 96, loss = 0.58211565\n",
            "Iteration 97, loss = 0.58087095\n",
            "Iteration 98, loss = 0.58051411\n",
            "Iteration 99, loss = 0.58321603\n",
            "Iteration 100, loss = 0.57937168\n",
            "Iteration 101, loss = 0.58060144\n",
            "Iteration 102, loss = 0.57945469\n",
            "Iteration 103, loss = 0.57831947\n",
            "Iteration 104, loss = 0.57942389\n",
            "Iteration 105, loss = 0.57720051\n",
            "Iteration 106, loss = 0.57758224\n",
            "Iteration 107, loss = 0.57730447\n",
            "Iteration 108, loss = 0.57561906\n",
            "Iteration 109, loss = 0.57710149\n",
            "Iteration 110, loss = 0.57581971\n",
            "Iteration 111, loss = 0.57485368\n",
            "Iteration 112, loss = 0.57513301\n",
            "Iteration 113, loss = 0.57527079\n",
            "Iteration 114, loss = 0.57543589\n",
            "Iteration 115, loss = 0.57417386\n",
            "Iteration 116, loss = 0.57468401\n",
            "Iteration 117, loss = 0.57445402\n",
            "Iteration 118, loss = 0.57340922\n",
            "Iteration 119, loss = 0.57395011\n",
            "Iteration 120, loss = 0.57404926\n",
            "Iteration 121, loss = 0.57481415\n",
            "Iteration 122, loss = 0.57407855\n",
            "Iteration 123, loss = 0.57241264\n",
            "Iteration 124, loss = 0.57171655\n",
            "Iteration 125, loss = 0.57101727\n",
            "Iteration 126, loss = 0.57182782\n",
            "Iteration 127, loss = 0.57200723\n",
            "Iteration 128, loss = 0.57309359\n",
            "Iteration 129, loss = 0.57277536\n",
            "Iteration 130, loss = 0.57232423\n",
            "Iteration 131, loss = 0.57424117\n",
            "Iteration 132, loss = 0.57355620\n",
            "Iteration 133, loss = 0.57126677\n",
            "Iteration 134, loss = 0.57068786\n",
            "Iteration 135, loss = 0.57075870\n",
            "Iteration 136, loss = 0.57252279\n",
            "Iteration 137, loss = 0.57065764\n",
            "Iteration 138, loss = 0.56953494\n",
            "Iteration 139, loss = 0.56973161\n",
            "Iteration 140, loss = 0.56699371\n",
            "Iteration 141, loss = 0.57022114\n",
            "Iteration 142, loss = 0.56680948\n",
            "Iteration 143, loss = 0.56725643\n",
            "Iteration 144, loss = 0.56631602\n",
            "Iteration 145, loss = 0.56985202\n",
            "Iteration 146, loss = 0.56610714\n",
            "Iteration 147, loss = 0.56568020\n",
            "Iteration 148, loss = 0.56588940\n",
            "Iteration 149, loss = 0.56744644\n",
            "Iteration 150, loss = 0.56527635\n",
            "Iteration 151, loss = 0.56661094\n",
            "Iteration 152, loss = 0.56568292\n",
            "Iteration 153, loss = 0.56842891\n",
            "Iteration 154, loss = 0.56474750\n",
            "Iteration 155, loss = 0.56634269\n",
            "Iteration 156, loss = 0.56681156\n",
            "Iteration 157, loss = 0.56330332\n",
            "Iteration 158, loss = 0.57458554\n",
            "Iteration 159, loss = 0.56598056\n",
            "Iteration 160, loss = 0.56308176\n",
            "Iteration 161, loss = 0.56258075\n",
            "Iteration 162, loss = 0.56483491\n",
            "Iteration 163, loss = 0.56352980\n",
            "Iteration 164, loss = 0.56471960\n",
            "Iteration 165, loss = 0.56185505\n",
            "Iteration 166, loss = 0.56257923\n",
            "Iteration 167, loss = 0.56118261\n",
            "Iteration 168, loss = 0.56108870\n",
            "Iteration 169, loss = 0.56197134\n",
            "Iteration 170, loss = 0.55961888\n",
            "Iteration 171, loss = 0.56742353\n",
            "Iteration 172, loss = 0.56006944\n",
            "Iteration 173, loss = 0.56133586\n",
            "Iteration 174, loss = 0.56127419\n",
            "Iteration 175, loss = 0.56376402\n",
            "Iteration 176, loss = 0.56069511\n",
            "Iteration 177, loss = 0.55837641\n",
            "Iteration 178, loss = 0.55925988\n",
            "Iteration 179, loss = 0.56001730\n",
            "Iteration 180, loss = 0.56019837\n",
            "Iteration 181, loss = 0.55808387\n",
            "Iteration 182, loss = 0.55823295\n",
            "Iteration 183, loss = 0.55937152\n",
            "Iteration 184, loss = 0.56312910\n",
            "Iteration 185, loss = 0.55968016\n",
            "Iteration 186, loss = 0.56579033\n",
            "Iteration 187, loss = 0.55597179\n",
            "Iteration 188, loss = 0.55712664\n",
            "Iteration 189, loss = 0.56199151\n",
            "Iteration 190, loss = 0.55979576\n",
            "Iteration 191, loss = 0.56204572\n",
            "Iteration 192, loss = 0.55775331\n",
            "Iteration 193, loss = 0.56118083\n",
            "Iteration 194, loss = 0.55836586\n",
            "Iteration 195, loss = 0.56328336\n",
            "Iteration 196, loss = 0.55444265\n",
            "Iteration 197, loss = 0.55674478\n",
            "Iteration 198, loss = 0.55671454\n",
            "Iteration 199, loss = 0.57154350\n",
            "Iteration 200, loss = 0.55499735\n",
            "Iteration 1, loss = 0.75776585\n",
            "Iteration 2, loss = 0.69135439\n",
            "Iteration 3, loss = 0.65437952\n",
            "Iteration 4, loss = 0.63349782\n",
            "Iteration 5, loss = 0.62191191\n",
            "Iteration 6, loss = 0.61586789\n",
            "Iteration 7, loss = 0.61145167\n",
            "Iteration 8, loss = 0.60897865\n",
            "Iteration 9, loss = 0.60626484\n",
            "Iteration 10, loss = 0.60440697\n",
            "Iteration 11, loss = 0.60269674\n",
            "Iteration 12, loss = 0.60183147\n",
            "Iteration 13, loss = 0.60096155\n",
            "Iteration 14, loss = 0.60092622\n",
            "Iteration 15, loss = 0.60095458\n",
            "Iteration 16, loss = 0.60041212\n",
            "Iteration 17, loss = 0.59917620\n",
            "Iteration 18, loss = 0.59919283\n",
            "Iteration 19, loss = 0.59957186\n",
            "Iteration 20, loss = 0.59883162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 21, loss = 0.59897187\n",
            "Iteration 22, loss = 0.59811050\n",
            "Iteration 23, loss = 0.59780528\n",
            "Iteration 24, loss = 0.59756111\n",
            "Iteration 25, loss = 0.59758153\n",
            "Iteration 26, loss = 0.59829047\n",
            "Iteration 27, loss = 0.59824851\n",
            "Iteration 28, loss = 0.59744865\n",
            "Iteration 29, loss = 0.59651378\n",
            "Iteration 30, loss = 0.59654961\n",
            "Iteration 31, loss = 0.59619106\n",
            "Iteration 32, loss = 0.59613208\n",
            "Iteration 33, loss = 0.59596928\n",
            "Iteration 34, loss = 0.59569512\n",
            "Iteration 35, loss = 0.59590704\n",
            "Iteration 36, loss = 0.59549283\n",
            "Iteration 37, loss = 0.59542183\n",
            "Iteration 38, loss = 0.59527060\n",
            "Iteration 39, loss = 0.59489087\n",
            "Iteration 40, loss = 0.59472558\n",
            "Iteration 41, loss = 0.59460429\n",
            "Iteration 42, loss = 0.59463118\n",
            "Iteration 43, loss = 0.59537118\n",
            "Iteration 44, loss = 0.59562522\n",
            "Iteration 45, loss = 0.59528059\n",
            "Iteration 46, loss = 0.59565802\n",
            "Iteration 47, loss = 0.59494771\n",
            "Iteration 48, loss = 0.59512221\n",
            "Iteration 49, loss = 0.59614614\n",
            "Iteration 50, loss = 0.59626742\n",
            "Iteration 51, loss = 0.59505590\n",
            "Iteration 52, loss = 0.59583760\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.63640573\n",
            "Iteration 2, loss = 0.62405705\n",
            "Iteration 3, loss = 0.61725900\n",
            "Iteration 4, loss = 0.61295697\n",
            "Iteration 5, loss = 0.61059122\n",
            "Iteration 6, loss = 0.60737332\n",
            "Iteration 7, loss = 0.60431171\n",
            "Iteration 8, loss = 0.60156539\n",
            "Iteration 9, loss = 0.60077419\n",
            "Iteration 10, loss = 0.60032758\n",
            "Iteration 11, loss = 0.60020997\n",
            "Iteration 12, loss = 0.59816069\n",
            "Iteration 13, loss = 0.59856991\n",
            "Iteration 14, loss = 0.59699370\n",
            "Iteration 15, loss = 0.59711691\n",
            "Iteration 16, loss = 0.59760502\n",
            "Iteration 17, loss = 0.59676309\n",
            "Iteration 18, loss = 0.59658816\n",
            "Iteration 19, loss = 0.59595757\n",
            "Iteration 20, loss = 0.59491961\n",
            "Iteration 21, loss = 0.59431184\n",
            "Iteration 22, loss = 0.59426521\n",
            "Iteration 23, loss = 0.59344776\n",
            "Iteration 24, loss = 0.59324322\n",
            "Iteration 25, loss = 0.59215722\n",
            "Iteration 26, loss = 0.59170180\n",
            "Iteration 27, loss = 0.59107264\n",
            "Iteration 28, loss = 0.59066893\n",
            "Iteration 29, loss = 0.59175044\n",
            "Iteration 30, loss = 0.59149832\n",
            "Iteration 31, loss = 0.59034567\n",
            "Iteration 32, loss = 0.59005121\n",
            "Iteration 33, loss = 0.58997658\n",
            "Iteration 34, loss = 0.58996308\n",
            "Iteration 35, loss = 0.58901725\n",
            "Iteration 36, loss = 0.58931373\n",
            "Iteration 37, loss = 0.59098025\n",
            "Iteration 38, loss = 0.58943117\n",
            "Iteration 39, loss = 0.58893967\n",
            "Iteration 40, loss = 0.58908849\n",
            "Iteration 41, loss = 0.58835743\n",
            "Iteration 42, loss = 0.58846149\n",
            "Iteration 43, loss = 0.58789026\n",
            "Iteration 44, loss = 0.58737814\n",
            "Iteration 45, loss = 0.58733486\n",
            "Iteration 46, loss = 0.58791314\n",
            "Iteration 47, loss = 0.58782736\n",
            "Iteration 48, loss = 0.58694859\n",
            "Iteration 49, loss = 0.58739352\n",
            "Iteration 50, loss = 0.58633596\n",
            "Iteration 51, loss = 0.58710841\n",
            "Iteration 52, loss = 0.58644012\n",
            "Iteration 53, loss = 0.58626186\n",
            "Iteration 54, loss = 0.58702185\n",
            "Iteration 55, loss = 0.58574367\n",
            "Iteration 56, loss = 0.58487848\n",
            "Iteration 57, loss = 0.58475998\n",
            "Iteration 58, loss = 0.58501294\n",
            "Iteration 59, loss = 0.58726858\n",
            "Iteration 60, loss = 0.58610783\n",
            "Iteration 61, loss = 0.58543665\n",
            "Iteration 62, loss = 0.58523350\n",
            "Iteration 63, loss = 0.58259227\n",
            "Iteration 64, loss = 0.58371356\n",
            "Iteration 65, loss = 0.58248219\n",
            "Iteration 66, loss = 0.58408726\n",
            "Iteration 67, loss = 0.58728796\n",
            "Iteration 68, loss = 0.58535170\n",
            "Iteration 69, loss = 0.58134156\n",
            "Iteration 70, loss = 0.58213104\n",
            "Iteration 71, loss = 0.58282887\n",
            "Iteration 72, loss = 0.58019427\n",
            "Iteration 73, loss = 0.58021105\n",
            "Iteration 74, loss = 0.57968750\n",
            "Iteration 75, loss = 0.58039464\n",
            "Iteration 76, loss = 0.58073657\n",
            "Iteration 77, loss = 0.57965130\n",
            "Iteration 78, loss = 0.57845345\n",
            "Iteration 79, loss = 0.57797256\n",
            "Iteration 80, loss = 0.57731838\n",
            "Iteration 81, loss = 0.57856280\n",
            "Iteration 82, loss = 0.57722619\n",
            "Iteration 83, loss = 0.57532358\n",
            "Iteration 84, loss = 0.57423411\n",
            "Iteration 85, loss = 0.57478152\n",
            "Iteration 86, loss = 0.57473075\n",
            "Iteration 87, loss = 0.57232575\n",
            "Iteration 88, loss = 0.56940304\n",
            "Iteration 89, loss = 0.57150674\n",
            "Iteration 90, loss = 0.57089486\n",
            "Iteration 91, loss = 0.57740609\n",
            "Iteration 92, loss = 0.57674166\n",
            "Iteration 93, loss = 0.56565814\n",
            "Iteration 94, loss = 0.56684938\n",
            "Iteration 95, loss = 0.56438181\n",
            "Iteration 96, loss = 0.56568835\n",
            "Iteration 97, loss = 0.56093770\n",
            "Iteration 98, loss = 0.56694410\n",
            "Iteration 99, loss = 0.56224764\n",
            "Iteration 100, loss = 0.56580448\n",
            "Iteration 101, loss = 0.55935564\n",
            "Iteration 102, loss = 0.56204341\n",
            "Iteration 103, loss = 0.56151937\n",
            "Iteration 104, loss = 0.55869252\n",
            "Iteration 105, loss = 0.55908981\n",
            "Iteration 106, loss = 0.56077291\n",
            "Iteration 107, loss = 0.55679419\n",
            "Iteration 108, loss = 0.55558704\n",
            "Iteration 109, loss = 0.56485101\n",
            "Iteration 110, loss = 0.55388430\n",
            "Iteration 111, loss = 0.55453041\n",
            "Iteration 112, loss = 0.55245962\n",
            "Iteration 113, loss = 0.55914501\n",
            "Iteration 114, loss = 0.55127591\n",
            "Iteration 115, loss = 0.55091322\n",
            "Iteration 116, loss = 0.55003389\n",
            "Iteration 117, loss = 0.55383004\n",
            "Iteration 118, loss = 0.55231221\n",
            "Iteration 119, loss = 0.55577977\n",
            "Iteration 120, loss = 0.55891631\n",
            "Iteration 121, loss = 0.54708448\n",
            "Iteration 122, loss = 0.55346556\n",
            "Iteration 123, loss = 0.56266975\n",
            "Iteration 124, loss = 0.56894553\n",
            "Iteration 125, loss = 0.54857818\n",
            "Iteration 126, loss = 0.55196066\n",
            "Iteration 127, loss = 0.54458154\n",
            "Iteration 128, loss = 0.54287343\n",
            "Iteration 129, loss = 0.54115020\n",
            "Iteration 130, loss = 0.54212764\n",
            "Iteration 131, loss = 0.54399849\n",
            "Iteration 132, loss = 0.55252820\n",
            "Iteration 133, loss = 0.54380095\n",
            "Iteration 134, loss = 0.54436568\n",
            "Iteration 135, loss = 0.53852515\n",
            "Iteration 136, loss = 0.53938352\n",
            "Iteration 137, loss = 0.53405387\n",
            "Iteration 138, loss = 0.53889878\n",
            "Iteration 139, loss = 0.53574462\n",
            "Iteration 140, loss = 0.53960203\n",
            "Iteration 141, loss = 0.53486934\n",
            "Iteration 142, loss = 0.53342934\n",
            "Iteration 143, loss = 0.55232298\n",
            "Iteration 144, loss = 0.54433322\n",
            "Iteration 145, loss = 0.53736198\n",
            "Iteration 146, loss = 0.53208625\n",
            "Iteration 147, loss = 0.53339491\n",
            "Iteration 148, loss = 0.53265144\n",
            "Iteration 149, loss = 0.52991638\n",
            "Iteration 150, loss = 0.52982587\n",
            "Iteration 151, loss = 0.52651646\n",
            "Iteration 152, loss = 0.53799010\n",
            "Iteration 153, loss = 0.53231057\n",
            "Iteration 154, loss = 0.52299597\n",
            "Iteration 155, loss = 0.52728472\n",
            "Iteration 156, loss = 0.53444238\n",
            "Iteration 157, loss = 0.52507358\n",
            "Iteration 158, loss = 0.52190337\n",
            "Iteration 159, loss = 0.52166533\n",
            "Iteration 160, loss = 0.54283291\n",
            "Iteration 161, loss = 0.51957471\n",
            "Iteration 162, loss = 0.52043314\n",
            "Iteration 163, loss = 0.52097194\n",
            "Iteration 164, loss = 0.52926834\n",
            "Iteration 165, loss = 0.52400765\n",
            "Iteration 166, loss = 0.51538415\n",
            "Iteration 167, loss = 0.52362417\n",
            "Iteration 168, loss = 0.54028524\n",
            "Iteration 169, loss = 0.51966616\n",
            "Iteration 170, loss = 0.51340503\n",
            "Iteration 171, loss = 0.51509120\n",
            "Iteration 172, loss = 0.51640671\n",
            "Iteration 173, loss = 0.51554743\n",
            "Iteration 174, loss = 0.51701363\n",
            "Iteration 175, loss = 0.51186202\n",
            "Iteration 176, loss = 0.50737419\n",
            "Iteration 177, loss = 0.51513668\n",
            "Iteration 178, loss = 0.51250378\n",
            "Iteration 179, loss = 0.50549327\n",
            "Iteration 180, loss = 0.51250795\n",
            "Iteration 181, loss = 0.52133814\n",
            "Iteration 182, loss = 0.51170252\n",
            "Iteration 183, loss = 0.50690513\n",
            "Iteration 184, loss = 0.50915573\n",
            "Iteration 185, loss = 0.50950551\n",
            "Iteration 186, loss = 0.50586695\n",
            "Iteration 187, loss = 0.53527654\n",
            "Iteration 188, loss = 0.51728602\n",
            "Iteration 189, loss = 0.51021987\n",
            "Iteration 190, loss = 0.54570014\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.88446876\n",
            "Iteration 2, loss = 0.97095305\n",
            "Iteration 3, loss = 0.77029692\n",
            "Iteration 4, loss = 0.76184617\n",
            "Iteration 5, loss = 0.70647368\n",
            "Iteration 6, loss = 0.68412931\n",
            "Iteration 7, loss = 0.65372007\n",
            "Iteration 8, loss = 0.63107095\n",
            "Iteration 9, loss = 0.60817835\n",
            "Iteration 10, loss = 0.59947263\n",
            "Iteration 11, loss = 0.63166673\n",
            "Iteration 12, loss = 0.63131229\n",
            "Iteration 13, loss = 0.61936379\n",
            "Iteration 14, loss = 0.57019582\n",
            "Iteration 15, loss = 0.57476704\n",
            "Iteration 16, loss = 0.56079813\n",
            "Iteration 17, loss = 0.55616272\n",
            "Iteration 18, loss = 0.56271426\n",
            "Iteration 19, loss = 0.54698153\n",
            "Iteration 20, loss = 0.61592263\n",
            "Iteration 21, loss = 0.59085864\n",
            "Iteration 22, loss = 0.58568916\n",
            "Iteration 23, loss = 0.53600546\n",
            "Iteration 24, loss = 0.53699433\n",
            "Iteration 25, loss = 0.53238767\n",
            "Iteration 26, loss = 0.52523616\n",
            "Iteration 27, loss = 0.51858459\n",
            "Iteration 28, loss = 0.54135205\n",
            "Iteration 29, loss = 0.56214600\n",
            "Iteration 30, loss = 0.60533833\n",
            "Iteration 31, loss = 0.57613111\n",
            "Iteration 32, loss = 0.54346491\n",
            "Iteration 33, loss = 0.54321850\n",
            "Iteration 34, loss = 0.52007846\n",
            "Iteration 35, loss = 0.50990098\n",
            "Iteration 36, loss = 0.49997731\n",
            "Iteration 37, loss = 0.50133563\n",
            "Iteration 38, loss = 0.49908692\n",
            "Iteration 39, loss = 0.50233095\n",
            "Iteration 40, loss = 0.51337210\n",
            "Iteration 41, loss = 0.49409841\n",
            "Iteration 42, loss = 0.48902380\n",
            "Iteration 43, loss = 0.49021965\n",
            "Iteration 44, loss = 0.48813172\n",
            "Iteration 45, loss = 0.48394315\n",
            "Iteration 46, loss = 0.49711697\n",
            "Iteration 47, loss = 0.50494218\n",
            "Iteration 48, loss = 0.49029675\n",
            "Iteration 49, loss = 0.48855336\n",
            "Iteration 50, loss = 0.48078210\n",
            "Iteration 51, loss = 0.49376426\n",
            "Iteration 52, loss = 0.47254672\n",
            "Iteration 53, loss = 0.47138780\n",
            "Iteration 54, loss = 0.47154212\n",
            "Iteration 55, loss = 0.46575403\n",
            "Iteration 56, loss = 0.46805935\n",
            "Iteration 57, loss = 0.48574826\n",
            "Iteration 58, loss = 0.48646031\n",
            "Iteration 59, loss = 0.47405956\n",
            "Iteration 60, loss = 0.46189525\n",
            "Iteration 61, loss = 0.45958703\n",
            "Iteration 62, loss = 0.46115214\n",
            "Iteration 63, loss = 0.45838132\n",
            "Iteration 64, loss = 0.46014558\n",
            "Iteration 65, loss = 0.45604025\n",
            "Iteration 66, loss = 0.45676428\n",
            "Iteration 67, loss = 0.47822797\n",
            "Iteration 68, loss = 0.46865224\n",
            "Iteration 69, loss = 0.47551930\n",
            "Iteration 70, loss = 0.45613376\n",
            "Iteration 71, loss = 0.47699899\n",
            "Iteration 72, loss = 0.46193884\n",
            "Iteration 73, loss = 0.45494088\n",
            "Iteration 74, loss = 0.47924961\n",
            "Iteration 75, loss = 0.45544480\n",
            "Iteration 76, loss = 0.45672387\n",
            "Iteration 77, loss = 0.47616200\n",
            "Iteration 78, loss = 0.49288786\n",
            "Iteration 79, loss = 0.46786232\n",
            "Iteration 80, loss = 0.45026984\n",
            "Iteration 81, loss = 0.44472526\n",
            "Iteration 82, loss = 0.44558872\n",
            "Iteration 83, loss = 0.44342197\n",
            "Iteration 84, loss = 0.44776160\n",
            "Iteration 85, loss = 0.45596159\n",
            "Iteration 86, loss = 0.44604333\n",
            "Iteration 87, loss = 0.44618623\n",
            "Iteration 88, loss = 0.45137991\n",
            "Iteration 89, loss = 0.44059764\n",
            "Iteration 90, loss = 0.43989886\n",
            "Iteration 91, loss = 0.44485062\n",
            "Iteration 92, loss = 0.45284632\n",
            "Iteration 93, loss = 0.44785118\n",
            "Iteration 94, loss = 0.46561618\n",
            "Iteration 95, loss = 0.44324519\n",
            "Iteration 96, loss = 0.44111948\n",
            "Iteration 97, loss = 0.45607379\n",
            "Iteration 98, loss = 0.46001718\n",
            "Iteration 99, loss = 0.44756290\n",
            "Iteration 100, loss = 0.44277243\n",
            "Iteration 101, loss = 0.43825831\n",
            "Iteration 102, loss = 0.44153951\n",
            "Iteration 103, loss = 0.43447353\n",
            "Iteration 104, loss = 0.43494211\n",
            "Iteration 105, loss = 0.43690587\n",
            "Iteration 106, loss = 0.44518940\n",
            "Iteration 107, loss = 0.43704006\n",
            "Iteration 108, loss = 0.43369528\n",
            "Iteration 109, loss = 0.43090668\n",
            "Iteration 110, loss = 0.43453589\n",
            "Iteration 111, loss = 0.44822568\n",
            "Iteration 112, loss = 0.50931100\n",
            "Iteration 113, loss = 0.48573326\n",
            "Iteration 114, loss = 0.44570427\n",
            "Iteration 115, loss = 0.43742150\n",
            "Iteration 116, loss = 0.43345434\n",
            "Iteration 117, loss = 0.43781260\n",
            "Iteration 118, loss = 0.43221221\n",
            "Iteration 119, loss = 0.45376163\n",
            "Iteration 120, loss = 0.44562525\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.45309678\n",
            "Iteration 2, loss = 1.31721800\n",
            "Iteration 3, loss = 0.76434332\n",
            "Iteration 4, loss = 0.74528561\n",
            "Iteration 5, loss = 0.67843513\n",
            "Iteration 6, loss = 0.63086923\n",
            "Iteration 7, loss = 0.61319125\n",
            "Iteration 8, loss = 0.60175589\n",
            "Iteration 9, loss = 0.60564076\n",
            "Iteration 10, loss = 0.59969417\n",
            "Iteration 11, loss = 0.58973524\n",
            "Iteration 12, loss = 0.58634124\n",
            "Iteration 13, loss = 0.59464063\n",
            "Iteration 14, loss = 0.58771315\n",
            "Iteration 15, loss = 0.57756611\n",
            "Iteration 16, loss = 0.59844714\n",
            "Iteration 17, loss = 0.60942808\n",
            "Iteration 18, loss = 0.60070735\n",
            "Iteration 19, loss = 0.59060218\n",
            "Iteration 20, loss = 0.61688841\n",
            "Iteration 21, loss = 0.59880813\n",
            "Iteration 22, loss = 0.57357971\n",
            "Iteration 23, loss = 0.56529451\n",
            "Iteration 24, loss = 0.56487463\n",
            "Iteration 25, loss = 0.55338047\n",
            "Iteration 26, loss = 0.56591146\n",
            "Iteration 27, loss = 0.55641936\n",
            "Iteration 28, loss = 0.55194186\n",
            "Iteration 29, loss = 0.55192562\n",
            "Iteration 30, loss = 0.54357538\n",
            "Iteration 31, loss = 0.55386023\n",
            "Iteration 32, loss = 0.54469496\n",
            "Iteration 33, loss = 0.56926065\n",
            "Iteration 34, loss = 0.53865472\n",
            "Iteration 35, loss = 0.53314463\n",
            "Iteration 36, loss = 0.53485398\n",
            "Iteration 37, loss = 0.52973508\n",
            "Iteration 38, loss = 0.53754334\n",
            "Iteration 39, loss = 0.53156259\n",
            "Iteration 40, loss = 0.52254817\n",
            "Iteration 41, loss = 0.53312118\n",
            "Iteration 42, loss = 0.52197590\n",
            "Iteration 43, loss = 0.52008682\n",
            "Iteration 44, loss = 0.51125948\n",
            "Iteration 45, loss = 0.51949470\n",
            "Iteration 46, loss = 0.52588080\n",
            "Iteration 47, loss = 0.50121658\n",
            "Iteration 48, loss = 0.49888735\n",
            "Iteration 49, loss = 0.49940773\n",
            "Iteration 50, loss = 0.51164007\n",
            "Iteration 51, loss = 0.55548450\n",
            "Iteration 52, loss = 0.50653701\n",
            "Iteration 53, loss = 0.51139064\n",
            "Iteration 54, loss = 0.48787562\n",
            "Iteration 55, loss = 0.51136712\n",
            "Iteration 56, loss = 0.48948027\n",
            "Iteration 57, loss = 0.48810691\n",
            "Iteration 58, loss = 0.49927723\n",
            "Iteration 59, loss = 0.49191781\n",
            "Iteration 60, loss = 0.50695958\n",
            "Iteration 61, loss = 0.50487581\n",
            "Iteration 62, loss = 0.50946558\n",
            "Iteration 63, loss = 0.47470697\n",
            "Iteration 64, loss = 0.47249553\n",
            "Iteration 65, loss = 0.47678726\n",
            "Iteration 66, loss = 0.46725648\n",
            "Iteration 67, loss = 0.46868305\n",
            "Iteration 68, loss = 0.46224052\n",
            "Iteration 69, loss = 0.46976907\n",
            "Iteration 70, loss = 0.46084828\n",
            "Iteration 71, loss = 0.45965427\n",
            "Iteration 72, loss = 0.46329263\n",
            "Iteration 73, loss = 0.46240686\n",
            "Iteration 74, loss = 0.46396396\n",
            "Iteration 75, loss = 0.45985744\n",
            "Iteration 76, loss = 0.45477363\n",
            "Iteration 77, loss = 0.45741103\n",
            "Iteration 78, loss = 0.45872904\n",
            "Iteration 79, loss = 0.45262422\n",
            "Iteration 80, loss = 0.44904160\n",
            "Iteration 81, loss = 0.45982251\n",
            "Iteration 82, loss = 0.49483902\n",
            "Iteration 83, loss = 0.67871982\n",
            "Iteration 84, loss = 0.59449892\n",
            "Iteration 85, loss = 0.48374461\n",
            "Iteration 86, loss = 0.46213037\n",
            "Iteration 87, loss = 0.45648695\n",
            "Iteration 88, loss = 0.45836846\n",
            "Iteration 89, loss = 0.47133172\n",
            "Iteration 90, loss = 0.45151635\n",
            "Iteration 91, loss = 0.44506456\n",
            "Iteration 92, loss = 0.43999865\n",
            "Iteration 93, loss = 0.45697620\n",
            "Iteration 94, loss = 0.48917345\n",
            "Iteration 95, loss = 0.48611435\n",
            "Iteration 96, loss = 0.45151632\n",
            "Iteration 97, loss = 0.46605722\n",
            "Iteration 98, loss = 0.44929961\n",
            "Iteration 99, loss = 0.46257455\n",
            "Iteration 100, loss = 0.50139679\n",
            "Iteration 101, loss = 0.60063934\n",
            "Iteration 102, loss = 0.54321377\n",
            "Iteration 103, loss = 0.49703675\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.50611598\n",
            "Iteration 2, loss = 1.30967806\n",
            "Iteration 3, loss = 1.01302166\n",
            "Iteration 4, loss = 0.79839894\n",
            "Iteration 5, loss = 0.68812169\n",
            "Iteration 6, loss = 0.66164482\n",
            "Iteration 7, loss = 0.64430237\n",
            "Iteration 8, loss = 0.63213278\n",
            "Iteration 9, loss = 0.62254592\n",
            "Iteration 10, loss = 0.62247839\n",
            "Iteration 11, loss = 0.61438007\n",
            "Iteration 12, loss = 0.61075096\n",
            "Iteration 13, loss = 0.60900048\n",
            "Iteration 14, loss = 0.60190146\n",
            "Iteration 15, loss = 0.59985267\n",
            "Iteration 16, loss = 0.60195608\n",
            "Iteration 17, loss = 0.59541504\n",
            "Iteration 18, loss = 0.59175699\n",
            "Iteration 19, loss = 0.58695019\n",
            "Iteration 20, loss = 0.58942897\n",
            "Iteration 21, loss = 0.58558915\n",
            "Iteration 22, loss = 0.58309161\n",
            "Iteration 23, loss = 0.57927221\n",
            "Iteration 24, loss = 0.57865167\n",
            "Iteration 25, loss = 0.57682654\n",
            "Iteration 26, loss = 0.57089956\n",
            "Iteration 27, loss = 0.56546570\n",
            "Iteration 28, loss = 0.56354805\n",
            "Iteration 29, loss = 0.56074334\n",
            "Iteration 30, loss = 0.55728525\n",
            "Iteration 31, loss = 0.55227404\n",
            "Iteration 32, loss = 0.54960869\n",
            "Iteration 33, loss = 0.55206433\n",
            "Iteration 34, loss = 0.54627697\n",
            "Iteration 35, loss = 0.54636561\n",
            "Iteration 36, loss = 0.53860155\n",
            "Iteration 37, loss = 0.53724381\n",
            "Iteration 38, loss = 0.53228272\n",
            "Iteration 39, loss = 0.53188329\n",
            "Iteration 40, loss = 0.52585619\n",
            "Iteration 41, loss = 0.52423857\n",
            "Iteration 42, loss = 0.52282787\n",
            "Iteration 43, loss = 0.52457843\n",
            "Iteration 44, loss = 0.53183468\n",
            "Iteration 45, loss = 0.54277554\n",
            "Iteration 46, loss = 0.53656280\n",
            "Iteration 47, loss = 0.50991822\n",
            "Iteration 48, loss = 0.50888483\n",
            "Iteration 49, loss = 0.51167928\n",
            "Iteration 50, loss = 0.50645272\n",
            "Iteration 51, loss = 0.55265101\n",
            "Iteration 52, loss = 0.50765732\n",
            "Iteration 53, loss = 0.50765815\n",
            "Iteration 54, loss = 0.49916383\n",
            "Iteration 55, loss = 0.50835143\n",
            "Iteration 56, loss = 0.48840499\n",
            "Iteration 57, loss = 0.48369429\n",
            "Iteration 58, loss = 0.47918242\n",
            "Iteration 59, loss = 0.47618147\n",
            "Iteration 60, loss = 0.48802506\n",
            "Iteration 61, loss = 0.48764724\n",
            "Iteration 62, loss = 0.47296512\n",
            "Iteration 63, loss = 0.46407214\n",
            "Iteration 64, loss = 0.47031560\n",
            "Iteration 65, loss = 0.49713349\n",
            "Iteration 66, loss = 0.47442337\n",
            "Iteration 67, loss = 0.46384208\n",
            "Iteration 68, loss = 0.46816484\n",
            "Iteration 69, loss = 0.46048542\n",
            "Iteration 70, loss = 0.46175465\n",
            "Iteration 71, loss = 0.45863785\n",
            "Iteration 72, loss = 0.47242006\n",
            "Iteration 73, loss = 0.47063782\n",
            "Iteration 74, loss = 0.46317053\n",
            "Iteration 75, loss = 0.45921716\n",
            "Iteration 76, loss = 0.45154138\n",
            "Iteration 77, loss = 0.45282217\n",
            "Iteration 78, loss = 0.49754117\n",
            "Iteration 79, loss = 0.50100771\n",
            "Iteration 80, loss = 0.46320110\n",
            "Iteration 81, loss = 0.46203037\n",
            "Iteration 82, loss = 0.45513601\n",
            "Iteration 83, loss = 0.45384051\n",
            "Iteration 84, loss = 0.44906310\n",
            "Iteration 85, loss = 0.45166842\n",
            "Iteration 86, loss = 0.46414754\n",
            "Iteration 87, loss = 0.48601495\n",
            "Iteration 88, loss = 0.45772102\n",
            "Iteration 89, loss = 0.44964979\n",
            "Iteration 90, loss = 0.44999503\n",
            "Iteration 91, loss = 0.46434580\n",
            "Iteration 92, loss = 0.44309336\n",
            "Iteration 93, loss = 0.44104662\n",
            "Iteration 94, loss = 0.44688831\n",
            "Iteration 95, loss = 0.46266597\n",
            "Iteration 96, loss = 0.45118964\n",
            "Iteration 97, loss = 0.44836174\n",
            "Iteration 98, loss = 0.44524267\n",
            "Iteration 99, loss = 0.44163003\n",
            "Iteration 100, loss = 0.44360111\n",
            "Iteration 101, loss = 0.44050094\n",
            "Iteration 102, loss = 0.46301182\n",
            "Iteration 103, loss = 0.44671674\n",
            "Iteration 104, loss = 0.45180717\n",
            "Iteration 105, loss = 0.44745995\n",
            "Iteration 106, loss = 0.43882743\n",
            "Iteration 107, loss = 0.43683792\n",
            "Iteration 108, loss = 0.43580610\n",
            "Iteration 109, loss = 0.44110742\n",
            "Iteration 110, loss = 0.45344713\n",
            "Iteration 111, loss = 0.43750136\n",
            "Iteration 112, loss = 0.43898614\n",
            "Iteration 113, loss = 0.44237204\n",
            "Iteration 114, loss = 0.43833299\n",
            "Iteration 115, loss = 0.44120624\n",
            "Iteration 116, loss = 0.44380845\n",
            "Iteration 117, loss = 0.46194658\n",
            "Iteration 118, loss = 0.53230905\n",
            "Iteration 119, loss = 0.50703093\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.91365040\n",
            "Iteration 2, loss = 0.72595228\n",
            "Iteration 3, loss = 0.65226079\n",
            "Iteration 4, loss = 0.63269316\n",
            "Iteration 5, loss = 0.60920824\n",
            "Iteration 6, loss = 0.59851824\n",
            "Iteration 7, loss = 0.59664657\n",
            "Iteration 8, loss = 0.59263239\n",
            "Iteration 9, loss = 0.58828393\n",
            "Iteration 10, loss = 0.59034765\n",
            "Iteration 11, loss = 0.58741933\n",
            "Iteration 12, loss = 0.59036329\n",
            "Iteration 13, loss = 0.57912885\n",
            "Iteration 14, loss = 0.57300979\n",
            "Iteration 15, loss = 0.57372509\n",
            "Iteration 16, loss = 0.57347615\n",
            "Iteration 17, loss = 0.57075581\n",
            "Iteration 18, loss = 0.56711705\n",
            "Iteration 19, loss = 0.56434948\n",
            "Iteration 20, loss = 0.56426527\n",
            "Iteration 21, loss = 0.55825216\n",
            "Iteration 22, loss = 0.55622044\n",
            "Iteration 23, loss = 0.55422841\n",
            "Iteration 24, loss = 0.55083328\n",
            "Iteration 25, loss = 0.54905665\n",
            "Iteration 26, loss = 0.54976995\n",
            "Iteration 27, loss = 0.54208235\n",
            "Iteration 28, loss = 0.54339155\n",
            "Iteration 29, loss = 0.53885347\n",
            "Iteration 30, loss = 0.54274882\n",
            "Iteration 31, loss = 0.53576450\n",
            "Iteration 32, loss = 0.53473835\n",
            "Iteration 33, loss = 0.53389768\n",
            "Iteration 34, loss = 0.52637614\n",
            "Iteration 35, loss = 0.52463330\n",
            "Iteration 36, loss = 0.52562279\n",
            "Iteration 37, loss = 0.52673669\n",
            "Iteration 38, loss = 0.52451080\n",
            "Iteration 39, loss = 0.51939437\n",
            "Iteration 40, loss = 0.52060226\n",
            "Iteration 41, loss = 0.51191317\n",
            "Iteration 42, loss = 0.51461946\n",
            "Iteration 43, loss = 0.51717841\n",
            "Iteration 44, loss = 0.51670263\n",
            "Iteration 45, loss = 0.50641622\n",
            "Iteration 46, loss = 0.51089612\n",
            "Iteration 47, loss = 0.55876582\n",
            "Iteration 48, loss = 0.55855590\n",
            "Iteration 49, loss = 0.50890937\n",
            "Iteration 50, loss = 0.49997800\n",
            "Iteration 51, loss = 0.49303406\n",
            "Iteration 52, loss = 0.49039605\n",
            "Iteration 53, loss = 0.48998272\n",
            "Iteration 54, loss = 0.49014251\n",
            "Iteration 55, loss = 0.48673911\n",
            "Iteration 56, loss = 0.48195334\n",
            "Iteration 57, loss = 0.48811926\n",
            "Iteration 58, loss = 0.48342559\n",
            "Iteration 59, loss = 0.48169445\n",
            "Iteration 60, loss = 0.48065106\n",
            "Iteration 61, loss = 0.47780728\n",
            "Iteration 62, loss = 0.47510502\n",
            "Iteration 63, loss = 0.47296576\n",
            "Iteration 64, loss = 0.47664751\n",
            "Iteration 65, loss = 0.47446594\n",
            "Iteration 66, loss = 0.47507853\n",
            "Iteration 67, loss = 0.46830359\n",
            "Iteration 68, loss = 0.47409651\n",
            "Iteration 69, loss = 0.46324385\n",
            "Iteration 70, loss = 0.46541787\n",
            "Iteration 71, loss = 0.46601250\n",
            "Iteration 72, loss = 0.51227054\n",
            "Iteration 73, loss = 0.52334974\n",
            "Iteration 74, loss = 0.48063007\n",
            "Iteration 75, loss = 0.46033938\n",
            "Iteration 76, loss = 0.46870917\n",
            "Iteration 77, loss = 0.46559237\n",
            "Iteration 78, loss = 0.45759931\n",
            "Iteration 79, loss = 0.45563598\n",
            "Iteration 80, loss = 0.45923524\n",
            "Iteration 81, loss = 0.45326384\n",
            "Iteration 82, loss = 0.47152602\n",
            "Iteration 83, loss = 0.46022967\n",
            "Iteration 84, loss = 0.45096016\n",
            "Iteration 85, loss = 0.45269251\n",
            "Iteration 86, loss = 0.45023012\n",
            "Iteration 87, loss = 0.44834965\n",
            "Iteration 88, loss = 0.45796787\n",
            "Iteration 89, loss = 0.45810231\n",
            "Iteration 90, loss = 0.45883621\n",
            "Iteration 91, loss = 0.45747583\n",
            "Iteration 92, loss = 0.44508572\n",
            "Iteration 93, loss = 0.44589078\n",
            "Iteration 94, loss = 0.44771778\n",
            "Iteration 95, loss = 0.45663982\n",
            "Iteration 96, loss = 0.44837876\n",
            "Iteration 97, loss = 0.44784846\n",
            "Iteration 98, loss = 0.45484299\n",
            "Iteration 99, loss = 0.44354722\n",
            "Iteration 100, loss = 0.44394716\n",
            "Iteration 101, loss = 0.44274768\n",
            "Iteration 102, loss = 0.44086018\n",
            "Iteration 103, loss = 0.44233915\n",
            "Iteration 104, loss = 0.44127817\n",
            "Iteration 105, loss = 0.43884127\n",
            "Iteration 106, loss = 0.43749044\n",
            "Iteration 107, loss = 0.43868478\n",
            "Iteration 108, loss = 0.43470318\n",
            "Iteration 109, loss = 0.43448771\n",
            "Iteration 110, loss = 0.43852778\n",
            "Iteration 111, loss = 0.43397754\n",
            "Iteration 112, loss = 0.48337676\n",
            "Iteration 113, loss = 0.46648144\n",
            "Iteration 114, loss = 0.44263115\n",
            "Iteration 115, loss = 0.43020209\n",
            "Iteration 116, loss = 0.43233200\n",
            "Iteration 117, loss = 0.43519264\n",
            "Iteration 118, loss = 0.43196844\n",
            "Iteration 119, loss = 0.44014426\n",
            "Iteration 120, loss = 0.43866009\n",
            "Iteration 121, loss = 0.42927802\n",
            "Iteration 122, loss = 0.43047801\n",
            "Iteration 123, loss = 0.42595397\n",
            "Iteration 124, loss = 0.42789582\n",
            "Iteration 125, loss = 0.42609739\n",
            "Iteration 126, loss = 0.43878018\n",
            "Iteration 127, loss = 0.43514896\n",
            "Iteration 128, loss = 0.43106278\n",
            "Iteration 129, loss = 0.43743054\n",
            "Iteration 130, loss = 0.43649193\n",
            "Iteration 131, loss = 0.42466460\n",
            "Iteration 132, loss = 0.43051041\n",
            "Iteration 133, loss = 0.42974015\n",
            "Iteration 134, loss = 0.42437467\n",
            "Iteration 135, loss = 0.42951004\n",
            "Iteration 136, loss = 0.42963098\n",
            "Iteration 137, loss = 0.42171364\n",
            "Iteration 138, loss = 0.42434299\n",
            "Iteration 139, loss = 0.42355362\n",
            "Iteration 140, loss = 0.42107560\n",
            "Iteration 141, loss = 0.42419130\n",
            "Iteration 142, loss = 0.42544978\n",
            "Iteration 143, loss = 0.42567552\n",
            "Iteration 144, loss = 0.42286151\n",
            "Iteration 145, loss = 0.42956727\n",
            "Iteration 146, loss = 0.41888724\n",
            "Iteration 147, loss = 0.41931258\n",
            "Iteration 148, loss = 0.42666866\n",
            "Iteration 149, loss = 0.42472486\n",
            "Iteration 150, loss = 0.42601331\n",
            "Iteration 151, loss = 0.42029585\n",
            "Iteration 152, loss = 0.41984580\n",
            "Iteration 153, loss = 0.43374805\n",
            "Iteration 154, loss = 0.43539627\n",
            "Iteration 155, loss = 0.43644480\n",
            "Iteration 156, loss = 0.42273200\n",
            "Iteration 157, loss = 0.41846215\n",
            "Iteration 158, loss = 0.41986636\n",
            "Iteration 159, loss = 0.42127259\n",
            "Iteration 160, loss = 0.42980935\n",
            "Iteration 161, loss = 0.41267609\n",
            "Iteration 162, loss = 0.41649120\n",
            "Iteration 163, loss = 0.41540304\n",
            "Iteration 164, loss = 0.41662727\n",
            "Iteration 165, loss = 0.41961684\n",
            "Iteration 166, loss = 0.42662398\n",
            "Iteration 167, loss = 0.43549884\n",
            "Iteration 168, loss = 0.41388250\n",
            "Iteration 169, loss = 0.41550408\n",
            "Iteration 170, loss = 0.41263871\n",
            "Iteration 171, loss = 0.42426278\n",
            "Iteration 172, loss = 0.41548341\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.55699782\n",
            "Iteration 2, loss = 0.85352218\n",
            "Iteration 3, loss = 0.73471486\n",
            "Iteration 4, loss = 0.67107096\n",
            "Iteration 5, loss = 0.63800248\n",
            "Iteration 6, loss = 0.63461662\n",
            "Iteration 7, loss = 0.61399325\n",
            "Iteration 8, loss = 0.60922707\n",
            "Iteration 9, loss = 0.60115081\n",
            "Iteration 10, loss = 0.59708580\n",
            "Iteration 11, loss = 0.59709560\n",
            "Iteration 12, loss = 0.58929857\n",
            "Iteration 13, loss = 0.57912704\n",
            "Iteration 14, loss = 0.57559498\n",
            "Iteration 15, loss = 0.57666044\n",
            "Iteration 16, loss = 0.56015375\n",
            "Iteration 17, loss = 0.55486088\n",
            "Iteration 18, loss = 0.55920103\n",
            "Iteration 19, loss = 0.56427466\n",
            "Iteration 20, loss = 0.54566478\n",
            "Iteration 21, loss = 0.57225812\n",
            "Iteration 22, loss = 0.53484490\n",
            "Iteration 23, loss = 0.55391893\n",
            "Iteration 24, loss = 0.53804750\n",
            "Iteration 25, loss = 0.52727372\n",
            "Iteration 26, loss = 0.53433511\n",
            "Iteration 27, loss = 0.57402502\n",
            "Iteration 28, loss = 0.56387138\n",
            "Iteration 29, loss = 0.52398212\n",
            "Iteration 30, loss = 0.51987898\n",
            "Iteration 31, loss = 0.52026885\n",
            "Iteration 32, loss = 0.51317429\n",
            "Iteration 33, loss = 0.50449666\n",
            "Iteration 34, loss = 0.51358025\n",
            "Iteration 35, loss = 0.50013548\n",
            "Iteration 36, loss = 0.50081802\n",
            "Iteration 37, loss = 0.50242489\n",
            "Iteration 38, loss = 0.49889971\n",
            "Iteration 39, loss = 0.51350248\n",
            "Iteration 40, loss = 0.49443319\n",
            "Iteration 41, loss = 0.49452631\n",
            "Iteration 42, loss = 0.55388918\n",
            "Iteration 43, loss = 0.50830221\n",
            "Iteration 44, loss = 0.48868219\n",
            "Iteration 45, loss = 0.49418221\n",
            "Iteration 46, loss = 0.48629696\n",
            "Iteration 47, loss = 0.50838731\n",
            "Iteration 48, loss = 0.48331348\n",
            "Iteration 49, loss = 0.50426482\n",
            "Iteration 50, loss = 0.47778561\n",
            "Iteration 51, loss = 0.49108838\n",
            "Iteration 52, loss = 0.52842311\n",
            "Iteration 53, loss = 0.52639713\n",
            "Iteration 54, loss = 0.49077804\n",
            "Iteration 55, loss = 0.48608344\n",
            "Iteration 56, loss = 0.47427952\n",
            "Iteration 57, loss = 0.47412876\n",
            "Iteration 58, loss = 0.48243408\n",
            "Iteration 59, loss = 0.47498964\n",
            "Iteration 60, loss = 0.47033239\n",
            "Iteration 61, loss = 0.47183519\n",
            "Iteration 62, loss = 0.46880939\n",
            "Iteration 63, loss = 0.46919456\n",
            "Iteration 64, loss = 0.47537406\n",
            "Iteration 65, loss = 0.48218575\n",
            "Iteration 66, loss = 0.50649918\n",
            "Iteration 67, loss = 0.50803460\n",
            "Iteration 68, loss = 0.49986232\n",
            "Iteration 69, loss = 0.47850971\n",
            "Iteration 70, loss = 0.46771712\n",
            "Iteration 71, loss = 0.46372102\n",
            "Iteration 72, loss = 0.47554016\n",
            "Iteration 73, loss = 0.49334259\n",
            "Iteration 74, loss = 0.47691459\n",
            "Iteration 75, loss = 0.46767516\n",
            "Iteration 76, loss = 0.47619990\n",
            "Iteration 77, loss = 0.48075658\n",
            "Iteration 78, loss = 0.46278252\n",
            "Iteration 79, loss = 0.45790980\n",
            "Iteration 80, loss = 0.47287075\n",
            "Iteration 81, loss = 0.46485727\n",
            "Iteration 82, loss = 0.45624048\n",
            "Iteration 83, loss = 0.48101537\n",
            "Iteration 84, loss = 0.51906750\n",
            "Iteration 85, loss = 0.47787886\n",
            "Iteration 86, loss = 0.47847204\n",
            "Iteration 87, loss = 0.45633830\n",
            "Iteration 88, loss = 0.45647230\n",
            "Iteration 89, loss = 0.48445635\n",
            "Iteration 90, loss = 0.47683381\n",
            "Iteration 91, loss = 0.47611058\n",
            "Iteration 92, loss = 0.46143264\n",
            "Iteration 93, loss = 0.45225036\n",
            "Iteration 94, loss = 0.45173011\n",
            "Iteration 95, loss = 0.44958173\n",
            "Iteration 96, loss = 0.44778261\n",
            "Iteration 97, loss = 0.45475034\n",
            "Iteration 98, loss = 0.45615378\n",
            "Iteration 99, loss = 0.44679843\n",
            "Iteration 100, loss = 0.44592987\n",
            "Iteration 101, loss = 0.47205086\n",
            "Iteration 102, loss = 0.45049636\n",
            "Iteration 103, loss = 0.44986573\n",
            "Iteration 104, loss = 0.45705748\n",
            "Iteration 105, loss = 0.44971030\n",
            "Iteration 106, loss = 0.44553859\n",
            "Iteration 107, loss = 0.45147801\n",
            "Iteration 108, loss = 0.45197994\n",
            "Iteration 109, loss = 0.45148365\n",
            "Iteration 110, loss = 0.44973339\n",
            "Iteration 111, loss = 0.44964958\n",
            "Iteration 112, loss = 0.44803791\n",
            "Iteration 113, loss = 0.45413931\n",
            "Iteration 114, loss = 0.45197432\n",
            "Iteration 115, loss = 0.46879928\n",
            "Iteration 116, loss = 0.46279484\n",
            "Iteration 117, loss = 0.46826672\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66039619\n",
            "Iteration 2, loss = 0.61587908\n",
            "Iteration 3, loss = 0.56551445\n",
            "Iteration 4, loss = 0.53186524\n",
            "Iteration 5, loss = 0.55238933\n",
            "Iteration 6, loss = 0.50313331\n",
            "Iteration 7, loss = 0.53943300\n",
            "Iteration 8, loss = 0.50707258\n",
            "Iteration 9, loss = 0.50104008\n",
            "Iteration 10, loss = 0.50579950\n",
            "Iteration 11, loss = 0.48325978\n",
            "Iteration 12, loss = 0.46237070\n",
            "Iteration 13, loss = 0.45912147\n",
            "Iteration 14, loss = 0.45872551\n",
            "Iteration 15, loss = 0.45498465\n",
            "Iteration 16, loss = 0.45947406\n",
            "Iteration 17, loss = 0.44572626\n",
            "Iteration 18, loss = 0.43697982\n",
            "Iteration 19, loss = 0.44005879\n",
            "Iteration 20, loss = 0.46945884\n",
            "Iteration 21, loss = 0.44355183\n",
            "Iteration 22, loss = 0.43346500\n",
            "Iteration 23, loss = 0.43356460\n",
            "Iteration 24, loss = 0.41687198\n",
            "Iteration 25, loss = 0.44665998\n",
            "Iteration 26, loss = 0.42871887\n",
            "Iteration 27, loss = 0.43656236\n",
            "Iteration 28, loss = 0.43494735\n",
            "Iteration 29, loss = 0.40976919\n",
            "Iteration 30, loss = 0.42402898\n",
            "Iteration 31, loss = 0.41358085\n",
            "Iteration 32, loss = 0.41960086\n",
            "Iteration 33, loss = 0.42258376\n",
            "Iteration 34, loss = 0.42694956\n",
            "Iteration 35, loss = 0.42649343\n",
            "Iteration 36, loss = 0.41477527\n",
            "Iteration 37, loss = 0.41357424\n",
            "Iteration 38, loss = 0.42184386\n",
            "Iteration 39, loss = 0.43239247\n",
            "Iteration 40, loss = 0.40653934\n",
            "Iteration 41, loss = 0.40160600\n",
            "Iteration 42, loss = 0.40481422\n",
            "Iteration 43, loss = 0.41528005\n",
            "Iteration 44, loss = 0.40422150\n",
            "Iteration 45, loss = 0.40439736\n",
            "Iteration 46, loss = 0.40477507\n",
            "Iteration 47, loss = 0.40223883\n",
            "Iteration 48, loss = 0.40737525\n",
            "Iteration 49, loss = 0.39824004\n",
            "Iteration 50, loss = 0.41169098\n",
            "Iteration 51, loss = 0.39485898\n",
            "Iteration 52, loss = 0.41250602\n",
            "Iteration 53, loss = 0.41535870\n",
            "Iteration 54, loss = 0.39251529\n",
            "Iteration 55, loss = 0.39233013\n",
            "Iteration 56, loss = 0.39366629\n",
            "Iteration 57, loss = 0.38887080\n",
            "Iteration 58, loss = 0.39175254\n",
            "Iteration 59, loss = 0.39680513\n",
            "Iteration 60, loss = 0.38959482\n",
            "Iteration 61, loss = 0.41323451\n",
            "Iteration 62, loss = 0.40708786\n",
            "Iteration 63, loss = 0.38553151\n",
            "Iteration 64, loss = 0.38102306\n",
            "Iteration 65, loss = 0.39060132\n",
            "Iteration 66, loss = 0.38808435\n",
            "Iteration 67, loss = 0.39345412\n",
            "Iteration 68, loss = 0.38575794\n",
            "Iteration 69, loss = 0.38777345\n",
            "Iteration 70, loss = 0.38740780\n",
            "Iteration 71, loss = 0.37956843\n",
            "Iteration 72, loss = 0.38222600\n",
            "Iteration 73, loss = 0.37281691\n",
            "Iteration 74, loss = 0.38035573\n",
            "Iteration 75, loss = 0.37342986\n",
            "Iteration 76, loss = 0.36470782\n",
            "Iteration 77, loss = 0.37598355\n",
            "Iteration 78, loss = 0.36790966\n",
            "Iteration 79, loss = 0.38537498\n",
            "Iteration 80, loss = 0.37247414\n",
            "Iteration 81, loss = 0.37355189\n",
            "Iteration 82, loss = 0.36514087\n",
            "Iteration 83, loss = 0.37349373\n",
            "Iteration 84, loss = 0.38583373\n",
            "Iteration 85, loss = 0.38236884\n",
            "Iteration 86, loss = 0.37485120\n",
            "Iteration 87, loss = 0.35962559\n",
            "Iteration 88, loss = 0.36758146\n",
            "Iteration 89, loss = 0.35877000\n",
            "Iteration 90, loss = 0.36302264\n",
            "Iteration 91, loss = 0.36378554\n",
            "Iteration 92, loss = 0.39015321\n",
            "Iteration 93, loss = 0.38021050\n",
            "Iteration 94, loss = 0.36937510\n",
            "Iteration 95, loss = 0.37808432\n",
            "Iteration 96, loss = 0.37903754\n",
            "Iteration 97, loss = 0.36583195\n",
            "Iteration 98, loss = 0.35963041\n",
            "Iteration 99, loss = 0.36181323\n",
            "Iteration 100, loss = 0.37105769\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66236428\n",
            "Iteration 2, loss = 0.58457617\n",
            "Iteration 3, loss = 0.53935163\n",
            "Iteration 4, loss = 0.53241886\n",
            "Iteration 5, loss = 0.52107352\n",
            "Iteration 6, loss = 0.49361636\n",
            "Iteration 7, loss = 0.50878886\n",
            "Iteration 8, loss = 0.47596821\n",
            "Iteration 9, loss = 0.48767747\n",
            "Iteration 10, loss = 0.46975252\n",
            "Iteration 11, loss = 0.46411367\n",
            "Iteration 12, loss = 0.47364356\n",
            "Iteration 13, loss = 0.46700213\n",
            "Iteration 14, loss = 0.44121589\n",
            "Iteration 15, loss = 0.44655696\n",
            "Iteration 16, loss = 0.44714110\n",
            "Iteration 17, loss = 0.44825717\n",
            "Iteration 18, loss = 0.43604697\n",
            "Iteration 19, loss = 0.44190292\n",
            "Iteration 20, loss = 0.41812303\n",
            "Iteration 21, loss = 0.45561959\n",
            "Iteration 22, loss = 0.44417074\n",
            "Iteration 23, loss = 0.43503628\n",
            "Iteration 24, loss = 0.45263619\n",
            "Iteration 25, loss = 0.42171287\n",
            "Iteration 26, loss = 0.44254672\n",
            "Iteration 27, loss = 0.43353792\n",
            "Iteration 28, loss = 0.42083375\n",
            "Iteration 29, loss = 0.44163382\n",
            "Iteration 30, loss = 0.40803616\n",
            "Iteration 31, loss = 0.43020260\n",
            "Iteration 32, loss = 0.45397871\n",
            "Iteration 33, loss = 0.42584697\n",
            "Iteration 34, loss = 0.41094286\n",
            "Iteration 35, loss = 0.41347771\n",
            "Iteration 36, loss = 0.41823476\n",
            "Iteration 37, loss = 0.41450648\n",
            "Iteration 38, loss = 0.40624876\n",
            "Iteration 39, loss = 0.42011024\n",
            "Iteration 40, loss = 0.41570570\n",
            "Iteration 41, loss = 0.39946501\n",
            "Iteration 42, loss = 0.42262430\n",
            "Iteration 43, loss = 0.41060628\n",
            "Iteration 44, loss = 0.40044586\n",
            "Iteration 45, loss = 0.40975872\n",
            "Iteration 46, loss = 0.40637132\n",
            "Iteration 47, loss = 0.39675148\n",
            "Iteration 48, loss = 0.41497477\n",
            "Iteration 49, loss = 0.40469725\n",
            "Iteration 50, loss = 0.40363034\n",
            "Iteration 51, loss = 0.39619998\n",
            "Iteration 52, loss = 0.41289237\n",
            "Iteration 53, loss = 0.40682085\n",
            "Iteration 54, loss = 0.39715325\n",
            "Iteration 55, loss = 0.40327266\n",
            "Iteration 56, loss = 0.40439976\n",
            "Iteration 57, loss = 0.42514255\n",
            "Iteration 58, loss = 0.40501700\n",
            "Iteration 59, loss = 0.39844085\n",
            "Iteration 60, loss = 0.39541004\n",
            "Iteration 61, loss = 0.38762515\n",
            "Iteration 62, loss = 0.39701114\n",
            "Iteration 63, loss = 0.38817156\n",
            "Iteration 64, loss = 0.38867843\n",
            "Iteration 65, loss = 0.40785748\n",
            "Iteration 66, loss = 0.39323305\n",
            "Iteration 67, loss = 0.39934678\n",
            "Iteration 68, loss = 0.39157304\n",
            "Iteration 69, loss = 0.38654591\n",
            "Iteration 70, loss = 0.39654446\n",
            "Iteration 71, loss = 0.38296585\n",
            "Iteration 72, loss = 0.38859026\n",
            "Iteration 73, loss = 0.39942285\n",
            "Iteration 74, loss = 0.38806922\n",
            "Iteration 75, loss = 0.38140905\n",
            "Iteration 76, loss = 0.38439627\n",
            "Iteration 77, loss = 0.37906998\n",
            "Iteration 78, loss = 0.40459548\n",
            "Iteration 79, loss = 0.37587937\n",
            "Iteration 80, loss = 0.37896782\n",
            "Iteration 81, loss = 0.38991035\n",
            "Iteration 82, loss = 0.38174084\n",
            "Iteration 83, loss = 0.40164969\n",
            "Iteration 84, loss = 0.37595427\n",
            "Iteration 85, loss = 0.37166426\n",
            "Iteration 86, loss = 0.37309693\n",
            "Iteration 87, loss = 0.37572001\n",
            "Iteration 88, loss = 0.37867432\n",
            "Iteration 89, loss = 0.36902963\n",
            "Iteration 90, loss = 0.38658217\n",
            "Iteration 91, loss = 0.39125945\n",
            "Iteration 92, loss = 0.36093314\n",
            "Iteration 93, loss = 0.38892257\n",
            "Iteration 94, loss = 0.39681668\n",
            "Iteration 95, loss = 0.39981588\n",
            "Iteration 96, loss = 0.38955952\n",
            "Iteration 97, loss = 0.38389276\n",
            "Iteration 98, loss = 0.38678873\n",
            "Iteration 99, loss = 0.37330973\n",
            "Iteration 100, loss = 0.36395546\n",
            "Iteration 101, loss = 0.37592228\n",
            "Iteration 102, loss = 0.36917220\n",
            "Iteration 103, loss = 0.37376593\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.65476821\n",
            "Iteration 2, loss = 0.61028702\n",
            "Iteration 3, loss = 0.58253074\n",
            "Iteration 4, loss = 0.56660623\n",
            "Iteration 5, loss = 0.58594302\n",
            "Iteration 6, loss = 0.53595544\n",
            "Iteration 7, loss = 0.53851725\n",
            "Iteration 8, loss = 0.49556117\n",
            "Iteration 9, loss = 0.48810153\n",
            "Iteration 10, loss = 0.49678847\n",
            "Iteration 11, loss = 0.51129755\n",
            "Iteration 12, loss = 0.46060712\n",
            "Iteration 13, loss = 0.47209247\n",
            "Iteration 14, loss = 0.47440498\n",
            "Iteration 15, loss = 0.46651854\n",
            "Iteration 16, loss = 0.48352212\n",
            "Iteration 17, loss = 0.46938413\n",
            "Iteration 18, loss = 0.48022539\n",
            "Iteration 19, loss = 0.46081446\n",
            "Iteration 20, loss = 0.45100376\n",
            "Iteration 21, loss = 0.47520240\n",
            "Iteration 22, loss = 0.46649872\n",
            "Iteration 23, loss = 0.47689691\n",
            "Iteration 24, loss = 0.46275650\n",
            "Iteration 25, loss = 0.45810004\n",
            "Iteration 26, loss = 0.44843035\n",
            "Iteration 27, loss = 0.44454828\n",
            "Iteration 28, loss = 0.45109605\n",
            "Iteration 29, loss = 0.43326891\n",
            "Iteration 30, loss = 0.44645338\n",
            "Iteration 31, loss = 0.45250080\n",
            "Iteration 32, loss = 0.45457802\n",
            "Iteration 33, loss = 0.43659031\n",
            "Iteration 34, loss = 0.43737142\n",
            "Iteration 35, loss = 0.45023987\n",
            "Iteration 36, loss = 0.44117298\n",
            "Iteration 37, loss = 0.44029456\n",
            "Iteration 38, loss = 0.43669618\n",
            "Iteration 39, loss = 0.43208880\n",
            "Iteration 40, loss = 0.42417209\n",
            "Iteration 41, loss = 0.42326577\n",
            "Iteration 42, loss = 0.43614286\n",
            "Iteration 43, loss = 0.43310111\n",
            "Iteration 44, loss = 0.42194484\n",
            "Iteration 45, loss = 0.42919988\n",
            "Iteration 46, loss = 0.42533564\n",
            "Iteration 47, loss = 0.41497477\n",
            "Iteration 48, loss = 0.41917017\n",
            "Iteration 49, loss = 0.40961092\n",
            "Iteration 50, loss = 0.42099537\n",
            "Iteration 51, loss = 0.41746356\n",
            "Iteration 52, loss = 0.43977773\n",
            "Iteration 53, loss = 0.43040022\n",
            "Iteration 54, loss = 0.41443103\n",
            "Iteration 55, loss = 0.41805271\n",
            "Iteration 56, loss = 0.42958293\n",
            "Iteration 57, loss = 0.41560891\n",
            "Iteration 58, loss = 0.43095442\n",
            "Iteration 59, loss = 0.42788858\n",
            "Iteration 60, loss = 0.41997041\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70431917\n",
            "Iteration 2, loss = 0.62513020\n",
            "Iteration 3, loss = 0.58567801\n",
            "Iteration 4, loss = 0.56919983\n",
            "Iteration 5, loss = 0.56319471\n",
            "Iteration 6, loss = 0.57012542\n",
            "Iteration 7, loss = 0.51456843\n",
            "Iteration 8, loss = 0.50991191\n",
            "Iteration 9, loss = 0.48559813\n",
            "Iteration 10, loss = 0.48039539\n",
            "Iteration 11, loss = 0.46950493\n",
            "Iteration 12, loss = 0.49116050\n",
            "Iteration 13, loss = 0.46753775\n",
            "Iteration 14, loss = 0.47349849\n",
            "Iteration 15, loss = 0.46898704\n",
            "Iteration 16, loss = 0.44361068\n",
            "Iteration 17, loss = 0.44514193\n",
            "Iteration 18, loss = 0.44241057\n",
            "Iteration 19, loss = 0.44341156\n",
            "Iteration 20, loss = 0.43584638\n",
            "Iteration 21, loss = 0.42508314\n",
            "Iteration 22, loss = 0.42667542\n",
            "Iteration 23, loss = 0.44436705\n",
            "Iteration 24, loss = 0.43312165\n",
            "Iteration 25, loss = 0.43708510\n",
            "Iteration 26, loss = 0.42682815\n",
            "Iteration 27, loss = 0.42873481\n",
            "Iteration 28, loss = 0.41903274\n",
            "Iteration 29, loss = 0.41157337\n",
            "Iteration 30, loss = 0.41726859\n",
            "Iteration 31, loss = 0.44279435\n",
            "Iteration 32, loss = 0.41438255\n",
            "Iteration 33, loss = 0.43178698\n",
            "Iteration 34, loss = 0.41415216\n",
            "Iteration 35, loss = 0.40949509\n",
            "Iteration 36, loss = 0.40699297\n",
            "Iteration 37, loss = 0.39501790\n",
            "Iteration 38, loss = 0.42113655\n",
            "Iteration 39, loss = 0.40273611\n",
            "Iteration 40, loss = 0.40475622\n",
            "Iteration 41, loss = 0.39123172\n",
            "Iteration 42, loss = 0.39750254\n",
            "Iteration 43, loss = 0.39614075\n",
            "Iteration 44, loss = 0.40882278\n",
            "Iteration 45, loss = 0.40892266\n",
            "Iteration 46, loss = 0.38382619\n",
            "Iteration 47, loss = 0.43590277\n",
            "Iteration 48, loss = 0.39518933\n",
            "Iteration 49, loss = 0.41212841\n",
            "Iteration 50, loss = 0.39656109\n",
            "Iteration 51, loss = 0.39720264\n",
            "Iteration 52, loss = 0.38616129\n",
            "Iteration 53, loss = 0.39826701\n",
            "Iteration 54, loss = 0.38075357\n",
            "Iteration 55, loss = 0.39852188\n",
            "Iteration 56, loss = 0.39357821\n",
            "Iteration 57, loss = 0.41380207\n",
            "Iteration 58, loss = 0.38532187\n",
            "Iteration 59, loss = 0.39787931\n",
            "Iteration 60, loss = 0.38800040\n",
            "Iteration 61, loss = 0.38744976\n",
            "Iteration 62, loss = 0.39182693\n",
            "Iteration 63, loss = 0.38756413\n",
            "Iteration 64, loss = 0.37891099\n",
            "Iteration 65, loss = 0.37231562\n",
            "Iteration 66, loss = 0.36805429\n",
            "Iteration 67, loss = 0.38408559\n",
            "Iteration 68, loss = 0.37696099\n",
            "Iteration 69, loss = 0.39879678\n",
            "Iteration 70, loss = 0.37679246\n",
            "Iteration 71, loss = 0.37112802\n",
            "Iteration 72, loss = 0.36897160\n",
            "Iteration 73, loss = 0.37145921\n",
            "Iteration 74, loss = 0.37383317\n",
            "Iteration 75, loss = 0.37310013\n",
            "Iteration 76, loss = 0.36327641\n",
            "Iteration 77, loss = 0.37828130\n",
            "Iteration 78, loss = 0.36148725\n",
            "Iteration 79, loss = 0.36924398\n",
            "Iteration 80, loss = 0.36043574\n",
            "Iteration 81, loss = 0.36920234\n",
            "Iteration 82, loss = 0.38845158\n",
            "Iteration 83, loss = 0.36859082\n",
            "Iteration 84, loss = 0.37019232\n",
            "Iteration 85, loss = 0.36451005\n",
            "Iteration 86, loss = 0.35776256\n",
            "Iteration 87, loss = 0.35680540\n",
            "Iteration 88, loss = 0.35547506\n",
            "Iteration 89, loss = 0.35019215\n",
            "Iteration 90, loss = 0.35787043\n",
            "Iteration 91, loss = 0.36220787\n",
            "Iteration 92, loss = 0.37020968\n",
            "Iteration 93, loss = 0.35110683\n",
            "Iteration 94, loss = 0.34869449\n",
            "Iteration 95, loss = 0.36006196\n",
            "Iteration 96, loss = 0.37011564\n",
            "Iteration 97, loss = 0.34812076\n",
            "Iteration 98, loss = 0.34370870\n",
            "Iteration 99, loss = 0.37124982\n",
            "Iteration 100, loss = 0.35976311\n",
            "Iteration 101, loss = 0.35525991\n",
            "Iteration 102, loss = 0.34213455\n",
            "Iteration 103, loss = 0.34197733\n",
            "Iteration 104, loss = 0.34228545\n",
            "Iteration 105, loss = 0.35146546\n",
            "Iteration 106, loss = 0.36347597\n",
            "Iteration 107, loss = 0.34431621\n",
            "Iteration 108, loss = 0.35699156\n",
            "Iteration 109, loss = 0.34374339\n",
            "Iteration 110, loss = 0.34940062\n",
            "Iteration 111, loss = 0.33143259\n",
            "Iteration 112, loss = 0.33981379\n",
            "Iteration 113, loss = 0.33233962\n",
            "Iteration 114, loss = 0.34897705\n",
            "Iteration 115, loss = 0.38470198\n",
            "Iteration 116, loss = 0.34374906\n",
            "Iteration 117, loss = 0.34355035\n",
            "Iteration 118, loss = 0.34623551\n",
            "Iteration 119, loss = 0.33580375\n",
            "Iteration 120, loss = 0.33169194\n",
            "Iteration 121, loss = 0.33792057\n",
            "Iteration 122, loss = 0.34176977\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67431521\n",
            "Iteration 2, loss = 0.63884954\n",
            "Iteration 3, loss = 0.57942458\n",
            "Iteration 4, loss = 0.56364786\n",
            "Iteration 5, loss = 0.56191614\n",
            "Iteration 6, loss = 0.51596113\n",
            "Iteration 7, loss = 0.53150015\n",
            "Iteration 8, loss = 0.49789562\n",
            "Iteration 9, loss = 0.50819060\n",
            "Iteration 10, loss = 0.48435613\n",
            "Iteration 11, loss = 0.50826122\n",
            "Iteration 12, loss = 0.47082428\n",
            "Iteration 13, loss = 0.47526068\n",
            "Iteration 14, loss = 0.49476201\n",
            "Iteration 15, loss = 0.49302801\n",
            "Iteration 16, loss = 0.47688480\n",
            "Iteration 17, loss = 0.46024275\n",
            "Iteration 18, loss = 0.46938365\n",
            "Iteration 19, loss = 0.45647336\n",
            "Iteration 20, loss = 0.44082189\n",
            "Iteration 21, loss = 0.46377991\n",
            "Iteration 22, loss = 0.45074503\n",
            "Iteration 23, loss = 0.46695747\n",
            "Iteration 24, loss = 0.45208718\n",
            "Iteration 25, loss = 0.44458517\n",
            "Iteration 26, loss = 0.44359298\n",
            "Iteration 27, loss = 0.43506062\n",
            "Iteration 28, loss = 0.42454325\n",
            "Iteration 29, loss = 0.42392905\n",
            "Iteration 30, loss = 0.43411889\n",
            "Iteration 31, loss = 0.45079314\n",
            "Iteration 32, loss = 0.44439733\n",
            "Iteration 33, loss = 0.43507371\n",
            "Iteration 34, loss = 0.43304906\n",
            "Iteration 35, loss = 0.41775981\n",
            "Iteration 36, loss = 0.42580065\n",
            "Iteration 37, loss = 0.41530453\n",
            "Iteration 38, loss = 0.42657400\n",
            "Iteration 39, loss = 0.41641363\n",
            "Iteration 40, loss = 0.40777744\n",
            "Iteration 41, loss = 0.41114478\n",
            "Iteration 42, loss = 0.42922758\n",
            "Iteration 43, loss = 0.41786329\n",
            "Iteration 44, loss = 0.41580123\n",
            "Iteration 45, loss = 0.40072754\n",
            "Iteration 46, loss = 0.42190512\n",
            "Iteration 47, loss = 0.40828485\n",
            "Iteration 48, loss = 0.41410209\n",
            "Iteration 49, loss = 0.41637971\n",
            "Iteration 50, loss = 0.39151312\n",
            "Iteration 51, loss = 0.39912553\n",
            "Iteration 52, loss = 0.41228774\n",
            "Iteration 53, loss = 0.39757648\n",
            "Iteration 54, loss = 0.39816821\n",
            "Iteration 55, loss = 0.39726607\n",
            "Iteration 56, loss = 0.38717625\n",
            "Iteration 57, loss = 0.40035964\n",
            "Iteration 58, loss = 0.38963275\n",
            "Iteration 59, loss = 0.39780597\n",
            "Iteration 60, loss = 0.40207066\n",
            "Iteration 61, loss = 0.39008501\n",
            "Iteration 62, loss = 0.39191271\n",
            "Iteration 63, loss = 0.39917714\n",
            "Iteration 64, loss = 0.37877171\n",
            "Iteration 65, loss = 0.39392065\n",
            "Iteration 66, loss = 0.41513492\n",
            "Iteration 67, loss = 0.39721281\n",
            "Iteration 68, loss = 0.39068770\n",
            "Iteration 69, loss = 0.38171053\n",
            "Iteration 70, loss = 0.38913789\n",
            "Iteration 71, loss = 0.37940575\n",
            "Iteration 72, loss = 0.40392839\n",
            "Iteration 73, loss = 0.37062942\n",
            "Iteration 74, loss = 0.37956131\n",
            "Iteration 75, loss = 0.37636179\n",
            "Iteration 76, loss = 0.39061353\n",
            "Iteration 77, loss = 0.37958456\n",
            "Iteration 78, loss = 0.37787526\n",
            "Iteration 79, loss = 0.36357206\n",
            "Iteration 80, loss = 0.37908959\n",
            "Iteration 81, loss = 0.38188047\n",
            "Iteration 82, loss = 0.36371751\n",
            "Iteration 83, loss = 0.37493941\n",
            "Iteration 84, loss = 0.36474239\n",
            "Iteration 85, loss = 0.37001992\n",
            "Iteration 86, loss = 0.37294061\n",
            "Iteration 87, loss = 0.36512718\n",
            "Iteration 88, loss = 0.38553619\n",
            "Iteration 89, loss = 0.37068929\n",
            "Iteration 90, loss = 0.40933862\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66127911\n",
            "Iteration 2, loss = 0.64908167\n",
            "Iteration 3, loss = 0.64139798\n",
            "Iteration 4, loss = 0.63294951\n",
            "Iteration 5, loss = 0.62428751\n",
            "Iteration 6, loss = 0.61851383\n",
            "Iteration 7, loss = 0.61230203\n",
            "Iteration 8, loss = 0.60825294\n",
            "Iteration 9, loss = 0.60582508\n",
            "Iteration 10, loss = 0.60641500\n",
            "Iteration 11, loss = 0.60390530\n",
            "Iteration 12, loss = 0.60345134\n",
            "Iteration 13, loss = 0.60314522\n",
            "Iteration 14, loss = 0.60186651\n",
            "Iteration 15, loss = 0.60259065\n",
            "Iteration 16, loss = 0.60013975\n",
            "Iteration 17, loss = 0.59783993\n",
            "Iteration 18, loss = 0.59764743\n",
            "Iteration 19, loss = 0.59676307\n",
            "Iteration 20, loss = 0.59588362\n",
            "Iteration 21, loss = 0.59428166\n",
            "Iteration 22, loss = 0.59259750\n",
            "Iteration 23, loss = 0.59146351\n",
            "Iteration 24, loss = 0.59079514\n",
            "Iteration 25, loss = 0.58703472\n",
            "Iteration 26, loss = 0.58525922\n",
            "Iteration 27, loss = 0.58294344\n",
            "Iteration 28, loss = 0.58033612\n",
            "Iteration 29, loss = 0.58552345\n",
            "Iteration 30, loss = 0.57715609\n",
            "Iteration 31, loss = 0.57601470\n",
            "Iteration 32, loss = 0.57579176\n",
            "Iteration 33, loss = 0.57047136\n",
            "Iteration 34, loss = 0.56896760\n",
            "Iteration 35, loss = 0.56731625\n",
            "Iteration 36, loss = 0.56414229\n",
            "Iteration 37, loss = 0.56840455\n",
            "Iteration 38, loss = 0.55816852\n",
            "Iteration 39, loss = 0.56276319\n",
            "Iteration 40, loss = 0.55510243\n",
            "Iteration 41, loss = 0.55119913\n",
            "Iteration 42, loss = 0.54757918\n",
            "Iteration 43, loss = 0.54505666\n",
            "Iteration 44, loss = 0.54088577\n",
            "Iteration 45, loss = 0.53755302\n",
            "Iteration 46, loss = 0.53425950\n",
            "Iteration 47, loss = 0.53390326\n",
            "Iteration 48, loss = 0.52817809\n",
            "Iteration 49, loss = 0.52377187\n",
            "Iteration 50, loss = 0.52119308\n",
            "Iteration 51, loss = 0.51675859\n",
            "Iteration 52, loss = 0.51550586\n",
            "Iteration 53, loss = 0.51649722\n",
            "Iteration 54, loss = 0.50649174\n",
            "Iteration 55, loss = 0.50684534\n",
            "Iteration 56, loss = 0.50094825\n",
            "Iteration 57, loss = 0.50109031\n",
            "Iteration 58, loss = 0.49470002\n",
            "Iteration 59, loss = 0.49162449\n",
            "Iteration 60, loss = 0.49157388\n",
            "Iteration 61, loss = 0.48652282\n",
            "Iteration 62, loss = 0.48267157\n",
            "Iteration 63, loss = 0.48470553\n",
            "Iteration 64, loss = 0.48366371\n",
            "Iteration 65, loss = 0.47935279\n",
            "Iteration 66, loss = 0.47931666\n",
            "Iteration 67, loss = 0.47076109\n",
            "Iteration 68, loss = 0.47044960\n",
            "Iteration 69, loss = 0.46756914\n",
            "Iteration 70, loss = 0.46658587\n",
            "Iteration 71, loss = 0.46498748\n",
            "Iteration 72, loss = 0.46142669\n",
            "Iteration 73, loss = 0.46027282\n",
            "Iteration 74, loss = 0.45981401\n",
            "Iteration 75, loss = 0.45861648\n",
            "Iteration 76, loss = 0.45723907\n",
            "Iteration 77, loss = 0.45677698\n",
            "Iteration 78, loss = 0.45589517\n",
            "Iteration 79, loss = 0.46410693\n",
            "Iteration 80, loss = 0.45561077\n",
            "Iteration 81, loss = 0.45126008\n",
            "Iteration 82, loss = 0.45244451\n",
            "Iteration 83, loss = 0.45081966\n",
            "Iteration 84, loss = 0.45303339\n",
            "Iteration 85, loss = 0.45653455\n",
            "Iteration 86, loss = 0.45025417\n",
            "Iteration 87, loss = 0.44832932\n",
            "Iteration 88, loss = 0.45164007\n",
            "Iteration 89, loss = 0.45643291\n",
            "Iteration 90, loss = 0.45906994\n",
            "Iteration 91, loss = 0.45591120\n",
            "Iteration 92, loss = 0.44880982\n",
            "Iteration 93, loss = 0.45091588\n",
            "Iteration 94, loss = 0.44624682\n",
            "Iteration 95, loss = 0.44361035\n",
            "Iteration 96, loss = 0.44641279\n",
            "Iteration 97, loss = 0.44696502\n",
            "Iteration 98, loss = 0.45122077\n",
            "Iteration 99, loss = 0.45317230\n",
            "Iteration 100, loss = 0.44761800\n",
            "Iteration 101, loss = 0.44495659\n",
            "Iteration 102, loss = 0.44307873\n",
            "Iteration 103, loss = 0.44486796\n",
            "Iteration 104, loss = 0.44175564\n",
            "Iteration 105, loss = 0.44465971\n",
            "Iteration 106, loss = 0.46006772\n",
            "Iteration 107, loss = 0.44918482\n",
            "Iteration 108, loss = 0.44405720\n",
            "Iteration 109, loss = 0.45197709\n",
            "Iteration 110, loss = 0.44454096\n",
            "Iteration 111, loss = 0.44285512\n",
            "Iteration 112, loss = 0.43997407\n",
            "Iteration 113, loss = 0.43984137\n",
            "Iteration 114, loss = 0.44048559\n",
            "Iteration 115, loss = 0.43949812\n",
            "Iteration 116, loss = 0.44078348\n",
            "Iteration 117, loss = 0.43948669\n",
            "Iteration 118, loss = 0.43954126\n",
            "Iteration 119, loss = 0.44175167\n",
            "Iteration 120, loss = 0.43820695\n",
            "Iteration 121, loss = 0.43926410\n",
            "Iteration 122, loss = 0.44044686\n",
            "Iteration 123, loss = 0.44317940\n",
            "Iteration 124, loss = 0.44915331\n",
            "Iteration 125, loss = 0.44757149\n",
            "Iteration 126, loss = 0.43516045\n",
            "Iteration 127, loss = 0.43827842\n",
            "Iteration 128, loss = 0.43595806\n",
            "Iteration 129, loss = 0.43856569\n",
            "Iteration 130, loss = 0.43699091\n",
            "Iteration 131, loss = 0.43584332\n",
            "Iteration 132, loss = 0.44343420\n",
            "Iteration 133, loss = 0.44222125\n",
            "Iteration 134, loss = 0.43809567\n",
            "Iteration 135, loss = 0.44100367\n",
            "Iteration 136, loss = 0.43875221\n",
            "Iteration 137, loss = 0.43532375\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.65526942\n",
            "Iteration 2, loss = 0.64126724\n",
            "Iteration 3, loss = 0.63188772\n",
            "Iteration 4, loss = 0.62302938\n",
            "Iteration 5, loss = 0.61607257\n",
            "Iteration 6, loss = 0.61011432\n",
            "Iteration 7, loss = 0.60564724\n",
            "Iteration 8, loss = 0.60321082\n",
            "Iteration 9, loss = 0.60092344\n",
            "Iteration 10, loss = 0.59891819\n",
            "Iteration 11, loss = 0.59708536\n",
            "Iteration 12, loss = 0.59541209\n",
            "Iteration 13, loss = 0.59481390\n",
            "Iteration 14, loss = 0.59330510\n",
            "Iteration 15, loss = 0.59157878\n",
            "Iteration 16, loss = 0.59041451\n",
            "Iteration 17, loss = 0.59000232\n",
            "Iteration 18, loss = 0.58806402\n",
            "Iteration 19, loss = 0.58662931\n",
            "Iteration 20, loss = 0.58663490\n",
            "Iteration 21, loss = 0.58433030\n",
            "Iteration 22, loss = 0.58392223\n",
            "Iteration 23, loss = 0.58173480\n",
            "Iteration 24, loss = 0.57887196\n",
            "Iteration 25, loss = 0.58100443\n",
            "Iteration 26, loss = 0.57847799\n",
            "Iteration 27, loss = 0.57460002\n",
            "Iteration 28, loss = 0.57081085\n",
            "Iteration 29, loss = 0.56861008\n",
            "Iteration 30, loss = 0.56626461\n",
            "Iteration 31, loss = 0.56336623\n",
            "Iteration 32, loss = 0.56105237\n",
            "Iteration 33, loss = 0.55853200\n",
            "Iteration 34, loss = 0.55515673\n",
            "Iteration 35, loss = 0.55105460\n",
            "Iteration 36, loss = 0.54988063\n",
            "Iteration 37, loss = 0.54242674\n",
            "Iteration 38, loss = 0.53894938\n",
            "Iteration 39, loss = 0.53526297\n",
            "Iteration 40, loss = 0.52966323\n",
            "Iteration 41, loss = 0.52489898\n",
            "Iteration 42, loss = 0.52085588\n",
            "Iteration 43, loss = 0.52154471\n",
            "Iteration 44, loss = 0.51774130\n",
            "Iteration 45, loss = 0.51569568\n",
            "Iteration 46, loss = 0.50742713\n",
            "Iteration 47, loss = 0.51078639\n",
            "Iteration 48, loss = 0.50496767\n",
            "Iteration 49, loss = 0.50015626\n",
            "Iteration 50, loss = 0.49482089\n",
            "Iteration 51, loss = 0.49145920\n",
            "Iteration 52, loss = 0.49401962\n",
            "Iteration 53, loss = 0.48556746\n",
            "Iteration 54, loss = 0.48282239\n",
            "Iteration 55, loss = 0.47878959\n",
            "Iteration 56, loss = 0.48409898\n",
            "Iteration 57, loss = 0.48705755\n",
            "Iteration 58, loss = 0.47814830\n",
            "Iteration 59, loss = 0.47415695\n",
            "Iteration 60, loss = 0.46849438\n",
            "Iteration 61, loss = 0.47058738\n",
            "Iteration 62, loss = 0.46624566\n",
            "Iteration 63, loss = 0.46705122\n",
            "Iteration 64, loss = 0.46325679\n",
            "Iteration 65, loss = 0.45859647\n",
            "Iteration 66, loss = 0.46322955\n",
            "Iteration 67, loss = 0.46629711\n",
            "Iteration 68, loss = 0.45674253\n",
            "Iteration 69, loss = 0.46152720\n",
            "Iteration 70, loss = 0.45782728\n",
            "Iteration 71, loss = 0.45344137\n",
            "Iteration 72, loss = 0.45642125\n",
            "Iteration 73, loss = 0.45393958\n",
            "Iteration 74, loss = 0.45198091\n",
            "Iteration 75, loss = 0.44954936\n",
            "Iteration 76, loss = 0.44878125\n",
            "Iteration 77, loss = 0.45063991\n",
            "Iteration 78, loss = 0.44774754\n",
            "Iteration 79, loss = 0.44805709\n",
            "Iteration 80, loss = 0.44569020\n",
            "Iteration 81, loss = 0.44563098\n",
            "Iteration 82, loss = 0.44604258\n",
            "Iteration 83, loss = 0.44719829\n",
            "Iteration 84, loss = 0.44497433\n",
            "Iteration 85, loss = 0.44298510\n",
            "Iteration 86, loss = 0.45511005\n",
            "Iteration 87, loss = 0.44482577\n",
            "Iteration 88, loss = 0.44120587\n",
            "Iteration 89, loss = 0.46542021\n",
            "Iteration 90, loss = 0.44047558\n",
            "Iteration 91, loss = 0.43940640\n",
            "Iteration 92, loss = 0.44271563\n",
            "Iteration 93, loss = 0.44539694\n",
            "Iteration 94, loss = 0.43877563\n",
            "Iteration 95, loss = 0.43788412\n",
            "Iteration 96, loss = 0.44303480\n",
            "Iteration 97, loss = 0.44133042\n",
            "Iteration 98, loss = 0.43926866\n",
            "Iteration 99, loss = 0.44182396\n",
            "Iteration 100, loss = 0.43612922\n",
            "Iteration 101, loss = 0.43745514\n",
            "Iteration 102, loss = 0.43859039\n",
            "Iteration 103, loss = 0.44050768\n",
            "Iteration 104, loss = 0.43689192\n",
            "Iteration 105, loss = 0.43669758\n",
            "Iteration 106, loss = 0.43887599\n",
            "Iteration 107, loss = 0.43425802\n",
            "Iteration 108, loss = 0.43405694\n",
            "Iteration 109, loss = 0.43346037\n",
            "Iteration 110, loss = 0.43446336\n",
            "Iteration 111, loss = 0.43402400\n",
            "Iteration 112, loss = 0.43528187\n",
            "Iteration 113, loss = 0.43523179\n",
            "Iteration 114, loss = 0.44045073\n",
            "Iteration 115, loss = 0.43135497\n",
            "Iteration 116, loss = 0.43561721\n",
            "Iteration 117, loss = 0.43172675\n",
            "Iteration 118, loss = 0.43636619\n",
            "Iteration 119, loss = 0.43181588\n",
            "Iteration 120, loss = 0.43630570\n",
            "Iteration 121, loss = 0.43328319\n",
            "Iteration 122, loss = 0.42946719\n",
            "Iteration 123, loss = 0.43697533\n",
            "Iteration 124, loss = 0.42945847\n",
            "Iteration 125, loss = 0.43082292\n",
            "Iteration 126, loss = 0.43223338\n",
            "Iteration 127, loss = 0.42792843\n",
            "Iteration 128, loss = 0.42821196\n",
            "Iteration 129, loss = 0.42907737\n",
            "Iteration 130, loss = 0.42621944\n",
            "Iteration 131, loss = 0.43344946\n",
            "Iteration 132, loss = 0.43781765\n",
            "Iteration 133, loss = 0.43350613\n",
            "Iteration 134, loss = 0.43028702\n",
            "Iteration 135, loss = 0.42588376\n",
            "Iteration 136, loss = 0.42612693\n",
            "Iteration 137, loss = 0.42725479\n",
            "Iteration 138, loss = 0.42930332\n",
            "Iteration 139, loss = 0.42996392\n",
            "Iteration 140, loss = 0.43016536\n",
            "Iteration 141, loss = 0.42844231\n",
            "Iteration 142, loss = 0.43126227\n",
            "Iteration 143, loss = 0.42526307\n",
            "Iteration 144, loss = 0.42468193\n",
            "Iteration 145, loss = 0.42684849\n",
            "Iteration 146, loss = 0.42886971\n",
            "Iteration 147, loss = 0.42863763\n",
            "Iteration 148, loss = 0.42029037\n",
            "Iteration 149, loss = 0.43284633\n",
            "Iteration 150, loss = 0.42464886\n",
            "Iteration 151, loss = 0.42451429\n",
            "Iteration 152, loss = 0.42422755\n",
            "Iteration 153, loss = 0.42610881\n",
            "Iteration 154, loss = 0.43321148\n",
            "Iteration 155, loss = 0.43065200\n",
            "Iteration 156, loss = 0.42498037\n",
            "Iteration 157, loss = 0.42461889\n",
            "Iteration 158, loss = 0.42326357\n",
            "Iteration 159, loss = 0.42124259\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69491543\n",
            "Iteration 2, loss = 0.65952061\n",
            "Iteration 3, loss = 0.65200985\n",
            "Iteration 4, loss = 0.64559010\n",
            "Iteration 5, loss = 0.64038046\n",
            "Iteration 6, loss = 0.63408460\n",
            "Iteration 7, loss = 0.62965679\n",
            "Iteration 8, loss = 0.62491611\n",
            "Iteration 9, loss = 0.62128227\n",
            "Iteration 10, loss = 0.61873850\n",
            "Iteration 11, loss = 0.61588505\n",
            "Iteration 12, loss = 0.61547291\n",
            "Iteration 13, loss = 0.61386132\n",
            "Iteration 14, loss = 0.61276126\n",
            "Iteration 15, loss = 0.61205315\n",
            "Iteration 16, loss = 0.60965750\n",
            "Iteration 17, loss = 0.60886744\n",
            "Iteration 18, loss = 0.60978877\n",
            "Iteration 19, loss = 0.60726372\n",
            "Iteration 20, loss = 0.60472036\n",
            "Iteration 21, loss = 0.60314319\n",
            "Iteration 22, loss = 0.59966736\n",
            "Iteration 23, loss = 0.59764346\n",
            "Iteration 24, loss = 0.59550614\n",
            "Iteration 25, loss = 0.59178028\n",
            "Iteration 26, loss = 0.58687163\n",
            "Iteration 27, loss = 0.58675854\n",
            "Iteration 28, loss = 0.58163751\n",
            "Iteration 29, loss = 0.57715020\n",
            "Iteration 30, loss = 0.57394918\n",
            "Iteration 31, loss = 0.56937949\n",
            "Iteration 32, loss = 0.56417844\n",
            "Iteration 33, loss = 0.55916460\n",
            "Iteration 34, loss = 0.55650741\n",
            "Iteration 35, loss = 0.55729440\n",
            "Iteration 36, loss = 0.54624534\n",
            "Iteration 37, loss = 0.54435544\n",
            "Iteration 38, loss = 0.53879020\n",
            "Iteration 39, loss = 0.53802489\n",
            "Iteration 40, loss = 0.52967706\n",
            "Iteration 41, loss = 0.52601645\n",
            "Iteration 42, loss = 0.52286077\n",
            "Iteration 43, loss = 0.52520064\n",
            "Iteration 44, loss = 0.52006411\n",
            "Iteration 45, loss = 0.51383684\n",
            "Iteration 46, loss = 0.51509512\n",
            "Iteration 47, loss = 0.50254789\n",
            "Iteration 48, loss = 0.50051883\n",
            "Iteration 49, loss = 0.49650420\n",
            "Iteration 50, loss = 0.49357797\n",
            "Iteration 51, loss = 0.49245223\n",
            "Iteration 52, loss = 0.48736526\n",
            "Iteration 53, loss = 0.48612748\n",
            "Iteration 54, loss = 0.48384862\n",
            "Iteration 55, loss = 0.48115584\n",
            "Iteration 56, loss = 0.47613475\n",
            "Iteration 57, loss = 0.47618714\n",
            "Iteration 58, loss = 0.47301377\n",
            "Iteration 59, loss = 0.47271646\n",
            "Iteration 60, loss = 0.47650177\n",
            "Iteration 61, loss = 0.46665819\n",
            "Iteration 62, loss = 0.46708150\n",
            "Iteration 63, loss = 0.46696872\n",
            "Iteration 64, loss = 0.46298840\n",
            "Iteration 65, loss = 0.46155357\n",
            "Iteration 66, loss = 0.46022053\n",
            "Iteration 67, loss = 0.46087877\n",
            "Iteration 68, loss = 0.46140685\n",
            "Iteration 69, loss = 0.46057514\n",
            "Iteration 70, loss = 0.45633976\n",
            "Iteration 71, loss = 0.45654709\n",
            "Iteration 72, loss = 0.45609522\n",
            "Iteration 73, loss = 0.45277714\n",
            "Iteration 74, loss = 0.45605043\n",
            "Iteration 75, loss = 0.45503606\n",
            "Iteration 76, loss = 0.45726235\n",
            "Iteration 77, loss = 0.45627129\n",
            "Iteration 78, loss = 0.45441070\n",
            "Iteration 79, loss = 0.45381908\n",
            "Iteration 80, loss = 0.46306767\n",
            "Iteration 81, loss = 0.47003603\n",
            "Iteration 82, loss = 0.45727139\n",
            "Iteration 83, loss = 0.45435326\n",
            "Iteration 84, loss = 0.45663886\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67125057\n",
            "Iteration 2, loss = 0.64711038\n",
            "Iteration 3, loss = 0.63685157\n",
            "Iteration 4, loss = 0.62805157\n",
            "Iteration 5, loss = 0.62087081\n",
            "Iteration 6, loss = 0.61492552\n",
            "Iteration 7, loss = 0.61034187\n",
            "Iteration 8, loss = 0.60638409\n",
            "Iteration 9, loss = 0.60380159\n",
            "Iteration 10, loss = 0.60105355\n",
            "Iteration 11, loss = 0.60274635\n",
            "Iteration 12, loss = 0.59963968\n",
            "Iteration 13, loss = 0.59789159\n",
            "Iteration 14, loss = 0.59848939\n",
            "Iteration 15, loss = 0.59401938\n",
            "Iteration 16, loss = 0.59561785\n",
            "Iteration 17, loss = 0.59148149\n",
            "Iteration 18, loss = 0.59276158\n",
            "Iteration 19, loss = 0.58878382\n",
            "Iteration 20, loss = 0.58930635\n",
            "Iteration 21, loss = 0.58688709\n",
            "Iteration 22, loss = 0.58293070\n",
            "Iteration 23, loss = 0.58225150\n",
            "Iteration 24, loss = 0.58018132\n",
            "Iteration 25, loss = 0.57878565\n",
            "Iteration 26, loss = 0.57867837\n",
            "Iteration 27, loss = 0.57431604\n",
            "Iteration 28, loss = 0.57381565\n",
            "Iteration 29, loss = 0.57003021\n",
            "Iteration 30, loss = 0.56845221\n",
            "Iteration 31, loss = 0.56637336\n",
            "Iteration 32, loss = 0.56425325\n",
            "Iteration 33, loss = 0.55848783\n",
            "Iteration 34, loss = 0.55700931\n",
            "Iteration 35, loss = 0.55384379\n",
            "Iteration 36, loss = 0.55062976\n",
            "Iteration 37, loss = 0.54794459\n",
            "Iteration 38, loss = 0.54761989\n",
            "Iteration 39, loss = 0.54236394\n",
            "Iteration 40, loss = 0.53715314\n",
            "Iteration 41, loss = 0.53491021\n",
            "Iteration 42, loss = 0.53144481\n",
            "Iteration 43, loss = 0.52661452\n",
            "Iteration 44, loss = 0.52319705\n",
            "Iteration 45, loss = 0.51812437\n",
            "Iteration 46, loss = 0.51401766\n",
            "Iteration 47, loss = 0.51790493\n",
            "Iteration 48, loss = 0.51779947\n",
            "Iteration 49, loss = 0.50326600\n",
            "Iteration 50, loss = 0.49781870\n",
            "Iteration 51, loss = 0.49660347\n",
            "Iteration 52, loss = 0.49017914\n",
            "Iteration 53, loss = 0.49028122\n",
            "Iteration 54, loss = 0.48651952\n",
            "Iteration 55, loss = 0.48884876\n",
            "Iteration 56, loss = 0.48103590\n",
            "Iteration 57, loss = 0.47939041\n",
            "Iteration 58, loss = 0.47681726\n",
            "Iteration 59, loss = 0.47271570\n",
            "Iteration 60, loss = 0.46983167\n",
            "Iteration 61, loss = 0.47311948\n",
            "Iteration 62, loss = 0.46607067\n",
            "Iteration 63, loss = 0.47122859\n",
            "Iteration 64, loss = 0.46211036\n",
            "Iteration 65, loss = 0.46026983\n",
            "Iteration 66, loss = 0.46097530\n",
            "Iteration 67, loss = 0.45645902\n",
            "Iteration 68, loss = 0.45525392\n",
            "Iteration 69, loss = 0.45478417\n",
            "Iteration 70, loss = 0.45851897\n",
            "Iteration 71, loss = 0.45154004\n",
            "Iteration 72, loss = 0.44962193\n",
            "Iteration 73, loss = 0.44963751\n",
            "Iteration 74, loss = 0.44979100\n",
            "Iteration 75, loss = 0.44831221\n",
            "Iteration 76, loss = 0.44747237\n",
            "Iteration 77, loss = 0.45791666\n",
            "Iteration 78, loss = 0.45398350\n",
            "Iteration 79, loss = 0.44669405\n",
            "Iteration 80, loss = 0.44200140\n",
            "Iteration 81, loss = 0.44143240\n",
            "Iteration 82, loss = 0.44506979\n",
            "Iteration 83, loss = 0.45370640\n",
            "Iteration 84, loss = 0.44697477\n",
            "Iteration 85, loss = 0.44285209\n",
            "Iteration 86, loss = 0.44237420\n",
            "Iteration 87, loss = 0.43944599\n",
            "Iteration 88, loss = 0.45043022\n",
            "Iteration 89, loss = 0.45027412\n",
            "Iteration 90, loss = 0.44338526\n",
            "Iteration 91, loss = 0.44525771\n",
            "Iteration 92, loss = 0.45605998\n",
            "Iteration 93, loss = 0.43926252\n",
            "Iteration 94, loss = 0.43953805\n",
            "Iteration 95, loss = 0.43916804\n",
            "Iteration 96, loss = 0.44536449\n",
            "Iteration 97, loss = 0.43803546\n",
            "Iteration 98, loss = 0.43748820\n",
            "Iteration 99, loss = 0.43467611\n",
            "Iteration 100, loss = 0.43521534\n",
            "Iteration 101, loss = 0.43541817\n",
            "Iteration 102, loss = 0.43392481\n",
            "Iteration 103, loss = 0.43519720\n",
            "Iteration 104, loss = 0.43254770\n",
            "Iteration 105, loss = 0.43169663\n",
            "Iteration 106, loss = 0.43252262\n",
            "Iteration 107, loss = 0.43255733\n",
            "Iteration 108, loss = 0.43371824\n",
            "Iteration 109, loss = 0.43875286\n",
            "Iteration 110, loss = 0.43218714\n",
            "Iteration 111, loss = 0.43570284\n",
            "Iteration 112, loss = 0.43112029\n",
            "Iteration 113, loss = 0.43508939\n",
            "Iteration 114, loss = 0.43191877\n",
            "Iteration 115, loss = 0.42988856\n",
            "Iteration 116, loss = 0.43125247\n",
            "Iteration 117, loss = 0.43502109\n",
            "Iteration 118, loss = 0.43332786\n",
            "Iteration 119, loss = 0.44378325\n",
            "Iteration 120, loss = 0.42862725\n",
            "Iteration 121, loss = 0.43010837\n",
            "Iteration 122, loss = 0.42982813\n",
            "Iteration 123, loss = 0.42911547\n",
            "Iteration 124, loss = 0.42647784\n",
            "Iteration 125, loss = 0.43363756\n",
            "Iteration 126, loss = 0.43245431\n",
            "Iteration 127, loss = 0.43426241\n",
            "Iteration 128, loss = 0.43401726\n",
            "Iteration 129, loss = 0.42988819\n",
            "Iteration 130, loss = 0.42826403\n",
            "Iteration 131, loss = 0.42907736\n",
            "Iteration 132, loss = 0.42750367\n",
            "Iteration 133, loss = 0.42480049\n",
            "Iteration 134, loss = 0.42764172\n",
            "Iteration 135, loss = 0.42786648\n",
            "Iteration 136, loss = 0.42705524\n",
            "Iteration 137, loss = 0.42425758\n",
            "Iteration 138, loss = 0.42551895\n",
            "Iteration 139, loss = 0.43017612\n",
            "Iteration 140, loss = 0.43461387\n",
            "Iteration 141, loss = 0.43048865\n",
            "Iteration 142, loss = 0.42234518\n",
            "Iteration 143, loss = 0.42679880\n",
            "Iteration 144, loss = 0.42339966\n",
            "Iteration 145, loss = 0.43035327\n",
            "Iteration 146, loss = 0.42390138\n",
            "Iteration 147, loss = 0.42515561\n",
            "Iteration 148, loss = 0.42368833\n",
            "Iteration 149, loss = 0.42389673\n",
            "Iteration 150, loss = 0.42245889\n",
            "Iteration 151, loss = 0.42207670\n",
            "Iteration 152, loss = 0.42246877\n",
            "Iteration 153, loss = 0.42106282\n",
            "Iteration 154, loss = 0.42183746\n",
            "Iteration 155, loss = 0.42377221\n",
            "Iteration 156, loss = 0.42267692\n",
            "Iteration 157, loss = 0.42106970\n",
            "Iteration 158, loss = 0.42116485\n",
            "Iteration 159, loss = 0.42242388\n",
            "Iteration 160, loss = 0.42082372\n",
            "Iteration 161, loss = 0.42106017\n",
            "Iteration 162, loss = 0.42269958\n",
            "Iteration 163, loss = 0.42599156\n",
            "Iteration 164, loss = 0.42217708\n",
            "Iteration 165, loss = 0.42379369\n",
            "Iteration 166, loss = 0.41819546\n",
            "Iteration 167, loss = 0.43412344\n",
            "Iteration 168, loss = 0.42161407\n",
            "Iteration 169, loss = 0.41987668\n",
            "Iteration 170, loss = 0.42631742\n",
            "Iteration 171, loss = 0.42349578\n",
            "Iteration 172, loss = 0.41885883\n",
            "Iteration 173, loss = 0.41843962\n",
            "Iteration 174, loss = 0.41783971\n",
            "Iteration 175, loss = 0.41984170\n",
            "Iteration 176, loss = 0.41853435\n",
            "Iteration 177, loss = 0.41837888\n",
            "Iteration 178, loss = 0.41911444\n",
            "Iteration 179, loss = 0.42201084\n",
            "Iteration 180, loss = 0.42814095\n",
            "Iteration 181, loss = 0.41902236\n",
            "Iteration 182, loss = 0.41657680\n",
            "Iteration 183, loss = 0.41658684\n",
            "Iteration 184, loss = 0.41892953\n",
            "Iteration 185, loss = 0.41597376\n",
            "Iteration 186, loss = 0.41636929\n",
            "Iteration 187, loss = 0.43570940\n",
            "Iteration 188, loss = 0.42012035\n",
            "Iteration 189, loss = 0.41884714\n",
            "Iteration 190, loss = 0.42105280\n",
            "Iteration 191, loss = 0.42321411\n",
            "Iteration 192, loss = 0.42204532\n",
            "Iteration 193, loss = 0.42691377\n",
            "Iteration 194, loss = 0.42188378\n",
            "Iteration 195, loss = 0.41784448\n",
            "Iteration 196, loss = 0.41609625\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66272485\n",
            "Iteration 2, loss = 0.64829618\n",
            "Iteration 3, loss = 0.64069931\n",
            "Iteration 4, loss = 0.63328871\n",
            "Iteration 5, loss = 0.62628442\n",
            "Iteration 6, loss = 0.61939563\n",
            "Iteration 7, loss = 0.61440539\n",
            "Iteration 8, loss = 0.61021421\n",
            "Iteration 9, loss = 0.60722858\n",
            "Iteration 10, loss = 0.60584965\n",
            "Iteration 11, loss = 0.60555840\n",
            "Iteration 12, loss = 0.60359201\n",
            "Iteration 13, loss = 0.60199869\n",
            "Iteration 14, loss = 0.60131552\n",
            "Iteration 15, loss = 0.59984075\n",
            "Iteration 16, loss = 0.60110506\n",
            "Iteration 17, loss = 0.59725614\n",
            "Iteration 18, loss = 0.59489607\n",
            "Iteration 19, loss = 0.59363907\n",
            "Iteration 20, loss = 0.59227708\n",
            "Iteration 21, loss = 0.59127439\n",
            "Iteration 22, loss = 0.58965332\n",
            "Iteration 23, loss = 0.58883483\n",
            "Iteration 24, loss = 0.58711022\n",
            "Iteration 25, loss = 0.58635150\n",
            "Iteration 26, loss = 0.58385152\n",
            "Iteration 27, loss = 0.58952413\n",
            "Iteration 28, loss = 0.58171815\n",
            "Iteration 29, loss = 0.58328586\n",
            "Iteration 30, loss = 0.57951843\n",
            "Iteration 31, loss = 0.57611843\n",
            "Iteration 32, loss = 0.57618144\n",
            "Iteration 33, loss = 0.57270118\n",
            "Iteration 34, loss = 0.57209495\n",
            "Iteration 35, loss = 0.56856438\n",
            "Iteration 36, loss = 0.56436820\n",
            "Iteration 37, loss = 0.56732430\n",
            "Iteration 38, loss = 0.56020755\n",
            "Iteration 39, loss = 0.55490663\n",
            "Iteration 40, loss = 0.55493965\n",
            "Iteration 41, loss = 0.55025378\n",
            "Iteration 42, loss = 0.54731412\n",
            "Iteration 43, loss = 0.54095570\n",
            "Iteration 44, loss = 0.53737610\n",
            "Iteration 45, loss = 0.53543337\n",
            "Iteration 46, loss = 0.52799484\n",
            "Iteration 47, loss = 0.52308166\n",
            "Iteration 48, loss = 0.53217439\n",
            "Iteration 49, loss = 0.52795902\n",
            "Iteration 50, loss = 0.51360704\n",
            "Iteration 51, loss = 0.51043801\n",
            "Iteration 52, loss = 0.50820726\n",
            "Iteration 53, loss = 0.50974112\n",
            "Iteration 54, loss = 0.50181935\n",
            "Iteration 55, loss = 0.50010659\n",
            "Iteration 56, loss = 0.50011537\n",
            "Iteration 57, loss = 0.49522678\n",
            "Iteration 58, loss = 0.49190304\n",
            "Iteration 59, loss = 0.50127259\n",
            "Iteration 60, loss = 0.50891297\n",
            "Iteration 61, loss = 0.49487184\n",
            "Iteration 62, loss = 0.49492849\n",
            "Iteration 63, loss = 0.49095560\n",
            "Iteration 64, loss = 0.48190921\n",
            "Iteration 65, loss = 0.47670062\n",
            "Iteration 66, loss = 0.48591265\n",
            "Iteration 67, loss = 0.47639515\n",
            "Iteration 68, loss = 0.47305230\n",
            "Iteration 69, loss = 0.47324361\n",
            "Iteration 70, loss = 0.47083458\n",
            "Iteration 71, loss = 0.46818262\n",
            "Iteration 72, loss = 0.46757810\n",
            "Iteration 73, loss = 0.46752878\n",
            "Iteration 74, loss = 0.46644373\n",
            "Iteration 75, loss = 0.46651652\n",
            "Iteration 76, loss = 0.46402244\n",
            "Iteration 77, loss = 0.46342359\n",
            "Iteration 78, loss = 0.48207718\n",
            "Iteration 79, loss = 0.47079272\n",
            "Iteration 80, loss = 0.46592405\n",
            "Iteration 81, loss = 0.46726294\n",
            "Iteration 82, loss = 0.46205483\n",
            "Iteration 83, loss = 0.46026947\n",
            "Iteration 84, loss = 0.45994428\n",
            "Iteration 85, loss = 0.45822435\n",
            "Iteration 86, loss = 0.46070207\n",
            "Iteration 87, loss = 0.46004279\n",
            "Iteration 88, loss = 0.46390702\n",
            "Iteration 89, loss = 0.45881230\n",
            "Iteration 90, loss = 0.45733329\n",
            "Iteration 91, loss = 0.45958537\n",
            "Iteration 92, loss = 0.45766965\n",
            "Iteration 93, loss = 0.45623968\n",
            "Iteration 94, loss = 0.45594767\n",
            "Iteration 95, loss = 0.45517883\n",
            "Iteration 96, loss = 0.45746426\n",
            "Iteration 97, loss = 0.45593870\n",
            "Iteration 98, loss = 0.45236643\n",
            "Iteration 99, loss = 0.45814793\n",
            "Iteration 100, loss = 0.45777215\n",
            "Iteration 101, loss = 0.45657027\n",
            "Iteration 102, loss = 0.45563270\n",
            "Iteration 103, loss = 0.45145175\n",
            "Iteration 104, loss = 0.45177403\n",
            "Iteration 105, loss = 0.45277880\n",
            "Iteration 106, loss = 0.45071934\n",
            "Iteration 107, loss = 0.45254099\n",
            "Iteration 108, loss = 0.45227657\n",
            "Iteration 109, loss = 0.45577155\n",
            "Iteration 110, loss = 0.45032442\n",
            "Iteration 111, loss = 0.45246817\n",
            "Iteration 112, loss = 0.45174308\n",
            "Iteration 113, loss = 0.44981534\n",
            "Iteration 114, loss = 0.44797397\n",
            "Iteration 115, loss = 0.45601395\n",
            "Iteration 116, loss = 0.44810048\n",
            "Iteration 117, loss = 0.44643030\n",
            "Iteration 118, loss = 0.45566621\n",
            "Iteration 119, loss = 0.44921936\n",
            "Iteration 120, loss = 0.44740553\n",
            "Iteration 121, loss = 0.44731518\n",
            "Iteration 122, loss = 0.44677515\n",
            "Iteration 123, loss = 0.45306457\n",
            "Iteration 124, loss = 0.44764735\n",
            "Iteration 125, loss = 0.45219595\n",
            "Iteration 126, loss = 0.46212186\n",
            "Iteration 127, loss = 0.44720669\n",
            "Iteration 128, loss = 0.45783500\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64822708\n",
            "Iteration 2, loss = 0.60725323\n",
            "Iteration 3, loss = 0.58234275\n",
            "Iteration 4, loss = 0.57041243\n",
            "Iteration 5, loss = 0.55319844\n",
            "Iteration 6, loss = 0.53634307\n",
            "Iteration 7, loss = 0.51693419\n",
            "Iteration 8, loss = 0.50630704\n",
            "Iteration 9, loss = 0.49379205\n",
            "Iteration 10, loss = 0.48443244\n",
            "Iteration 11, loss = 0.46775972\n",
            "Iteration 12, loss = 0.45761168\n",
            "Iteration 13, loss = 0.46108306\n",
            "Iteration 14, loss = 0.45176898\n",
            "Iteration 15, loss = 0.44306567\n",
            "Iteration 16, loss = 0.44489863\n",
            "Iteration 17, loss = 0.43951486\n",
            "Iteration 18, loss = 0.43604844\n",
            "Iteration 19, loss = 0.43458907\n",
            "Iteration 20, loss = 0.43153802\n",
            "Iteration 21, loss = 0.42916566\n",
            "Iteration 22, loss = 0.42925720\n",
            "Iteration 23, loss = 0.42897288\n",
            "Iteration 24, loss = 0.43098416\n",
            "Iteration 25, loss = 0.43017912\n",
            "Iteration 26, loss = 0.43188763\n",
            "Iteration 27, loss = 0.43854047\n",
            "Iteration 28, loss = 0.42630924\n",
            "Iteration 29, loss = 0.42026265\n",
            "Iteration 30, loss = 0.42892954\n",
            "Iteration 31, loss = 0.42073749\n",
            "Iteration 32, loss = 0.43235910\n",
            "Iteration 33, loss = 0.41963391\n",
            "Iteration 34, loss = 0.42076857\n",
            "Iteration 35, loss = 0.41677869\n",
            "Iteration 36, loss = 0.41692623\n",
            "Iteration 37, loss = 0.41806666\n",
            "Iteration 38, loss = 0.40812009\n",
            "Iteration 39, loss = 0.41490075\n",
            "Iteration 40, loss = 0.40819099\n",
            "Iteration 41, loss = 0.41392027\n",
            "Iteration 42, loss = 0.43278805\n",
            "Iteration 43, loss = 0.41110248\n",
            "Iteration 44, loss = 0.40999145\n",
            "Iteration 45, loss = 0.41120186\n",
            "Iteration 46, loss = 0.42928270\n",
            "Iteration 47, loss = 0.41997777\n",
            "Iteration 48, loss = 0.39716825\n",
            "Iteration 49, loss = 0.40487698\n",
            "Iteration 50, loss = 0.39724742\n",
            "Iteration 51, loss = 0.40627883\n",
            "Iteration 52, loss = 0.40334675\n",
            "Iteration 53, loss = 0.41056552\n",
            "Iteration 54, loss = 0.40571404\n",
            "Iteration 55, loss = 0.39805079\n",
            "Iteration 56, loss = 0.40291816\n",
            "Iteration 57, loss = 0.40369977\n",
            "Iteration 58, loss = 0.40351661\n",
            "Iteration 59, loss = 0.40084205\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62506511\n",
            "Iteration 2, loss = 0.60195889\n",
            "Iteration 3, loss = 0.59609083\n",
            "Iteration 4, loss = 0.58912691\n",
            "Iteration 5, loss = 0.58798781\n",
            "Iteration 6, loss = 0.58055371\n",
            "Iteration 7, loss = 0.57351441\n",
            "Iteration 8, loss = 0.57142061\n",
            "Iteration 9, loss = 0.55736780\n",
            "Iteration 10, loss = 0.54788510\n",
            "Iteration 11, loss = 0.53354523\n",
            "Iteration 12, loss = 0.52399218\n",
            "Iteration 13, loss = 0.50337285\n",
            "Iteration 14, loss = 0.49362995\n",
            "Iteration 15, loss = 0.48621686\n",
            "Iteration 16, loss = 0.47928627\n",
            "Iteration 17, loss = 0.46647463\n",
            "Iteration 18, loss = 0.46689603\n",
            "Iteration 19, loss = 0.45859705\n",
            "Iteration 20, loss = 0.45352680\n",
            "Iteration 21, loss = 0.44821699\n",
            "Iteration 22, loss = 0.45386680\n",
            "Iteration 23, loss = 0.46006368\n",
            "Iteration 24, loss = 0.44205703\n",
            "Iteration 25, loss = 0.43913298\n",
            "Iteration 26, loss = 0.43478586\n",
            "Iteration 27, loss = 0.43932125\n",
            "Iteration 28, loss = 0.44387187\n",
            "Iteration 29, loss = 0.42818504\n",
            "Iteration 30, loss = 0.43057510\n",
            "Iteration 31, loss = 0.42998470\n",
            "Iteration 32, loss = 0.42357387\n",
            "Iteration 33, loss = 0.42048095\n",
            "Iteration 34, loss = 0.42224097\n",
            "Iteration 35, loss = 0.41978351\n",
            "Iteration 36, loss = 0.41384827\n",
            "Iteration 37, loss = 0.42554025\n",
            "Iteration 38, loss = 0.41976500\n",
            "Iteration 39, loss = 0.41702033\n",
            "Iteration 40, loss = 0.41649626\n",
            "Iteration 41, loss = 0.41372552\n",
            "Iteration 42, loss = 0.41349061\n",
            "Iteration 43, loss = 0.41031201\n",
            "Iteration 44, loss = 0.41155988\n",
            "Iteration 45, loss = 0.40890185\n",
            "Iteration 46, loss = 0.41044720\n",
            "Iteration 47, loss = 0.40643738\n",
            "Iteration 48, loss = 0.41286986\n",
            "Iteration 49, loss = 0.40875908\n",
            "Iteration 50, loss = 0.40605972\n",
            "Iteration 51, loss = 0.40855100\n",
            "Iteration 52, loss = 0.41184517\n",
            "Iteration 53, loss = 0.40236056\n",
            "Iteration 54, loss = 0.40582906\n",
            "Iteration 55, loss = 0.41009107\n",
            "Iteration 56, loss = 0.40540233\n",
            "Iteration 57, loss = 0.40652348\n",
            "Iteration 58, loss = 0.40179419\n",
            "Iteration 59, loss = 0.41169507\n",
            "Iteration 60, loss = 0.40314669\n",
            "Iteration 61, loss = 0.40432192\n",
            "Iteration 62, loss = 0.40278355\n",
            "Iteration 63, loss = 0.40038107\n",
            "Iteration 64, loss = 0.40190494\n",
            "Iteration 65, loss = 0.40496667\n",
            "Iteration 66, loss = 0.39778753\n",
            "Iteration 67, loss = 0.40123943\n",
            "Iteration 68, loss = 0.41580866\n",
            "Iteration 69, loss = 0.39612820\n",
            "Iteration 70, loss = 0.40443070\n",
            "Iteration 71, loss = 0.39022850\n",
            "Iteration 72, loss = 0.40026859\n",
            "Iteration 73, loss = 0.39861994\n",
            "Iteration 74, loss = 0.39838124\n",
            "Iteration 75, loss = 0.39011440\n",
            "Iteration 76, loss = 0.39047971\n",
            "Iteration 77, loss = 0.38762633\n",
            "Iteration 78, loss = 0.39544150\n",
            "Iteration 79, loss = 0.39125494\n",
            "Iteration 80, loss = 0.39167320\n",
            "Iteration 81, loss = 0.39009480\n",
            "Iteration 82, loss = 0.38833444\n",
            "Iteration 83, loss = 0.39203225\n",
            "Iteration 84, loss = 0.39022804\n",
            "Iteration 85, loss = 0.38830906\n",
            "Iteration 86, loss = 0.38344714\n",
            "Iteration 87, loss = 0.40394684\n",
            "Iteration 88, loss = 0.39413378\n",
            "Iteration 89, loss = 0.39371227\n",
            "Iteration 90, loss = 0.39340043\n",
            "Iteration 91, loss = 0.38456036\n",
            "Iteration 92, loss = 0.39139296\n",
            "Iteration 93, loss = 0.38264271\n",
            "Iteration 94, loss = 0.38222276\n",
            "Iteration 95, loss = 0.38225737\n",
            "Iteration 96, loss = 0.38598066\n",
            "Iteration 97, loss = 0.38482967\n",
            "Iteration 98, loss = 0.38377432\n",
            "Iteration 99, loss = 0.38147033\n",
            "Iteration 100, loss = 0.38262840\n",
            "Iteration 101, loss = 0.38854795\n",
            "Iteration 102, loss = 0.39139896\n",
            "Iteration 103, loss = 0.38690335\n",
            "Iteration 104, loss = 0.38201834\n",
            "Iteration 105, loss = 0.38439009\n",
            "Iteration 106, loss = 0.38253985\n",
            "Iteration 107, loss = 0.38548613\n",
            "Iteration 108, loss = 0.38151314\n",
            "Iteration 109, loss = 0.38092852\n",
            "Iteration 110, loss = 0.37914702\n",
            "Iteration 111, loss = 0.37602698\n",
            "Iteration 112, loss = 0.37599759\n",
            "Iteration 113, loss = 0.37789737\n",
            "Iteration 114, loss = 0.37645720\n",
            "Iteration 115, loss = 0.38375465\n",
            "Iteration 116, loss = 0.38866442\n",
            "Iteration 117, loss = 0.37308641\n",
            "Iteration 118, loss = 0.38129684\n",
            "Iteration 119, loss = 0.38479357\n",
            "Iteration 120, loss = 0.37681867\n",
            "Iteration 121, loss = 0.37656108\n",
            "Iteration 122, loss = 0.37207409\n",
            "Iteration 123, loss = 0.37499479\n",
            "Iteration 124, loss = 0.37453735\n",
            "Iteration 125, loss = 0.38791804\n",
            "Iteration 126, loss = 0.38576901\n",
            "Iteration 127, loss = 0.38323213\n",
            "Iteration 128, loss = 0.37666805\n",
            "Iteration 129, loss = 0.36995954\n",
            "Iteration 130, loss = 0.37942613\n",
            "Iteration 131, loss = 0.37008440\n",
            "Iteration 132, loss = 0.37225156\n",
            "Iteration 133, loss = 0.37548778\n",
            "Iteration 134, loss = 0.37420621\n",
            "Iteration 135, loss = 0.37047430\n",
            "Iteration 136, loss = 0.37853793\n",
            "Iteration 137, loss = 0.37074927\n",
            "Iteration 138, loss = 0.37289052\n",
            "Iteration 139, loss = 0.37448976\n",
            "Iteration 140, loss = 0.37180767\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67409818\n",
            "Iteration 2, loss = 0.62742412\n",
            "Iteration 3, loss = 0.62283900\n",
            "Iteration 4, loss = 0.61410388\n",
            "Iteration 5, loss = 0.61087019\n",
            "Iteration 6, loss = 0.59632056\n",
            "Iteration 7, loss = 0.58445836\n",
            "Iteration 8, loss = 0.56540040\n",
            "Iteration 9, loss = 0.55045035\n",
            "Iteration 10, loss = 0.52856477\n",
            "Iteration 11, loss = 0.52058188\n",
            "Iteration 12, loss = 0.50430176\n",
            "Iteration 13, loss = 0.49154271\n",
            "Iteration 14, loss = 0.48644313\n",
            "Iteration 15, loss = 0.48040868\n",
            "Iteration 16, loss = 0.47164145\n",
            "Iteration 17, loss = 0.46867262\n",
            "Iteration 18, loss = 0.46589874\n",
            "Iteration 19, loss = 0.45858214\n",
            "Iteration 20, loss = 0.45900553\n",
            "Iteration 21, loss = 0.45507680\n",
            "Iteration 22, loss = 0.44985054\n",
            "Iteration 23, loss = 0.45912161\n",
            "Iteration 24, loss = 0.46147713\n",
            "Iteration 25, loss = 0.45527458\n",
            "Iteration 26, loss = 0.44668940\n",
            "Iteration 27, loss = 0.45931340\n",
            "Iteration 28, loss = 0.43950163\n",
            "Iteration 29, loss = 0.45246858\n",
            "Iteration 30, loss = 0.44136238\n",
            "Iteration 31, loss = 0.44316857\n",
            "Iteration 32, loss = 0.45126913\n",
            "Iteration 33, loss = 0.43516185\n",
            "Iteration 34, loss = 0.43269757\n",
            "Iteration 35, loss = 0.43594243\n",
            "Iteration 36, loss = 0.43342469\n",
            "Iteration 37, loss = 0.43354773\n",
            "Iteration 38, loss = 0.43647213\n",
            "Iteration 39, loss = 0.42629492\n",
            "Iteration 40, loss = 0.43727476\n",
            "Iteration 41, loss = 0.43477614\n",
            "Iteration 42, loss = 0.43464959\n",
            "Iteration 43, loss = 0.42766184\n",
            "Iteration 44, loss = 0.43150508\n",
            "Iteration 45, loss = 0.42362859\n",
            "Iteration 46, loss = 0.43289719\n",
            "Iteration 47, loss = 0.42639030\n",
            "Iteration 48, loss = 0.42908140\n",
            "Iteration 49, loss = 0.42179641\n",
            "Iteration 50, loss = 0.43112828\n",
            "Iteration 51, loss = 0.42517528\n",
            "Iteration 52, loss = 0.42605657\n",
            "Iteration 53, loss = 0.42144126\n",
            "Iteration 54, loss = 0.42345127\n",
            "Iteration 55, loss = 0.42141044\n",
            "Iteration 56, loss = 0.42371042\n",
            "Iteration 57, loss = 0.41870986\n",
            "Iteration 58, loss = 0.41145003\n",
            "Iteration 59, loss = 0.41974690\n",
            "Iteration 60, loss = 0.41530406\n",
            "Iteration 61, loss = 0.42003635\n",
            "Iteration 62, loss = 0.41451977\n",
            "Iteration 63, loss = 0.41271423\n",
            "Iteration 64, loss = 0.41339424\n",
            "Iteration 65, loss = 0.42518199\n",
            "Iteration 66, loss = 0.42025992\n",
            "Iteration 67, loss = 0.42181910\n",
            "Iteration 68, loss = 0.41996033\n",
            "Iteration 69, loss = 0.41559706\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61589689\n",
            "Iteration 2, loss = 0.60359683\n",
            "Iteration 3, loss = 0.59923604\n",
            "Iteration 4, loss = 0.59737299\n",
            "Iteration 5, loss = 0.58367203\n",
            "Iteration 6, loss = 0.57075211\n",
            "Iteration 7, loss = 0.56217173\n",
            "Iteration 8, loss = 0.54596179\n",
            "Iteration 9, loss = 0.52950499\n",
            "Iteration 10, loss = 0.52113245\n",
            "Iteration 11, loss = 0.50725389\n",
            "Iteration 12, loss = 0.48515361\n",
            "Iteration 13, loss = 0.47619238\n",
            "Iteration 14, loss = 0.47318494\n",
            "Iteration 15, loss = 0.46118154\n",
            "Iteration 16, loss = 0.46393338\n",
            "Iteration 17, loss = 0.46188122\n",
            "Iteration 18, loss = 0.45788597\n",
            "Iteration 19, loss = 0.44838372\n",
            "Iteration 20, loss = 0.45274359\n",
            "Iteration 21, loss = 0.44269791\n",
            "Iteration 22, loss = 0.43589178\n",
            "Iteration 23, loss = 0.45396793\n",
            "Iteration 24, loss = 0.43884548\n",
            "Iteration 25, loss = 0.43506377\n",
            "Iteration 26, loss = 0.43086647\n",
            "Iteration 27, loss = 0.43610726\n",
            "Iteration 28, loss = 0.43407478\n",
            "Iteration 29, loss = 0.43727404\n",
            "Iteration 30, loss = 0.42383990\n",
            "Iteration 31, loss = 0.43185084\n",
            "Iteration 32, loss = 0.43024267\n",
            "Iteration 33, loss = 0.41834729\n",
            "Iteration 34, loss = 0.42772747\n",
            "Iteration 35, loss = 0.43193354\n",
            "Iteration 36, loss = 0.42101551\n",
            "Iteration 37, loss = 0.42139882\n",
            "Iteration 38, loss = 0.42129546\n",
            "Iteration 39, loss = 0.41750566\n",
            "Iteration 40, loss = 0.42969562\n",
            "Iteration 41, loss = 0.41814153\n",
            "Iteration 42, loss = 0.41297233\n",
            "Iteration 43, loss = 0.41685046\n",
            "Iteration 44, loss = 0.42657888\n",
            "Iteration 45, loss = 0.40812727\n",
            "Iteration 46, loss = 0.41867384\n",
            "Iteration 47, loss = 0.41374220\n",
            "Iteration 48, loss = 0.41177668\n",
            "Iteration 49, loss = 0.41199711\n",
            "Iteration 50, loss = 0.41109392\n",
            "Iteration 51, loss = 0.40657189\n",
            "Iteration 52, loss = 0.41091110\n",
            "Iteration 53, loss = 0.40502807\n",
            "Iteration 54, loss = 0.40658922\n",
            "Iteration 55, loss = 0.39827750\n",
            "Iteration 56, loss = 0.40278990\n",
            "Iteration 57, loss = 0.42239844\n",
            "Iteration 58, loss = 0.40476853\n",
            "Iteration 59, loss = 0.40970521\n",
            "Iteration 60, loss = 0.40513594\n",
            "Iteration 61, loss = 0.41272493\n",
            "Iteration 62, loss = 0.41149238\n",
            "Iteration 63, loss = 0.40115741\n",
            "Iteration 64, loss = 0.40356397\n",
            "Iteration 65, loss = 0.39653765\n",
            "Iteration 66, loss = 0.40306964\n",
            "Iteration 67, loss = 0.39607684\n",
            "Iteration 68, loss = 0.40873652\n",
            "Iteration 69, loss = 0.39780962\n",
            "Iteration 70, loss = 0.39162838\n",
            "Iteration 71, loss = 0.39427025\n",
            "Iteration 72, loss = 0.39004247\n",
            "Iteration 73, loss = 0.40274218\n",
            "Iteration 74, loss = 0.40009789\n",
            "Iteration 75, loss = 0.39014612\n",
            "Iteration 76, loss = 0.39328881\n",
            "Iteration 77, loss = 0.39038555\n",
            "Iteration 78, loss = 0.45596559\n",
            "Iteration 79, loss = 0.39851715\n",
            "Iteration 80, loss = 0.39306541\n",
            "Iteration 81, loss = 0.39311829\n",
            "Iteration 82, loss = 0.38755647\n",
            "Iteration 83, loss = 0.38934261\n",
            "Iteration 84, loss = 0.38861228\n",
            "Iteration 85, loss = 0.38407875\n",
            "Iteration 86, loss = 0.39119322\n",
            "Iteration 87, loss = 0.38859690\n",
            "Iteration 88, loss = 0.38472209\n",
            "Iteration 89, loss = 0.38601167\n",
            "Iteration 90, loss = 0.38029498\n",
            "Iteration 91, loss = 0.38017992\n",
            "Iteration 92, loss = 0.37800001\n",
            "Iteration 93, loss = 0.38485765\n",
            "Iteration 94, loss = 0.37721602\n",
            "Iteration 95, loss = 0.38306517\n",
            "Iteration 96, loss = 0.39003683\n",
            "Iteration 97, loss = 0.37824590\n",
            "Iteration 98, loss = 0.38332097\n",
            "Iteration 99, loss = 0.37627894\n",
            "Iteration 100, loss = 0.37752039\n",
            "Iteration 101, loss = 0.37839861\n",
            "Iteration 102, loss = 0.38441842\n",
            "Iteration 103, loss = 0.37924915\n",
            "Iteration 104, loss = 0.38026743\n",
            "Iteration 105, loss = 0.37297048\n",
            "Iteration 106, loss = 0.37798893\n",
            "Iteration 107, loss = 0.37591848\n",
            "Iteration 108, loss = 0.37281578\n",
            "Iteration 109, loss = 0.37811341\n",
            "Iteration 110, loss = 0.37132999\n",
            "Iteration 111, loss = 0.37853599\n",
            "Iteration 112, loss = 0.36910543\n",
            "Iteration 113, loss = 0.37231462\n",
            "Iteration 114, loss = 0.36933893\n",
            "Iteration 115, loss = 0.37113245\n",
            "Iteration 116, loss = 0.37442328\n",
            "Iteration 117, loss = 0.37730904\n",
            "Iteration 118, loss = 0.36617012\n",
            "Iteration 119, loss = 0.37260569\n",
            "Iteration 120, loss = 0.37066586\n",
            "Iteration 121, loss = 0.37162268\n",
            "Iteration 122, loss = 0.36949911\n",
            "Iteration 123, loss = 0.37016920\n",
            "Iteration 124, loss = 0.37475475\n",
            "Iteration 125, loss = 0.37289659\n",
            "Iteration 126, loss = 0.36812004\n",
            "Iteration 127, loss = 0.36646709\n",
            "Iteration 128, loss = 0.37226873\n",
            "Iteration 129, loss = 0.37110249\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67039746\n",
            "Iteration 2, loss = 0.61521437\n",
            "Iteration 3, loss = 0.60700155\n",
            "Iteration 4, loss = 0.59420328\n",
            "Iteration 5, loss = 0.58442895\n",
            "Iteration 6, loss = 0.57615927\n",
            "Iteration 7, loss = 0.56792275\n",
            "Iteration 8, loss = 0.55473548\n",
            "Iteration 9, loss = 0.54809798\n",
            "Iteration 10, loss = 0.52852557\n",
            "Iteration 11, loss = 0.51631107\n",
            "Iteration 12, loss = 0.50916316\n",
            "Iteration 13, loss = 0.50005639\n",
            "Iteration 14, loss = 0.50038794\n",
            "Iteration 15, loss = 0.49179957\n",
            "Iteration 16, loss = 0.47683753\n",
            "Iteration 17, loss = 0.48260991\n",
            "Iteration 18, loss = 0.47652157\n",
            "Iteration 19, loss = 0.46959579\n",
            "Iteration 20, loss = 0.48613741\n",
            "Iteration 21, loss = 0.46628067\n",
            "Iteration 22, loss = 0.46758711\n",
            "Iteration 23, loss = 0.46422663\n",
            "Iteration 24, loss = 0.46208163\n",
            "Iteration 25, loss = 0.45817645\n",
            "Iteration 26, loss = 0.45367472\n",
            "Iteration 27, loss = 0.45717799\n",
            "Iteration 28, loss = 0.44520107\n",
            "Iteration 29, loss = 0.45076023\n",
            "Iteration 30, loss = 0.45011758\n",
            "Iteration 31, loss = 0.44669240\n",
            "Iteration 32, loss = 0.44112888\n",
            "Iteration 33, loss = 0.44023075\n",
            "Iteration 34, loss = 0.44151911\n",
            "Iteration 35, loss = 0.45882016\n",
            "Iteration 36, loss = 0.44499784\n",
            "Iteration 37, loss = 0.43365037\n",
            "Iteration 38, loss = 0.43435679\n",
            "Iteration 39, loss = 0.42954909\n",
            "Iteration 40, loss = 0.43524038\n",
            "Iteration 41, loss = 0.43793241\n",
            "Iteration 42, loss = 0.44558640\n",
            "Iteration 43, loss = 0.42986441\n",
            "Iteration 44, loss = 0.42607147\n",
            "Iteration 45, loss = 0.43199111\n",
            "Iteration 46, loss = 0.42265739\n",
            "Iteration 47, loss = 0.43293349\n",
            "Iteration 48, loss = 0.42205468\n",
            "Iteration 49, loss = 0.42004651\n",
            "Iteration 50, loss = 0.42123320\n",
            "Iteration 51, loss = 0.41778948\n",
            "Iteration 52, loss = 0.41688976\n",
            "Iteration 53, loss = 0.41468089\n",
            "Iteration 54, loss = 0.41290757\n",
            "Iteration 55, loss = 0.41924719\n",
            "Iteration 56, loss = 0.41594172\n",
            "Iteration 57, loss = 0.42070833\n",
            "Iteration 58, loss = 0.40946669\n",
            "Iteration 59, loss = 0.41988478\n",
            "Iteration 60, loss = 0.41456185\n",
            "Iteration 61, loss = 0.41286740\n",
            "Iteration 62, loss = 0.41155507\n",
            "Iteration 63, loss = 0.40929476\n",
            "Iteration 64, loss = 0.41479324\n",
            "Iteration 65, loss = 0.41123450\n",
            "Iteration 66, loss = 0.40288802\n",
            "Iteration 67, loss = 0.39876125\n",
            "Iteration 68, loss = 0.40505255\n",
            "Iteration 69, loss = 0.41202997\n",
            "Iteration 70, loss = 0.40566525\n",
            "Iteration 71, loss = 0.40337396\n",
            "Iteration 72, loss = 0.40547904\n",
            "Iteration 73, loss = 0.40398498\n",
            "Iteration 74, loss = 0.40324844\n",
            "Iteration 75, loss = 0.40418981\n",
            "Iteration 76, loss = 0.40645623\n",
            "Iteration 77, loss = 0.39631838\n",
            "Iteration 78, loss = 0.39235947\n",
            "Iteration 79, loss = 0.38788584\n",
            "Iteration 80, loss = 0.40073485\n",
            "Iteration 81, loss = 0.40321218\n",
            "Iteration 82, loss = 0.39970445\n",
            "Iteration 83, loss = 0.39931581\n",
            "Iteration 84, loss = 0.40215423\n",
            "Iteration 85, loss = 0.39056813\n",
            "Iteration 86, loss = 0.39879660\n",
            "Iteration 87, loss = 0.40147046\n",
            "Iteration 88, loss = 0.39195756\n",
            "Iteration 89, loss = 0.39373289\n",
            "Iteration 90, loss = 0.38908885\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68730124\n",
            "Iteration 2, loss = 0.68059218\n",
            "Iteration 3, loss = 0.67336585\n",
            "Iteration 4, loss = 0.66928889\n",
            "Iteration 5, loss = 0.66712642\n",
            "Iteration 6, loss = 0.66578859\n",
            "Iteration 7, loss = 0.66472897\n",
            "Iteration 8, loss = 0.66445702\n",
            "Iteration 9, loss = 0.66365472\n",
            "Iteration 10, loss = 0.66337175\n",
            "Iteration 11, loss = 0.66244978\n",
            "Iteration 12, loss = 0.66204519\n",
            "Iteration 13, loss = 0.66126016\n",
            "Iteration 14, loss = 0.66111800\n",
            "Iteration 15, loss = 0.66004702\n",
            "Iteration 16, loss = 0.65962100\n",
            "Iteration 17, loss = 0.65919153\n",
            "Iteration 18, loss = 0.65893790\n",
            "Iteration 19, loss = 0.65840697\n",
            "Iteration 20, loss = 0.65799735\n",
            "Iteration 21, loss = 0.65769693\n",
            "Iteration 22, loss = 0.65723198\n",
            "Iteration 23, loss = 0.65658102\n",
            "Iteration 24, loss = 0.65649404\n",
            "Iteration 25, loss = 0.65568186\n",
            "Iteration 26, loss = 0.65537910\n",
            "Iteration 27, loss = 0.65498060\n",
            "Iteration 28, loss = 0.65452975\n",
            "Iteration 29, loss = 0.65409420\n",
            "Iteration 30, loss = 0.65384788\n",
            "Iteration 31, loss = 0.65364541\n",
            "Iteration 32, loss = 0.65316513\n",
            "Iteration 33, loss = 0.65278544\n",
            "Iteration 34, loss = 0.65232522\n",
            "Iteration 35, loss = 0.65202005\n",
            "Iteration 36, loss = 0.65143980\n",
            "Iteration 37, loss = 0.65141757\n",
            "Iteration 38, loss = 0.65103813\n",
            "Iteration 39, loss = 0.65113368\n",
            "Iteration 40, loss = 0.65099075\n",
            "Iteration 41, loss = 0.64953545\n",
            "Iteration 42, loss = 0.64925736\n",
            "Iteration 43, loss = 0.64868242\n",
            "Iteration 44, loss = 0.64841056\n",
            "Iteration 45, loss = 0.64800518\n",
            "Iteration 46, loss = 0.64761163\n",
            "Iteration 47, loss = 0.64728625\n",
            "Iteration 48, loss = 0.64716619\n",
            "Iteration 49, loss = 0.64661739\n",
            "Iteration 50, loss = 0.64659925\n",
            "Iteration 51, loss = 0.64570741\n",
            "Iteration 52, loss = 0.64548253\n",
            "Iteration 53, loss = 0.64501384\n",
            "Iteration 54, loss = 0.64492767\n",
            "Iteration 55, loss = 0.64449671\n",
            "Iteration 56, loss = 0.64418698\n",
            "Iteration 57, loss = 0.64402135\n",
            "Iteration 58, loss = 0.64347028\n",
            "Iteration 59, loss = 0.64331300\n",
            "Iteration 60, loss = 0.64268653\n",
            "Iteration 61, loss = 0.64210829\n",
            "Iteration 62, loss = 0.64176683\n",
            "Iteration 63, loss = 0.64143035\n",
            "Iteration 64, loss = 0.64100932\n",
            "Iteration 65, loss = 0.64059541\n",
            "Iteration 66, loss = 0.64025649\n",
            "Iteration 67, loss = 0.63996405\n",
            "Iteration 68, loss = 0.63948612\n",
            "Iteration 69, loss = 0.63935519\n",
            "Iteration 70, loss = 0.63886727\n",
            "Iteration 71, loss = 0.63849429\n",
            "Iteration 72, loss = 0.63832099\n",
            "Iteration 73, loss = 0.63792621\n",
            "Iteration 74, loss = 0.63746050\n",
            "Iteration 75, loss = 0.63706748\n",
            "Iteration 76, loss = 0.63696373\n",
            "Iteration 77, loss = 0.63645026\n",
            "Iteration 78, loss = 0.63638350\n",
            "Iteration 79, loss = 0.63588168\n",
            "Iteration 80, loss = 0.63522805\n",
            "Iteration 81, loss = 0.63503176\n",
            "Iteration 82, loss = 0.63459439\n",
            "Iteration 83, loss = 0.63439984\n",
            "Iteration 84, loss = 0.63391178\n",
            "Iteration 85, loss = 0.63340220\n",
            "Iteration 86, loss = 0.63314630\n",
            "Iteration 87, loss = 0.63264449\n",
            "Iteration 88, loss = 0.63242515\n",
            "Iteration 89, loss = 0.63211446\n",
            "Iteration 90, loss = 0.63152415\n",
            "Iteration 91, loss = 0.63122268\n",
            "Iteration 92, loss = 0.63104154\n",
            "Iteration 93, loss = 0.63059498\n",
            "Iteration 94, loss = 0.63027872\n",
            "Iteration 95, loss = 0.62999412\n",
            "Iteration 96, loss = 0.62977322\n",
            "Iteration 97, loss = 0.62914262\n",
            "Iteration 98, loss = 0.62877184\n",
            "Iteration 99, loss = 0.62838903\n",
            "Iteration 100, loss = 0.62802935\n",
            "Iteration 101, loss = 0.62777900\n",
            "Iteration 102, loss = 0.62731404\n",
            "Iteration 103, loss = 0.62719491\n",
            "Iteration 104, loss = 0.62686827\n",
            "Iteration 105, loss = 0.62656199\n",
            "Iteration 106, loss = 0.62629358\n",
            "Iteration 107, loss = 0.62607370\n",
            "Iteration 108, loss = 0.62537208\n",
            "Iteration 109, loss = 0.62512209\n",
            "Iteration 110, loss = 0.62481049\n",
            "Iteration 111, loss = 0.62443886\n",
            "Iteration 112, loss = 0.62398502\n",
            "Iteration 113, loss = 0.62388515\n",
            "Iteration 114, loss = 0.62356487\n",
            "Iteration 115, loss = 0.62299527\n",
            "Iteration 116, loss = 0.62266986\n",
            "Iteration 117, loss = 0.62236572\n",
            "Iteration 118, loss = 0.62214455\n",
            "Iteration 119, loss = 0.62197457\n",
            "Iteration 120, loss = 0.62169124\n",
            "Iteration 121, loss = 0.62154861\n",
            "Iteration 122, loss = 0.62103244\n",
            "Iteration 123, loss = 0.62057587\n",
            "Iteration 124, loss = 0.62031111\n",
            "Iteration 125, loss = 0.61999788\n",
            "Iteration 126, loss = 0.62001031\n",
            "Iteration 127, loss = 0.61972754\n",
            "Iteration 128, loss = 0.61955999\n",
            "Iteration 129, loss = 0.61914701\n",
            "Iteration 130, loss = 0.61911056\n",
            "Iteration 131, loss = 0.61855289\n",
            "Iteration 132, loss = 0.61862570\n",
            "Iteration 133, loss = 0.61819667\n",
            "Iteration 134, loss = 0.61775528\n",
            "Iteration 135, loss = 0.61774295\n",
            "Iteration 136, loss = 0.61754639\n",
            "Iteration 137, loss = 0.61728054\n",
            "Iteration 138, loss = 0.61697152\n",
            "Iteration 139, loss = 0.61668965\n",
            "Iteration 140, loss = 0.61650272\n",
            "Iteration 141, loss = 0.61624079\n",
            "Iteration 142, loss = 0.61613441\n",
            "Iteration 143, loss = 0.61577908\n",
            "Iteration 144, loss = 0.61541549\n",
            "Iteration 145, loss = 0.61496399\n",
            "Iteration 146, loss = 0.61487908\n",
            "Iteration 147, loss = 0.61446667\n",
            "Iteration 148, loss = 0.61425101\n",
            "Iteration 149, loss = 0.61424662\n",
            "Iteration 150, loss = 0.61391644\n",
            "Iteration 151, loss = 0.61383963\n",
            "Iteration 152, loss = 0.61384978\n",
            "Iteration 153, loss = 0.61347462\n",
            "Iteration 154, loss = 0.61324147\n",
            "Iteration 155, loss = 0.61309531\n",
            "Iteration 156, loss = 0.61302160\n",
            "Iteration 157, loss = 0.61275132\n",
            "Iteration 158, loss = 0.61259371\n",
            "Iteration 159, loss = 0.61240222\n",
            "Iteration 160, loss = 0.61238846\n",
            "Iteration 161, loss = 0.61215446\n",
            "Iteration 162, loss = 0.61179911\n",
            "Iteration 163, loss = 0.61189187\n",
            "Iteration 164, loss = 0.61148051\n",
            "Iteration 165, loss = 0.61142266\n",
            "Iteration 166, loss = 0.61136370\n",
            "Iteration 167, loss = 0.61114547\n",
            "Iteration 168, loss = 0.61090970\n",
            "Iteration 169, loss = 0.61112715\n",
            "Iteration 170, loss = 0.61096188\n",
            "Iteration 171, loss = 0.61062933\n",
            "Iteration 172, loss = 0.61053865\n",
            "Iteration 173, loss = 0.61042670\n",
            "Iteration 174, loss = 0.61034792\n",
            "Iteration 175, loss = 0.61025427\n",
            "Iteration 176, loss = 0.61020434\n",
            "Iteration 177, loss = 0.61007099\n",
            "Iteration 178, loss = 0.60995967\n",
            "Iteration 179, loss = 0.60967949\n",
            "Iteration 180, loss = 0.61035928\n",
            "Iteration 181, loss = 0.61024637\n",
            "Iteration 182, loss = 0.60947141\n",
            "Iteration 183, loss = 0.60943727\n",
            "Iteration 184, loss = 0.60932019\n",
            "Iteration 185, loss = 0.60970335\n",
            "Iteration 186, loss = 0.60916721\n",
            "Iteration 187, loss = 0.60927506\n",
            "Iteration 188, loss = 0.60952182\n",
            "Iteration 189, loss = 0.60951579\n",
            "Iteration 190, loss = 0.60912967\n",
            "Iteration 191, loss = 0.60886543\n",
            "Iteration 192, loss = 0.60913771\n",
            "Iteration 193, loss = 0.60910112\n",
            "Iteration 194, loss = 0.60883781\n",
            "Iteration 195, loss = 0.60875185\n",
            "Iteration 196, loss = 0.60871869\n",
            "Iteration 197, loss = 0.60887563\n",
            "Iteration 198, loss = 0.60881556\n",
            "Iteration 199, loss = 0.60875393\n",
            "Iteration 200, loss = 0.60854113\n",
            "Iteration 1, loss = 0.79765100\n",
            "Iteration 2, loss = 0.74112182\n",
            "Iteration 3, loss = 0.69660525\n",
            "Iteration 4, loss = 0.67399893\n",
            "Iteration 5, loss = 0.66137869\n",
            "Iteration 6, loss = 0.65963592\n",
            "Iteration 7, loss = 0.65853773\n",
            "Iteration 8, loss = 0.65783134\n",
            "Iteration 9, loss = 0.65717111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 10, loss = 0.65656101\n",
            "Iteration 11, loss = 0.65615184\n",
            "Iteration 12, loss = 0.65561694\n",
            "Iteration 13, loss = 0.65521051\n",
            "Iteration 14, loss = 0.65473367\n",
            "Iteration 15, loss = 0.65431076\n",
            "Iteration 16, loss = 0.65397662\n",
            "Iteration 17, loss = 0.65364161\n",
            "Iteration 18, loss = 0.65327491\n",
            "Iteration 19, loss = 0.65263226\n",
            "Iteration 20, loss = 0.65207692\n",
            "Iteration 21, loss = 0.65158045\n",
            "Iteration 22, loss = 0.65101337\n",
            "Iteration 23, loss = 0.65054189\n",
            "Iteration 24, loss = 0.65009563\n",
            "Iteration 25, loss = 0.64966366\n",
            "Iteration 26, loss = 0.64927734\n",
            "Iteration 27, loss = 0.64899759\n",
            "Iteration 28, loss = 0.64858085\n",
            "Iteration 29, loss = 0.64840085\n",
            "Iteration 30, loss = 0.64783037\n",
            "Iteration 31, loss = 0.64748092\n",
            "Iteration 32, loss = 0.64724044\n",
            "Iteration 33, loss = 0.64668833\n",
            "Iteration 34, loss = 0.64652736\n",
            "Iteration 35, loss = 0.64612697\n",
            "Iteration 36, loss = 0.64594465\n",
            "Iteration 37, loss = 0.64530696\n",
            "Iteration 38, loss = 0.64481766\n",
            "Iteration 39, loss = 0.64453397\n",
            "Iteration 40, loss = 0.64432893\n",
            "Iteration 41, loss = 0.64403377\n",
            "Iteration 42, loss = 0.64346184\n",
            "Iteration 43, loss = 0.64314844\n",
            "Iteration 44, loss = 0.64263979\n",
            "Iteration 45, loss = 0.64223368\n",
            "Iteration 46, loss = 0.64188128\n",
            "Iteration 47, loss = 0.64168903\n",
            "Iteration 48, loss = 0.64142374\n",
            "Iteration 49, loss = 0.64079484\n",
            "Iteration 50, loss = 0.64030421\n",
            "Iteration 51, loss = 0.64013607\n",
            "Iteration 52, loss = 0.63981607\n",
            "Iteration 53, loss = 0.63946073\n",
            "Iteration 54, loss = 0.63888450\n",
            "Iteration 55, loss = 0.63831048\n",
            "Iteration 56, loss = 0.63839205\n",
            "Iteration 57, loss = 0.63802354\n",
            "Iteration 58, loss = 0.63743821\n",
            "Iteration 59, loss = 0.63849512\n",
            "Iteration 60, loss = 0.63690838\n",
            "Iteration 61, loss = 0.63653528\n",
            "Iteration 62, loss = 0.63598913\n",
            "Iteration 63, loss = 0.63560819\n",
            "Iteration 64, loss = 0.63520041\n",
            "Iteration 65, loss = 0.63482836\n",
            "Iteration 66, loss = 0.63455874\n",
            "Iteration 67, loss = 0.63415272\n",
            "Iteration 68, loss = 0.63372756\n",
            "Iteration 69, loss = 0.63348579\n",
            "Iteration 70, loss = 0.63331938\n",
            "Iteration 71, loss = 0.63259693\n",
            "Iteration 72, loss = 0.63225928\n",
            "Iteration 73, loss = 0.63196041\n",
            "Iteration 74, loss = 0.63147841\n",
            "Iteration 75, loss = 0.63111211\n",
            "Iteration 76, loss = 0.63073626\n",
            "Iteration 77, loss = 0.63115368\n",
            "Iteration 78, loss = 0.63062213\n",
            "Iteration 79, loss = 0.62973456\n",
            "Iteration 80, loss = 0.62938872\n",
            "Iteration 81, loss = 0.62903329\n",
            "Iteration 82, loss = 0.62881640\n",
            "Iteration 83, loss = 0.62831412\n",
            "Iteration 84, loss = 0.62794587\n",
            "Iteration 85, loss = 0.62754912\n",
            "Iteration 86, loss = 0.62711274\n",
            "Iteration 87, loss = 0.62689831\n",
            "Iteration 88, loss = 0.62651675\n",
            "Iteration 89, loss = 0.62611241\n",
            "Iteration 90, loss = 0.62592612\n",
            "Iteration 91, loss = 0.62570343\n",
            "Iteration 92, loss = 0.62529091\n",
            "Iteration 93, loss = 0.62510070\n",
            "Iteration 94, loss = 0.62467876\n",
            "Iteration 95, loss = 0.62420574\n",
            "Iteration 96, loss = 0.62437985\n",
            "Iteration 97, loss = 0.62365842\n",
            "Iteration 98, loss = 0.62370781\n",
            "Iteration 99, loss = 0.62327616\n",
            "Iteration 100, loss = 0.62296292\n",
            "Iteration 101, loss = 0.62259934\n",
            "Iteration 102, loss = 0.62253725\n",
            "Iteration 103, loss = 0.62264361\n",
            "Iteration 104, loss = 0.62185245\n",
            "Iteration 105, loss = 0.62136465\n",
            "Iteration 106, loss = 0.62086834\n",
            "Iteration 107, loss = 0.62060913\n",
            "Iteration 108, loss = 0.62037083\n",
            "Iteration 109, loss = 0.62003693\n",
            "Iteration 110, loss = 0.61979042\n",
            "Iteration 111, loss = 0.61983496\n",
            "Iteration 112, loss = 0.61940987\n",
            "Iteration 113, loss = 0.61897013\n",
            "Iteration 114, loss = 0.61896778\n",
            "Iteration 115, loss = 0.61864006\n",
            "Iteration 116, loss = 0.61844371\n",
            "Iteration 117, loss = 0.61817775\n",
            "Iteration 118, loss = 0.61809627\n",
            "Iteration 119, loss = 0.61770959\n",
            "Iteration 120, loss = 0.61717797\n",
            "Iteration 121, loss = 0.61703909\n",
            "Iteration 122, loss = 0.61657521\n",
            "Iteration 123, loss = 0.61620791\n",
            "Iteration 124, loss = 0.61610830\n",
            "Iteration 125, loss = 0.61586077\n",
            "Iteration 126, loss = 0.61568367\n",
            "Iteration 127, loss = 0.61535616\n",
            "Iteration 128, loss = 0.61518462\n",
            "Iteration 129, loss = 0.61492853\n",
            "Iteration 130, loss = 0.61480041\n",
            "Iteration 131, loss = 0.61444003\n",
            "Iteration 132, loss = 0.61407724\n",
            "Iteration 133, loss = 0.61388490\n",
            "Iteration 134, loss = 0.61371818\n",
            "Iteration 135, loss = 0.61359494\n",
            "Iteration 136, loss = 0.61351131\n",
            "Iteration 137, loss = 0.61292951\n",
            "Iteration 138, loss = 0.61268101\n",
            "Iteration 139, loss = 0.61252633\n",
            "Iteration 140, loss = 0.61277796\n",
            "Iteration 141, loss = 0.61295605\n",
            "Iteration 142, loss = 0.61247687\n",
            "Iteration 143, loss = 0.61192856\n",
            "Iteration 144, loss = 0.61158153\n",
            "Iteration 145, loss = 0.61143757\n",
            "Iteration 146, loss = 0.61117823\n",
            "Iteration 147, loss = 0.61116403\n",
            "Iteration 148, loss = 0.61107418\n",
            "Iteration 149, loss = 0.61067722\n",
            "Iteration 150, loss = 0.61048319\n",
            "Iteration 151, loss = 0.61033898\n",
            "Iteration 152, loss = 0.61014147\n",
            "Iteration 153, loss = 0.61003549\n",
            "Iteration 154, loss = 0.60976605\n",
            "Iteration 155, loss = 0.60963036\n",
            "Iteration 156, loss = 0.60938203\n",
            "Iteration 157, loss = 0.60941563\n",
            "Iteration 158, loss = 0.60904100\n",
            "Iteration 159, loss = 0.60928160\n",
            "Iteration 160, loss = 0.60911674\n",
            "Iteration 161, loss = 0.60897629\n",
            "Iteration 162, loss = 0.60908748\n",
            "Iteration 163, loss = 0.60879117\n",
            "Iteration 164, loss = 0.60845713\n",
            "Iteration 165, loss = 0.60845357\n",
            "Iteration 166, loss = 0.60848601\n",
            "Iteration 167, loss = 0.60868844\n",
            "Iteration 168, loss = 0.60855399\n",
            "Iteration 169, loss = 0.60843729\n",
            "Iteration 170, loss = 0.60848515\n",
            "Iteration 171, loss = 0.60784701\n",
            "Iteration 172, loss = 0.60786526\n",
            "Iteration 173, loss = 0.60773011\n",
            "Iteration 174, loss = 0.60758050\n",
            "Iteration 175, loss = 0.60739764\n",
            "Iteration 176, loss = 0.60700664\n",
            "Iteration 177, loss = 0.60716274\n",
            "Iteration 178, loss = 0.60710592\n",
            "Iteration 179, loss = 0.60671953\n",
            "Iteration 180, loss = 0.60666740\n",
            "Iteration 181, loss = 0.60645091\n",
            "Iteration 182, loss = 0.60644133\n",
            "Iteration 183, loss = 0.60625532\n",
            "Iteration 184, loss = 0.60619422\n",
            "Iteration 185, loss = 0.60601481\n",
            "Iteration 186, loss = 0.60594873\n",
            "Iteration 187, loss = 0.60576991\n",
            "Iteration 188, loss = 0.60568781\n",
            "Iteration 189, loss = 0.60563471\n",
            "Iteration 190, loss = 0.60560313\n",
            "Iteration 191, loss = 0.60563469\n",
            "Iteration 192, loss = 0.60558730\n",
            "Iteration 193, loss = 0.60541705\n",
            "Iteration 194, loss = 0.60536129\n",
            "Iteration 195, loss = 0.60472716\n",
            "Iteration 196, loss = 0.60579782\n",
            "Iteration 197, loss = 0.60541774\n",
            "Iteration 198, loss = 0.60513150\n",
            "Iteration 199, loss = 0.60496002\n",
            "Iteration 200, loss = 0.60478054\n",
            "Iteration 1, loss = 0.66342918\n",
            "Iteration 2, loss = 0.66311960\n",
            "Iteration 3, loss = 0.66288392\n",
            "Iteration 4, loss = 0.66229936\n",
            "Iteration 5, loss = 0.66233354\n",
            "Iteration 6, loss = 0.66183515\n",
            "Iteration 7, loss = 0.66177171\n",
            "Iteration 8, loss = 0.66138676\n",
            "Iteration 9, loss = 0.66094753\n",
            "Iteration 10, loss = 0.66094219\n",
            "Iteration 11, loss = 0.66049563\n",
            "Iteration 12, loss = 0.66015924\n",
            "Iteration 13, loss = 0.65980685\n",
            "Iteration 14, loss = 0.65964955\n",
            "Iteration 15, loss = 0.65933151\n",
            "Iteration 16, loss = 0.65929405\n",
            "Iteration 17, loss = 0.65909258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 18, loss = 0.65882560\n",
            "Iteration 19, loss = 0.65876414\n",
            "Iteration 20, loss = 0.65820225\n",
            "Iteration 21, loss = 0.65789573\n",
            "Iteration 22, loss = 0.65771494\n",
            "Iteration 23, loss = 0.65735433\n",
            "Iteration 24, loss = 0.65710543\n",
            "Iteration 25, loss = 0.65685597\n",
            "Iteration 26, loss = 0.65664263\n",
            "Iteration 27, loss = 0.65643353\n",
            "Iteration 28, loss = 0.65632213\n",
            "Iteration 29, loss = 0.65602373\n",
            "Iteration 30, loss = 0.65566672\n",
            "Iteration 31, loss = 0.65566913\n",
            "Iteration 32, loss = 0.65514122\n",
            "Iteration 33, loss = 0.65479526\n",
            "Iteration 34, loss = 0.65462716\n",
            "Iteration 35, loss = 0.65440491\n",
            "Iteration 36, loss = 0.65423294\n",
            "Iteration 37, loss = 0.65409750\n",
            "Iteration 38, loss = 0.65388711\n",
            "Iteration 39, loss = 0.65358113\n",
            "Iteration 40, loss = 0.65337185\n",
            "Iteration 41, loss = 0.65332516\n",
            "Iteration 42, loss = 0.65332692\n",
            "Iteration 43, loss = 0.65369777\n",
            "Iteration 44, loss = 0.65270321\n",
            "Iteration 45, loss = 0.65208559\n",
            "Iteration 46, loss = 0.65176030\n",
            "Iteration 47, loss = 0.65146305\n",
            "Iteration 48, loss = 0.65122862\n",
            "Iteration 49, loss = 0.65110383\n",
            "Iteration 50, loss = 0.65095385\n",
            "Iteration 51, loss = 0.65107025\n",
            "Iteration 52, loss = 0.65056804\n",
            "Iteration 53, loss = 0.65027041\n",
            "Iteration 54, loss = 0.64977190\n",
            "Iteration 55, loss = 0.64951100\n",
            "Iteration 56, loss = 0.64938414\n",
            "Iteration 57, loss = 0.64910298\n",
            "Iteration 58, loss = 0.64865563\n",
            "Iteration 59, loss = 0.64850071\n",
            "Iteration 60, loss = 0.64848984\n",
            "Iteration 61, loss = 0.64810285\n",
            "Iteration 62, loss = 0.64777742\n",
            "Iteration 63, loss = 0.64750496\n",
            "Iteration 64, loss = 0.64734553\n",
            "Iteration 65, loss = 0.64706040\n",
            "Iteration 66, loss = 0.64693518\n",
            "Iteration 67, loss = 0.64659336\n",
            "Iteration 68, loss = 0.64629224\n",
            "Iteration 69, loss = 0.64654093\n",
            "Iteration 70, loss = 0.64619696\n",
            "Iteration 71, loss = 0.64622659\n",
            "Iteration 72, loss = 0.64569254\n",
            "Iteration 73, loss = 0.64530985\n",
            "Iteration 74, loss = 0.64487255\n",
            "Iteration 75, loss = 0.64475200\n",
            "Iteration 76, loss = 0.64444629\n",
            "Iteration 77, loss = 0.64427319\n",
            "Iteration 78, loss = 0.64411276\n",
            "Iteration 79, loss = 0.64392211\n",
            "Iteration 80, loss = 0.64381689\n",
            "Iteration 81, loss = 0.64352429\n",
            "Iteration 82, loss = 0.64325440\n",
            "Iteration 83, loss = 0.64291217\n",
            "Iteration 84, loss = 0.64268391\n",
            "Iteration 85, loss = 0.64227013\n",
            "Iteration 86, loss = 0.64222582\n",
            "Iteration 87, loss = 0.64239397\n",
            "Iteration 88, loss = 0.64232603\n",
            "Iteration 89, loss = 0.64187962\n",
            "Iteration 90, loss = 0.64128713\n",
            "Iteration 91, loss = 0.64081685\n",
            "Iteration 92, loss = 0.64092806\n",
            "Iteration 93, loss = 0.64042459\n",
            "Iteration 94, loss = 0.64008043\n",
            "Iteration 95, loss = 0.63987694\n",
            "Iteration 96, loss = 0.63962438\n",
            "Iteration 97, loss = 0.63941977\n",
            "Iteration 98, loss = 0.63917005\n",
            "Iteration 99, loss = 0.63902755\n",
            "Iteration 100, loss = 0.63860731\n",
            "Iteration 101, loss = 0.63834504\n",
            "Iteration 102, loss = 0.63809885\n",
            "Iteration 103, loss = 0.63785576\n",
            "Iteration 104, loss = 0.63786749\n",
            "Iteration 105, loss = 0.63814015\n",
            "Iteration 106, loss = 0.63851343\n",
            "Iteration 107, loss = 0.63738076\n",
            "Iteration 108, loss = 0.63706519\n",
            "Iteration 109, loss = 0.63658761\n",
            "Iteration 110, loss = 0.63612288\n",
            "Iteration 111, loss = 0.63600699\n",
            "Iteration 112, loss = 0.63583587\n",
            "Iteration 113, loss = 0.63580406\n",
            "Iteration 114, loss = 0.63524103\n",
            "Iteration 115, loss = 0.63504264\n",
            "Iteration 116, loss = 0.63476278\n",
            "Iteration 117, loss = 0.63446604\n",
            "Iteration 118, loss = 0.63423080\n",
            "Iteration 119, loss = 0.63405050\n",
            "Iteration 120, loss = 0.63388143\n",
            "Iteration 121, loss = 0.63376310\n",
            "Iteration 122, loss = 0.63339575\n",
            "Iteration 123, loss = 0.63312825\n",
            "Iteration 124, loss = 0.63290794\n",
            "Iteration 125, loss = 0.63251513\n",
            "Iteration 126, loss = 0.63271687\n",
            "Iteration 127, loss = 0.63265640\n",
            "Iteration 128, loss = 0.63213215\n",
            "Iteration 129, loss = 0.63215681\n",
            "Iteration 130, loss = 0.63164553\n",
            "Iteration 131, loss = 0.63149486\n",
            "Iteration 132, loss = 0.63151920\n",
            "Iteration 133, loss = 0.63127542\n",
            "Iteration 134, loss = 0.63129345\n",
            "Iteration 135, loss = 0.63101231\n",
            "Iteration 136, loss = 0.63037916\n",
            "Iteration 137, loss = 0.63016789\n",
            "Iteration 138, loss = 0.62999889\n",
            "Iteration 139, loss = 0.62970543\n",
            "Iteration 140, loss = 0.62938698\n",
            "Iteration 141, loss = 0.62924547\n",
            "Iteration 142, loss = 0.62914196\n",
            "Iteration 143, loss = 0.62871722\n",
            "Iteration 144, loss = 0.62934587\n",
            "Iteration 145, loss = 0.62887839\n",
            "Iteration 146, loss = 0.62831058\n",
            "Iteration 147, loss = 0.62796225\n",
            "Iteration 148, loss = 0.62777360\n",
            "Iteration 149, loss = 0.62758973\n",
            "Iteration 150, loss = 0.62739151\n",
            "Iteration 151, loss = 0.62726085\n",
            "Iteration 152, loss = 0.62704838\n",
            "Iteration 153, loss = 0.62684006\n",
            "Iteration 154, loss = 0.62675356\n",
            "Iteration 155, loss = 0.62648504\n",
            "Iteration 156, loss = 0.62644722\n",
            "Iteration 157, loss = 0.62656462\n",
            "Iteration 158, loss = 0.62593549\n",
            "Iteration 159, loss = 0.62584119\n",
            "Iteration 160, loss = 0.62568061\n",
            "Iteration 161, loss = 0.62540017\n",
            "Iteration 162, loss = 0.62527590\n",
            "Iteration 163, loss = 0.62545097\n",
            "Iteration 164, loss = 0.62543513\n",
            "Iteration 165, loss = 0.62508401\n",
            "Iteration 166, loss = 0.62484397\n",
            "Iteration 167, loss = 0.62463926\n",
            "Iteration 168, loss = 0.62449244\n",
            "Iteration 169, loss = 0.62433797\n",
            "Iteration 170, loss = 0.62442783\n",
            "Iteration 171, loss = 0.62404639\n",
            "Iteration 172, loss = 0.62389665\n",
            "Iteration 173, loss = 0.62416945\n",
            "Iteration 174, loss = 0.62368994\n",
            "Iteration 175, loss = 0.62384699\n",
            "Iteration 176, loss = 0.62377712\n",
            "Iteration 177, loss = 0.62350175\n",
            "Iteration 178, loss = 0.62336012\n",
            "Iteration 179, loss = 0.62314833\n",
            "Iteration 180, loss = 0.62306713\n",
            "Iteration 181, loss = 0.62335371\n",
            "Iteration 182, loss = 0.62311620\n",
            "Iteration 183, loss = 0.62275648\n",
            "Iteration 184, loss = 0.62258229\n",
            "Iteration 185, loss = 0.62275944\n",
            "Iteration 186, loss = 0.62240337\n",
            "Iteration 187, loss = 0.62203231\n",
            "Iteration 188, loss = 0.62225410\n",
            "Iteration 189, loss = 0.62205421\n",
            "Iteration 190, loss = 0.62216032\n",
            "Iteration 191, loss = 0.62176598\n",
            "Iteration 192, loss = 0.62157513\n",
            "Iteration 193, loss = 0.62170530\n",
            "Iteration 194, loss = 0.62193854\n",
            "Iteration 195, loss = 0.62189962\n",
            "Iteration 196, loss = 0.62183177\n",
            "Iteration 197, loss = 0.62123356\n",
            "Iteration 198, loss = 0.62118824\n",
            "Iteration 199, loss = 0.62097915\n",
            "Iteration 200, loss = 0.62111116\n",
            "Iteration 1, loss = 0.66565607\n",
            "Iteration 2, loss = 0.66506000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3, loss = 0.66453978\n",
            "Iteration 4, loss = 0.66385970\n",
            "Iteration 5, loss = 0.66335776\n",
            "Iteration 6, loss = 0.66311845\n",
            "Iteration 7, loss = 0.66281166\n",
            "Iteration 8, loss = 0.66247915\n",
            "Iteration 9, loss = 0.66195498\n",
            "Iteration 10, loss = 0.66188298\n",
            "Iteration 11, loss = 0.66115170\n",
            "Iteration 12, loss = 0.66082664\n",
            "Iteration 13, loss = 0.66053060\n",
            "Iteration 14, loss = 0.66011517\n",
            "Iteration 15, loss = 0.65988414\n",
            "Iteration 16, loss = 0.65937099\n",
            "Iteration 17, loss = 0.65910007\n",
            "Iteration 18, loss = 0.65872185\n",
            "Iteration 19, loss = 0.65822788\n",
            "Iteration 20, loss = 0.65799158\n",
            "Iteration 21, loss = 0.65773728\n",
            "Iteration 22, loss = 0.65740405\n",
            "Iteration 23, loss = 0.65720346\n",
            "Iteration 24, loss = 0.65717092\n",
            "Iteration 25, loss = 0.65659133\n",
            "Iteration 26, loss = 0.65595097\n",
            "Iteration 27, loss = 0.65551041\n",
            "Iteration 28, loss = 0.65510469\n",
            "Iteration 29, loss = 0.65477917\n",
            "Iteration 30, loss = 0.65447934\n",
            "Iteration 31, loss = 0.65413215\n",
            "Iteration 32, loss = 0.65401122\n",
            "Iteration 33, loss = 0.65345777\n",
            "Iteration 34, loss = 0.65323571\n",
            "Iteration 35, loss = 0.65277598\n",
            "Iteration 36, loss = 0.65256124\n",
            "Iteration 37, loss = 0.65178221\n",
            "Iteration 38, loss = 0.65121532\n",
            "Iteration 39, loss = 0.65080205\n",
            "Iteration 40, loss = 0.65060820\n",
            "Iteration 41, loss = 0.65009049\n",
            "Iteration 42, loss = 0.64972172\n",
            "Iteration 43, loss = 0.64958660\n",
            "Iteration 44, loss = 0.64906795\n",
            "Iteration 45, loss = 0.64857776\n",
            "Iteration 46, loss = 0.64807817\n",
            "Iteration 47, loss = 0.64812379\n",
            "Iteration 48, loss = 0.64795879\n",
            "Iteration 49, loss = 0.64735624\n",
            "Iteration 50, loss = 0.64693520\n",
            "Iteration 51, loss = 0.64693018\n",
            "Iteration 52, loss = 0.64603289\n",
            "Iteration 53, loss = 0.64581651\n",
            "Iteration 54, loss = 0.64529220\n",
            "Iteration 55, loss = 0.64475369\n",
            "Iteration 56, loss = 0.64441574\n",
            "Iteration 57, loss = 0.64393040\n",
            "Iteration 58, loss = 0.64363787\n",
            "Iteration 59, loss = 0.64398363\n",
            "Iteration 60, loss = 0.64346396\n",
            "Iteration 61, loss = 0.64254107\n",
            "Iteration 62, loss = 0.64213228\n",
            "Iteration 63, loss = 0.64190341\n",
            "Iteration 64, loss = 0.64149587\n",
            "Iteration 65, loss = 0.64086503\n",
            "Iteration 66, loss = 0.64047680\n",
            "Iteration 67, loss = 0.64006008\n",
            "Iteration 68, loss = 0.63975222\n",
            "Iteration 69, loss = 0.63927116\n",
            "Iteration 70, loss = 0.63926911\n",
            "Iteration 71, loss = 0.63908560\n",
            "Iteration 72, loss = 0.63858114\n",
            "Iteration 73, loss = 0.63828448\n",
            "Iteration 74, loss = 0.63757746\n",
            "Iteration 75, loss = 0.63701379\n",
            "Iteration 76, loss = 0.63660701\n",
            "Iteration 77, loss = 0.63620231\n",
            "Iteration 78, loss = 0.63589630\n",
            "Iteration 79, loss = 0.63554245\n",
            "Iteration 80, loss = 0.63516949\n",
            "Iteration 81, loss = 0.63459495\n",
            "Iteration 82, loss = 0.63417957\n",
            "Iteration 83, loss = 0.63350396\n",
            "Iteration 84, loss = 0.63335756\n",
            "Iteration 85, loss = 0.63320412\n",
            "Iteration 86, loss = 0.63273331\n",
            "Iteration 87, loss = 0.63241627\n",
            "Iteration 88, loss = 0.63203335\n",
            "Iteration 89, loss = 0.63149035\n",
            "Iteration 90, loss = 0.63124802\n",
            "Iteration 91, loss = 0.63076533\n",
            "Iteration 92, loss = 0.63045318\n",
            "Iteration 93, loss = 0.63002600\n",
            "Iteration 94, loss = 0.62969599\n",
            "Iteration 95, loss = 0.62918605\n",
            "Iteration 96, loss = 0.62909174\n",
            "Iteration 97, loss = 0.62935828\n",
            "Iteration 98, loss = 0.62859671\n",
            "Iteration 99, loss = 0.62784503\n",
            "Iteration 100, loss = 0.62752412\n",
            "Iteration 101, loss = 0.62729943\n",
            "Iteration 102, loss = 0.62683866\n",
            "Iteration 103, loss = 0.62617343\n",
            "Iteration 104, loss = 0.62580598\n",
            "Iteration 105, loss = 0.62543171\n",
            "Iteration 106, loss = 0.62513323\n",
            "Iteration 107, loss = 0.62496514\n",
            "Iteration 108, loss = 0.62431684\n",
            "Iteration 109, loss = 0.62387235\n",
            "Iteration 110, loss = 0.62399179\n",
            "Iteration 111, loss = 0.62377445\n",
            "Iteration 112, loss = 0.62318838\n",
            "Iteration 113, loss = 0.62275826\n",
            "Iteration 114, loss = 0.62251602\n",
            "Iteration 115, loss = 0.62213293\n",
            "Iteration 116, loss = 0.62182282\n",
            "Iteration 117, loss = 0.62157101\n",
            "Iteration 118, loss = 0.62124373\n",
            "Iteration 119, loss = 0.62088730\n",
            "Iteration 120, loss = 0.62065910\n",
            "Iteration 121, loss = 0.62035866\n",
            "Iteration 122, loss = 0.62026711\n",
            "Iteration 123, loss = 0.62008479\n",
            "Iteration 124, loss = 0.61949998\n",
            "Iteration 125, loss = 0.61888415\n",
            "Iteration 126, loss = 0.61889274\n",
            "Iteration 127, loss = 0.61838071\n",
            "Iteration 128, loss = 0.61809778\n",
            "Iteration 129, loss = 0.61812016\n",
            "Iteration 130, loss = 0.61770575\n",
            "Iteration 131, loss = 0.61734629\n",
            "Iteration 132, loss = 0.61697606\n",
            "Iteration 133, loss = 0.61676425\n",
            "Iteration 134, loss = 0.61649664\n",
            "Iteration 135, loss = 0.61624940\n",
            "Iteration 136, loss = 0.61584784\n",
            "Iteration 137, loss = 0.61564569\n",
            "Iteration 138, loss = 0.61542925\n",
            "Iteration 139, loss = 0.61500766\n",
            "Iteration 140, loss = 0.61472433\n",
            "Iteration 141, loss = 0.61443555\n",
            "Iteration 142, loss = 0.61428261\n",
            "Iteration 143, loss = 0.61393242\n",
            "Iteration 144, loss = 0.61383500\n",
            "Iteration 145, loss = 0.61360694\n",
            "Iteration 146, loss = 0.61330324\n",
            "Iteration 147, loss = 0.61316081\n",
            "Iteration 148, loss = 0.61288577\n",
            "Iteration 149, loss = 0.61256786\n",
            "Iteration 150, loss = 0.61234274\n",
            "Iteration 151, loss = 0.61207834\n",
            "Iteration 152, loss = 0.61184805\n",
            "Iteration 153, loss = 0.61150911\n",
            "Iteration 154, loss = 0.61141490\n",
            "Iteration 155, loss = 0.61125287\n",
            "Iteration 156, loss = 0.61099740\n",
            "Iteration 157, loss = 0.61082110\n",
            "Iteration 158, loss = 0.61067885\n",
            "Iteration 159, loss = 0.61055677\n",
            "Iteration 160, loss = 0.61028516\n",
            "Iteration 161, loss = 0.60996823\n",
            "Iteration 162, loss = 0.60976172\n",
            "Iteration 163, loss = 0.60979563\n",
            "Iteration 164, loss = 0.60950034\n",
            "Iteration 165, loss = 0.60918083\n",
            "Iteration 166, loss = 0.60899968\n",
            "Iteration 167, loss = 0.60874440\n",
            "Iteration 168, loss = 0.60855461\n",
            "Iteration 169, loss = 0.60847708\n",
            "Iteration 170, loss = 0.60832651\n",
            "Iteration 171, loss = 0.60817980\n",
            "Iteration 172, loss = 0.60796239\n",
            "Iteration 173, loss = 0.60764796\n",
            "Iteration 174, loss = 0.60749778\n",
            "Iteration 175, loss = 0.60736989\n",
            "Iteration 176, loss = 0.60716376\n",
            "Iteration 177, loss = 0.60700539\n",
            "Iteration 178, loss = 0.60683938\n",
            "Iteration 179, loss = 0.60678966\n",
            "Iteration 180, loss = 0.60666881\n",
            "Iteration 181, loss = 0.60654416\n",
            "Iteration 182, loss = 0.60644367\n",
            "Iteration 183, loss = 0.60657456\n",
            "Iteration 184, loss = 0.60613400\n",
            "Iteration 185, loss = 0.60613915\n",
            "Iteration 186, loss = 0.60589949\n",
            "Iteration 187, loss = 0.60569010\n",
            "Iteration 188, loss = 0.60551328\n",
            "Iteration 189, loss = 0.60543566\n",
            "Iteration 190, loss = 0.60536116\n",
            "Iteration 191, loss = 0.60527073\n",
            "Iteration 192, loss = 0.60503177\n",
            "Iteration 193, loss = 0.60498138\n",
            "Iteration 194, loss = 0.60500784\n",
            "Iteration 195, loss = 0.60486550\n",
            "Iteration 196, loss = 0.60482587\n",
            "Iteration 197, loss = 0.60453480\n",
            "Iteration 198, loss = 0.60463674\n",
            "Iteration 199, loss = 0.60456885\n",
            "Iteration 200, loss = 0.60425567\n",
            "Iteration 1, loss = 0.68172535\n",
            "Iteration 2, loss = 0.67292633\n",
            "Iteration 3, loss = 0.66632365\n",
            "Iteration 4, loss = 0.66371302\n",
            "Iteration 5, loss = 0.66279151\n",
            "Iteration 6, loss = 0.66201819\n",
            "Iteration 7, loss = 0.66151281\n",
            "Iteration 8, loss = 0.66105995\n",
            "Iteration 9, loss = 0.66062247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 10, loss = 0.66027313\n",
            "Iteration 11, loss = 0.65965269\n",
            "Iteration 12, loss = 0.65934735\n",
            "Iteration 13, loss = 0.65914591\n",
            "Iteration 14, loss = 0.65832439\n",
            "Iteration 15, loss = 0.65833984\n",
            "Iteration 16, loss = 0.65764943\n",
            "Iteration 17, loss = 0.65712685\n",
            "Iteration 18, loss = 0.65669575\n",
            "Iteration 19, loss = 0.65633678\n",
            "Iteration 20, loss = 0.65610290\n",
            "Iteration 21, loss = 0.65578308\n",
            "Iteration 22, loss = 0.65532839\n",
            "Iteration 23, loss = 0.65508576\n",
            "Iteration 24, loss = 0.65454909\n",
            "Iteration 25, loss = 0.65440809\n",
            "Iteration 26, loss = 0.65428494\n",
            "Iteration 27, loss = 0.65382666\n",
            "Iteration 28, loss = 0.65348062\n",
            "Iteration 29, loss = 0.65309731\n",
            "Iteration 30, loss = 0.65301218\n",
            "Iteration 31, loss = 0.65216982\n",
            "Iteration 32, loss = 0.65180308\n",
            "Iteration 33, loss = 0.65141916\n",
            "Iteration 34, loss = 0.65104606\n",
            "Iteration 35, loss = 0.65076788\n",
            "Iteration 36, loss = 0.65054889\n",
            "Iteration 37, loss = 0.65018646\n",
            "Iteration 38, loss = 0.65001855\n",
            "Iteration 39, loss = 0.64956476\n",
            "Iteration 40, loss = 0.64902911\n",
            "Iteration 41, loss = 0.64865206\n",
            "Iteration 42, loss = 0.64820132\n",
            "Iteration 43, loss = 0.64777331\n",
            "Iteration 44, loss = 0.64744375\n",
            "Iteration 45, loss = 0.64718803\n",
            "Iteration 46, loss = 0.64682307\n",
            "Iteration 47, loss = 0.64677277\n",
            "Iteration 48, loss = 0.64660461\n",
            "Iteration 49, loss = 0.64611906\n",
            "Iteration 50, loss = 0.64521898\n",
            "Iteration 51, loss = 0.64502339\n",
            "Iteration 52, loss = 0.64449231\n",
            "Iteration 53, loss = 0.64449663\n",
            "Iteration 54, loss = 0.64380249\n",
            "Iteration 55, loss = 0.64343551\n",
            "Iteration 56, loss = 0.64299505\n",
            "Iteration 57, loss = 0.64264681\n",
            "Iteration 58, loss = 0.64211871\n",
            "Iteration 59, loss = 0.64191758\n",
            "Iteration 60, loss = 0.64153150\n",
            "Iteration 61, loss = 0.64109047\n",
            "Iteration 62, loss = 0.64066758\n",
            "Iteration 63, loss = 0.64013991\n",
            "Iteration 64, loss = 0.63979993\n",
            "Iteration 65, loss = 0.63931035\n",
            "Iteration 66, loss = 0.63931185\n",
            "Iteration 67, loss = 0.63946669\n",
            "Iteration 68, loss = 0.63938323\n",
            "Iteration 69, loss = 0.63920842\n",
            "Iteration 70, loss = 0.63873543\n",
            "Iteration 71, loss = 0.63786007\n",
            "Iteration 72, loss = 0.63705923\n",
            "Iteration 73, loss = 0.63682988\n",
            "Iteration 74, loss = 0.63636133\n",
            "Iteration 75, loss = 0.63589524\n",
            "Iteration 76, loss = 0.63563684\n",
            "Iteration 77, loss = 0.63534783\n",
            "Iteration 78, loss = 0.63499141\n",
            "Iteration 79, loss = 0.63456701\n",
            "Iteration 80, loss = 0.63418579\n",
            "Iteration 81, loss = 0.63371989\n",
            "Iteration 82, loss = 0.63356585\n",
            "Iteration 83, loss = 0.63326684\n",
            "Iteration 84, loss = 0.63286169\n",
            "Iteration 85, loss = 0.63255278\n",
            "Iteration 86, loss = 0.63220755\n",
            "Iteration 87, loss = 0.63178701\n",
            "Iteration 88, loss = 0.63132269\n",
            "Iteration 89, loss = 0.63108466\n",
            "Iteration 90, loss = 0.63082822\n",
            "Iteration 91, loss = 0.63046096\n",
            "Iteration 92, loss = 0.63010660\n",
            "Iteration 93, loss = 0.62980362\n",
            "Iteration 94, loss = 0.62948783\n",
            "Iteration 95, loss = 0.62932924\n",
            "Iteration 96, loss = 0.62879760\n",
            "Iteration 97, loss = 0.62865514\n",
            "Iteration 98, loss = 0.62862972\n",
            "Iteration 99, loss = 0.62830716\n",
            "Iteration 100, loss = 0.62723578\n",
            "Iteration 101, loss = 0.62752943\n",
            "Iteration 102, loss = 0.62695184\n",
            "Iteration 103, loss = 0.62687412\n",
            "Iteration 104, loss = 0.62705258\n",
            "Iteration 105, loss = 0.62652888\n",
            "Iteration 106, loss = 0.62608055\n",
            "Iteration 107, loss = 0.62644374\n",
            "Iteration 108, loss = 0.62557258\n",
            "Iteration 109, loss = 0.62536286\n",
            "Iteration 110, loss = 0.62503697\n",
            "Iteration 111, loss = 0.62459530\n",
            "Iteration 112, loss = 0.62426066\n",
            "Iteration 113, loss = 0.62380448\n",
            "Iteration 114, loss = 0.62360366\n",
            "Iteration 115, loss = 0.62333510\n",
            "Iteration 116, loss = 0.62312919\n",
            "Iteration 117, loss = 0.62286331\n",
            "Iteration 118, loss = 0.62258714\n",
            "Iteration 119, loss = 0.62243599\n",
            "Iteration 120, loss = 0.62224932\n",
            "Iteration 121, loss = 0.62170408\n",
            "Iteration 122, loss = 0.62152483\n",
            "Iteration 123, loss = 0.62117716\n",
            "Iteration 124, loss = 0.62087604\n",
            "Iteration 125, loss = 0.62068952\n",
            "Iteration 126, loss = 0.62058471\n",
            "Iteration 127, loss = 0.62021449\n",
            "Iteration 128, loss = 0.61999418\n",
            "Iteration 129, loss = 0.61959999\n",
            "Iteration 130, loss = 0.61930105\n",
            "Iteration 131, loss = 0.61931327\n",
            "Iteration 132, loss = 0.61903181\n",
            "Iteration 133, loss = 0.61872143\n",
            "Iteration 134, loss = 0.61839981\n",
            "Iteration 135, loss = 0.61849404\n",
            "Iteration 136, loss = 0.61790461\n",
            "Iteration 137, loss = 0.61811253\n",
            "Iteration 138, loss = 0.61838987\n",
            "Iteration 139, loss = 0.61760996\n",
            "Iteration 140, loss = 0.61726417\n",
            "Iteration 141, loss = 0.61703198\n",
            "Iteration 142, loss = 0.61672255\n",
            "Iteration 143, loss = 0.61649098\n",
            "Iteration 144, loss = 0.61644670\n",
            "Iteration 145, loss = 0.61617710\n",
            "Iteration 146, loss = 0.61665443\n",
            "Iteration 147, loss = 0.61616227\n",
            "Iteration 148, loss = 0.61593595\n",
            "Iteration 149, loss = 0.61571652\n",
            "Iteration 150, loss = 0.61553680\n",
            "Iteration 151, loss = 0.61542509\n",
            "Iteration 152, loss = 0.61528123\n",
            "Iteration 153, loss = 0.61507893\n",
            "Iteration 154, loss = 0.61489697\n",
            "Iteration 155, loss = 0.61440951\n",
            "Iteration 156, loss = 0.61442882\n",
            "Iteration 157, loss = 0.61428369\n",
            "Iteration 158, loss = 0.61401794\n",
            "Iteration 159, loss = 0.61404169\n",
            "Iteration 160, loss = 0.61381948\n",
            "Iteration 161, loss = 0.61364368\n",
            "Iteration 162, loss = 0.61345078\n",
            "Iteration 163, loss = 0.61329774\n",
            "Iteration 164, loss = 0.61305639\n",
            "Iteration 165, loss = 0.61287959\n",
            "Iteration 166, loss = 0.61293496\n",
            "Iteration 167, loss = 0.61344354\n",
            "Iteration 168, loss = 0.61345188\n",
            "Iteration 169, loss = 0.61249146\n",
            "Iteration 170, loss = 0.61247410\n",
            "Iteration 171, loss = 0.61223510\n",
            "Iteration 172, loss = 0.61199774\n",
            "Iteration 173, loss = 0.61182210\n",
            "Iteration 174, loss = 0.61179439\n",
            "Iteration 175, loss = 0.61155370\n",
            "Iteration 176, loss = 0.61178978\n",
            "Iteration 177, loss = 0.61181478\n",
            "Iteration 178, loss = 0.61124849\n",
            "Iteration 179, loss = 0.61116361\n",
            "Iteration 180, loss = 0.61128818\n",
            "Iteration 181, loss = 0.61125116\n",
            "Iteration 182, loss = 0.61093939\n",
            "Iteration 183, loss = 0.61074940\n",
            "Iteration 184, loss = 0.61079158\n",
            "Iteration 185, loss = 0.61070389\n",
            "Iteration 186, loss = 0.61067545\n",
            "Iteration 187, loss = 0.61065043\n",
            "Iteration 188, loss = 0.61054263\n",
            "Iteration 189, loss = 0.61068330\n",
            "Iteration 190, loss = 0.61056623\n",
            "Iteration 191, loss = 0.61016267\n",
            "Iteration 192, loss = 0.61008326\n",
            "Iteration 193, loss = 0.61000131\n",
            "Iteration 194, loss = 0.61061478\n",
            "Iteration 195, loss = 0.61060395\n",
            "Iteration 196, loss = 0.60971418\n",
            "Iteration 197, loss = 0.60965333\n",
            "Iteration 198, loss = 0.60952764\n",
            "Iteration 199, loss = 0.60941617\n",
            "Iteration 200, loss = 0.60946801\n",
            "Iteration 1, loss = 0.62454829\n",
            "Iteration 2, loss = 0.60458341\n",
            "Iteration 3, loss = 0.60528520\n",
            "Iteration 4, loss = 0.58478802\n",
            "Iteration 5, loss = 0.58371135\n",
            "Iteration 6, loss = 0.57221819\n",
            "Iteration 7, loss = 0.57021847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8, loss = 0.56506354\n",
            "Iteration 9, loss = 0.55578725\n",
            "Iteration 10, loss = 0.55459920\n",
            "Iteration 11, loss = 0.53475753\n",
            "Iteration 12, loss = 0.55003311\n",
            "Iteration 13, loss = 0.53975836\n",
            "Iteration 14, loss = 0.53372916\n",
            "Iteration 15, loss = 0.53456286\n",
            "Iteration 16, loss = 0.49944434\n",
            "Iteration 17, loss = 0.50109154\n",
            "Iteration 18, loss = 0.48212333\n",
            "Iteration 19, loss = 0.47863176\n",
            "Iteration 20, loss = 0.47205799\n",
            "Iteration 21, loss = 0.48420280\n",
            "Iteration 22, loss = 0.47547705\n",
            "Iteration 23, loss = 0.46423634\n",
            "Iteration 24, loss = 0.46559616\n",
            "Iteration 25, loss = 0.44876722\n",
            "Iteration 26, loss = 0.45557033\n",
            "Iteration 27, loss = 0.46709632\n",
            "Iteration 28, loss = 0.45311081\n",
            "Iteration 29, loss = 0.45201343\n",
            "Iteration 30, loss = 0.45460353\n",
            "Iteration 31, loss = 0.45733748\n",
            "Iteration 32, loss = 0.45566732\n",
            "Iteration 33, loss = 0.45098767\n",
            "Iteration 34, loss = 0.43362749\n",
            "Iteration 35, loss = 0.43782137\n",
            "Iteration 36, loss = 0.47412377\n",
            "Iteration 37, loss = 0.44204257\n",
            "Iteration 38, loss = 0.42829531\n",
            "Iteration 39, loss = 0.43801322\n",
            "Iteration 40, loss = 0.44152352\n",
            "Iteration 41, loss = 0.44081064\n",
            "Iteration 42, loss = 0.45019483\n",
            "Iteration 43, loss = 0.42034226\n",
            "Iteration 44, loss = 0.41700007\n",
            "Iteration 45, loss = 0.42668745\n",
            "Iteration 46, loss = 0.41706890\n",
            "Iteration 47, loss = 0.42160554\n",
            "Iteration 48, loss = 0.42460933\n",
            "Iteration 49, loss = 0.41454541\n",
            "Iteration 50, loss = 0.41316500\n",
            "Iteration 51, loss = 0.42273455\n",
            "Iteration 52, loss = 0.43056387\n",
            "Iteration 53, loss = 0.41695702\n",
            "Iteration 54, loss = 0.42099954\n",
            "Iteration 55, loss = 0.43881796\n",
            "Iteration 56, loss = 0.40634278\n",
            "Iteration 57, loss = 0.41780514\n",
            "Iteration 58, loss = 0.42812315\n",
            "Iteration 59, loss = 0.40872745\n",
            "Iteration 60, loss = 0.40374609\n",
            "Iteration 61, loss = 0.41006894\n",
            "Iteration 62, loss = 0.39952292\n",
            "Iteration 63, loss = 0.42338066\n",
            "Iteration 64, loss = 0.41225753\n",
            "Iteration 65, loss = 0.40476869\n",
            "Iteration 66, loss = 0.40371268\n",
            "Iteration 67, loss = 0.41369248\n",
            "Iteration 68, loss = 0.41077295\n",
            "Iteration 69, loss = 0.40357394\n",
            "Iteration 70, loss = 0.40202589\n",
            "Iteration 71, loss = 0.40649173\n",
            "Iteration 72, loss = 0.41776726\n",
            "Iteration 73, loss = 0.41673397\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62875986\n",
            "Iteration 2, loss = 0.59916226\n",
            "Iteration 3, loss = 0.61458257\n",
            "Iteration 4, loss = 0.59262842\n",
            "Iteration 5, loss = 0.58993909\n",
            "Iteration 6, loss = 0.59468246\n",
            "Iteration 7, loss = 0.60213801\n",
            "Iteration 8, loss = 0.57358125\n",
            "Iteration 9, loss = 0.56037566\n",
            "Iteration 10, loss = 0.55163167\n",
            "Iteration 11, loss = 0.54639939\n",
            "Iteration 12, loss = 0.54054668\n",
            "Iteration 13, loss = 0.53869169\n",
            "Iteration 14, loss = 0.53689366\n",
            "Iteration 15, loss = 0.51518198\n",
            "Iteration 16, loss = 0.50681091\n",
            "Iteration 17, loss = 0.49648966\n",
            "Iteration 18, loss = 0.48870215\n",
            "Iteration 19, loss = 0.49731183\n",
            "Iteration 20, loss = 0.47697526\n",
            "Iteration 21, loss = 0.47134309\n",
            "Iteration 22, loss = 0.46291532\n",
            "Iteration 23, loss = 0.47870488\n",
            "Iteration 24, loss = 0.47567015\n",
            "Iteration 25, loss = 0.48281213\n",
            "Iteration 26, loss = 0.46183356\n",
            "Iteration 27, loss = 0.45027474\n",
            "Iteration 28, loss = 0.44606925\n",
            "Iteration 29, loss = 0.44151534\n",
            "Iteration 30, loss = 0.45228527\n",
            "Iteration 31, loss = 0.44769900\n",
            "Iteration 32, loss = 0.44078437\n",
            "Iteration 33, loss = 0.44906324\n",
            "Iteration 34, loss = 0.42941391\n",
            "Iteration 35, loss = 0.42875982\n",
            "Iteration 36, loss = 0.45134127\n",
            "Iteration 37, loss = 0.45811394\n",
            "Iteration 38, loss = 0.42748008\n",
            "Iteration 39, loss = 0.42209352\n",
            "Iteration 40, loss = 0.41736277\n",
            "Iteration 41, loss = 0.42533100\n",
            "Iteration 42, loss = 0.41474123\n",
            "Iteration 43, loss = 0.41874454\n",
            "Iteration 44, loss = 0.43119478\n",
            "Iteration 45, loss = 0.44105758\n",
            "Iteration 46, loss = 0.42244998\n",
            "Iteration 47, loss = 0.41656427\n",
            "Iteration 48, loss = 0.41142474\n",
            "Iteration 49, loss = 0.41274969\n",
            "Iteration 50, loss = 0.40693032\n",
            "Iteration 51, loss = 0.41777206\n",
            "Iteration 52, loss = 0.41332953\n",
            "Iteration 53, loss = 0.42079288\n",
            "Iteration 54, loss = 0.40923662\n",
            "Iteration 55, loss = 0.40670888\n",
            "Iteration 56, loss = 0.40754840\n",
            "Iteration 57, loss = 0.40919584\n",
            "Iteration 58, loss = 0.41847876\n",
            "Iteration 59, loss = 0.39833290\n",
            "Iteration 60, loss = 0.40702030\n",
            "Iteration 61, loss = 0.42200942\n",
            "Iteration 62, loss = 0.44282759\n",
            "Iteration 63, loss = 0.43187331\n",
            "Iteration 64, loss = 0.42700563\n",
            "Iteration 65, loss = 0.43215578\n",
            "Iteration 66, loss = 0.41324754\n",
            "Iteration 67, loss = 0.40112667\n",
            "Iteration 68, loss = 0.39914637\n",
            "Iteration 69, loss = 0.40556803\n",
            "Iteration 70, loss = 0.39560180\n",
            "Iteration 71, loss = 0.39671043\n",
            "Iteration 72, loss = 0.39567825\n",
            "Iteration 73, loss = 0.39789253\n",
            "Iteration 74, loss = 0.39664226\n",
            "Iteration 75, loss = 0.41304751\n",
            "Iteration 76, loss = 0.40064705\n",
            "Iteration 77, loss = 0.40751618\n",
            "Iteration 78, loss = 0.41095229\n",
            "Iteration 79, loss = 0.41855797\n",
            "Iteration 80, loss = 0.39795867\n",
            "Iteration 81, loss = 0.39540275\n",
            "Iteration 82, loss = 0.38926418\n",
            "Iteration 83, loss = 0.40036896\n",
            "Iteration 84, loss = 0.39394959\n",
            "Iteration 85, loss = 0.39689196\n",
            "Iteration 86, loss = 0.38509736\n",
            "Iteration 87, loss = 0.39967317\n",
            "Iteration 88, loss = 0.39121162\n",
            "Iteration 89, loss = 0.39172659\n",
            "Iteration 90, loss = 0.40479710\n",
            "Iteration 91, loss = 0.43222236\n",
            "Iteration 92, loss = 0.41192591\n",
            "Iteration 93, loss = 0.38837322\n",
            "Iteration 94, loss = 0.38613683\n",
            "Iteration 95, loss = 0.41123986\n",
            "Iteration 96, loss = 0.39132173\n",
            "Iteration 97, loss = 0.38174148\n",
            "Iteration 98, loss = 0.41042957\n",
            "Iteration 99, loss = 0.40227879\n",
            "Iteration 100, loss = 0.40075759\n",
            "Iteration 101, loss = 0.39824854\n",
            "Iteration 102, loss = 0.39077724\n",
            "Iteration 103, loss = 0.38728650\n",
            "Iteration 104, loss = 0.38495026\n",
            "Iteration 105, loss = 0.38215319\n",
            "Iteration 106, loss = 0.38294890\n",
            "Iteration 107, loss = 0.39101608\n",
            "Iteration 108, loss = 0.40117521\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67303467\n",
            "Iteration 2, loss = 0.62254411\n",
            "Iteration 3, loss = 0.61607421\n",
            "Iteration 4, loss = 0.60464144\n",
            "Iteration 5, loss = 0.60181289\n",
            "Iteration 6, loss = 0.59492337\n",
            "Iteration 7, loss = 0.58674581\n",
            "Iteration 8, loss = 0.61628210\n",
            "Iteration 9, loss = 0.57772002\n",
            "Iteration 10, loss = 0.56334136\n",
            "Iteration 11, loss = 0.56397015\n",
            "Iteration 12, loss = 0.54630558\n",
            "Iteration 13, loss = 0.53103172\n",
            "Iteration 14, loss = 0.54395954\n",
            "Iteration 15, loss = 0.52991205\n",
            "Iteration 16, loss = 0.50897594\n",
            "Iteration 17, loss = 0.49635895\n",
            "Iteration 18, loss = 0.49556626\n",
            "Iteration 19, loss = 0.49694352\n",
            "Iteration 20, loss = 0.50304850\n",
            "Iteration 21, loss = 0.50112334\n",
            "Iteration 22, loss = 0.47944636\n",
            "Iteration 23, loss = 0.46908715\n",
            "Iteration 24, loss = 0.47321494\n",
            "Iteration 25, loss = 0.46383196\n",
            "Iteration 26, loss = 0.46255239\n",
            "Iteration 27, loss = 0.45956849\n",
            "Iteration 28, loss = 0.47849334\n",
            "Iteration 29, loss = 0.52516075\n",
            "Iteration 30, loss = 0.48502608\n",
            "Iteration 31, loss = 0.46293657\n",
            "Iteration 32, loss = 0.47437840\n",
            "Iteration 33, loss = 0.46316122\n",
            "Iteration 34, loss = 0.46719730\n",
            "Iteration 35, loss = 0.50167594\n",
            "Iteration 36, loss = 0.45831597\n",
            "Iteration 37, loss = 0.45383868\n",
            "Iteration 38, loss = 0.45928894\n",
            "Iteration 39, loss = 0.47867239\n",
            "Iteration 40, loss = 0.46914986\n",
            "Iteration 41, loss = 0.46600312\n",
            "Iteration 42, loss = 0.44915793\n",
            "Iteration 43, loss = 0.44809968\n",
            "Iteration 44, loss = 0.44170356\n",
            "Iteration 45, loss = 0.44156830\n",
            "Iteration 46, loss = 0.43952288\n",
            "Iteration 47, loss = 0.43819503\n",
            "Iteration 48, loss = 0.44109159\n",
            "Iteration 49, loss = 0.43593087\n",
            "Iteration 50, loss = 0.43393217\n",
            "Iteration 51, loss = 0.43086122\n",
            "Iteration 52, loss = 0.43605000\n",
            "Iteration 53, loss = 0.44932423\n",
            "Iteration 54, loss = 0.43384929\n",
            "Iteration 55, loss = 0.44169643\n",
            "Iteration 56, loss = 0.43698345\n",
            "Iteration 57, loss = 0.44470202\n",
            "Iteration 58, loss = 0.42625323\n",
            "Iteration 59, loss = 0.43142331\n",
            "Iteration 60, loss = 0.42512851\n",
            "Iteration 61, loss = 0.42619071\n",
            "Iteration 62, loss = 0.45601413\n",
            "Iteration 63, loss = 0.44587799\n",
            "Iteration 64, loss = 0.44751715\n",
            "Iteration 65, loss = 0.47398965\n",
            "Iteration 66, loss = 0.43572714\n",
            "Iteration 67, loss = 0.43330635\n",
            "Iteration 68, loss = 0.44682419\n",
            "Iteration 69, loss = 0.42572147\n",
            "Iteration 70, loss = 0.42416372\n",
            "Iteration 71, loss = 0.42286176\n",
            "Iteration 72, loss = 0.43102673\n",
            "Iteration 73, loss = 0.43080747\n",
            "Iteration 74, loss = 0.42256312\n",
            "Iteration 75, loss = 0.41816884\n",
            "Iteration 76, loss = 0.41535855\n",
            "Iteration 77, loss = 0.41648981\n",
            "Iteration 78, loss = 0.41362393\n",
            "Iteration 79, loss = 0.41486611\n",
            "Iteration 80, loss = 0.42211640\n",
            "Iteration 81, loss = 0.42847664\n",
            "Iteration 82, loss = 0.44570173\n",
            "Iteration 83, loss = 0.42391679\n",
            "Iteration 84, loss = 0.41708818\n",
            "Iteration 85, loss = 0.47567769\n",
            "Iteration 86, loss = 0.42293569\n",
            "Iteration 87, loss = 0.41890696\n",
            "Iteration 88, loss = 0.41609305\n",
            "Iteration 89, loss = 0.41143803\n",
            "Iteration 90, loss = 0.41176124\n",
            "Iteration 91, loss = 0.41935481\n",
            "Iteration 92, loss = 0.40683117\n",
            "Iteration 93, loss = 0.41582484\n",
            "Iteration 94, loss = 0.40576059\n",
            "Iteration 95, loss = 0.41877399\n",
            "Iteration 96, loss = 0.42886457\n",
            "Iteration 97, loss = 0.42912513\n",
            "Iteration 98, loss = 0.43291573\n",
            "Iteration 99, loss = 0.46566396\n",
            "Iteration 100, loss = 0.46250708\n",
            "Iteration 101, loss = 0.43052398\n",
            "Iteration 102, loss = 0.40755047\n",
            "Iteration 103, loss = 0.40725064\n",
            "Iteration 104, loss = 0.42613735\n",
            "Iteration 105, loss = 0.40619106\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66141157\n",
            "Iteration 2, loss = 0.63202612\n",
            "Iteration 3, loss = 0.60234409\n",
            "Iteration 4, loss = 0.60840950\n",
            "Iteration 5, loss = 0.59085856\n",
            "Iteration 6, loss = 0.58850666\n",
            "Iteration 7, loss = 0.59088088\n",
            "Iteration 8, loss = 0.58131570\n",
            "Iteration 9, loss = 0.57156307\n",
            "Iteration 10, loss = 0.57128751\n",
            "Iteration 11, loss = 0.55961830\n",
            "Iteration 12, loss = 0.56455414\n",
            "Iteration 13, loss = 0.54813344\n",
            "Iteration 14, loss = 0.55433400\n",
            "Iteration 15, loss = 0.53854548\n",
            "Iteration 16, loss = 0.53247588\n",
            "Iteration 17, loss = 0.54411726\n",
            "Iteration 18, loss = 0.53457717\n",
            "Iteration 19, loss = 0.54969743\n",
            "Iteration 20, loss = 0.51553375\n",
            "Iteration 21, loss = 0.50456800\n",
            "Iteration 22, loss = 0.50222451\n",
            "Iteration 23, loss = 0.50891475\n",
            "Iteration 24, loss = 0.48681044\n",
            "Iteration 25, loss = 0.48479027\n",
            "Iteration 26, loss = 0.47236593\n",
            "Iteration 27, loss = 0.50879311\n",
            "Iteration 28, loss = 0.47433321\n",
            "Iteration 29, loss = 0.46508554\n",
            "Iteration 30, loss = 0.46274249\n",
            "Iteration 31, loss = 0.47368836\n",
            "Iteration 32, loss = 0.46325844\n",
            "Iteration 33, loss = 0.47743576\n",
            "Iteration 34, loss = 0.47245956\n",
            "Iteration 35, loss = 0.46175867\n",
            "Iteration 36, loss = 0.48679232\n",
            "Iteration 37, loss = 0.45062961\n",
            "Iteration 38, loss = 0.46180062\n",
            "Iteration 39, loss = 0.46377326\n",
            "Iteration 40, loss = 0.46179420\n",
            "Iteration 41, loss = 0.46184649\n",
            "Iteration 42, loss = 0.47368457\n",
            "Iteration 43, loss = 0.45198944\n",
            "Iteration 44, loss = 0.44545624\n",
            "Iteration 45, loss = 0.44138248\n",
            "Iteration 46, loss = 0.44714809\n",
            "Iteration 47, loss = 0.45964624\n",
            "Iteration 48, loss = 0.45685400\n",
            "Iteration 49, loss = 0.44173264\n",
            "Iteration 50, loss = 0.47867224\n",
            "Iteration 51, loss = 0.46416423\n",
            "Iteration 52, loss = 0.43210379\n",
            "Iteration 53, loss = 0.44947080\n",
            "Iteration 54, loss = 0.43847622\n",
            "Iteration 55, loss = 0.44063487\n",
            "Iteration 56, loss = 0.43265545\n",
            "Iteration 57, loss = 0.44794048\n",
            "Iteration 58, loss = 0.42919677\n",
            "Iteration 59, loss = 0.42506428\n",
            "Iteration 60, loss = 0.41886806\n",
            "Iteration 61, loss = 0.41851588\n",
            "Iteration 62, loss = 0.43766443\n",
            "Iteration 63, loss = 0.42174136\n",
            "Iteration 64, loss = 0.41749472\n",
            "Iteration 65, loss = 0.41109784\n",
            "Iteration 66, loss = 0.42165267\n",
            "Iteration 67, loss = 0.41059041\n",
            "Iteration 68, loss = 0.41013107\n",
            "Iteration 69, loss = 0.41525878\n",
            "Iteration 70, loss = 0.42270651\n",
            "Iteration 71, loss = 0.40973689\n",
            "Iteration 72, loss = 0.40850663\n",
            "Iteration 73, loss = 0.40708318\n",
            "Iteration 74, loss = 0.41593436\n",
            "Iteration 75, loss = 0.42642222\n",
            "Iteration 76, loss = 0.40581273\n",
            "Iteration 77, loss = 0.41503443\n",
            "Iteration 78, loss = 0.40528785\n",
            "Iteration 79, loss = 0.40679949\n",
            "Iteration 80, loss = 0.40492650\n",
            "Iteration 81, loss = 0.39938739\n",
            "Iteration 82, loss = 0.43615909\n",
            "Iteration 83, loss = 0.40786630\n",
            "Iteration 84, loss = 0.41181670\n",
            "Iteration 85, loss = 0.40554549\n",
            "Iteration 86, loss = 0.40043165\n",
            "Iteration 87, loss = 0.41385874\n",
            "Iteration 88, loss = 0.39467135\n",
            "Iteration 89, loss = 0.41296402\n",
            "Iteration 90, loss = 0.40374562\n",
            "Iteration 91, loss = 0.40068644\n",
            "Iteration 92, loss = 0.40055425\n",
            "Iteration 93, loss = 0.39437523\n",
            "Iteration 94, loss = 0.40423008\n",
            "Iteration 95, loss = 0.39191085\n",
            "Iteration 96, loss = 0.40897044\n",
            "Iteration 97, loss = 0.42027833\n",
            "Iteration 98, loss = 0.40263303\n",
            "Iteration 99, loss = 0.39727811\n",
            "Iteration 100, loss = 0.39502832\n",
            "Iteration 101, loss = 0.40479717\n",
            "Iteration 102, loss = 0.41825799\n",
            "Iteration 103, loss = 0.39112005\n",
            "Iteration 104, loss = 0.39183066\n",
            "Iteration 105, loss = 0.39030966\n",
            "Iteration 106, loss = 0.39558479\n",
            "Iteration 107, loss = 0.40378888\n",
            "Iteration 108, loss = 0.38650011\n",
            "Iteration 109, loss = 0.38607839\n",
            "Iteration 110, loss = 0.38524248\n",
            "Iteration 111, loss = 0.38065125\n",
            "Iteration 112, loss = 0.38367592\n",
            "Iteration 113, loss = 0.39524325\n",
            "Iteration 114, loss = 0.39121616\n",
            "Iteration 115, loss = 0.38616696\n",
            "Iteration 116, loss = 0.38660096\n",
            "Iteration 117, loss = 0.37752919\n",
            "Iteration 118, loss = 0.38189122\n",
            "Iteration 119, loss = 0.37904283\n",
            "Iteration 120, loss = 0.38627817\n",
            "Iteration 121, loss = 0.38765578\n",
            "Iteration 122, loss = 0.38849527\n",
            "Iteration 123, loss = 0.39439976\n",
            "Iteration 124, loss = 0.37882485\n",
            "Iteration 125, loss = 0.37367743\n",
            "Iteration 126, loss = 0.37381880\n",
            "Iteration 127, loss = 0.37729314\n",
            "Iteration 128, loss = 0.39103576\n",
            "Iteration 129, loss = 0.39020186\n",
            "Iteration 130, loss = 0.39163726\n",
            "Iteration 131, loss = 0.36964531\n",
            "Iteration 132, loss = 0.39189321\n",
            "Iteration 133, loss = 0.37707148\n",
            "Iteration 134, loss = 0.37722877\n",
            "Iteration 135, loss = 0.37151009\n",
            "Iteration 136, loss = 0.37118777\n",
            "Iteration 137, loss = 0.37000936\n",
            "Iteration 138, loss = 0.36712418\n",
            "Iteration 139, loss = 0.36915168\n",
            "Iteration 140, loss = 0.37274947\n",
            "Iteration 141, loss = 0.38402059\n",
            "Iteration 142, loss = 0.36433128\n",
            "Iteration 143, loss = 0.37125743\n",
            "Iteration 144, loss = 0.37245495\n",
            "Iteration 145, loss = 0.38114221\n",
            "Iteration 146, loss = 0.37324266\n",
            "Iteration 147, loss = 0.37445555\n",
            "Iteration 148, loss = 0.36806563\n",
            "Iteration 149, loss = 0.38139326\n",
            "Iteration 150, loss = 0.37217203\n",
            "Iteration 151, loss = 0.36127388\n",
            "Iteration 152, loss = 0.36358266\n",
            "Iteration 153, loss = 0.36251566\n",
            "Iteration 154, loss = 0.36616243\n",
            "Iteration 155, loss = 0.36347628\n",
            "Iteration 156, loss = 0.39651429\n",
            "Iteration 157, loss = 0.41342347\n",
            "Iteration 158, loss = 0.37351303\n",
            "Iteration 159, loss = 0.36694136\n",
            "Iteration 160, loss = 0.39692939\n",
            "Iteration 161, loss = 0.37874406\n",
            "Iteration 162, loss = 0.38588889\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.65587918\n",
            "Iteration 2, loss = 0.60675960\n",
            "Iteration 3, loss = 0.60140068\n",
            "Iteration 4, loss = 0.60038246\n",
            "Iteration 5, loss = 0.58147614\n",
            "Iteration 6, loss = 0.58238068\n",
            "Iteration 7, loss = 0.58607500\n",
            "Iteration 8, loss = 0.57557688\n",
            "Iteration 9, loss = 0.57390423\n",
            "Iteration 10, loss = 0.57263024\n",
            "Iteration 11, loss = 0.54388124\n",
            "Iteration 12, loss = 0.56541873\n",
            "Iteration 13, loss = 0.54996381\n",
            "Iteration 14, loss = 0.54693921\n",
            "Iteration 15, loss = 0.53742232\n",
            "Iteration 16, loss = 0.52225404\n",
            "Iteration 17, loss = 0.50494906\n",
            "Iteration 18, loss = 0.49653656\n",
            "Iteration 19, loss = 0.49407520\n",
            "Iteration 20, loss = 0.50459952\n",
            "Iteration 21, loss = 0.50462767\n",
            "Iteration 22, loss = 0.47736438\n",
            "Iteration 23, loss = 0.48940154\n",
            "Iteration 24, loss = 0.47096952\n",
            "Iteration 25, loss = 0.46732773\n",
            "Iteration 26, loss = 0.47272542\n",
            "Iteration 27, loss = 0.46676724\n",
            "Iteration 28, loss = 0.46902951\n",
            "Iteration 29, loss = 0.47747934\n",
            "Iteration 30, loss = 0.47623990\n",
            "Iteration 31, loss = 0.48167013\n",
            "Iteration 32, loss = 0.52190227\n",
            "Iteration 33, loss = 0.46452378\n",
            "Iteration 34, loss = 0.45228415\n",
            "Iteration 35, loss = 0.46282633\n",
            "Iteration 36, loss = 0.46076319\n",
            "Iteration 37, loss = 0.45212285\n",
            "Iteration 38, loss = 0.46270804\n",
            "Iteration 39, loss = 0.47457947\n",
            "Iteration 40, loss = 0.48552717\n",
            "Iteration 41, loss = 0.45998962\n",
            "Iteration 42, loss = 0.44892966\n",
            "Iteration 43, loss = 0.45389834\n",
            "Iteration 44, loss = 0.44044028\n",
            "Iteration 45, loss = 0.44412225\n",
            "Iteration 46, loss = 0.44633635\n",
            "Iteration 47, loss = 0.44131177\n",
            "Iteration 48, loss = 0.44599532\n",
            "Iteration 49, loss = 0.44630601\n",
            "Iteration 50, loss = 0.42748168\n",
            "Iteration 51, loss = 0.43439684\n",
            "Iteration 52, loss = 0.43555359\n",
            "Iteration 53, loss = 0.43492179\n",
            "Iteration 54, loss = 0.43762290\n",
            "Iteration 55, loss = 0.44873671\n",
            "Iteration 56, loss = 0.43690168\n",
            "Iteration 57, loss = 0.42963250\n",
            "Iteration 58, loss = 0.43148283\n",
            "Iteration 59, loss = 0.43665170\n",
            "Iteration 60, loss = 0.42227758\n",
            "Iteration 61, loss = 0.44439575\n",
            "Iteration 62, loss = 0.44774791\n",
            "Iteration 63, loss = 0.45012832\n",
            "Iteration 64, loss = 0.44521111\n",
            "Iteration 65, loss = 0.43417288\n",
            "Iteration 66, loss = 0.43594272\n",
            "Iteration 67, loss = 0.43725915\n",
            "Iteration 68, loss = 0.43745704\n",
            "Iteration 69, loss = 0.42634787\n",
            "Iteration 70, loss = 0.42333952\n",
            "Iteration 71, loss = 0.42834681\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66355307\n",
            "Iteration 2, loss = 0.60713058\n",
            "Iteration 3, loss = 0.60584394\n",
            "Iteration 4, loss = 0.60714224\n",
            "Iteration 5, loss = 0.59696115\n",
            "Iteration 6, loss = 0.59544306\n",
            "Iteration 7, loss = 0.59075761\n",
            "Iteration 8, loss = 0.58916164\n",
            "Iteration 9, loss = 0.58768697\n",
            "Iteration 10, loss = 0.58463322\n",
            "Iteration 11, loss = 0.58529648\n",
            "Iteration 12, loss = 0.58933362\n",
            "Iteration 13, loss = 0.59807743\n",
            "Iteration 14, loss = 0.58302131\n",
            "Iteration 15, loss = 0.57650041\n",
            "Iteration 16, loss = 0.59460509\n",
            "Iteration 17, loss = 0.58762021\n",
            "Iteration 18, loss = 0.58124104\n",
            "Iteration 19, loss = 0.57479363\n",
            "Iteration 20, loss = 0.56830200\n",
            "Iteration 21, loss = 0.55977464\n",
            "Iteration 22, loss = 0.55681980\n",
            "Iteration 23, loss = 0.57773387\n",
            "Iteration 24, loss = 0.56114934\n",
            "Iteration 25, loss = 0.56605539\n",
            "Iteration 26, loss = 0.56600795\n",
            "Iteration 27, loss = 0.56979928\n",
            "Iteration 28, loss = 0.56647342\n",
            "Iteration 29, loss = 0.58711234\n",
            "Iteration 30, loss = 0.57688650\n",
            "Iteration 31, loss = 0.56513771\n",
            "Iteration 32, loss = 0.57393235\n",
            "Iteration 33, loss = 0.55812237\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.60873419\n",
            "Iteration 2, loss = 0.60087311\n",
            "Iteration 3, loss = 0.59445528\n",
            "Iteration 4, loss = 0.59285440\n",
            "Iteration 5, loss = 0.59367866\n",
            "Iteration 6, loss = 0.58546316\n",
            "Iteration 7, loss = 0.58949089\n",
            "Iteration 8, loss = 0.58875049\n",
            "Iteration 9, loss = 0.59424744\n",
            "Iteration 10, loss = 0.58727898\n",
            "Iteration 11, loss = 0.57647737\n",
            "Iteration 12, loss = 0.58341697\n",
            "Iteration 13, loss = 0.57520213\n",
            "Iteration 14, loss = 0.58500465\n",
            "Iteration 15, loss = 0.58020416\n",
            "Iteration 16, loss = 0.57428467\n",
            "Iteration 17, loss = 0.57692586\n",
            "Iteration 18, loss = 0.56803117\n",
            "Iteration 19, loss = 0.56979904\n",
            "Iteration 20, loss = 0.56423824\n",
            "Iteration 21, loss = 0.56072555\n",
            "Iteration 22, loss = 0.56866378\n",
            "Iteration 23, loss = 0.56764267\n",
            "Iteration 24, loss = 0.56463628\n",
            "Iteration 25, loss = 0.57054102\n",
            "Iteration 26, loss = 0.56251432\n",
            "Iteration 27, loss = 0.56631263\n",
            "Iteration 28, loss = 0.56951382\n",
            "Iteration 29, loss = 0.56209803\n",
            "Iteration 30, loss = 0.55507274\n",
            "Iteration 31, loss = 0.56256235\n",
            "Iteration 32, loss = 0.54895785\n",
            "Iteration 33, loss = 0.56188222\n",
            "Iteration 34, loss = 0.56552615\n",
            "Iteration 35, loss = 0.57248593\n",
            "Iteration 36, loss = 0.55880659\n",
            "Iteration 37, loss = 0.56434260\n",
            "Iteration 38, loss = 0.54653963\n",
            "Iteration 39, loss = 0.55367244\n",
            "Iteration 40, loss = 0.54396949\n",
            "Iteration 41, loss = 0.54579803\n",
            "Iteration 42, loss = 0.54767017\n",
            "Iteration 43, loss = 0.55279350\n",
            "Iteration 44, loss = 0.54945657\n",
            "Iteration 45, loss = 0.54655236\n",
            "Iteration 46, loss = 0.55618749\n",
            "Iteration 47, loss = 0.55014095\n",
            "Iteration 48, loss = 0.54470681\n",
            "Iteration 49, loss = 0.53621586\n",
            "Iteration 50, loss = 0.53174634\n",
            "Iteration 51, loss = 0.54066377\n",
            "Iteration 52, loss = 0.52930066\n",
            "Iteration 53, loss = 0.54112331\n",
            "Iteration 54, loss = 0.55104180\n",
            "Iteration 55, loss = 0.53354550\n",
            "Iteration 56, loss = 0.53972670\n",
            "Iteration 57, loss = 0.53403008\n",
            "Iteration 58, loss = 0.51462605\n",
            "Iteration 59, loss = 0.51548740\n",
            "Iteration 60, loss = 0.51906232\n",
            "Iteration 61, loss = 0.51894590\n",
            "Iteration 62, loss = 0.53754560\n",
            "Iteration 63, loss = 0.53345537\n",
            "Iteration 64, loss = 0.52821522\n",
            "Iteration 65, loss = 0.53571273\n",
            "Iteration 66, loss = 0.53898130\n",
            "Iteration 67, loss = 0.53703540\n",
            "Iteration 68, loss = 0.53057998\n",
            "Iteration 69, loss = 0.53711267\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64536530\n",
            "Iteration 2, loss = 0.62009869\n",
            "Iteration 3, loss = 0.61709464\n",
            "Iteration 4, loss = 0.61343739\n",
            "Iteration 5, loss = 0.60946079\n",
            "Iteration 6, loss = 0.60888949\n",
            "Iteration 7, loss = 0.61323983\n",
            "Iteration 8, loss = 0.60738774\n",
            "Iteration 9, loss = 0.60549578\n",
            "Iteration 10, loss = 0.60631186\n",
            "Iteration 11, loss = 0.60328338\n",
            "Iteration 12, loss = 0.60289475\n",
            "Iteration 13, loss = 0.60089190\n",
            "Iteration 14, loss = 0.60453521\n",
            "Iteration 15, loss = 0.59956332\n",
            "Iteration 16, loss = 0.60951227\n",
            "Iteration 17, loss = 0.60191531\n",
            "Iteration 18, loss = 0.59848614\n",
            "Iteration 19, loss = 0.59906290\n",
            "Iteration 20, loss = 0.59689407\n",
            "Iteration 21, loss = 0.59030199\n",
            "Iteration 22, loss = 0.59377074\n",
            "Iteration 23, loss = 0.59528179\n",
            "Iteration 24, loss = 0.59055309\n",
            "Iteration 25, loss = 0.58781200\n",
            "Iteration 26, loss = 0.58916070\n",
            "Iteration 27, loss = 0.58444044\n",
            "Iteration 28, loss = 0.59730393\n",
            "Iteration 29, loss = 0.59313059\n",
            "Iteration 30, loss = 0.59230837\n",
            "Iteration 31, loss = 0.59319186\n",
            "Iteration 32, loss = 0.59204601\n",
            "Iteration 33, loss = 0.58392047\n",
            "Iteration 34, loss = 0.58307747\n",
            "Iteration 35, loss = 0.57960632\n",
            "Iteration 36, loss = 0.58256414\n",
            "Iteration 37, loss = 0.57595927\n",
            "Iteration 38, loss = 0.59295115\n",
            "Iteration 39, loss = 0.57903761\n",
            "Iteration 40, loss = 0.57917466\n",
            "Iteration 41, loss = 0.57885174\n",
            "Iteration 42, loss = 0.57554807\n",
            "Iteration 43, loss = 0.57190342\n",
            "Iteration 44, loss = 0.56705725\n",
            "Iteration 45, loss = 0.57640329\n",
            "Iteration 46, loss = 0.55730511\n",
            "Iteration 47, loss = 0.55668512\n",
            "Iteration 48, loss = 0.56368402\n",
            "Iteration 49, loss = 0.56066714\n",
            "Iteration 50, loss = 0.56747212\n",
            "Iteration 51, loss = 0.57548593\n",
            "Iteration 52, loss = 0.55078552\n",
            "Iteration 53, loss = 0.55341218\n",
            "Iteration 54, loss = 0.55068052\n",
            "Iteration 55, loss = 0.56510620\n",
            "Iteration 56, loss = 0.56707802\n",
            "Iteration 57, loss = 0.55345910\n",
            "Iteration 58, loss = 0.54072683\n",
            "Iteration 59, loss = 0.54847941\n",
            "Iteration 60, loss = 0.54716933\n",
            "Iteration 61, loss = 0.56059051\n",
            "Iteration 62, loss = 0.58979770\n",
            "Iteration 63, loss = 0.58471705\n",
            "Iteration 64, loss = 0.57549636\n",
            "Iteration 65, loss = 0.57409999\n",
            "Iteration 66, loss = 0.58441559\n",
            "Iteration 67, loss = 0.57457643\n",
            "Iteration 68, loss = 0.56452885\n",
            "Iteration 69, loss = 0.57262269\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64108471\n",
            "Iteration 2, loss = 0.60424360\n",
            "Iteration 3, loss = 0.60301269\n",
            "Iteration 4, loss = 0.59888016\n",
            "Iteration 5, loss = 0.59615468\n",
            "Iteration 6, loss = 0.59541610\n",
            "Iteration 7, loss = 0.59091697\n",
            "Iteration 8, loss = 0.58774511\n",
            "Iteration 9, loss = 0.58143367\n",
            "Iteration 10, loss = 0.58303527\n",
            "Iteration 11, loss = 0.58458128\n",
            "Iteration 12, loss = 0.58541621\n",
            "Iteration 13, loss = 0.58493157\n",
            "Iteration 14, loss = 0.57724910\n",
            "Iteration 15, loss = 0.58420864\n",
            "Iteration 16, loss = 0.57214871\n",
            "Iteration 17, loss = 0.57071783\n",
            "Iteration 18, loss = 0.56909306\n",
            "Iteration 19, loss = 0.57668291\n",
            "Iteration 20, loss = 0.58194109\n",
            "Iteration 21, loss = 0.58545383\n",
            "Iteration 22, loss = 0.57975950\n",
            "Iteration 23, loss = 0.57384778\n",
            "Iteration 24, loss = 0.57357808\n",
            "Iteration 25, loss = 0.56586062\n",
            "Iteration 26, loss = 0.56813115\n",
            "Iteration 27, loss = 0.57536879\n",
            "Iteration 28, loss = 0.56287618\n",
            "Iteration 29, loss = 0.56105237\n",
            "Iteration 30, loss = 0.55408147\n",
            "Iteration 31, loss = 0.56139363\n",
            "Iteration 32, loss = 0.55894692\n",
            "Iteration 33, loss = 0.54587291\n",
            "Iteration 34, loss = 0.56193211\n",
            "Iteration 35, loss = 0.55256709\n",
            "Iteration 36, loss = 0.55117799\n",
            "Iteration 37, loss = 0.55270961\n",
            "Iteration 38, loss = 0.54216338\n",
            "Iteration 39, loss = 0.53706609\n",
            "Iteration 40, loss = 0.54373516\n",
            "Iteration 41, loss = 0.53558015\n",
            "Iteration 42, loss = 0.56160383\n",
            "Iteration 43, loss = 0.54244987\n",
            "Iteration 44, loss = 0.54624659\n",
            "Iteration 45, loss = 0.53550497\n",
            "Iteration 46, loss = 0.54236565\n",
            "Iteration 47, loss = 0.53441143\n",
            "Iteration 48, loss = 0.53742590\n",
            "Iteration 49, loss = 0.53389026\n",
            "Iteration 50, loss = 0.54294270\n",
            "Iteration 51, loss = 0.53596422\n",
            "Iteration 52, loss = 0.52409095\n",
            "Iteration 53, loss = 0.56744475\n",
            "Iteration 54, loss = 0.54761738\n",
            "Iteration 55, loss = 0.53834284\n",
            "Iteration 56, loss = 0.53265123\n",
            "Iteration 57, loss = 0.53227870\n",
            "Iteration 58, loss = 0.53328250\n",
            "Iteration 59, loss = 0.51541169\n",
            "Iteration 60, loss = 0.54128318\n",
            "Iteration 61, loss = 0.54881385\n",
            "Iteration 62, loss = 0.53817398\n",
            "Iteration 63, loss = 0.54496627\n",
            "Iteration 64, loss = 0.55146439\n",
            "Iteration 65, loss = 0.53379573\n",
            "Iteration 66, loss = 0.55236776\n",
            "Iteration 67, loss = 0.56534253\n",
            "Iteration 68, loss = 0.56476495\n",
            "Iteration 69, loss = 0.54418504\n",
            "Iteration 70, loss = 0.53276139\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.65075769\n",
            "Iteration 2, loss = 0.59776942\n",
            "Iteration 3, loss = 0.60045385\n",
            "Iteration 4, loss = 0.59629759\n",
            "Iteration 5, loss = 0.58972511\n",
            "Iteration 6, loss = 0.58780209\n",
            "Iteration 7, loss = 0.58923528\n",
            "Iteration 8, loss = 0.58277487\n",
            "Iteration 9, loss = 0.58865264\n",
            "Iteration 10, loss = 0.58300936\n",
            "Iteration 11, loss = 0.57954147\n",
            "Iteration 12, loss = 0.58332059\n",
            "Iteration 13, loss = 0.58470832\n",
            "Iteration 14, loss = 0.58063193\n",
            "Iteration 15, loss = 0.57985095\n",
            "Iteration 16, loss = 0.57613766\n",
            "Iteration 17, loss = 0.58929057\n",
            "Iteration 18, loss = 0.57326241\n",
            "Iteration 19, loss = 0.56869179\n",
            "Iteration 20, loss = 0.57913781\n",
            "Iteration 21, loss = 0.56202851\n",
            "Iteration 22, loss = 0.57052511\n",
            "Iteration 23, loss = 0.55853262\n",
            "Iteration 24, loss = 0.56036879\n",
            "Iteration 25, loss = 0.56142819\n",
            "Iteration 26, loss = 0.57776152\n",
            "Iteration 27, loss = 0.55912974\n",
            "Iteration 28, loss = 0.55382940\n",
            "Iteration 29, loss = 0.56586875\n",
            "Iteration 30, loss = 0.55983858\n",
            "Iteration 31, loss = 0.55872344\n",
            "Iteration 32, loss = 0.54534483\n",
            "Iteration 33, loss = 0.54721315\n",
            "Iteration 34, loss = 0.57331810\n",
            "Iteration 35, loss = 0.56445940\n",
            "Iteration 36, loss = 0.55898972\n",
            "Iteration 37, loss = 0.53702931\n",
            "Iteration 38, loss = 0.54987359\n",
            "Iteration 39, loss = 0.55702513\n",
            "Iteration 40, loss = 0.54920668\n",
            "Iteration 41, loss = 0.54969015\n",
            "Iteration 42, loss = 0.56036545\n",
            "Iteration 43, loss = 0.55185488\n",
            "Iteration 44, loss = 0.55532441\n",
            "Iteration 45, loss = 0.54723259\n",
            "Iteration 46, loss = 0.57422159\n",
            "Iteration 47, loss = 0.55504475\n",
            "Iteration 48, loss = 0.56293846\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.01805074\n",
            "Iteration 2, loss = 0.64865061\n",
            "Iteration 3, loss = 0.70693840\n",
            "Iteration 4, loss = 0.61854134\n",
            "Iteration 5, loss = 0.61969477\n",
            "Iteration 6, loss = 0.60147002\n",
            "Iteration 7, loss = 0.59881878\n",
            "Iteration 8, loss = 0.59866112\n",
            "Iteration 9, loss = 0.59502067\n",
            "Iteration 10, loss = 0.59165318\n",
            "Iteration 11, loss = 0.59230934\n",
            "Iteration 12, loss = 0.58854096\n",
            "Iteration 13, loss = 0.58675388\n",
            "Iteration 14, loss = 0.57693404\n",
            "Iteration 15, loss = 0.58338554\n",
            "Iteration 16, loss = 0.58502807\n",
            "Iteration 17, loss = 0.58680759\n",
            "Iteration 18, loss = 0.57437314\n",
            "Iteration 19, loss = 0.59251792\n",
            "Iteration 20, loss = 0.58134137\n",
            "Iteration 21, loss = 0.56954387\n",
            "Iteration 22, loss = 0.56876836\n",
            "Iteration 23, loss = 0.57015626\n",
            "Iteration 24, loss = 0.57185097\n",
            "Iteration 25, loss = 0.56594254\n",
            "Iteration 26, loss = 0.55602919\n",
            "Iteration 27, loss = 0.55487542\n",
            "Iteration 28, loss = 0.55640279\n",
            "Iteration 29, loss = 0.55444333\n",
            "Iteration 30, loss = 0.55500941\n",
            "Iteration 31, loss = 0.55932100\n",
            "Iteration 32, loss = 0.56351462\n",
            "Iteration 33, loss = 0.55867641\n",
            "Iteration 34, loss = 0.54425492\n",
            "Iteration 35, loss = 0.55506044\n",
            "Iteration 36, loss = 0.54017512\n",
            "Iteration 37, loss = 0.54589469\n",
            "Iteration 38, loss = 0.53898606\n",
            "Iteration 39, loss = 0.55059257\n",
            "Iteration 40, loss = 0.53495014\n",
            "Iteration 41, loss = 0.53377676\n",
            "Iteration 42, loss = 0.54432651\n",
            "Iteration 43, loss = 0.53493125\n",
            "Iteration 44, loss = 0.52832763\n",
            "Iteration 45, loss = 0.53954829\n",
            "Iteration 46, loss = 0.53101624\n",
            "Iteration 47, loss = 0.53092545\n",
            "Iteration 48, loss = 0.53177624\n",
            "Iteration 49, loss = 0.52886875\n",
            "Iteration 50, loss = 0.52033980\n",
            "Iteration 51, loss = 0.51777184\n",
            "Iteration 52, loss = 0.52149400\n",
            "Iteration 53, loss = 0.51809332\n",
            "Iteration 54, loss = 0.54122650\n",
            "Iteration 55, loss = 0.53537513\n",
            "Iteration 56, loss = 0.51068596\n",
            "Iteration 57, loss = 0.52868657\n",
            "Iteration 58, loss = 0.51742554\n",
            "Iteration 59, loss = 0.51611432\n",
            "Iteration 60, loss = 0.52805765\n",
            "Iteration 61, loss = 0.51709425\n",
            "Iteration 62, loss = 0.50905657\n",
            "Iteration 63, loss = 0.51010057\n",
            "Iteration 64, loss = 0.50456395\n",
            "Iteration 65, loss = 0.50851661\n",
            "Iteration 66, loss = 0.53386784\n",
            "Iteration 67, loss = 0.52500751\n",
            "Iteration 68, loss = 0.49876107\n",
            "Iteration 69, loss = 0.48590470\n",
            "Iteration 70, loss = 0.52296338\n",
            "Iteration 71, loss = 0.50493306\n",
            "Iteration 72, loss = 0.48096111\n",
            "Iteration 73, loss = 0.50976963\n",
            "Iteration 74, loss = 0.49919889\n",
            "Iteration 75, loss = 0.51067550\n",
            "Iteration 76, loss = 0.49440661\n",
            "Iteration 77, loss = 0.49761717\n",
            "Iteration 78, loss = 0.52078622\n",
            "Iteration 79, loss = 0.49312716\n",
            "Iteration 80, loss = 0.48578726\n",
            "Iteration 81, loss = 0.51606999\n",
            "Iteration 82, loss = 0.48199126\n",
            "Iteration 83, loss = 0.47989672\n",
            "Iteration 84, loss = 0.49238689\n",
            "Iteration 85, loss = 0.48513843\n",
            "Iteration 86, loss = 0.48836394\n",
            "Iteration 87, loss = 0.48955461\n",
            "Iteration 88, loss = 0.50510533\n",
            "Iteration 89, loss = 0.50458672\n",
            "Iteration 90, loss = 0.49625301\n",
            "Iteration 91, loss = 0.51383739\n",
            "Iteration 92, loss = 0.50763925\n",
            "Iteration 93, loss = 0.48219356\n",
            "Iteration 94, loss = 0.49309687\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.12895916\n",
            "Iteration 2, loss = 0.68263427\n",
            "Iteration 3, loss = 0.61701052\n",
            "Iteration 4, loss = 0.60667113\n",
            "Iteration 5, loss = 0.59632394\n",
            "Iteration 6, loss = 0.59590150\n",
            "Iteration 7, loss = 0.59474586\n",
            "Iteration 8, loss = 0.59320870\n",
            "Iteration 9, loss = 0.58795244\n",
            "Iteration 10, loss = 0.59134553\n",
            "Iteration 11, loss = 0.58252939\n",
            "Iteration 12, loss = 0.59126262\n",
            "Iteration 13, loss = 0.58196001\n",
            "Iteration 14, loss = 0.57487793\n",
            "Iteration 15, loss = 0.57423371\n",
            "Iteration 16, loss = 0.58271565\n",
            "Iteration 17, loss = 0.58332195\n",
            "Iteration 18, loss = 0.56188298\n",
            "Iteration 19, loss = 0.55736342\n",
            "Iteration 20, loss = 0.56345688\n",
            "Iteration 21, loss = 0.56372159\n",
            "Iteration 22, loss = 0.55477852\n",
            "Iteration 23, loss = 0.57434875\n",
            "Iteration 24, loss = 0.56736548\n",
            "Iteration 25, loss = 0.56084167\n",
            "Iteration 26, loss = 0.56238006\n",
            "Iteration 27, loss = 0.54387484\n",
            "Iteration 28, loss = 0.55483382\n",
            "Iteration 29, loss = 0.55701967\n",
            "Iteration 30, loss = 0.55549595\n",
            "Iteration 31, loss = 0.53703811\n",
            "Iteration 32, loss = 0.53191282\n",
            "Iteration 33, loss = 0.53860831\n",
            "Iteration 34, loss = 0.53720475\n",
            "Iteration 35, loss = 0.52202342\n",
            "Iteration 36, loss = 0.54137119\n",
            "Iteration 37, loss = 0.53472182\n",
            "Iteration 38, loss = 0.51766996\n",
            "Iteration 39, loss = 0.52842655\n",
            "Iteration 40, loss = 0.53458467\n",
            "Iteration 41, loss = 0.52358301\n",
            "Iteration 42, loss = 0.52245934\n",
            "Iteration 43, loss = 0.52104949\n",
            "Iteration 44, loss = 0.51892638\n",
            "Iteration 45, loss = 0.52209829\n",
            "Iteration 46, loss = 0.52422190\n",
            "Iteration 47, loss = 0.50599170\n",
            "Iteration 48, loss = 0.52739308\n",
            "Iteration 49, loss = 0.52264284\n",
            "Iteration 50, loss = 0.51614105\n",
            "Iteration 51, loss = 0.51740043\n",
            "Iteration 52, loss = 0.51210037\n",
            "Iteration 53, loss = 0.50950060\n",
            "Iteration 54, loss = 0.52743918\n",
            "Iteration 55, loss = 0.50737068\n",
            "Iteration 56, loss = 0.51808963\n",
            "Iteration 57, loss = 0.50580270\n",
            "Iteration 58, loss = 0.51160821\n",
            "Iteration 59, loss = 0.49959138\n",
            "Iteration 60, loss = 0.50519861\n",
            "Iteration 61, loss = 0.50831284\n",
            "Iteration 62, loss = 0.50218340\n",
            "Iteration 63, loss = 0.49984816\n",
            "Iteration 64, loss = 0.50361511\n",
            "Iteration 65, loss = 0.50463161\n",
            "Iteration 66, loss = 0.48826068\n",
            "Iteration 67, loss = 0.49995162\n",
            "Iteration 68, loss = 0.52977909\n",
            "Iteration 69, loss = 0.50655642\n",
            "Iteration 70, loss = 0.49475824\n",
            "Iteration 71, loss = 0.49192451\n",
            "Iteration 72, loss = 0.48871984\n",
            "Iteration 73, loss = 0.53616527\n",
            "Iteration 74, loss = 0.50342295\n",
            "Iteration 75, loss = 0.50328919\n",
            "Iteration 76, loss = 0.49548890\n",
            "Iteration 77, loss = 0.48135510\n",
            "Iteration 78, loss = 0.49058010\n",
            "Iteration 79, loss = 0.49141626\n",
            "Iteration 80, loss = 0.48343558\n",
            "Iteration 81, loss = 0.49195643\n",
            "Iteration 82, loss = 0.48896540\n",
            "Iteration 83, loss = 0.48173841\n",
            "Iteration 84, loss = 0.49213003\n",
            "Iteration 85, loss = 0.51144777\n",
            "Iteration 86, loss = 0.48525826\n",
            "Iteration 87, loss = 0.48103933\n",
            "Iteration 88, loss = 0.48652958\n",
            "Iteration 89, loss = 0.48929964\n",
            "Iteration 90, loss = 0.48238458\n",
            "Iteration 91, loss = 0.48823299\n",
            "Iteration 92, loss = 0.47443118\n",
            "Iteration 93, loss = 0.49697461\n",
            "Iteration 94, loss = 0.49854407\n",
            "Iteration 95, loss = 0.49607705\n",
            "Iteration 96, loss = 0.48629612\n",
            "Iteration 97, loss = 0.49126681\n",
            "Iteration 98, loss = 0.49330522\n",
            "Iteration 99, loss = 0.50181490\n",
            "Iteration 100, loss = 0.49732261\n",
            "Iteration 101, loss = 0.49274308\n",
            "Iteration 102, loss = 0.49364916\n",
            "Iteration 103, loss = 0.48659257\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.88474471\n",
            "Iteration 2, loss = 0.75430425\n",
            "Iteration 3, loss = 0.62732195\n",
            "Iteration 4, loss = 0.63057336\n",
            "Iteration 5, loss = 0.65531466\n",
            "Iteration 6, loss = 0.67868875\n",
            "Iteration 7, loss = 0.62727765\n",
            "Iteration 8, loss = 0.61327217\n",
            "Iteration 9, loss = 0.61497960\n",
            "Iteration 10, loss = 0.60210578\n",
            "Iteration 11, loss = 0.60742573\n",
            "Iteration 12, loss = 0.61112523\n",
            "Iteration 13, loss = 0.59986082\n",
            "Iteration 14, loss = 0.60389849\n",
            "Iteration 15, loss = 0.59180248\n",
            "Iteration 16, loss = 0.59346050\n",
            "Iteration 17, loss = 0.58838563\n",
            "Iteration 18, loss = 0.59562010\n",
            "Iteration 19, loss = 0.58980484\n",
            "Iteration 20, loss = 0.58086638\n",
            "Iteration 21, loss = 0.57677210\n",
            "Iteration 22, loss = 0.58541419\n",
            "Iteration 23, loss = 0.57583124\n",
            "Iteration 24, loss = 0.56596768\n",
            "Iteration 25, loss = 0.57713874\n",
            "Iteration 26, loss = 0.56903715\n",
            "Iteration 27, loss = 0.56037864\n",
            "Iteration 28, loss = 0.56293726\n",
            "Iteration 29, loss = 0.57488706\n",
            "Iteration 30, loss = 0.56416024\n",
            "Iteration 31, loss = 0.55763987\n",
            "Iteration 32, loss = 0.58402981\n",
            "Iteration 33, loss = 0.55673500\n",
            "Iteration 34, loss = 0.54713702\n",
            "Iteration 35, loss = 0.55875929\n",
            "Iteration 36, loss = 0.54861747\n",
            "Iteration 37, loss = 0.55061916\n",
            "Iteration 38, loss = 0.54368639\n",
            "Iteration 39, loss = 0.54242927\n",
            "Iteration 40, loss = 0.57453555\n",
            "Iteration 41, loss = 0.54578934\n",
            "Iteration 42, loss = 0.53222209\n",
            "Iteration 43, loss = 0.54254590\n",
            "Iteration 44, loss = 0.53396083\n",
            "Iteration 45, loss = 0.53065357\n",
            "Iteration 46, loss = 0.53324037\n",
            "Iteration 47, loss = 0.51640104\n",
            "Iteration 48, loss = 0.52506412\n",
            "Iteration 49, loss = 0.52071399\n",
            "Iteration 50, loss = 0.52315961\n",
            "Iteration 51, loss = 0.52394688\n",
            "Iteration 52, loss = 0.53101891\n",
            "Iteration 53, loss = 0.52852492\n",
            "Iteration 54, loss = 0.52642038\n",
            "Iteration 55, loss = 0.52447663\n",
            "Iteration 56, loss = 0.51688082\n",
            "Iteration 57, loss = 0.52105062\n",
            "Iteration 58, loss = 0.50666583\n",
            "Iteration 59, loss = 0.49458439\n",
            "Iteration 60, loss = 0.56784614\n",
            "Iteration 61, loss = 0.51254405\n",
            "Iteration 62, loss = 0.51107499\n",
            "Iteration 63, loss = 0.51246118\n",
            "Iteration 64, loss = 0.52773479\n",
            "Iteration 65, loss = 0.53343054\n",
            "Iteration 66, loss = 0.51731975\n",
            "Iteration 67, loss = 0.52357414\n",
            "Iteration 68, loss = 0.50558848\n",
            "Iteration 69, loss = 0.50706044\n",
            "Iteration 70, loss = 0.50516800\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.85904426\n",
            "Iteration 2, loss = 0.71793837\n",
            "Iteration 3, loss = 0.69963847\n",
            "Iteration 4, loss = 0.64623383\n",
            "Iteration 5, loss = 0.61752302\n",
            "Iteration 6, loss = 0.61106350\n",
            "Iteration 7, loss = 0.60289309\n",
            "Iteration 8, loss = 0.59062462\n",
            "Iteration 9, loss = 0.59147719\n",
            "Iteration 10, loss = 0.61170261\n",
            "Iteration 11, loss = 0.60049097\n",
            "Iteration 12, loss = 0.58179829\n",
            "Iteration 13, loss = 0.57686223\n",
            "Iteration 14, loss = 0.58247749\n",
            "Iteration 15, loss = 0.57238869\n",
            "Iteration 16, loss = 0.57705564\n",
            "Iteration 17, loss = 0.57631542\n",
            "Iteration 18, loss = 0.57309611\n",
            "Iteration 19, loss = 0.58464303\n",
            "Iteration 20, loss = 0.58277825\n",
            "Iteration 21, loss = 0.57190674\n",
            "Iteration 22, loss = 0.57338208\n",
            "Iteration 23, loss = 0.56555286\n",
            "Iteration 24, loss = 0.57412026\n",
            "Iteration 25, loss = 0.56413155\n",
            "Iteration 26, loss = 0.56426566\n",
            "Iteration 27, loss = 0.56270896\n",
            "Iteration 28, loss = 0.54810563\n",
            "Iteration 29, loss = 0.57317809\n",
            "Iteration 30, loss = 0.55992053\n",
            "Iteration 31, loss = 0.55139129\n",
            "Iteration 32, loss = 0.55985986\n",
            "Iteration 33, loss = 0.55196906\n",
            "Iteration 34, loss = 0.54454336\n",
            "Iteration 35, loss = 0.53979190\n",
            "Iteration 36, loss = 0.54712825\n",
            "Iteration 37, loss = 0.55136513\n",
            "Iteration 38, loss = 0.55654038\n",
            "Iteration 39, loss = 0.53864778\n",
            "Iteration 40, loss = 0.54533491\n",
            "Iteration 41, loss = 0.54127084\n",
            "Iteration 42, loss = 0.53170885\n",
            "Iteration 43, loss = 0.52814173\n",
            "Iteration 44, loss = 0.53041872\n",
            "Iteration 45, loss = 0.54785252\n",
            "Iteration 46, loss = 0.53267857\n",
            "Iteration 47, loss = 0.52198532\n",
            "Iteration 48, loss = 0.52837172\n",
            "Iteration 49, loss = 0.53860340\n",
            "Iteration 50, loss = 0.53848759\n",
            "Iteration 51, loss = 0.52330018\n",
            "Iteration 52, loss = 0.52043828\n",
            "Iteration 53, loss = 0.52101760\n",
            "Iteration 54, loss = 0.51823413\n",
            "Iteration 55, loss = 0.54302140\n",
            "Iteration 56, loss = 0.51913946\n",
            "Iteration 57, loss = 0.52272557\n",
            "Iteration 58, loss = 0.52834706\n",
            "Iteration 59, loss = 0.52173630\n",
            "Iteration 60, loss = 0.50413783\n",
            "Iteration 61, loss = 0.51224975\n",
            "Iteration 62, loss = 0.50485962\n",
            "Iteration 63, loss = 0.50744702\n",
            "Iteration 64, loss = 0.51193974\n",
            "Iteration 65, loss = 0.51099212\n",
            "Iteration 66, loss = 0.52611986\n",
            "Iteration 67, loss = 0.51989535\n",
            "Iteration 68, loss = 0.51539416\n",
            "Iteration 69, loss = 0.50763019\n",
            "Iteration 70, loss = 0.51272228\n",
            "Iteration 71, loss = 0.50763665\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.96877651\n",
            "Iteration 2, loss = 0.75100049\n",
            "Iteration 3, loss = 0.67535782\n",
            "Iteration 4, loss = 0.65220727\n",
            "Iteration 5, loss = 0.61266270\n",
            "Iteration 6, loss = 0.62553020\n",
            "Iteration 7, loss = 0.60383113\n",
            "Iteration 8, loss = 0.59897627\n",
            "Iteration 9, loss = 0.59228936\n",
            "Iteration 10, loss = 0.60055329\n",
            "Iteration 11, loss = 0.59857017\n",
            "Iteration 12, loss = 0.59340058\n",
            "Iteration 13, loss = 0.58148165\n",
            "Iteration 14, loss = 0.58833101\n",
            "Iteration 15, loss = 0.59270270\n",
            "Iteration 16, loss = 0.57293472\n",
            "Iteration 17, loss = 0.58038374\n",
            "Iteration 18, loss = 0.57925323\n",
            "Iteration 19, loss = 0.57672723\n",
            "Iteration 20, loss = 0.57149583\n",
            "Iteration 21, loss = 0.58392405\n",
            "Iteration 22, loss = 0.56584164\n",
            "Iteration 23, loss = 0.56752840\n",
            "Iteration 24, loss = 0.57203706\n",
            "Iteration 25, loss = 0.56761595\n",
            "Iteration 26, loss = 0.55237397\n",
            "Iteration 27, loss = 0.57602577\n",
            "Iteration 28, loss = 0.56127756\n",
            "Iteration 29, loss = 0.56023220\n",
            "Iteration 30, loss = 0.55376367\n",
            "Iteration 31, loss = 0.55249253\n",
            "Iteration 32, loss = 0.54861579\n",
            "Iteration 33, loss = 0.56784961\n",
            "Iteration 34, loss = 0.55036988\n",
            "Iteration 35, loss = 0.55764654\n",
            "Iteration 36, loss = 0.54853460\n",
            "Iteration 37, loss = 0.58912180\n",
            "Iteration 38, loss = 0.55999774\n",
            "Iteration 39, loss = 0.53957760\n",
            "Iteration 40, loss = 0.53732107\n",
            "Iteration 41, loss = 0.53685360\n",
            "Iteration 42, loss = 0.54087514\n",
            "Iteration 43, loss = 0.54904039\n",
            "Iteration 44, loss = 0.54514862\n",
            "Iteration 45, loss = 0.53241680\n",
            "Iteration 46, loss = 0.53685307\n",
            "Iteration 47, loss = 0.53595508\n",
            "Iteration 48, loss = 0.54015178\n",
            "Iteration 49, loss = 0.54252078\n",
            "Iteration 50, loss = 0.54627289\n",
            "Iteration 51, loss = 0.54604979\n",
            "Iteration 52, loss = 0.53317134\n",
            "Iteration 53, loss = 0.51695697\n",
            "Iteration 54, loss = 0.54635067\n",
            "Iteration 55, loss = 0.53159802\n",
            "Iteration 56, loss = 0.55597062\n",
            "Iteration 57, loss = 0.51702729\n",
            "Iteration 58, loss = 0.55409863\n",
            "Iteration 59, loss = 0.52916590\n",
            "Iteration 60, loss = 0.52467914\n",
            "Iteration 61, loss = 0.51585959\n",
            "Iteration 62, loss = 0.51846500\n",
            "Iteration 63, loss = 0.51823480\n",
            "Iteration 64, loss = 0.52968504\n",
            "Iteration 65, loss = 0.52530299\n",
            "Iteration 66, loss = 0.54063274\n",
            "Iteration 67, loss = 0.51916906\n",
            "Iteration 68, loss = 0.55218854\n",
            "Iteration 69, loss = 0.50613720\n",
            "Iteration 70, loss = 0.52147463\n",
            "Iteration 71, loss = 0.50775669\n",
            "Iteration 72, loss = 0.50718376\n",
            "Iteration 73, loss = 0.51661915\n",
            "Iteration 74, loss = 0.50459042\n",
            "Iteration 75, loss = 0.50933129\n",
            "Iteration 76, loss = 0.51231265\n",
            "Iteration 77, loss = 0.51331139\n",
            "Iteration 78, loss = 0.51918741\n",
            "Iteration 79, loss = 0.50126115\n",
            "Iteration 80, loss = 0.50891033\n",
            "Iteration 81, loss = 0.51721179\n",
            "Iteration 82, loss = 0.50432537\n",
            "Iteration 83, loss = 0.50831111\n",
            "Iteration 84, loss = 0.51195470\n",
            "Iteration 85, loss = 0.50474795\n",
            "Iteration 86, loss = 0.50509980\n",
            "Iteration 87, loss = 0.50917822\n",
            "Iteration 88, loss = 0.49560300\n",
            "Iteration 89, loss = 0.49294737\n",
            "Iteration 90, loss = 0.52078917\n",
            "Iteration 91, loss = 0.49981170\n",
            "Iteration 92, loss = 0.49960617\n",
            "Iteration 93, loss = 0.49449238\n",
            "Iteration 94, loss = 0.48249423\n",
            "Iteration 95, loss = 0.50442669\n",
            "Iteration 96, loss = 0.50859279\n",
            "Iteration 97, loss = 0.50244170\n",
            "Iteration 98, loss = 0.49720457\n",
            "Iteration 99, loss = 0.49129163\n",
            "Iteration 100, loss = 0.49873969\n",
            "Iteration 101, loss = 0.49339646\n",
            "Iteration 102, loss = 0.50334912\n",
            "Iteration 103, loss = 0.49924982\n",
            "Iteration 104, loss = 0.49909336\n",
            "Iteration 105, loss = 0.48185906\n",
            "Iteration 106, loss = 0.49964280\n",
            "Iteration 107, loss = 0.51012435\n",
            "Iteration 108, loss = 0.48949010\n",
            "Iteration 109, loss = 0.48637944\n",
            "Iteration 110, loss = 0.50349917\n",
            "Iteration 111, loss = 0.49184457\n",
            "Iteration 112, loss = 0.48903096\n",
            "Iteration 113, loss = 0.50653060\n",
            "Iteration 114, loss = 0.48913841\n",
            "Iteration 115, loss = 0.48953479\n",
            "Iteration 116, loss = 0.50104273\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.80849017\n",
            "Iteration 2, loss = 0.67785156\n",
            "Iteration 3, loss = 0.63880830\n",
            "Iteration 4, loss = 0.60373830\n",
            "Iteration 5, loss = 0.59752312\n",
            "Iteration 6, loss = 0.60283494\n",
            "Iteration 7, loss = 0.59255122\n",
            "Iteration 8, loss = 0.58878641\n",
            "Iteration 9, loss = 0.58776524\n",
            "Iteration 10, loss = 0.58778231\n",
            "Iteration 11, loss = 0.58482573\n",
            "Iteration 12, loss = 0.57336849\n",
            "Iteration 13, loss = 0.59765615\n",
            "Iteration 14, loss = 0.57440663\n",
            "Iteration 15, loss = 0.57646451\n",
            "Iteration 16, loss = 0.56734069\n",
            "Iteration 17, loss = 0.57725579\n",
            "Iteration 18, loss = 0.56101487\n",
            "Iteration 19, loss = 0.57135924\n",
            "Iteration 20, loss = 0.55610088\n",
            "Iteration 21, loss = 0.54741004\n",
            "Iteration 22, loss = 0.57039612\n",
            "Iteration 23, loss = 0.55585354\n",
            "Iteration 24, loss = 0.55300855\n",
            "Iteration 25, loss = 0.54691474\n",
            "Iteration 26, loss = 0.54763419\n",
            "Iteration 27, loss = 0.57437409\n",
            "Iteration 28, loss = 0.53787843\n",
            "Iteration 29, loss = 0.54715588\n",
            "Iteration 30, loss = 0.53614776\n",
            "Iteration 31, loss = 0.53637394\n",
            "Iteration 32, loss = 0.55085774\n",
            "Iteration 33, loss = 0.54710656\n",
            "Iteration 34, loss = 0.54269120\n",
            "Iteration 35, loss = 0.52990402\n",
            "Iteration 36, loss = 0.52483211\n",
            "Iteration 37, loss = 0.51807536\n",
            "Iteration 38, loss = 0.52475373\n",
            "Iteration 39, loss = 0.52130965\n",
            "Iteration 40, loss = 0.54303517\n",
            "Iteration 41, loss = 0.51642088\n",
            "Iteration 42, loss = 0.51847055\n",
            "Iteration 43, loss = 0.51331659\n",
            "Iteration 44, loss = 0.53076732\n",
            "Iteration 45, loss = 0.51985864\n",
            "Iteration 46, loss = 0.53413142\n",
            "Iteration 47, loss = 0.50097621\n",
            "Iteration 48, loss = 0.53844285\n",
            "Iteration 49, loss = 0.51133525\n",
            "Iteration 50, loss = 0.51831602\n",
            "Iteration 51, loss = 0.51709628\n",
            "Iteration 52, loss = 0.54964430\n",
            "Iteration 53, loss = 0.52252645\n",
            "Iteration 54, loss = 0.52491917\n",
            "Iteration 55, loss = 0.51219966\n",
            "Iteration 56, loss = 0.50385341\n",
            "Iteration 57, loss = 0.48904227\n",
            "Iteration 58, loss = 0.50707833\n",
            "Iteration 59, loss = 0.49625456\n",
            "Iteration 60, loss = 0.50265988\n",
            "Iteration 61, loss = 0.49471800\n",
            "Iteration 62, loss = 0.50241447\n",
            "Iteration 63, loss = 0.51331188\n",
            "Iteration 64, loss = 0.49468232\n",
            "Iteration 65, loss = 0.51290153\n",
            "Iteration 66, loss = 0.50108623\n",
            "Iteration 67, loss = 0.50261989\n",
            "Iteration 68, loss = 0.48771730\n",
            "Iteration 69, loss = 0.50000559\n",
            "Iteration 70, loss = 0.49578483\n",
            "Iteration 71, loss = 0.49076435\n",
            "Iteration 72, loss = 0.49355639\n",
            "Iteration 73, loss = 0.50975777\n",
            "Iteration 74, loss = 0.50557673\n",
            "Iteration 75, loss = 0.50096409\n",
            "Iteration 76, loss = 0.49873248\n",
            "Iteration 77, loss = 0.49801436\n",
            "Iteration 78, loss = 0.50899106\n",
            "Iteration 79, loss = 0.49626666\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.30172543\n",
            "Iteration 2, loss = 0.67143284\n",
            "Iteration 3, loss = 0.67206704\n",
            "Iteration 4, loss = 0.63299457\n",
            "Iteration 5, loss = 0.63696118\n",
            "Iteration 6, loss = 0.62031961\n",
            "Iteration 7, loss = 0.61839997\n",
            "Iteration 8, loss = 0.61436751\n",
            "Iteration 9, loss = 0.61763547\n",
            "Iteration 10, loss = 0.63302944\n",
            "Iteration 11, loss = 0.58529143\n",
            "Iteration 12, loss = 0.58058442\n",
            "Iteration 13, loss = 0.59368138\n",
            "Iteration 14, loss = 0.57504149\n",
            "Iteration 15, loss = 0.58446789\n",
            "Iteration 16, loss = 0.57143510\n",
            "Iteration 17, loss = 0.57613698\n",
            "Iteration 18, loss = 0.56905863\n",
            "Iteration 19, loss = 0.56738668\n",
            "Iteration 20, loss = 0.57171653\n",
            "Iteration 21, loss = 0.55823365\n",
            "Iteration 22, loss = 0.56329236\n",
            "Iteration 23, loss = 0.57106616\n",
            "Iteration 24, loss = 0.55781196\n",
            "Iteration 25, loss = 0.55205317\n",
            "Iteration 26, loss = 0.54544488\n",
            "Iteration 27, loss = 0.54634311\n",
            "Iteration 28, loss = 0.54828772\n",
            "Iteration 29, loss = 0.54356871\n",
            "Iteration 30, loss = 0.53366277\n",
            "Iteration 31, loss = 0.53948409\n",
            "Iteration 32, loss = 0.53604080\n",
            "Iteration 33, loss = 0.55023642\n",
            "Iteration 34, loss = 0.53607900\n",
            "Iteration 35, loss = 0.54258543\n",
            "Iteration 36, loss = 0.53445223\n",
            "Iteration 37, loss = 0.54299399\n",
            "Iteration 38, loss = 0.53831160\n",
            "Iteration 39, loss = 0.53092062\n",
            "Iteration 40, loss = 0.52855287\n",
            "Iteration 41, loss = 0.53563527\n",
            "Iteration 42, loss = 0.52057671\n",
            "Iteration 43, loss = 0.51718453\n",
            "Iteration 44, loss = 0.52275751\n",
            "Iteration 45, loss = 0.52032083\n",
            "Iteration 46, loss = 0.51217749\n",
            "Iteration 47, loss = 0.51719907\n",
            "Iteration 48, loss = 0.51070788\n",
            "Iteration 49, loss = 0.51005586\n",
            "Iteration 50, loss = 0.53056788\n",
            "Iteration 51, loss = 0.51536444\n",
            "Iteration 52, loss = 0.50831356\n",
            "Iteration 53, loss = 0.50149468\n",
            "Iteration 54, loss = 0.50175380\n",
            "Iteration 55, loss = 0.48847001\n",
            "Iteration 56, loss = 0.52731523\n",
            "Iteration 57, loss = 0.51554885\n",
            "Iteration 58, loss = 0.51293801\n",
            "Iteration 59, loss = 0.50425933\n",
            "Iteration 60, loss = 0.51496628\n",
            "Iteration 61, loss = 0.49445512\n",
            "Iteration 62, loss = 0.49683282\n",
            "Iteration 63, loss = 0.49029236\n",
            "Iteration 64, loss = 0.49151134\n",
            "Iteration 65, loss = 0.51943175\n",
            "Iteration 66, loss = 0.48684476\n",
            "Iteration 67, loss = 0.49011678\n",
            "Iteration 68, loss = 0.49961852\n",
            "Iteration 69, loss = 0.49744515\n",
            "Iteration 70, loss = 0.48456378\n",
            "Iteration 71, loss = 0.49641221\n",
            "Iteration 72, loss = 0.49616855\n",
            "Iteration 73, loss = 0.49049354\n",
            "Iteration 74, loss = 0.50193795\n",
            "Iteration 75, loss = 0.50650657\n",
            "Iteration 76, loss = 0.47830542\n",
            "Iteration 77, loss = 0.48865285\n",
            "Iteration 78, loss = 0.49382850\n",
            "Iteration 79, loss = 0.48941126\n",
            "Iteration 80, loss = 0.47193596\n",
            "Iteration 81, loss = 0.50422061\n",
            "Iteration 82, loss = 0.48150688\n",
            "Iteration 83, loss = 0.48621815\n",
            "Iteration 84, loss = 0.50632258\n",
            "Iteration 85, loss = 0.49180338\n",
            "Iteration 86, loss = 0.48319763\n",
            "Iteration 87, loss = 0.50635297\n",
            "Iteration 88, loss = 0.49252229\n",
            "Iteration 89, loss = 0.47424478\n",
            "Iteration 90, loss = 0.46987836\n",
            "Iteration 91, loss = 0.48800749\n",
            "Iteration 92, loss = 0.50866647\n",
            "Iteration 93, loss = 0.48360717\n",
            "Iteration 94, loss = 0.49090580\n",
            "Iteration 95, loss = 0.47624207\n",
            "Iteration 96, loss = 0.48315654\n",
            "Iteration 97, loss = 0.47789398\n",
            "Iteration 98, loss = 0.47152404\n",
            "Iteration 99, loss = 0.47074347\n",
            "Iteration 100, loss = 0.47304680\n",
            "Iteration 101, loss = 0.48605351\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.95370580\n",
            "Iteration 2, loss = 0.71791690\n",
            "Iteration 3, loss = 0.64224373\n",
            "Iteration 4, loss = 0.66574539\n",
            "Iteration 5, loss = 0.65002523\n",
            "Iteration 6, loss = 0.61902169\n",
            "Iteration 7, loss = 0.61664721\n",
            "Iteration 8, loss = 0.61930314\n",
            "Iteration 9, loss = 0.60562647\n",
            "Iteration 10, loss = 0.61579832\n",
            "Iteration 11, loss = 0.61796792\n",
            "Iteration 12, loss = 0.61750175\n",
            "Iteration 13, loss = 0.60715442\n",
            "Iteration 14, loss = 0.59502682\n",
            "Iteration 15, loss = 0.59783298\n",
            "Iteration 16, loss = 0.59288326\n",
            "Iteration 17, loss = 0.59576348\n",
            "Iteration 18, loss = 0.58072133\n",
            "Iteration 19, loss = 0.59306289\n",
            "Iteration 20, loss = 0.59224467\n",
            "Iteration 21, loss = 0.57475377\n",
            "Iteration 22, loss = 0.57740977\n",
            "Iteration 23, loss = 0.58253805\n",
            "Iteration 24, loss = 0.57924832\n",
            "Iteration 25, loss = 0.57178291\n",
            "Iteration 26, loss = 0.57169445\n",
            "Iteration 27, loss = 0.57599248\n",
            "Iteration 28, loss = 0.56407963\n",
            "Iteration 29, loss = 0.56226863\n",
            "Iteration 30, loss = 0.57472369\n",
            "Iteration 31, loss = 0.56030281\n",
            "Iteration 32, loss = 0.55999471\n",
            "Iteration 33, loss = 0.55687381\n",
            "Iteration 34, loss = 0.55910667\n",
            "Iteration 35, loss = 0.55885263\n",
            "Iteration 36, loss = 0.56247805\n",
            "Iteration 37, loss = 0.54597336\n",
            "Iteration 38, loss = 0.54934351\n",
            "Iteration 39, loss = 0.56978948\n",
            "Iteration 40, loss = 0.56258622\n",
            "Iteration 41, loss = 0.55234081\n",
            "Iteration 42, loss = 0.55744490\n",
            "Iteration 43, loss = 0.55168688\n",
            "Iteration 44, loss = 0.54251150\n",
            "Iteration 45, loss = 0.53432832\n",
            "Iteration 46, loss = 0.54809994\n",
            "Iteration 47, loss = 0.52663027\n",
            "Iteration 48, loss = 0.53425529\n",
            "Iteration 49, loss = 0.53205239\n",
            "Iteration 50, loss = 0.54232497\n",
            "Iteration 51, loss = 0.53480159\n",
            "Iteration 52, loss = 0.52050011\n",
            "Iteration 53, loss = 0.52617076\n",
            "Iteration 54, loss = 0.51782193\n",
            "Iteration 55, loss = 0.55005451\n",
            "Iteration 56, loss = 0.51854377\n",
            "Iteration 57, loss = 0.51337570\n",
            "Iteration 58, loss = 0.51432557\n",
            "Iteration 59, loss = 0.51693774\n",
            "Iteration 60, loss = 0.51824440\n",
            "Iteration 61, loss = 0.51437425\n",
            "Iteration 62, loss = 0.52811397\n",
            "Iteration 63, loss = 0.52025708\n",
            "Iteration 64, loss = 0.51430587\n",
            "Iteration 65, loss = 0.51373407\n",
            "Iteration 66, loss = 0.51435770\n",
            "Iteration 67, loss = 0.51171549\n",
            "Iteration 68, loss = 0.52448261\n",
            "Iteration 69, loss = 0.51905332\n",
            "Iteration 70, loss = 0.50895665\n",
            "Iteration 71, loss = 0.51281639\n",
            "Iteration 72, loss = 0.51423274\n",
            "Iteration 73, loss = 0.51805516\n",
            "Iteration 74, loss = 0.49705595\n",
            "Iteration 75, loss = 0.51238030\n",
            "Iteration 76, loss = 0.49889490\n",
            "Iteration 77, loss = 0.50816751\n",
            "Iteration 78, loss = 0.51189820\n",
            "Iteration 79, loss = 0.49931073\n",
            "Iteration 80, loss = 0.50048294\n",
            "Iteration 81, loss = 0.49491464\n",
            "Iteration 82, loss = 0.51404574\n",
            "Iteration 83, loss = 0.49868643\n",
            "Iteration 84, loss = 0.48695900\n",
            "Iteration 85, loss = 0.49337376\n",
            "Iteration 86, loss = 0.50773180\n",
            "Iteration 87, loss = 0.49951832\n",
            "Iteration 88, loss = 0.49991318\n",
            "Iteration 89, loss = 0.48793491\n",
            "Iteration 90, loss = 0.49349197\n",
            "Iteration 91, loss = 0.48494263\n",
            "Iteration 92, loss = 0.51105321\n",
            "Iteration 93, loss = 0.50189871\n",
            "Iteration 94, loss = 0.49239283\n",
            "Iteration 95, loss = 0.48001591\n",
            "Iteration 96, loss = 0.49716647\n",
            "Iteration 97, loss = 0.51020862\n",
            "Iteration 98, loss = 0.48657792\n",
            "Iteration 99, loss = 0.49921853\n",
            "Iteration 100, loss = 0.49206361\n",
            "Iteration 101, loss = 0.48218743\n",
            "Iteration 102, loss = 0.48775001\n",
            "Iteration 103, loss = 0.50120632\n",
            "Iteration 104, loss = 0.51150578\n",
            "Iteration 105, loss = 0.51536111\n",
            "Iteration 106, loss = 0.48811924\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.89365505\n",
            "Iteration 2, loss = 0.70164860\n",
            "Iteration 3, loss = 0.65463597\n",
            "Iteration 4, loss = 0.63937689\n",
            "Iteration 5, loss = 0.61894763\n",
            "Iteration 6, loss = 0.61103116\n",
            "Iteration 7, loss = 0.60841604\n",
            "Iteration 8, loss = 0.59800926\n",
            "Iteration 9, loss = 0.60061207\n",
            "Iteration 10, loss = 0.59963287\n",
            "Iteration 11, loss = 0.60007286\n",
            "Iteration 12, loss = 0.60313441\n",
            "Iteration 13, loss = 0.59643136\n",
            "Iteration 14, loss = 0.59192995\n",
            "Iteration 15, loss = 0.59281548\n",
            "Iteration 16, loss = 0.59153756\n",
            "Iteration 17, loss = 0.58912856\n",
            "Iteration 18, loss = 0.59018882\n",
            "Iteration 19, loss = 0.59167941\n",
            "Iteration 20, loss = 0.58780416\n",
            "Iteration 21, loss = 0.58382257\n",
            "Iteration 22, loss = 0.58954155\n",
            "Iteration 23, loss = 0.58961772\n",
            "Iteration 24, loss = 0.57413062\n",
            "Iteration 25, loss = 0.57415063\n",
            "Iteration 26, loss = 0.57461626\n",
            "Iteration 27, loss = 0.59627407\n",
            "Iteration 28, loss = 0.57811616\n",
            "Iteration 29, loss = 0.57231566\n",
            "Iteration 30, loss = 0.56660657\n",
            "Iteration 31, loss = 0.56839158\n",
            "Iteration 32, loss = 0.57941220\n",
            "Iteration 33, loss = 0.56748898\n",
            "Iteration 34, loss = 0.56123016\n",
            "Iteration 35, loss = 0.57376896\n",
            "Iteration 36, loss = 0.57017624\n",
            "Iteration 37, loss = 0.55522013\n",
            "Iteration 38, loss = 0.56144006\n",
            "Iteration 39, loss = 0.56213961\n",
            "Iteration 40, loss = 0.54685906\n",
            "Iteration 41, loss = 0.55133379\n",
            "Iteration 42, loss = 0.55325504\n",
            "Iteration 43, loss = 0.55524632\n",
            "Iteration 44, loss = 0.55905573\n",
            "Iteration 45, loss = 0.55463518\n",
            "Iteration 46, loss = 0.55899656\n",
            "Iteration 47, loss = 0.55866673\n",
            "Iteration 48, loss = 0.53930844\n",
            "Iteration 49, loss = 0.53718428\n",
            "Iteration 50, loss = 0.54340642\n",
            "Iteration 51, loss = 0.54084714\n",
            "Iteration 52, loss = 0.54497423\n",
            "Iteration 53, loss = 0.53683016\n",
            "Iteration 54, loss = 0.52747172\n",
            "Iteration 55, loss = 0.53114814\n",
            "Iteration 56, loss = 0.52404846\n",
            "Iteration 57, loss = 0.52995899\n",
            "Iteration 58, loss = 0.52444383\n",
            "Iteration 59, loss = 0.55946951\n",
            "Iteration 60, loss = 0.55740597\n",
            "Iteration 61, loss = 0.58743391\n",
            "Iteration 62, loss = 0.53283326\n",
            "Iteration 63, loss = 0.53282769\n",
            "Iteration 64, loss = 0.52568882\n",
            "Iteration 65, loss = 0.52297319\n",
            "Iteration 66, loss = 0.51974363\n",
            "Iteration 67, loss = 0.52017709\n",
            "Iteration 68, loss = 0.52300836\n",
            "Iteration 69, loss = 0.51209682\n",
            "Iteration 70, loss = 0.52427684\n",
            "Iteration 71, loss = 0.51791896\n",
            "Iteration 72, loss = 0.51823905\n",
            "Iteration 73, loss = 0.51011252\n",
            "Iteration 74, loss = 0.53799187\n",
            "Iteration 75, loss = 0.50626399\n",
            "Iteration 76, loss = 0.51330586\n",
            "Iteration 77, loss = 0.51904364\n",
            "Iteration 78, loss = 0.51177912\n",
            "Iteration 79, loss = 0.51686909\n",
            "Iteration 80, loss = 0.50990213\n",
            "Iteration 81, loss = 0.48799913\n",
            "Iteration 82, loss = 0.50610384\n",
            "Iteration 83, loss = 0.49869802\n",
            "Iteration 84, loss = 0.49218934\n",
            "Iteration 85, loss = 0.49752131\n",
            "Iteration 86, loss = 0.49930454\n",
            "Iteration 87, loss = 0.50556788\n",
            "Iteration 88, loss = 0.50530791\n",
            "Iteration 89, loss = 0.50635018\n",
            "Iteration 90, loss = 0.49964045\n",
            "Iteration 91, loss = 0.49946913\n",
            "Iteration 92, loss = 0.50535263\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.73029404\n",
            "Iteration 2, loss = 0.78559573\n",
            "Iteration 3, loss = 0.63656252\n",
            "Iteration 4, loss = 0.61729172\n",
            "Iteration 5, loss = 0.61151760\n",
            "Iteration 6, loss = 0.62014031\n",
            "Iteration 7, loss = 0.62549068\n",
            "Iteration 8, loss = 0.61244237\n",
            "Iteration 9, loss = 0.59953129\n",
            "Iteration 10, loss = 0.60154849\n",
            "Iteration 11, loss = 0.59995024\n",
            "Iteration 12, loss = 0.61253851\n",
            "Iteration 13, loss = 0.59560233\n",
            "Iteration 14, loss = 0.59777758\n",
            "Iteration 15, loss = 0.59918554\n",
            "Iteration 16, loss = 0.58666031\n",
            "Iteration 17, loss = 0.59109156\n",
            "Iteration 18, loss = 0.59054573\n",
            "Iteration 19, loss = 0.58302244\n",
            "Iteration 20, loss = 0.58216500\n",
            "Iteration 21, loss = 0.59897249\n",
            "Iteration 22, loss = 0.58079813\n",
            "Iteration 23, loss = 0.57868437\n",
            "Iteration 24, loss = 0.57494821\n",
            "Iteration 25, loss = 0.57914162\n",
            "Iteration 26, loss = 0.57021447\n",
            "Iteration 27, loss = 0.57216267\n",
            "Iteration 28, loss = 0.58878775\n",
            "Iteration 29, loss = 0.56449441\n",
            "Iteration 30, loss = 0.55940000\n",
            "Iteration 31, loss = 0.56035203\n",
            "Iteration 32, loss = 0.57059680\n",
            "Iteration 33, loss = 0.55987266\n",
            "Iteration 34, loss = 0.56975102\n",
            "Iteration 35, loss = 0.56367322\n",
            "Iteration 36, loss = 0.55203210\n",
            "Iteration 37, loss = 0.56895808\n",
            "Iteration 38, loss = 0.55700634\n",
            "Iteration 39, loss = 0.55071819\n",
            "Iteration 40, loss = 0.54569553\n",
            "Iteration 41, loss = 0.55740459\n",
            "Iteration 42, loss = 0.54326930\n",
            "Iteration 43, loss = 0.54003187\n",
            "Iteration 44, loss = 0.53876730\n",
            "Iteration 45, loss = 0.54635459\n",
            "Iteration 46, loss = 0.53303802\n",
            "Iteration 47, loss = 0.52593896\n",
            "Iteration 48, loss = 0.52482629\n",
            "Iteration 49, loss = 0.54665668\n",
            "Iteration 50, loss = 0.53406595\n",
            "Iteration 51, loss = 0.53812557\n",
            "Iteration 52, loss = 0.53593899\n",
            "Iteration 53, loss = 0.54243134\n",
            "Iteration 54, loss = 0.53503238\n",
            "Iteration 55, loss = 0.55430259\n",
            "Iteration 56, loss = 0.55155152\n",
            "Iteration 57, loss = 0.54053061\n",
            "Iteration 58, loss = 0.53768245\n",
            "Iteration 59, loss = 0.53206913\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.00470146\n",
            "Iteration 2, loss = 0.71750064\n",
            "Iteration 3, loss = 0.69157420\n",
            "Iteration 4, loss = 0.68170537\n",
            "Iteration 5, loss = 0.73545058\n",
            "Iteration 6, loss = 0.67516483\n",
            "Iteration 7, loss = 0.65275815\n",
            "Iteration 8, loss = 0.60991901\n",
            "Iteration 9, loss = 0.59022490\n",
            "Iteration 10, loss = 0.55345534\n",
            "Iteration 11, loss = 0.54114064\n",
            "Iteration 12, loss = 0.90755468\n",
            "Iteration 13, loss = 0.75978078\n",
            "Iteration 14, loss = 0.62660917\n",
            "Iteration 15, loss = 0.57306761\n",
            "Iteration 16, loss = 0.51527284\n",
            "Iteration 17, loss = 0.55105274\n",
            "Iteration 18, loss = 0.55926981\n",
            "Iteration 19, loss = 0.53234169\n",
            "Iteration 20, loss = 0.51765688\n",
            "Iteration 21, loss = 0.51793851\n",
            "Iteration 22, loss = 0.48444002\n",
            "Iteration 23, loss = 0.49213494\n",
            "Iteration 24, loss = 0.47504959\n",
            "Iteration 25, loss = 0.48774575\n",
            "Iteration 26, loss = 0.49885054\n",
            "Iteration 27, loss = 0.50955210\n",
            "Iteration 28, loss = 0.52907702\n",
            "Iteration 29, loss = 0.64142291\n",
            "Iteration 30, loss = 0.57913482\n",
            "Iteration 31, loss = 0.53623432\n",
            "Iteration 32, loss = 0.50220915\n",
            "Iteration 33, loss = 0.55034022\n",
            "Iteration 34, loss = 0.56018384\n",
            "Iteration 35, loss = 0.62463450\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.08016929\n",
            "Iteration 2, loss = 0.66173778\n",
            "Iteration 3, loss = 0.63636982\n",
            "Iteration 4, loss = 0.77055014\n",
            "Iteration 5, loss = 0.60466247\n",
            "Iteration 6, loss = 0.60287930\n",
            "Iteration 7, loss = 0.62505921\n",
            "Iteration 8, loss = 0.57114075\n",
            "Iteration 9, loss = 0.57148266\n",
            "Iteration 10, loss = 0.73429892\n",
            "Iteration 11, loss = 0.60063269\n",
            "Iteration 12, loss = 0.62342568\n",
            "Iteration 13, loss = 0.66113637\n",
            "Iteration 14, loss = 0.54887510\n",
            "Iteration 15, loss = 0.55462214\n",
            "Iteration 16, loss = 0.52959181\n",
            "Iteration 17, loss = 0.54143268\n",
            "Iteration 18, loss = 0.51229483\n",
            "Iteration 19, loss = 0.55938697\n",
            "Iteration 20, loss = 0.58208292\n",
            "Iteration 21, loss = 0.49630371\n",
            "Iteration 22, loss = 0.49947804\n",
            "Iteration 23, loss = 0.67423849\n",
            "Iteration 24, loss = 0.91104995\n",
            "Iteration 25, loss = 0.54246617\n",
            "Iteration 26, loss = 0.49032804\n",
            "Iteration 27, loss = 0.47486674\n",
            "Iteration 28, loss = 0.48969089\n",
            "Iteration 29, loss = 0.50216649\n",
            "Iteration 30, loss = 0.45227639\n",
            "Iteration 31, loss = 0.47605331\n",
            "Iteration 32, loss = 0.49415305\n",
            "Iteration 33, loss = 0.47363164\n",
            "Iteration 34, loss = 0.48319249\n",
            "Iteration 35, loss = 0.46583834\n",
            "Iteration 36, loss = 0.48282688\n",
            "Iteration 37, loss = 0.59070073\n",
            "Iteration 38, loss = 0.50114378\n",
            "Iteration 39, loss = 0.50490628\n",
            "Iteration 40, loss = 0.45540225\n",
            "Iteration 41, loss = 0.48678361\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.88987006\n",
            "Iteration 2, loss = 0.66013028\n",
            "Iteration 3, loss = 0.63690743\n",
            "Iteration 4, loss = 0.60823036\n",
            "Iteration 5, loss = 0.60766134\n",
            "Iteration 6, loss = 0.60453932\n",
            "Iteration 7, loss = 0.55825743\n",
            "Iteration 8, loss = 0.55706276\n",
            "Iteration 9, loss = 0.56568494\n",
            "Iteration 10, loss = 0.61243169\n",
            "Iteration 11, loss = 0.65822030\n",
            "Iteration 12, loss = 0.67735232\n",
            "Iteration 13, loss = 0.53997910\n",
            "Iteration 14, loss = 0.54963861\n",
            "Iteration 15, loss = 0.51525286\n",
            "Iteration 16, loss = 0.51048336\n",
            "Iteration 17, loss = 0.54884860\n",
            "Iteration 18, loss = 0.65610573\n",
            "Iteration 19, loss = 0.66970767\n",
            "Iteration 20, loss = 0.68016279\n",
            "Iteration 21, loss = 0.53327954\n",
            "Iteration 22, loss = 0.49790857\n",
            "Iteration 23, loss = 0.47736498\n",
            "Iteration 24, loss = 0.48470015\n",
            "Iteration 25, loss = 0.48556851\n",
            "Iteration 26, loss = 0.48093161\n",
            "Iteration 27, loss = 0.47965087\n",
            "Iteration 28, loss = 0.47029387\n",
            "Iteration 29, loss = 0.51800177\n",
            "Iteration 30, loss = 0.52202313\n",
            "Iteration 31, loss = 0.51921073\n",
            "Iteration 32, loss = 0.56132215\n",
            "Iteration 33, loss = 0.47893410\n",
            "Iteration 34, loss = 0.45206401\n",
            "Iteration 35, loss = 0.46620632\n",
            "Iteration 36, loss = 0.50823261\n",
            "Iteration 37, loss = 0.56481301\n",
            "Iteration 38, loss = 0.51168207\n",
            "Iteration 39, loss = 0.45446670\n",
            "Iteration 40, loss = 0.52638810\n",
            "Iteration 41, loss = 0.51431491\n",
            "Iteration 42, loss = 0.50494359\n",
            "Iteration 43, loss = 0.49373275\n",
            "Iteration 44, loss = 0.58537482\n",
            "Iteration 45, loss = 0.62210511\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.86988776\n",
            "Iteration 2, loss = 0.61541784\n",
            "Iteration 3, loss = 0.76225621\n",
            "Iteration 4, loss = 0.72642010\n",
            "Iteration 5, loss = 0.62338080\n",
            "Iteration 6, loss = 0.63508919\n",
            "Iteration 7, loss = 0.59141107\n",
            "Iteration 8, loss = 0.59944912\n",
            "Iteration 9, loss = 0.58185153\n",
            "Iteration 10, loss = 0.68352632\n",
            "Iteration 11, loss = 0.56726697\n",
            "Iteration 12, loss = 0.58474477\n",
            "Iteration 13, loss = 0.58669335\n",
            "Iteration 14, loss = 0.56572686\n",
            "Iteration 15, loss = 0.54791383\n",
            "Iteration 16, loss = 0.53905781\n",
            "Iteration 17, loss = 0.53757697\n",
            "Iteration 18, loss = 0.56306852\n",
            "Iteration 19, loss = 0.52054921\n",
            "Iteration 20, loss = 0.50609226\n",
            "Iteration 21, loss = 0.49273269\n",
            "Iteration 22, loss = 0.54766522\n",
            "Iteration 23, loss = 0.56684935\n",
            "Iteration 24, loss = 0.49192691\n",
            "Iteration 25, loss = 0.49011663\n",
            "Iteration 26, loss = 0.63594205\n",
            "Iteration 27, loss = 0.57816530\n",
            "Iteration 28, loss = 0.57500636\n",
            "Iteration 29, loss = 0.54431418\n",
            "Iteration 30, loss = 0.50211974\n",
            "Iteration 31, loss = 0.46893761\n",
            "Iteration 32, loss = 0.47765031\n",
            "Iteration 33, loss = 0.47726861\n",
            "Iteration 34, loss = 0.62763933\n",
            "Iteration 35, loss = 0.57161039\n",
            "Iteration 36, loss = 0.56623208\n",
            "Iteration 37, loss = 0.52914048\n",
            "Iteration 38, loss = 0.47477502\n",
            "Iteration 39, loss = 0.50264479\n",
            "Iteration 40, loss = 0.47891608\n",
            "Iteration 41, loss = 0.44542048\n",
            "Iteration 42, loss = 0.45578634\n",
            "Iteration 43, loss = 0.44147276\n",
            "Iteration 44, loss = 0.46195958\n",
            "Iteration 45, loss = 0.44501973\n",
            "Iteration 46, loss = 0.45122990\n",
            "Iteration 47, loss = 0.45721390\n",
            "Iteration 48, loss = 0.43942298\n",
            "Iteration 49, loss = 0.44270276\n",
            "Iteration 50, loss = 0.44508755\n",
            "Iteration 51, loss = 0.46016914\n",
            "Iteration 52, loss = 0.46897122\n",
            "Iteration 53, loss = 0.43595659\n",
            "Iteration 54, loss = 0.43743717\n",
            "Iteration 55, loss = 0.43906722\n",
            "Iteration 56, loss = 0.43261680\n",
            "Iteration 57, loss = 0.43783450\n",
            "Iteration 58, loss = 0.52921958\n",
            "Iteration 59, loss = 0.51052793\n",
            "Iteration 60, loss = 0.44756231\n",
            "Iteration 61, loss = 0.43558688\n",
            "Iteration 62, loss = 0.42773361\n",
            "Iteration 63, loss = 0.43505039\n",
            "Iteration 64, loss = 0.46219956\n",
            "Iteration 65, loss = 0.45966150\n",
            "Iteration 66, loss = 0.41926214\n",
            "Iteration 67, loss = 0.43307558\n",
            "Iteration 68, loss = 0.43268190\n",
            "Iteration 69, loss = 0.43184640\n",
            "Iteration 70, loss = 0.43037332\n",
            "Iteration 71, loss = 0.42124719\n",
            "Iteration 72, loss = 0.44208751\n",
            "Iteration 73, loss = 0.41736843\n",
            "Iteration 74, loss = 0.41816566\n",
            "Iteration 75, loss = 0.42680415\n",
            "Iteration 76, loss = 0.45260057\n",
            "Iteration 77, loss = 0.43859986\n",
            "Iteration 78, loss = 0.41384949\n",
            "Iteration 79, loss = 0.42349712\n",
            "Iteration 80, loss = 0.43028051\n",
            "Iteration 81, loss = 0.42155081\n",
            "Iteration 82, loss = 0.47393190\n",
            "Iteration 83, loss = 0.47817160\n",
            "Iteration 84, loss = 0.54148039\n",
            "Iteration 85, loss = 0.45734849\n",
            "Iteration 86, loss = 0.42063664\n",
            "Iteration 87, loss = 0.41263404\n",
            "Iteration 88, loss = 0.40838882\n",
            "Iteration 89, loss = 0.41971466\n",
            "Iteration 90, loss = 0.43627910\n",
            "Iteration 91, loss = 0.42292642\n",
            "Iteration 92, loss = 0.42636248\n",
            "Iteration 93, loss = 0.40900144\n",
            "Iteration 94, loss = 0.41072961\n",
            "Iteration 95, loss = 0.40098551\n",
            "Iteration 96, loss = 0.41600895\n",
            "Iteration 97, loss = 0.42011671\n",
            "Iteration 98, loss = 0.42400063\n",
            "Iteration 99, loss = 0.46477982\n",
            "Iteration 100, loss = 0.46565128\n",
            "Iteration 101, loss = 0.44385676\n",
            "Iteration 102, loss = 0.40458044\n",
            "Iteration 103, loss = 0.40028766\n",
            "Iteration 104, loss = 0.39817336\n",
            "Iteration 105, loss = 0.39333398\n",
            "Iteration 106, loss = 0.42463303\n",
            "Iteration 107, loss = 0.39965718\n",
            "Iteration 108, loss = 0.41916958\n",
            "Iteration 109, loss = 0.41665778\n",
            "Iteration 110, loss = 0.40339486\n",
            "Iteration 111, loss = 0.42518853\n",
            "Iteration 112, loss = 0.40406804\n",
            "Iteration 113, loss = 0.39859634\n",
            "Iteration 114, loss = 0.39770960\n",
            "Iteration 115, loss = 0.41336310\n",
            "Iteration 116, loss = 0.40095127\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.98888238\n",
            "Iteration 2, loss = 0.76955270\n",
            "Iteration 3, loss = 0.72751283\n",
            "Iteration 4, loss = 0.76954151\n",
            "Iteration 5, loss = 0.81336336\n",
            "Iteration 6, loss = 0.65398357\n",
            "Iteration 7, loss = 0.58357319\n",
            "Iteration 8, loss = 0.59800163\n",
            "Iteration 9, loss = 0.55171894\n",
            "Iteration 10, loss = 0.58331410\n",
            "Iteration 11, loss = 0.68899075\n",
            "Iteration 12, loss = 0.61146557\n",
            "Iteration 13, loss = 0.64794782\n",
            "Iteration 14, loss = 0.59534714\n",
            "Iteration 15, loss = 0.51970953\n",
            "Iteration 16, loss = 0.54670820\n",
            "Iteration 17, loss = 0.59072676\n",
            "Iteration 18, loss = 0.57203611\n",
            "Iteration 19, loss = 0.56501938\n",
            "Iteration 20, loss = 0.55742286\n",
            "Iteration 21, loss = 0.51722760\n",
            "Iteration 22, loss = 0.53912585\n",
            "Iteration 23, loss = 0.50299224\n",
            "Iteration 24, loss = 0.52903238\n",
            "Iteration 25, loss = 0.59854023\n",
            "Iteration 26, loss = 0.51204275\n",
            "Iteration 27, loss = 0.56669966\n",
            "Iteration 28, loss = 0.53470537\n",
            "Iteration 29, loss = 0.47399659\n",
            "Iteration 30, loss = 0.48346822\n",
            "Iteration 31, loss = 0.50330593\n",
            "Iteration 32, loss = 0.46470894\n",
            "Iteration 33, loss = 0.48487304\n",
            "Iteration 34, loss = 0.51399184\n",
            "Iteration 35, loss = 0.56317882\n",
            "Iteration 36, loss = 0.52312577\n",
            "Iteration 37, loss = 0.72611961\n",
            "Iteration 38, loss = 0.57663272\n",
            "Iteration 39, loss = 0.46683354\n",
            "Iteration 40, loss = 0.46635785\n",
            "Iteration 41, loss = 0.45225858\n",
            "Iteration 42, loss = 0.45837370\n",
            "Iteration 43, loss = 0.45970486\n",
            "Iteration 44, loss = 0.46584605\n",
            "Iteration 45, loss = 0.44357263\n",
            "Iteration 46, loss = 0.45273232\n",
            "Iteration 47, loss = 0.45984438\n",
            "Iteration 48, loss = 0.49200515\n",
            "Iteration 49, loss = 0.48140211\n",
            "Iteration 50, loss = 0.45987533\n",
            "Iteration 51, loss = 0.46125262\n",
            "Iteration 52, loss = 0.44594601\n",
            "Iteration 53, loss = 0.44244688\n",
            "Iteration 54, loss = 0.43624421\n",
            "Iteration 55, loss = 0.43438086\n",
            "Iteration 56, loss = 0.45449249\n",
            "Iteration 57, loss = 0.44442076\n",
            "Iteration 58, loss = 0.45700692\n",
            "Iteration 59, loss = 0.45684587\n",
            "Iteration 60, loss = 0.50097562\n",
            "Iteration 61, loss = 0.44440373\n",
            "Iteration 62, loss = 0.43685415\n",
            "Iteration 63, loss = 0.45583577\n",
            "Iteration 64, loss = 0.53555189\n",
            "Iteration 65, loss = 0.44695188\n",
            "Iteration 66, loss = 0.43237251\n",
            "Iteration 67, loss = 0.45508141\n",
            "Iteration 68, loss = 0.43712696\n",
            "Iteration 69, loss = 0.43107819\n",
            "Iteration 70, loss = 0.43509563\n",
            "Iteration 71, loss = 0.46484910\n",
            "Iteration 72, loss = 0.48910438\n",
            "Iteration 73, loss = 0.54275773\n",
            "Iteration 74, loss = 0.47825858\n",
            "Iteration 75, loss = 0.53704009\n",
            "Iteration 76, loss = 0.45910550\n",
            "Iteration 77, loss = 0.45941411\n",
            "Iteration 78, loss = 0.45323569\n",
            "Iteration 79, loss = 0.56502387\n",
            "Iteration 80, loss = 0.51861946\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.57541900\n",
            "Iteration 2, loss = 0.97566835\n",
            "Iteration 3, loss = 0.77904522\n",
            "Iteration 4, loss = 0.68381846\n",
            "Iteration 5, loss = 0.64799050\n",
            "Iteration 6, loss = 0.62707366\n",
            "Iteration 7, loss = 0.62111893\n",
            "Iteration 8, loss = 0.61294590\n",
            "Iteration 9, loss = 0.60398837\n",
            "Iteration 10, loss = 0.60171863\n",
            "Iteration 11, loss = 0.60357284\n",
            "Iteration 12, loss = 0.59807334\n",
            "Iteration 13, loss = 0.59137526\n",
            "Iteration 14, loss = 0.58292690\n",
            "Iteration 15, loss = 0.58098916\n",
            "Iteration 16, loss = 0.57799819\n",
            "Iteration 17, loss = 0.57568265\n",
            "Iteration 18, loss = 0.57211149\n",
            "Iteration 19, loss = 0.56729318\n",
            "Iteration 20, loss = 0.56636001\n",
            "Iteration 21, loss = 0.55914506\n",
            "Iteration 22, loss = 0.55670037\n",
            "Iteration 23, loss = 0.56294911\n",
            "Iteration 24, loss = 0.56447164\n",
            "Iteration 25, loss = 0.57653873\n",
            "Iteration 26, loss = 0.55783727\n",
            "Iteration 27, loss = 0.55941495\n",
            "Iteration 28, loss = 0.54150035\n",
            "Iteration 29, loss = 0.53034764\n",
            "Iteration 30, loss = 0.53800345\n",
            "Iteration 31, loss = 0.53785621\n",
            "Iteration 32, loss = 0.52176116\n",
            "Iteration 33, loss = 0.52472652\n",
            "Iteration 34, loss = 0.51589214\n",
            "Iteration 35, loss = 0.55868060\n",
            "Iteration 36, loss = 0.51359193\n",
            "Iteration 37, loss = 0.51149076\n",
            "Iteration 38, loss = 0.52221406\n",
            "Iteration 39, loss = 0.51153433\n",
            "Iteration 40, loss = 0.50655978\n",
            "Iteration 41, loss = 0.50449062\n",
            "Iteration 42, loss = 0.54254499\n",
            "Iteration 43, loss = 0.50454589\n",
            "Iteration 44, loss = 0.48694127\n",
            "Iteration 45, loss = 0.55872433\n",
            "Iteration 46, loss = 0.53067012\n",
            "Iteration 47, loss = 0.48703398\n",
            "Iteration 48, loss = 0.48358358\n",
            "Iteration 49, loss = 0.50052158\n",
            "Iteration 50, loss = 0.49363730\n",
            "Iteration 51, loss = 0.47351693\n",
            "Iteration 52, loss = 0.47824457\n",
            "Iteration 53, loss = 0.46604339\n",
            "Iteration 54, loss = 0.46414797\n",
            "Iteration 55, loss = 0.48951687\n",
            "Iteration 56, loss = 0.46717161\n",
            "Iteration 57, loss = 0.47695986\n",
            "Iteration 58, loss = 0.47887254\n",
            "Iteration 59, loss = 0.48133336\n",
            "Iteration 60, loss = 0.46458530\n",
            "Iteration 61, loss = 0.46375778\n",
            "Iteration 62, loss = 0.45216779\n",
            "Iteration 63, loss = 0.45570565\n",
            "Iteration 64, loss = 0.45272523\n",
            "Iteration 65, loss = 0.46049332\n",
            "Iteration 66, loss = 0.45962462\n",
            "Iteration 67, loss = 0.45097543\n",
            "Iteration 68, loss = 0.45524742\n",
            "Iteration 69, loss = 0.44538665\n",
            "Iteration 70, loss = 0.44467671\n",
            "Iteration 71, loss = 0.45239206\n",
            "Iteration 72, loss = 0.43678904\n",
            "Iteration 73, loss = 0.46124842\n",
            "Iteration 74, loss = 0.48358127\n",
            "Iteration 75, loss = 0.48433628\n",
            "Iteration 76, loss = 0.48994506\n",
            "Iteration 77, loss = 0.48660237\n",
            "Iteration 78, loss = 0.46681990\n",
            "Iteration 79, loss = 0.44751085\n",
            "Iteration 80, loss = 0.43929493\n",
            "Iteration 81, loss = 0.43647587\n",
            "Iteration 82, loss = 0.44106785\n",
            "Iteration 83, loss = 0.44268352\n",
            "Iteration 84, loss = 0.45763734\n",
            "Iteration 85, loss = 0.45632779\n",
            "Iteration 86, loss = 0.44961639\n",
            "Iteration 87, loss = 0.44415085\n",
            "Iteration 88, loss = 0.43853283\n",
            "Iteration 89, loss = 0.42995486\n",
            "Iteration 90, loss = 0.43254080\n",
            "Iteration 91, loss = 0.43261331\n",
            "Iteration 92, loss = 0.43165971\n",
            "Iteration 93, loss = 0.43738016\n",
            "Iteration 94, loss = 0.43055267\n",
            "Iteration 95, loss = 0.43199156\n",
            "Iteration 96, loss = 0.43210834\n",
            "Iteration 97, loss = 0.43200027\n",
            "Iteration 98, loss = 0.44212627\n",
            "Iteration 99, loss = 0.43237661\n",
            "Iteration 100, loss = 0.43384366\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.14146113\n",
            "Iteration 2, loss = 0.73932850\n",
            "Iteration 3, loss = 0.64053072\n",
            "Iteration 4, loss = 0.64579760\n",
            "Iteration 5, loss = 0.64495617\n",
            "Iteration 6, loss = 0.59149829\n",
            "Iteration 7, loss = 0.59531030\n",
            "Iteration 8, loss = 0.61504068\n",
            "Iteration 9, loss = 0.63457912\n",
            "Iteration 10, loss = 0.59134621\n",
            "Iteration 11, loss = 0.58317002\n",
            "Iteration 12, loss = 0.56689632\n",
            "Iteration 13, loss = 0.56262985\n",
            "Iteration 14, loss = 0.57849204\n",
            "Iteration 15, loss = 0.56784333\n",
            "Iteration 16, loss = 0.56648671\n",
            "Iteration 17, loss = 0.56465683\n",
            "Iteration 18, loss = 0.56507935\n",
            "Iteration 19, loss = 0.53979691\n",
            "Iteration 20, loss = 0.54775482\n",
            "Iteration 21, loss = 0.54685524\n",
            "Iteration 22, loss = 0.55015722\n",
            "Iteration 23, loss = 0.56261936\n",
            "Iteration 24, loss = 0.55545656\n",
            "Iteration 25, loss = 0.53726584\n",
            "Iteration 26, loss = 0.53796550\n",
            "Iteration 27, loss = 0.55608845\n",
            "Iteration 28, loss = 0.53441031\n",
            "Iteration 29, loss = 0.56057484\n",
            "Iteration 30, loss = 0.52706669\n",
            "Iteration 31, loss = 0.51216992\n",
            "Iteration 32, loss = 0.51138079\n",
            "Iteration 33, loss = 0.51032637\n",
            "Iteration 34, loss = 0.49802992\n",
            "Iteration 35, loss = 0.49216722\n",
            "Iteration 36, loss = 0.49211311\n",
            "Iteration 37, loss = 0.49790652\n",
            "Iteration 38, loss = 0.48739951\n",
            "Iteration 39, loss = 0.54632892\n",
            "Iteration 40, loss = 0.54331041\n",
            "Iteration 41, loss = 0.53132155\n",
            "Iteration 42, loss = 0.53193438\n",
            "Iteration 43, loss = 0.49284372\n",
            "Iteration 44, loss = 0.48514685\n",
            "Iteration 45, loss = 0.46746806\n",
            "Iteration 46, loss = 0.47123850\n",
            "Iteration 47, loss = 0.47841837\n",
            "Iteration 48, loss = 0.47332817\n",
            "Iteration 49, loss = 0.46221639\n",
            "Iteration 50, loss = 0.47543873\n",
            "Iteration 51, loss = 0.45873988\n",
            "Iteration 52, loss = 0.45939008\n",
            "Iteration 53, loss = 0.46649813\n",
            "Iteration 54, loss = 0.49835939\n",
            "Iteration 55, loss = 0.54376809\n",
            "Iteration 56, loss = 0.52097811\n",
            "Iteration 57, loss = 0.47681154\n",
            "Iteration 58, loss = 0.50456749\n",
            "Iteration 59, loss = 0.48507184\n",
            "Iteration 60, loss = 0.46255323\n",
            "Iteration 61, loss = 0.48928123\n",
            "Iteration 62, loss = 0.51475385\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.62440059\n",
            "Iteration 2, loss = 0.90916097\n",
            "Iteration 3, loss = 0.82728977\n",
            "Iteration 4, loss = 0.68264525\n",
            "Iteration 5, loss = 0.63502952\n",
            "Iteration 6, loss = 0.62123470\n",
            "Iteration 7, loss = 0.63522754\n",
            "Iteration 8, loss = 0.60628919\n",
            "Iteration 9, loss = 0.59928881\n",
            "Iteration 10, loss = 0.65150039\n",
            "Iteration 11, loss = 0.62656009\n",
            "Iteration 12, loss = 0.60764641\n",
            "Iteration 13, loss = 0.59687729\n",
            "Iteration 14, loss = 0.57902807\n",
            "Iteration 15, loss = 0.58397486\n",
            "Iteration 16, loss = 0.56895805\n",
            "Iteration 17, loss = 0.56893185\n",
            "Iteration 18, loss = 0.57549065\n",
            "Iteration 19, loss = 0.56117058\n",
            "Iteration 20, loss = 0.55290822\n",
            "Iteration 21, loss = 0.55800619\n",
            "Iteration 22, loss = 0.56374071\n",
            "Iteration 23, loss = 0.59976217\n",
            "Iteration 24, loss = 0.57315574\n",
            "Iteration 25, loss = 0.56955641\n",
            "Iteration 26, loss = 0.58685531\n",
            "Iteration 27, loss = 0.55735911\n",
            "Iteration 28, loss = 0.54982264\n",
            "Iteration 29, loss = 0.57223186\n",
            "Iteration 30, loss = 0.54818209\n",
            "Iteration 31, loss = 0.56530286\n",
            "Iteration 32, loss = 0.54457537\n",
            "Iteration 33, loss = 0.54504338\n",
            "Iteration 34, loss = 0.54513088\n",
            "Iteration 35, loss = 0.54266530\n",
            "Iteration 36, loss = 0.54602845\n",
            "Iteration 37, loss = 0.51214077\n",
            "Iteration 38, loss = 0.50834524\n",
            "Iteration 39, loss = 0.50715832\n",
            "Iteration 40, loss = 0.50600595\n",
            "Iteration 41, loss = 0.51065334\n",
            "Iteration 42, loss = 0.49816521\n",
            "Iteration 43, loss = 0.50390089\n",
            "Iteration 44, loss = 0.60398712\n",
            "Iteration 45, loss = 0.50000723\n",
            "Iteration 46, loss = 0.52408202\n",
            "Iteration 47, loss = 0.50471426\n",
            "Iteration 48, loss = 0.50334057\n",
            "Iteration 49, loss = 0.50090147\n",
            "Iteration 50, loss = 0.50082097\n",
            "Iteration 51, loss = 0.50795296\n",
            "Iteration 52, loss = 0.51076367\n",
            "Iteration 53, loss = 0.47878258\n",
            "Iteration 54, loss = 0.48629421\n",
            "Iteration 55, loss = 0.48378119\n",
            "Iteration 56, loss = 0.50644275\n",
            "Iteration 57, loss = 0.50334431\n",
            "Iteration 58, loss = 0.46759259\n",
            "Iteration 59, loss = 0.46786921\n",
            "Iteration 60, loss = 0.48408825\n",
            "Iteration 61, loss = 0.50966400\n",
            "Iteration 62, loss = 0.46432329\n",
            "Iteration 63, loss = 0.47353441\n",
            "Iteration 64, loss = 0.46937422\n",
            "Iteration 65, loss = 0.45051311\n",
            "Iteration 66, loss = 0.45654697\n",
            "Iteration 67, loss = 0.44968072\n",
            "Iteration 68, loss = 0.44762229\n",
            "Iteration 69, loss = 0.45773355\n",
            "Iteration 70, loss = 0.46585821\n",
            "Iteration 71, loss = 0.45648860\n",
            "Iteration 72, loss = 0.46353792\n",
            "Iteration 73, loss = 0.46019579\n",
            "Iteration 74, loss = 0.46088219\n",
            "Iteration 75, loss = 0.46451653\n",
            "Iteration 76, loss = 0.45899701\n",
            "Iteration 77, loss = 0.47974390\n",
            "Iteration 78, loss = 0.49033781\n",
            "Iteration 79, loss = 0.59766255\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.05705589\n",
            "Iteration 2, loss = 0.69390943\n",
            "Iteration 3, loss = 0.68348273\n",
            "Iteration 4, loss = 0.64849785\n",
            "Iteration 5, loss = 0.61281111\n",
            "Iteration 6, loss = 0.61401792\n",
            "Iteration 7, loss = 0.60562793\n",
            "Iteration 8, loss = 0.59239257\n",
            "Iteration 9, loss = 0.59142957\n",
            "Iteration 10, loss = 0.58499020\n",
            "Iteration 11, loss = 0.58716294\n",
            "Iteration 12, loss = 0.58138764\n",
            "Iteration 13, loss = 0.58571952\n",
            "Iteration 14, loss = 0.57806404\n",
            "Iteration 15, loss = 0.56485062\n",
            "Iteration 16, loss = 0.58482021\n",
            "Iteration 17, loss = 0.55959514\n",
            "Iteration 18, loss = 0.55653963\n",
            "Iteration 19, loss = 0.55304370\n",
            "Iteration 20, loss = 0.57673656\n",
            "Iteration 21, loss = 0.56751222\n",
            "Iteration 22, loss = 0.54427041\n",
            "Iteration 23, loss = 0.54650810\n",
            "Iteration 24, loss = 0.54195023\n",
            "Iteration 25, loss = 0.53618825\n",
            "Iteration 26, loss = 0.54140686\n",
            "Iteration 27, loss = 0.56590053\n",
            "Iteration 28, loss = 0.54627110\n",
            "Iteration 29, loss = 0.52497607\n",
            "Iteration 30, loss = 0.54082336\n",
            "Iteration 31, loss = 0.54013558\n",
            "Iteration 32, loss = 0.51707977\n",
            "Iteration 33, loss = 0.51567983\n",
            "Iteration 34, loss = 0.50936241\n",
            "Iteration 35, loss = 0.50681983\n",
            "Iteration 36, loss = 0.51180820\n",
            "Iteration 37, loss = 0.52118594\n",
            "Iteration 38, loss = 0.51618067\n",
            "Iteration 39, loss = 0.50335620\n",
            "Iteration 40, loss = 0.49965301\n",
            "Iteration 41, loss = 0.52015283\n",
            "Iteration 42, loss = 0.53506668\n",
            "Iteration 43, loss = 0.50873922\n",
            "Iteration 44, loss = 0.49022876\n",
            "Iteration 45, loss = 0.49163785\n",
            "Iteration 46, loss = 0.51344131\n",
            "Iteration 47, loss = 0.49517264\n",
            "Iteration 48, loss = 0.48545869\n",
            "Iteration 49, loss = 0.47466971\n",
            "Iteration 50, loss = 0.48039110\n",
            "Iteration 51, loss = 0.47393600\n",
            "Iteration 52, loss = 0.48144510\n",
            "Iteration 53, loss = 0.47855745\n",
            "Iteration 54, loss = 0.49978897\n",
            "Iteration 55, loss = 0.49417235\n",
            "Iteration 56, loss = 0.48984511\n",
            "Iteration 57, loss = 0.47849178\n",
            "Iteration 58, loss = 0.46952108\n",
            "Iteration 59, loss = 0.46291335\n",
            "Iteration 60, loss = 0.46911748\n",
            "Iteration 61, loss = 0.46656727\n",
            "Iteration 62, loss = 0.47281484\n",
            "Iteration 63, loss = 0.47008085\n",
            "Iteration 64, loss = 0.47215975\n",
            "Iteration 65, loss = 0.46394013\n",
            "Iteration 66, loss = 0.46407113\n",
            "Iteration 67, loss = 0.46460332\n",
            "Iteration 68, loss = 0.47833201\n",
            "Iteration 69, loss = 0.46523987\n",
            "Iteration 70, loss = 0.45501923\n",
            "Iteration 71, loss = 0.46123301\n",
            "Iteration 72, loss = 0.45171769\n",
            "Iteration 73, loss = 0.46022437\n",
            "Iteration 74, loss = 0.45017901\n",
            "Iteration 75, loss = 0.45183078\n",
            "Iteration 76, loss = 0.44502727\n",
            "Iteration 77, loss = 0.46860653\n",
            "Iteration 78, loss = 0.47191025\n",
            "Iteration 79, loss = 0.48372831\n",
            "Iteration 80, loss = 0.44853330\n",
            "Iteration 81, loss = 0.44715000\n",
            "Iteration 82, loss = 0.46056053\n",
            "Iteration 83, loss = 0.51353374\n",
            "Iteration 84, loss = 0.56168490\n",
            "Iteration 85, loss = 0.55733728\n",
            "Iteration 86, loss = 0.46697968\n",
            "Iteration 87, loss = 0.48650996\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.71634097\n",
            "Iteration 2, loss = 1.08234580\n",
            "Iteration 3, loss = 0.83854717\n",
            "Iteration 4, loss = 0.67073549\n",
            "Iteration 5, loss = 0.64294535\n",
            "Iteration 6, loss = 0.63538692\n",
            "Iteration 7, loss = 0.61042280\n",
            "Iteration 8, loss = 0.65165143\n",
            "Iteration 9, loss = 0.62597255\n",
            "Iteration 10, loss = 0.61897093\n",
            "Iteration 11, loss = 0.59883515\n",
            "Iteration 12, loss = 0.61860490\n",
            "Iteration 13, loss = 0.59553851\n",
            "Iteration 14, loss = 0.58768456\n",
            "Iteration 15, loss = 0.57981367\n",
            "Iteration 16, loss = 0.58093112\n",
            "Iteration 17, loss = 0.57530831\n",
            "Iteration 18, loss = 0.56484033\n",
            "Iteration 19, loss = 0.56238224\n",
            "Iteration 20, loss = 0.55716331\n",
            "Iteration 21, loss = 0.55718922\n",
            "Iteration 22, loss = 0.55202498\n",
            "Iteration 23, loss = 0.63822633\n",
            "Iteration 24, loss = 0.62394804\n",
            "Iteration 25, loss = 0.55659094\n",
            "Iteration 26, loss = 0.54153864\n",
            "Iteration 27, loss = 0.59205793\n",
            "Iteration 28, loss = 0.60145567\n",
            "Iteration 29, loss = 0.69884658\n",
            "Iteration 30, loss = 0.58294272\n",
            "Iteration 31, loss = 0.55926224\n",
            "Iteration 32, loss = 0.54125360\n",
            "Iteration 33, loss = 0.54943611\n",
            "Iteration 34, loss = 0.52068383\n",
            "Iteration 35, loss = 0.51711113\n",
            "Iteration 36, loss = 0.51093032\n",
            "Iteration 37, loss = 0.52472518\n",
            "Iteration 38, loss = 0.51263344\n",
            "Iteration 39, loss = 0.51422576\n",
            "Iteration 40, loss = 0.49782247\n",
            "Iteration 41, loss = 0.49896966\n",
            "Iteration 42, loss = 0.49443205\n",
            "Iteration 43, loss = 0.49525844\n",
            "Iteration 44, loss = 0.49150422\n",
            "Iteration 45, loss = 0.49248955\n",
            "Iteration 46, loss = 0.48812317\n",
            "Iteration 47, loss = 0.50006437\n",
            "Iteration 48, loss = 0.49050797\n",
            "Iteration 49, loss = 0.51327306\n",
            "Iteration 50, loss = 0.50814647\n",
            "Iteration 51, loss = 0.49874725\n",
            "Iteration 52, loss = 0.49024163\n",
            "Iteration 53, loss = 0.47234762\n",
            "Iteration 54, loss = 0.46818825\n",
            "Iteration 55, loss = 0.47994643\n",
            "Iteration 56, loss = 0.47154404\n",
            "Iteration 57, loss = 0.48246084\n",
            "Iteration 58, loss = 0.50208443\n",
            "Iteration 59, loss = 0.53831699\n",
            "Iteration 60, loss = 0.47244647\n",
            "Iteration 61, loss = 0.48817281\n",
            "Iteration 62, loss = 0.46167307\n",
            "Iteration 63, loss = 0.46050776\n",
            "Iteration 64, loss = 0.47578948\n",
            "Iteration 65, loss = 0.48530953\n",
            "Iteration 66, loss = 0.49036490\n",
            "Iteration 67, loss = 0.47852338\n",
            "Iteration 68, loss = 0.47608026\n",
            "Iteration 69, loss = 0.51142613\n",
            "Iteration 70, loss = 0.63511236\n",
            "Iteration 71, loss = 0.61306220\n",
            "Iteration 72, loss = 0.51638586\n",
            "Iteration 73, loss = 0.47305157\n",
            "Iteration 74, loss = 0.48494248\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.07372791\n",
            "Iteration 2, loss = 0.67637472\n",
            "Iteration 3, loss = 0.64618072\n",
            "Iteration 4, loss = 0.64748204\n",
            "Iteration 5, loss = 0.59289641\n",
            "Iteration 6, loss = 0.56444771\n",
            "Iteration 7, loss = 0.55188005\n",
            "Iteration 8, loss = 0.54258299\n",
            "Iteration 9, loss = 0.52981791\n",
            "Iteration 10, loss = 0.52668081\n",
            "Iteration 11, loss = 0.60872970\n",
            "Iteration 12, loss = 0.56851140\n",
            "Iteration 13, loss = 0.54609466\n",
            "Iteration 14, loss = 0.53108506\n",
            "Iteration 15, loss = 0.51224225\n",
            "Iteration 16, loss = 0.52776565\n",
            "Iteration 17, loss = 0.49895020\n",
            "Iteration 18, loss = 0.56457600\n",
            "Iteration 19, loss = 0.61074718\n",
            "Iteration 20, loss = 0.49609442\n",
            "Iteration 21, loss = 0.49993611\n",
            "Iteration 22, loss = 0.57684419\n",
            "Iteration 23, loss = 0.48779782\n",
            "Iteration 24, loss = 0.55486157\n",
            "Iteration 25, loss = 0.51127862\n",
            "Iteration 26, loss = 0.47395817\n",
            "Iteration 27, loss = 0.47188423\n",
            "Iteration 28, loss = 0.46510232\n",
            "Iteration 29, loss = 0.45228189\n",
            "Iteration 30, loss = 0.48753148\n",
            "Iteration 31, loss = 0.45606979\n",
            "Iteration 32, loss = 0.45240925\n",
            "Iteration 33, loss = 0.44724275\n",
            "Iteration 34, loss = 0.47707900\n",
            "Iteration 35, loss = 0.46780219\n",
            "Iteration 36, loss = 0.46651570\n",
            "Iteration 37, loss = 0.46516808\n",
            "Iteration 38, loss = 0.46120347\n",
            "Iteration 39, loss = 0.45383730\n",
            "Iteration 40, loss = 0.44813027\n",
            "Iteration 41, loss = 0.43970876\n",
            "Iteration 42, loss = 0.43902388\n",
            "Iteration 43, loss = 0.44661002\n",
            "Iteration 44, loss = 0.46224110\n",
            "Iteration 45, loss = 0.50294648\n",
            "Iteration 46, loss = 0.47426049\n",
            "Iteration 47, loss = 0.45055031\n",
            "Iteration 48, loss = 0.44539761\n",
            "Iteration 49, loss = 0.43703270\n",
            "Iteration 50, loss = 0.48429555\n",
            "Iteration 51, loss = 0.46166403\n",
            "Iteration 52, loss = 0.43503471\n",
            "Iteration 53, loss = 0.48035735\n",
            "Iteration 54, loss = 0.43683007\n",
            "Iteration 55, loss = 0.42198667\n",
            "Iteration 56, loss = 0.45744939\n",
            "Iteration 57, loss = 0.44618761\n",
            "Iteration 58, loss = 0.43796008\n",
            "Iteration 59, loss = 0.44071270\n",
            "Iteration 60, loss = 0.44008731\n",
            "Iteration 61, loss = 0.42939611\n",
            "Iteration 62, loss = 0.43359459\n",
            "Iteration 63, loss = 0.41683908\n",
            "Iteration 64, loss = 0.43947156\n",
            "Iteration 65, loss = 0.42769394\n",
            "Iteration 66, loss = 0.42119146\n",
            "Iteration 67, loss = 0.62538324\n",
            "Iteration 68, loss = 0.50890717\n",
            "Iteration 69, loss = 0.51956101\n",
            "Iteration 70, loss = 0.54643266\n",
            "Iteration 71, loss = 0.43748367\n",
            "Iteration 72, loss = 0.42169394\n",
            "Iteration 73, loss = 0.41333173\n",
            "Iteration 74, loss = 0.42520033\n",
            "Iteration 75, loss = 0.43848288\n",
            "Iteration 76, loss = 0.43468909\n",
            "Iteration 77, loss = 0.44379065\n",
            "Iteration 78, loss = 0.47324481\n",
            "Iteration 79, loss = 0.47240435\n",
            "Iteration 80, loss = 0.42820055\n",
            "Iteration 81, loss = 0.42981324\n",
            "Iteration 82, loss = 0.43604892\n",
            "Iteration 83, loss = 0.47206448\n",
            "Iteration 84, loss = 0.45057545\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.46829373\n",
            "Iteration 2, loss = 0.90455840\n",
            "Iteration 3, loss = 0.69462305\n",
            "Iteration 4, loss = 0.61973440\n",
            "Iteration 5, loss = 0.59286521\n",
            "Iteration 6, loss = 0.58020574\n",
            "Iteration 7, loss = 0.60260440\n",
            "Iteration 8, loss = 0.59440553\n",
            "Iteration 9, loss = 0.57452600\n",
            "Iteration 10, loss = 0.55628446\n",
            "Iteration 11, loss = 0.57104643\n",
            "Iteration 12, loss = 0.56880544\n",
            "Iteration 13, loss = 0.56897450\n",
            "Iteration 14, loss = 0.53487324\n",
            "Iteration 15, loss = 0.58686846\n",
            "Iteration 16, loss = 0.54046993\n",
            "Iteration 17, loss = 0.55920963\n",
            "Iteration 18, loss = 0.53296279\n",
            "Iteration 19, loss = 0.52800608\n",
            "Iteration 20, loss = 0.53258360\n",
            "Iteration 21, loss = 0.53443454\n",
            "Iteration 22, loss = 0.55706410\n",
            "Iteration 23, loss = 0.50673409\n",
            "Iteration 24, loss = 0.50939972\n",
            "Iteration 25, loss = 0.50077529\n",
            "Iteration 26, loss = 0.49246654\n",
            "Iteration 27, loss = 0.50726809\n",
            "Iteration 28, loss = 0.54408110\n",
            "Iteration 29, loss = 0.59129706\n",
            "Iteration 30, loss = 0.63403163\n",
            "Iteration 31, loss = 0.50259816\n",
            "Iteration 32, loss = 0.48725041\n",
            "Iteration 33, loss = 0.49963841\n",
            "Iteration 34, loss = 0.51166758\n",
            "Iteration 35, loss = 0.47576199\n",
            "Iteration 36, loss = 0.45479384\n",
            "Iteration 37, loss = 0.47432833\n",
            "Iteration 38, loss = 0.47917142\n",
            "Iteration 39, loss = 0.47558933\n",
            "Iteration 40, loss = 0.47454473\n",
            "Iteration 41, loss = 0.69656807\n",
            "Iteration 42, loss = 0.58434841\n",
            "Iteration 43, loss = 0.50617635\n",
            "Iteration 44, loss = 0.47269854\n",
            "Iteration 45, loss = 0.46120927\n",
            "Iteration 46, loss = 0.61906407\n",
            "Iteration 47, loss = 0.46580944\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.04558518\n",
            "Iteration 2, loss = 0.73611206\n",
            "Iteration 3, loss = 0.64523911\n",
            "Iteration 4, loss = 0.61449105\n",
            "Iteration 5, loss = 0.60998131\n",
            "Iteration 6, loss = 0.59673813\n",
            "Iteration 7, loss = 0.62883451\n",
            "Iteration 8, loss = 0.69516032\n",
            "Iteration 9, loss = 0.60000312\n",
            "Iteration 10, loss = 0.56800314\n",
            "Iteration 11, loss = 0.59478672\n",
            "Iteration 12, loss = 0.59124249\n",
            "Iteration 13, loss = 0.56472171\n",
            "Iteration 14, loss = 0.56160364\n",
            "Iteration 15, loss = 0.56909558\n",
            "Iteration 16, loss = 0.59365845\n",
            "Iteration 17, loss = 0.57852714\n",
            "Iteration 18, loss = 0.56907764\n",
            "Iteration 19, loss = 0.52702313\n",
            "Iteration 20, loss = 0.51733123\n",
            "Iteration 21, loss = 0.52053066\n",
            "Iteration 22, loss = 0.62098718\n",
            "Iteration 23, loss = 0.61225018\n",
            "Iteration 24, loss = 0.55069627\n",
            "Iteration 25, loss = 0.53667989\n",
            "Iteration 26, loss = 0.53472697\n",
            "Iteration 27, loss = 0.54305423\n",
            "Iteration 28, loss = 0.58586291\n",
            "Iteration 29, loss = 0.56518952\n",
            "Iteration 30, loss = 0.53098938\n",
            "Iteration 31, loss = 0.48717306\n",
            "Iteration 32, loss = 0.50520040\n",
            "Iteration 33, loss = 0.53020560\n",
            "Iteration 34, loss = 0.52615146\n",
            "Iteration 35, loss = 0.50049441\n",
            "Iteration 36, loss = 0.48359582\n",
            "Iteration 37, loss = 0.47585140\n",
            "Iteration 38, loss = 0.48874015\n",
            "Iteration 39, loss = 0.48106454\n",
            "Iteration 40, loss = 0.47476401\n",
            "Iteration 41, loss = 0.47786806\n",
            "Iteration 42, loss = 0.47320915\n",
            "Iteration 43, loss = 0.54399214\n",
            "Iteration 44, loss = 0.52883468\n",
            "Iteration 45, loss = 0.50233046\n",
            "Iteration 46, loss = 0.48935332\n",
            "Iteration 47, loss = 0.49566079\n",
            "Iteration 48, loss = 0.51700955\n",
            "Iteration 49, loss = 0.49495860\n",
            "Iteration 50, loss = 0.47418066\n",
            "Iteration 51, loss = 0.48342074\n",
            "Iteration 52, loss = 0.47724040\n",
            "Iteration 53, loss = 0.49499529\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75529151\n",
            "Iteration 2, loss = 0.61573877\n",
            "Iteration 3, loss = 0.64206082\n",
            "Iteration 4, loss = 0.58531085\n",
            "Iteration 5, loss = 0.60545594\n",
            "Iteration 6, loss = 0.79773666\n",
            "Iteration 7, loss = 0.74074596\n",
            "Iteration 8, loss = 0.69842353\n",
            "Iteration 9, loss = 0.58479279\n",
            "Iteration 10, loss = 0.65214568\n",
            "Iteration 11, loss = 0.56608532\n",
            "Iteration 12, loss = 0.54954020\n",
            "Iteration 13, loss = 0.56315241\n",
            "Iteration 14, loss = 0.56477931\n",
            "Iteration 15, loss = 0.68808154\n",
            "Iteration 16, loss = 0.64564414\n",
            "Iteration 17, loss = 0.55870126\n",
            "Iteration 18, loss = 0.58850879\n",
            "Iteration 19, loss = 0.54350594\n",
            "Iteration 20, loss = 0.51244851\n",
            "Iteration 21, loss = 0.53250603\n",
            "Iteration 22, loss = 0.52326778\n",
            "Iteration 23, loss = 0.49324376\n",
            "Iteration 24, loss = 0.49221878\n",
            "Iteration 25, loss = 0.48809779\n",
            "Iteration 26, loss = 0.49120594\n",
            "Iteration 27, loss = 0.48624642\n",
            "Iteration 28, loss = 0.48242571\n",
            "Iteration 29, loss = 0.48319415\n",
            "Iteration 30, loss = 0.48188077\n",
            "Iteration 31, loss = 0.47988681\n",
            "Iteration 32, loss = 0.46811675\n",
            "Iteration 33, loss = 0.46624713\n",
            "Iteration 34, loss = 0.45793297\n",
            "Iteration 35, loss = 0.48184818\n",
            "Iteration 36, loss = 0.54318085\n",
            "Iteration 37, loss = 0.70175666\n",
            "Iteration 38, loss = 0.56773787\n",
            "Iteration 39, loss = 0.47372464\n",
            "Iteration 40, loss = 0.46061421\n",
            "Iteration 41, loss = 0.48685693\n",
            "Iteration 42, loss = 0.49174410\n",
            "Iteration 43, loss = 0.46429189\n",
            "Iteration 44, loss = 0.47012629\n",
            "Iteration 45, loss = 0.46663409\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31088549\n",
            "Iteration 2, loss = 1.00633730\n",
            "Iteration 3, loss = 0.75605639\n",
            "Iteration 4, loss = 0.66783916\n",
            "Iteration 5, loss = 0.67774635\n",
            "Iteration 6, loss = 0.61782984\n",
            "Iteration 7, loss = 0.61004759\n",
            "Iteration 8, loss = 0.59210221\n",
            "Iteration 9, loss = 0.60833936\n",
            "Iteration 10, loss = 0.58256995\n",
            "Iteration 11, loss = 0.57885500\n",
            "Iteration 12, loss = 0.64503200\n",
            "Iteration 13, loss = 0.61198440\n",
            "Iteration 14, loss = 0.58794105\n",
            "Iteration 15, loss = 0.56873857\n",
            "Iteration 16, loss = 0.56191284\n",
            "Iteration 17, loss = 0.55108801\n",
            "Iteration 18, loss = 0.55943204\n",
            "Iteration 19, loss = 0.54743450\n",
            "Iteration 20, loss = 0.54129408\n",
            "Iteration 21, loss = 0.53497426\n",
            "Iteration 22, loss = 0.52951122\n",
            "Iteration 23, loss = 0.57667731\n",
            "Iteration 24, loss = 0.52670827\n",
            "Iteration 25, loss = 0.57657943\n",
            "Iteration 26, loss = 0.51327975\n",
            "Iteration 27, loss = 0.51044149\n",
            "Iteration 28, loss = 0.50198905\n",
            "Iteration 29, loss = 0.50969192\n",
            "Iteration 30, loss = 0.51904813\n",
            "Iteration 31, loss = 0.59509138\n",
            "Iteration 32, loss = 0.50118985\n",
            "Iteration 33, loss = 0.53870844\n",
            "Iteration 34, loss = 0.50134027\n",
            "Iteration 35, loss = 0.48526797\n",
            "Iteration 36, loss = 0.47951302\n",
            "Iteration 37, loss = 0.47477098\n",
            "Iteration 38, loss = 0.47257323\n",
            "Iteration 39, loss = 0.47642196\n",
            "Iteration 40, loss = 0.46455492\n",
            "Iteration 41, loss = 0.47429251\n",
            "Iteration 42, loss = 0.50844982\n",
            "Iteration 43, loss = 0.51614135\n",
            "Iteration 44, loss = 0.49209817\n",
            "Iteration 45, loss = 0.50445373\n",
            "Iteration 46, loss = 0.46434455\n",
            "Iteration 47, loss = 0.47795513\n",
            "Iteration 48, loss = 0.47696701\n",
            "Iteration 49, loss = 0.45975000\n",
            "Iteration 50, loss = 0.47449131\n",
            "Iteration 51, loss = 0.52924246\n",
            "Iteration 52, loss = 0.46988207\n",
            "Iteration 53, loss = 0.48511551\n",
            "Iteration 54, loss = 0.46017591\n",
            "Iteration 55, loss = 0.45335247\n",
            "Iteration 56, loss = 0.45192578\n",
            "Iteration 57, loss = 0.45396494\n",
            "Iteration 58, loss = 0.49804541\n",
            "Iteration 59, loss = 0.46729915\n",
            "Iteration 60, loss = 0.45915756\n",
            "Iteration 61, loss = 0.51515465\n",
            "Iteration 62, loss = 0.50675955\n",
            "Iteration 63, loss = 0.45115632\n",
            "Iteration 64, loss = 0.46882381\n",
            "Iteration 65, loss = 0.44634939\n",
            "Iteration 66, loss = 0.45054407\n",
            "Iteration 67, loss = 0.44667011\n",
            "Iteration 68, loss = 0.45627983\n",
            "Iteration 69, loss = 0.48704631\n",
            "Iteration 70, loss = 0.49147984\n",
            "Iteration 71, loss = 0.46752260\n",
            "Iteration 72, loss = 0.47471388\n",
            "Iteration 73, loss = 0.45450774\n",
            "Iteration 74, loss = 0.47921402\n",
            "Iteration 75, loss = 0.44226435\n",
            "Iteration 76, loss = 0.46172044\n",
            "Iteration 77, loss = 0.49804129\n",
            "Iteration 78, loss = 0.54246457\n",
            "Iteration 79, loss = 0.44375876\n",
            "Iteration 80, loss = 0.43979830\n",
            "Iteration 81, loss = 0.43861511\n",
            "Iteration 82, loss = 0.43604863\n",
            "Iteration 83, loss = 0.44250557\n",
            "Iteration 84, loss = 0.45579241\n",
            "Iteration 85, loss = 0.45876267\n",
            "Iteration 86, loss = 0.45471321\n",
            "Iteration 87, loss = 0.43161529\n",
            "Iteration 88, loss = 0.43529777\n",
            "Iteration 89, loss = 0.42949839\n",
            "Iteration 90, loss = 0.42406553\n",
            "Iteration 91, loss = 0.46705237\n",
            "Iteration 92, loss = 0.48027031\n",
            "Iteration 93, loss = 0.44774089\n",
            "Iteration 94, loss = 0.42623557\n",
            "Iteration 95, loss = 0.43055185\n",
            "Iteration 96, loss = 0.42538901\n",
            "Iteration 97, loss = 0.42735404\n",
            "Iteration 98, loss = 0.42523145\n",
            "Iteration 99, loss = 0.42732307\n",
            "Iteration 100, loss = 0.43392222\n",
            "Iteration 101, loss = 0.42551020\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.72507830\n",
            "Iteration 2, loss = 0.67163520\n",
            "Iteration 3, loss = 0.66425145\n",
            "Iteration 4, loss = 0.66188870\n",
            "Iteration 5, loss = 0.66027003\n",
            "Iteration 6, loss = 0.65811091\n",
            "Iteration 7, loss = 0.65726830\n",
            "Iteration 8, loss = 0.65500972\n",
            "Iteration 9, loss = 0.65388100\n",
            "Iteration 10, loss = 0.65272781\n",
            "Iteration 11, loss = 0.65146752\n",
            "Iteration 12, loss = 0.65031710\n",
            "Iteration 13, loss = 0.64903385\n",
            "Iteration 14, loss = 0.64802416\n",
            "Iteration 15, loss = 0.64670186\n",
            "Iteration 16, loss = 0.64552385\n",
            "Iteration 17, loss = 0.64499863\n",
            "Iteration 18, loss = 0.64322895\n",
            "Iteration 19, loss = 0.64205704\n",
            "Iteration 20, loss = 0.64099440\n",
            "Iteration 21, loss = 0.63989580\n",
            "Iteration 22, loss = 0.63872265\n",
            "Iteration 23, loss = 0.63743635\n",
            "Iteration 24, loss = 0.63723773\n",
            "Iteration 25, loss = 0.63536026\n",
            "Iteration 26, loss = 0.63486211\n",
            "Iteration 27, loss = 0.63324612\n",
            "Iteration 28, loss = 0.63175560\n",
            "Iteration 29, loss = 0.63082118\n",
            "Iteration 30, loss = 0.62956171\n",
            "Iteration 31, loss = 0.62866307\n",
            "Iteration 32, loss = 0.62783716\n",
            "Iteration 33, loss = 0.62691956\n",
            "Iteration 34, loss = 0.62580220\n",
            "Iteration 35, loss = 0.62450479\n",
            "Iteration 36, loss = 0.62379235\n",
            "Iteration 37, loss = 0.62346818\n",
            "Iteration 38, loss = 0.62199713\n",
            "Iteration 39, loss = 0.62136015\n",
            "Iteration 40, loss = 0.62038880\n",
            "Iteration 41, loss = 0.61988851\n",
            "Iteration 42, loss = 0.61894842\n",
            "Iteration 43, loss = 0.61840407\n",
            "Iteration 44, loss = 0.61747886\n",
            "Iteration 45, loss = 0.61674104\n",
            "Iteration 46, loss = 0.61566803\n",
            "Iteration 47, loss = 0.61511487\n",
            "Iteration 48, loss = 0.61438982\n",
            "Iteration 49, loss = 0.61436424\n",
            "Iteration 50, loss = 0.61356470\n",
            "Iteration 51, loss = 0.61274799\n",
            "Iteration 52, loss = 0.61224029\n",
            "Iteration 53, loss = 0.61190217\n",
            "Iteration 54, loss = 0.61178199\n",
            "Iteration 55, loss = 0.61121429\n",
            "Iteration 56, loss = 0.61053302\n",
            "Iteration 57, loss = 0.61087238\n",
            "Iteration 58, loss = 0.60984505\n",
            "Iteration 59, loss = 0.60998962\n",
            "Iteration 60, loss = 0.60945553\n",
            "Iteration 61, loss = 0.60879929\n",
            "Iteration 62, loss = 0.60907704\n",
            "Iteration 63, loss = 0.60853082\n",
            "Iteration 64, loss = 0.60846952\n",
            "Iteration 65, loss = 0.60800714\n",
            "Iteration 66, loss = 0.60763437\n",
            "Iteration 67, loss = 0.60758884\n",
            "Iteration 68, loss = 0.60686228\n",
            "Iteration 69, loss = 0.60734982\n",
            "Iteration 70, loss = 0.60644037\n",
            "Iteration 71, loss = 0.60635027\n",
            "Iteration 72, loss = 0.60604322\n",
            "Iteration 73, loss = 0.60621328\n",
            "Iteration 74, loss = 0.60547677\n",
            "Iteration 75, loss = 0.60617405\n",
            "Iteration 76, loss = 0.60531500\n",
            "Iteration 77, loss = 0.60470150\n",
            "Iteration 78, loss = 0.60527736\n",
            "Iteration 79, loss = 0.60459769\n",
            "Iteration 80, loss = 0.60531366\n",
            "Iteration 81, loss = 0.60396789\n",
            "Iteration 82, loss = 0.60341080\n",
            "Iteration 83, loss = 0.60402268\n",
            "Iteration 84, loss = 0.60286266\n",
            "Iteration 85, loss = 0.60250239\n",
            "Iteration 86, loss = 0.60161154\n",
            "Iteration 87, loss = 0.60197339\n",
            "Iteration 88, loss = 0.60165650\n",
            "Iteration 89, loss = 0.60140359\n",
            "Iteration 90, loss = 0.60246563\n",
            "Iteration 91, loss = 0.60065519\n",
            "Iteration 92, loss = 0.59984966\n",
            "Iteration 93, loss = 0.59987650\n",
            "Iteration 94, loss = 0.60005928\n",
            "Iteration 95, loss = 0.59981502\n",
            "Iteration 96, loss = 0.59984387\n",
            "Iteration 97, loss = 0.59967883\n",
            "Iteration 98, loss = 0.59894561\n",
            "Iteration 99, loss = 0.59851141\n",
            "Iteration 100, loss = 0.59857000\n",
            "Iteration 101, loss = 0.59925669\n",
            "Iteration 102, loss = 0.59837317\n",
            "Iteration 103, loss = 0.59904765\n",
            "Iteration 104, loss = 0.59739480\n",
            "Iteration 105, loss = 0.59700534\n",
            "Iteration 106, loss = 0.59692578\n",
            "Iteration 107, loss = 0.59697684\n",
            "Iteration 108, loss = 0.59677693\n",
            "Iteration 109, loss = 0.59706609\n",
            "Iteration 110, loss = 0.59690477\n",
            "Iteration 111, loss = 0.59622737\n",
            "Iteration 112, loss = 0.59595306\n",
            "Iteration 113, loss = 0.59551878\n",
            "Iteration 114, loss = 0.59558242\n",
            "Iteration 115, loss = 0.59495666\n",
            "Iteration 116, loss = 0.59498807\n",
            "Iteration 117, loss = 0.59530544\n",
            "Iteration 118, loss = 0.59561884\n",
            "Iteration 119, loss = 0.59483124\n",
            "Iteration 120, loss = 0.59467481\n",
            "Iteration 121, loss = 0.59423658\n",
            "Iteration 122, loss = 0.59427893\n",
            "Iteration 123, loss = 0.59341376\n",
            "Iteration 124, loss = 0.59259440\n",
            "Iteration 125, loss = 0.59265240\n",
            "Iteration 126, loss = 0.59264165\n",
            "Iteration 127, loss = 0.59198872\n",
            "Iteration 128, loss = 0.59209766\n",
            "Iteration 129, loss = 0.59169500\n",
            "Iteration 130, loss = 0.59120687\n",
            "Iteration 131, loss = 0.59137808\n",
            "Iteration 132, loss = 0.59127025\n",
            "Iteration 133, loss = 0.59096644\n",
            "Iteration 134, loss = 0.59062100\n",
            "Iteration 135, loss = 0.59094696\n",
            "Iteration 136, loss = 0.59057134\n",
            "Iteration 137, loss = 0.59003308\n",
            "Iteration 138, loss = 0.59176305\n",
            "Iteration 139, loss = 0.58981135\n",
            "Iteration 140, loss = 0.59112839\n",
            "Iteration 141, loss = 0.58908260\n",
            "Iteration 142, loss = 0.59034720\n",
            "Iteration 143, loss = 0.58823748\n",
            "Iteration 144, loss = 0.59002616\n",
            "Iteration 145, loss = 0.58877951\n",
            "Iteration 146, loss = 0.58698778\n",
            "Iteration 147, loss = 0.58761575\n",
            "Iteration 148, loss = 0.58789729\n",
            "Iteration 149, loss = 0.58716342\n",
            "Iteration 150, loss = 0.58763190\n",
            "Iteration 151, loss = 0.58655312\n",
            "Iteration 152, loss = 0.58762542\n",
            "Iteration 153, loss = 0.58733866\n",
            "Iteration 154, loss = 0.58682774\n",
            "Iteration 155, loss = 0.58571401\n",
            "Iteration 156, loss = 0.58633861\n",
            "Iteration 157, loss = 0.58600793\n",
            "Iteration 158, loss = 0.58485971\n",
            "Iteration 159, loss = 0.58459805\n",
            "Iteration 160, loss = 0.58516368\n",
            "Iteration 161, loss = 0.58446008\n",
            "Iteration 162, loss = 0.58367441\n",
            "Iteration 163, loss = 0.58368444\n",
            "Iteration 164, loss = 0.58534099\n",
            "Iteration 165, loss = 0.58293195\n",
            "Iteration 166, loss = 0.58568560\n",
            "Iteration 167, loss = 0.58318402\n",
            "Iteration 168, loss = 0.58217091\n",
            "Iteration 169, loss = 0.58228113\n",
            "Iteration 170, loss = 0.58489408\n",
            "Iteration 171, loss = 0.58184837\n",
            "Iteration 172, loss = 0.58086315\n",
            "Iteration 173, loss = 0.58151873\n",
            "Iteration 174, loss = 0.58234342\n",
            "Iteration 175, loss = 0.58127791\n",
            "Iteration 176, loss = 0.58111246\n",
            "Iteration 177, loss = 0.57928027\n",
            "Iteration 178, loss = 0.58031524\n",
            "Iteration 179, loss = 0.58118233\n",
            "Iteration 180, loss = 0.58068622\n",
            "Iteration 181, loss = 0.58179893\n",
            "Iteration 182, loss = 0.57877695\n",
            "Iteration 183, loss = 0.58054012\n",
            "Iteration 184, loss = 0.57950801\n",
            "Iteration 185, loss = 0.57780692\n",
            "Iteration 186, loss = 0.57804995\n",
            "Iteration 187, loss = 0.57852772\n",
            "Iteration 188, loss = 0.57824986\n",
            "Iteration 189, loss = 0.57659653\n",
            "Iteration 190, loss = 0.57698528\n",
            "Iteration 191, loss = 0.57770303\n",
            "Iteration 192, loss = 0.57626030\n",
            "Iteration 193, loss = 0.57716672\n",
            "Iteration 194, loss = 0.57537705\n",
            "Iteration 195, loss = 0.57538092\n",
            "Iteration 196, loss = 0.57459771\n",
            "Iteration 197, loss = 0.57509447\n",
            "Iteration 198, loss = 0.57418235\n",
            "Iteration 199, loss = 0.57383142\n",
            "Iteration 200, loss = 0.57388170\n",
            "Iteration 1, loss = 0.66094139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss = 0.65925426\n",
            "Iteration 3, loss = 0.65885032\n",
            "Iteration 4, loss = 0.65693291\n",
            "Iteration 5, loss = 0.65578043\n",
            "Iteration 6, loss = 0.65477448\n",
            "Iteration 7, loss = 0.65387669\n",
            "Iteration 8, loss = 0.65237051\n",
            "Iteration 9, loss = 0.65087299\n",
            "Iteration 10, loss = 0.64925876\n",
            "Iteration 11, loss = 0.64822372\n",
            "Iteration 12, loss = 0.64651755\n",
            "Iteration 13, loss = 0.64522090\n",
            "Iteration 14, loss = 0.64381246\n",
            "Iteration 15, loss = 0.64286462\n",
            "Iteration 16, loss = 0.64125456\n",
            "Iteration 17, loss = 0.64023889\n",
            "Iteration 18, loss = 0.63841424\n",
            "Iteration 19, loss = 0.63715546\n",
            "Iteration 20, loss = 0.63680251\n",
            "Iteration 21, loss = 0.63490822\n",
            "Iteration 22, loss = 0.63340311\n",
            "Iteration 23, loss = 0.63236756\n",
            "Iteration 24, loss = 0.63118938\n",
            "Iteration 25, loss = 0.62994367\n",
            "Iteration 26, loss = 0.62828064\n",
            "Iteration 27, loss = 0.62711868\n",
            "Iteration 28, loss = 0.62597686\n",
            "Iteration 29, loss = 0.62532152\n",
            "Iteration 30, loss = 0.62375944\n",
            "Iteration 31, loss = 0.62339037\n",
            "Iteration 32, loss = 0.62201658\n",
            "Iteration 33, loss = 0.62063693\n",
            "Iteration 34, loss = 0.62001988\n",
            "Iteration 35, loss = 0.61879986\n",
            "Iteration 36, loss = 0.61768337\n",
            "Iteration 37, loss = 0.61694258\n",
            "Iteration 38, loss = 0.61633013\n",
            "Iteration 39, loss = 0.61521404\n",
            "Iteration 40, loss = 0.61564496\n",
            "Iteration 41, loss = 0.61424238\n",
            "Iteration 42, loss = 0.61349775\n",
            "Iteration 43, loss = 0.61259950\n",
            "Iteration 44, loss = 0.61191113\n",
            "Iteration 45, loss = 0.61163845\n",
            "Iteration 46, loss = 0.61096305\n",
            "Iteration 47, loss = 0.61063034\n",
            "Iteration 48, loss = 0.60967328\n",
            "Iteration 49, loss = 0.60970067\n",
            "Iteration 50, loss = 0.60875756\n",
            "Iteration 51, loss = 0.60819839\n",
            "Iteration 52, loss = 0.60837121\n",
            "Iteration 53, loss = 0.60738824\n",
            "Iteration 54, loss = 0.60734294\n",
            "Iteration 55, loss = 0.60891191\n",
            "Iteration 56, loss = 0.60612267\n",
            "Iteration 57, loss = 0.60598478\n",
            "Iteration 58, loss = 0.60580016\n",
            "Iteration 59, loss = 0.60581563\n",
            "Iteration 60, loss = 0.60496953\n",
            "Iteration 61, loss = 0.60490610\n",
            "Iteration 62, loss = 0.60529374\n",
            "Iteration 63, loss = 0.60394217\n",
            "Iteration 64, loss = 0.60365637\n",
            "Iteration 65, loss = 0.60326187\n",
            "Iteration 66, loss = 0.60325200\n",
            "Iteration 67, loss = 0.60266608\n",
            "Iteration 68, loss = 0.60229860\n",
            "Iteration 69, loss = 0.60196061\n",
            "Iteration 70, loss = 0.60192371\n",
            "Iteration 71, loss = 0.60129114\n",
            "Iteration 72, loss = 0.60117799\n",
            "Iteration 73, loss = 0.60095182\n",
            "Iteration 74, loss = 0.60102269\n",
            "Iteration 75, loss = 0.60060628\n",
            "Iteration 76, loss = 0.60148484\n",
            "Iteration 77, loss = 0.60039945\n",
            "Iteration 78, loss = 0.60013009\n",
            "Iteration 79, loss = 0.59990941\n",
            "Iteration 80, loss = 0.59936096\n",
            "Iteration 81, loss = 0.59948801\n",
            "Iteration 82, loss = 0.59928745\n",
            "Iteration 83, loss = 0.59945350\n",
            "Iteration 84, loss = 0.59885712\n",
            "Iteration 85, loss = 0.59867718\n",
            "Iteration 86, loss = 0.59941531\n",
            "Iteration 87, loss = 0.59853973\n",
            "Iteration 88, loss = 0.59806211\n",
            "Iteration 89, loss = 0.59810460\n",
            "Iteration 90, loss = 0.59782015\n",
            "Iteration 91, loss = 0.59799805\n",
            "Iteration 92, loss = 0.59799969\n",
            "Iteration 93, loss = 0.59794711\n",
            "Iteration 94, loss = 0.59716440\n",
            "Iteration 95, loss = 0.59749069\n",
            "Iteration 96, loss = 0.59741600\n",
            "Iteration 97, loss = 0.59716899\n",
            "Iteration 98, loss = 0.59679888\n",
            "Iteration 99, loss = 0.59716169\n",
            "Iteration 100, loss = 0.59734778\n",
            "Iteration 101, loss = 0.59758378\n",
            "Iteration 102, loss = 0.59748861\n",
            "Iteration 103, loss = 0.59682014\n",
            "Iteration 104, loss = 0.59641357\n",
            "Iteration 105, loss = 0.59618930\n",
            "Iteration 106, loss = 0.59642907\n",
            "Iteration 107, loss = 0.59613785\n",
            "Iteration 108, loss = 0.59625257\n",
            "Iteration 109, loss = 0.59635413\n",
            "Iteration 110, loss = 0.59676927\n",
            "Iteration 111, loss = 0.59642556\n",
            "Iteration 112, loss = 0.59634661\n",
            "Iteration 113, loss = 0.59587363\n",
            "Iteration 114, loss = 0.59599271\n",
            "Iteration 115, loss = 0.59586633\n",
            "Iteration 116, loss = 0.59605157\n",
            "Iteration 117, loss = 0.59582205\n",
            "Iteration 118, loss = 0.59625585\n",
            "Iteration 119, loss = 0.59535838\n",
            "Iteration 120, loss = 0.59577291\n",
            "Iteration 121, loss = 0.59526697\n",
            "Iteration 122, loss = 0.59595975\n",
            "Iteration 123, loss = 0.59511108\n",
            "Iteration 124, loss = 0.59522800\n",
            "Iteration 125, loss = 0.59466795\n",
            "Iteration 126, loss = 0.59482907\n",
            "Iteration 127, loss = 0.59500860\n",
            "Iteration 128, loss = 0.59441476\n",
            "Iteration 129, loss = 0.59428571\n",
            "Iteration 130, loss = 0.59474703\n",
            "Iteration 131, loss = 0.59440126\n",
            "Iteration 132, loss = 0.59422856\n",
            "Iteration 133, loss = 0.59404357\n",
            "Iteration 134, loss = 0.59371954\n",
            "Iteration 135, loss = 0.59402182\n",
            "Iteration 136, loss = 0.59501032\n",
            "Iteration 137, loss = 0.59381455\n",
            "Iteration 138, loss = 0.59359936\n",
            "Iteration 139, loss = 0.59340424\n",
            "Iteration 140, loss = 0.59449719\n",
            "Iteration 141, loss = 0.59354984\n",
            "Iteration 142, loss = 0.59301590\n",
            "Iteration 143, loss = 0.59380488\n",
            "Iteration 144, loss = 0.59363743\n",
            "Iteration 145, loss = 0.59295843\n",
            "Iteration 146, loss = 0.59294084\n",
            "Iteration 147, loss = 0.59350187\n",
            "Iteration 148, loss = 0.59298529\n",
            "Iteration 149, loss = 0.59250138\n",
            "Iteration 150, loss = 0.59292917\n",
            "Iteration 151, loss = 0.59283799\n",
            "Iteration 152, loss = 0.59415980\n",
            "Iteration 153, loss = 0.59237474\n",
            "Iteration 154, loss = 0.59237108\n",
            "Iteration 155, loss = 0.59284515\n",
            "Iteration 156, loss = 0.59303140\n",
            "Iteration 157, loss = 0.59237918\n",
            "Iteration 158, loss = 0.59208930\n",
            "Iteration 159, loss = 0.59216672\n",
            "Iteration 160, loss = 0.59226545\n",
            "Iteration 161, loss = 0.59192508\n",
            "Iteration 162, loss = 0.59253764\n",
            "Iteration 163, loss = 0.59210246\n",
            "Iteration 164, loss = 0.59242444\n",
            "Iteration 165, loss = 0.59257990\n",
            "Iteration 166, loss = 0.59256574\n",
            "Iteration 167, loss = 0.59230040\n",
            "Iteration 168, loss = 0.59247090\n",
            "Iteration 169, loss = 0.59127373\n",
            "Iteration 170, loss = 0.59238582\n",
            "Iteration 171, loss = 0.59229222\n",
            "Iteration 172, loss = 0.59172969\n",
            "Iteration 173, loss = 0.59147504\n",
            "Iteration 174, loss = 0.59210614\n",
            "Iteration 175, loss = 0.59154362\n",
            "Iteration 176, loss = 0.59125838\n",
            "Iteration 177, loss = 0.59134247\n",
            "Iteration 178, loss = 0.59195795\n",
            "Iteration 179, loss = 0.59230532\n",
            "Iteration 180, loss = 0.59123381\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70770425\n",
            "Iteration 2, loss = 0.66328638\n",
            "Iteration 3, loss = 0.65969172\n",
            "Iteration 4, loss = 0.65947388\n",
            "Iteration 5, loss = 0.65755263\n",
            "Iteration 6, loss = 0.65682777\n",
            "Iteration 7, loss = 0.65582628\n",
            "Iteration 8, loss = 0.65499646\n",
            "Iteration 9, loss = 0.65372939\n",
            "Iteration 10, loss = 0.65275831\n",
            "Iteration 11, loss = 0.65165437\n",
            "Iteration 12, loss = 0.65069615\n",
            "Iteration 13, loss = 0.64969041\n",
            "Iteration 14, loss = 0.64852214\n",
            "Iteration 15, loss = 0.64768652\n",
            "Iteration 16, loss = 0.64634224\n",
            "Iteration 17, loss = 0.64534014\n",
            "Iteration 18, loss = 0.64431886\n",
            "Iteration 19, loss = 0.64279256\n",
            "Iteration 20, loss = 0.64190310\n",
            "Iteration 21, loss = 0.64090678\n",
            "Iteration 22, loss = 0.63958830\n",
            "Iteration 23, loss = 0.63894856\n",
            "Iteration 24, loss = 0.63746641\n",
            "Iteration 25, loss = 0.63648540\n",
            "Iteration 26, loss = 0.63561732\n",
            "Iteration 27, loss = 0.63470012\n",
            "Iteration 28, loss = 0.63363793\n",
            "Iteration 29, loss = 0.63347635\n",
            "Iteration 30, loss = 0.63217623\n",
            "Iteration 31, loss = 0.63131788\n",
            "Iteration 32, loss = 0.63026429\n",
            "Iteration 33, loss = 0.62964722\n",
            "Iteration 34, loss = 0.62901014\n",
            "Iteration 35, loss = 0.62831247\n",
            "Iteration 36, loss = 0.62774563\n",
            "Iteration 37, loss = 0.62718064\n",
            "Iteration 38, loss = 0.62613868\n",
            "Iteration 39, loss = 0.62539499\n",
            "Iteration 40, loss = 0.62502753\n",
            "Iteration 41, loss = 0.62487931\n",
            "Iteration 42, loss = 0.62383670\n",
            "Iteration 43, loss = 0.62377692\n",
            "Iteration 44, loss = 0.62303761\n",
            "Iteration 45, loss = 0.62261164\n",
            "Iteration 46, loss = 0.62242349\n",
            "Iteration 47, loss = 0.62340330\n",
            "Iteration 48, loss = 0.62229737\n",
            "Iteration 49, loss = 0.62126767\n",
            "Iteration 50, loss = 0.62091078\n",
            "Iteration 51, loss = 0.62006832\n",
            "Iteration 52, loss = 0.61978495\n",
            "Iteration 53, loss = 0.61967621\n",
            "Iteration 54, loss = 0.62011101\n",
            "Iteration 55, loss = 0.61904512\n",
            "Iteration 56, loss = 0.61889806\n",
            "Iteration 57, loss = 0.61859195\n",
            "Iteration 58, loss = 0.61902244\n",
            "Iteration 59, loss = 0.61886138\n",
            "Iteration 60, loss = 0.61755843\n",
            "Iteration 61, loss = 0.61757268\n",
            "Iteration 62, loss = 0.61738016\n",
            "Iteration 63, loss = 0.61733940\n",
            "Iteration 64, loss = 0.61737142\n",
            "Iteration 65, loss = 0.61672367\n",
            "Iteration 66, loss = 0.61676496\n",
            "Iteration 67, loss = 0.61606051\n",
            "Iteration 68, loss = 0.61616281\n",
            "Iteration 69, loss = 0.61595221\n",
            "Iteration 70, loss = 0.61602022\n",
            "Iteration 71, loss = 0.61494721\n",
            "Iteration 72, loss = 0.61530440\n",
            "Iteration 73, loss = 0.61475850\n",
            "Iteration 74, loss = 0.61494484\n",
            "Iteration 75, loss = 0.61458384\n",
            "Iteration 76, loss = 0.61407753\n",
            "Iteration 77, loss = 0.61432859\n",
            "Iteration 78, loss = 0.61378211\n",
            "Iteration 79, loss = 0.61351549\n",
            "Iteration 80, loss = 0.61311756\n",
            "Iteration 81, loss = 0.61348083\n",
            "Iteration 82, loss = 0.61307948\n",
            "Iteration 83, loss = 0.61310692\n",
            "Iteration 84, loss = 0.61273943\n",
            "Iteration 85, loss = 0.61285105\n",
            "Iteration 86, loss = 0.61237005\n",
            "Iteration 87, loss = 0.61341281\n",
            "Iteration 88, loss = 0.61257028\n",
            "Iteration 89, loss = 0.61230453\n",
            "Iteration 90, loss = 0.61225219\n",
            "Iteration 91, loss = 0.61183988\n",
            "Iteration 92, loss = 0.61219874\n",
            "Iteration 93, loss = 0.61245453\n",
            "Iteration 94, loss = 0.61193845\n",
            "Iteration 95, loss = 0.61165381\n",
            "Iteration 96, loss = 0.61098799\n",
            "Iteration 97, loss = 0.61093143\n",
            "Iteration 98, loss = 0.61114058\n",
            "Iteration 99, loss = 0.61085790\n",
            "Iteration 100, loss = 0.61155493\n",
            "Iteration 101, loss = 0.61058686\n",
            "Iteration 102, loss = 0.61036851\n",
            "Iteration 103, loss = 0.61032095\n",
            "Iteration 104, loss = 0.61030175\n",
            "Iteration 105, loss = 0.61018215\n",
            "Iteration 106, loss = 0.60941663\n",
            "Iteration 107, loss = 0.61190021\n",
            "Iteration 108, loss = 0.60991310\n",
            "Iteration 109, loss = 0.60990180\n",
            "Iteration 110, loss = 0.60981728\n",
            "Iteration 111, loss = 0.60946066\n",
            "Iteration 112, loss = 0.60901085\n",
            "Iteration 113, loss = 0.60919296\n",
            "Iteration 114, loss = 0.60915423\n",
            "Iteration 115, loss = 0.60892807\n",
            "Iteration 116, loss = 0.60950164\n",
            "Iteration 117, loss = 0.60839400\n",
            "Iteration 118, loss = 0.60820935\n",
            "Iteration 119, loss = 0.60859543\n",
            "Iteration 120, loss = 0.60782572\n",
            "Iteration 121, loss = 0.60807571\n",
            "Iteration 122, loss = 0.60763987\n",
            "Iteration 123, loss = 0.60757101\n",
            "Iteration 124, loss = 0.60805643\n",
            "Iteration 125, loss = 0.60796823\n",
            "Iteration 126, loss = 0.60747232\n",
            "Iteration 127, loss = 0.60700352\n",
            "Iteration 128, loss = 0.60733262\n",
            "Iteration 129, loss = 0.60671785\n",
            "Iteration 130, loss = 0.60696380\n",
            "Iteration 131, loss = 0.60646791\n",
            "Iteration 132, loss = 0.60704052\n",
            "Iteration 133, loss = 0.60633321\n",
            "Iteration 134, loss = 0.60634265\n",
            "Iteration 135, loss = 0.60599724\n",
            "Iteration 136, loss = 0.60576633\n",
            "Iteration 137, loss = 0.60663511\n",
            "Iteration 138, loss = 0.60514471\n",
            "Iteration 139, loss = 0.60553845\n",
            "Iteration 140, loss = 0.60685003\n",
            "Iteration 141, loss = 0.60485204\n",
            "Iteration 142, loss = 0.60496514\n",
            "Iteration 143, loss = 0.60609836\n",
            "Iteration 144, loss = 0.60581192\n",
            "Iteration 145, loss = 0.60558995\n",
            "Iteration 146, loss = 0.60448659\n",
            "Iteration 147, loss = 0.60413086\n",
            "Iteration 148, loss = 0.60444946\n",
            "Iteration 149, loss = 0.60453292\n",
            "Iteration 150, loss = 0.60344423\n",
            "Iteration 151, loss = 0.60418682\n",
            "Iteration 152, loss = 0.60414238\n",
            "Iteration 153, loss = 0.60451862\n",
            "Iteration 154, loss = 0.60350746\n",
            "Iteration 155, loss = 0.60356548\n",
            "Iteration 156, loss = 0.60343032\n",
            "Iteration 157, loss = 0.60375783\n",
            "Iteration 158, loss = 0.60245508\n",
            "Iteration 159, loss = 0.60274000\n",
            "Iteration 160, loss = 0.60272337\n",
            "Iteration 161, loss = 0.60364131\n",
            "Iteration 162, loss = 0.60239163\n",
            "Iteration 163, loss = 0.60211024\n",
            "Iteration 164, loss = 0.60237633\n",
            "Iteration 165, loss = 0.60259236\n",
            "Iteration 166, loss = 0.60195660\n",
            "Iteration 167, loss = 0.60157676\n",
            "Iteration 168, loss = 0.60185620\n",
            "Iteration 169, loss = 0.60167352\n",
            "Iteration 170, loss = 0.60059219\n",
            "Iteration 171, loss = 0.60073631\n",
            "Iteration 172, loss = 0.60059760\n",
            "Iteration 173, loss = 0.60164991\n",
            "Iteration 174, loss = 0.60175736\n",
            "Iteration 175, loss = 0.60056353\n",
            "Iteration 176, loss = 0.60035308\n",
            "Iteration 177, loss = 0.60056734\n",
            "Iteration 178, loss = 0.60008227\n",
            "Iteration 179, loss = 0.59945391\n",
            "Iteration 180, loss = 0.59956009\n",
            "Iteration 181, loss = 0.59964209\n",
            "Iteration 182, loss = 0.60153589\n",
            "Iteration 183, loss = 0.59845629\n",
            "Iteration 184, loss = 0.59967616\n",
            "Iteration 185, loss = 0.59907950\n",
            "Iteration 186, loss = 0.59843796\n",
            "Iteration 187, loss = 0.59808354\n",
            "Iteration 188, loss = 0.59768901\n",
            "Iteration 189, loss = 0.59820808\n",
            "Iteration 190, loss = 0.59757527\n",
            "Iteration 191, loss = 0.59768586\n",
            "Iteration 192, loss = 0.59800113\n",
            "Iteration 193, loss = 0.59878865\n",
            "Iteration 194, loss = 0.59777290\n",
            "Iteration 195, loss = 0.59775267\n",
            "Iteration 196, loss = 0.59711325\n",
            "Iteration 197, loss = 0.59637590\n",
            "Iteration 198, loss = 0.59786392\n",
            "Iteration 199, loss = 0.59564931\n",
            "Iteration 200, loss = 0.59525604\n",
            "Iteration 1, loss = 0.72426089\n",
            "Iteration 2, loss = 0.66210564\n",
            "Iteration 3, loss = 0.65664249\n",
            "Iteration 4, loss = 0.65516141\n",
            "Iteration 5, loss = 0.65379479\n",
            "Iteration 6, loss = 0.65272799\n",
            "Iteration 7, loss = 0.65156878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8, loss = 0.65041857\n",
            "Iteration 9, loss = 0.64945969\n",
            "Iteration 10, loss = 0.64867933\n",
            "Iteration 11, loss = 0.64737032\n",
            "Iteration 12, loss = 0.64611572\n",
            "Iteration 13, loss = 0.64523708\n",
            "Iteration 14, loss = 0.64419191\n",
            "Iteration 15, loss = 0.64369994\n",
            "Iteration 16, loss = 0.64221319\n",
            "Iteration 17, loss = 0.64123518\n",
            "Iteration 18, loss = 0.63981036\n",
            "Iteration 19, loss = 0.63867856\n",
            "Iteration 20, loss = 0.63751055\n",
            "Iteration 21, loss = 0.63606047\n",
            "Iteration 22, loss = 0.63470565\n",
            "Iteration 23, loss = 0.63393190\n",
            "Iteration 24, loss = 0.63305081\n",
            "Iteration 25, loss = 0.63160185\n",
            "Iteration 26, loss = 0.63110573\n",
            "Iteration 27, loss = 0.62911917\n",
            "Iteration 28, loss = 0.62832643\n",
            "Iteration 29, loss = 0.62707036\n",
            "Iteration 30, loss = 0.62639733\n",
            "Iteration 31, loss = 0.62493478\n",
            "Iteration 32, loss = 0.62420522\n",
            "Iteration 33, loss = 0.62285561\n",
            "Iteration 34, loss = 0.62166898\n",
            "Iteration 35, loss = 0.62067361\n",
            "Iteration 36, loss = 0.61974566\n",
            "Iteration 37, loss = 0.61883219\n",
            "Iteration 38, loss = 0.61815889\n",
            "Iteration 39, loss = 0.61727458\n",
            "Iteration 40, loss = 0.61647152\n",
            "Iteration 41, loss = 0.61528454\n",
            "Iteration 42, loss = 0.61448462\n",
            "Iteration 43, loss = 0.61430264\n",
            "Iteration 44, loss = 0.61340861\n",
            "Iteration 45, loss = 0.61249736\n",
            "Iteration 46, loss = 0.61185895\n",
            "Iteration 47, loss = 0.61128103\n",
            "Iteration 48, loss = 0.61124742\n",
            "Iteration 49, loss = 0.61005965\n",
            "Iteration 50, loss = 0.60903689\n",
            "Iteration 51, loss = 0.60881943\n",
            "Iteration 52, loss = 0.60818828\n",
            "Iteration 53, loss = 0.60794371\n",
            "Iteration 54, loss = 0.60812397\n",
            "Iteration 55, loss = 0.60639480\n",
            "Iteration 56, loss = 0.60586798\n",
            "Iteration 57, loss = 0.60532609\n",
            "Iteration 58, loss = 0.60514781\n",
            "Iteration 59, loss = 0.60473557\n",
            "Iteration 60, loss = 0.60438615\n",
            "Iteration 61, loss = 0.60463567\n",
            "Iteration 62, loss = 0.60343152\n",
            "Iteration 63, loss = 0.60332055\n",
            "Iteration 64, loss = 0.60296306\n",
            "Iteration 65, loss = 0.60249802\n",
            "Iteration 66, loss = 0.60226373\n",
            "Iteration 67, loss = 0.60197777\n",
            "Iteration 68, loss = 0.60115042\n",
            "Iteration 69, loss = 0.60137274\n",
            "Iteration 70, loss = 0.60088389\n",
            "Iteration 71, loss = 0.60060953\n",
            "Iteration 72, loss = 0.60053593\n",
            "Iteration 73, loss = 0.60017045\n",
            "Iteration 74, loss = 0.60045537\n",
            "Iteration 75, loss = 0.60094343\n",
            "Iteration 76, loss = 0.60036835\n",
            "Iteration 77, loss = 0.59890821\n",
            "Iteration 78, loss = 0.59893444\n",
            "Iteration 79, loss = 0.59972478\n",
            "Iteration 80, loss = 0.59837843\n",
            "Iteration 81, loss = 0.59898151\n",
            "Iteration 82, loss = 0.59841755\n",
            "Iteration 83, loss = 0.59802647\n",
            "Iteration 84, loss = 0.59786294\n",
            "Iteration 85, loss = 0.59744766\n",
            "Iteration 86, loss = 0.59752526\n",
            "Iteration 87, loss = 0.59775057\n",
            "Iteration 88, loss = 0.59696492\n",
            "Iteration 89, loss = 0.59721873\n",
            "Iteration 90, loss = 0.59679599\n",
            "Iteration 91, loss = 0.59661075\n",
            "Iteration 92, loss = 0.59647162\n",
            "Iteration 93, loss = 0.59634450\n",
            "Iteration 94, loss = 0.59606574\n",
            "Iteration 95, loss = 0.59579302\n",
            "Iteration 96, loss = 0.59617650\n",
            "Iteration 97, loss = 0.59604469\n",
            "Iteration 98, loss = 0.59552281\n",
            "Iteration 99, loss = 0.59546775\n",
            "Iteration 100, loss = 0.59543302\n",
            "Iteration 101, loss = 0.59508653\n",
            "Iteration 102, loss = 0.59573993\n",
            "Iteration 103, loss = 0.59510405\n",
            "Iteration 104, loss = 0.59428116\n",
            "Iteration 105, loss = 0.59468489\n",
            "Iteration 106, loss = 0.59417706\n",
            "Iteration 107, loss = 0.59413807\n",
            "Iteration 108, loss = 0.59405364\n",
            "Iteration 109, loss = 0.59420234\n",
            "Iteration 110, loss = 0.59382654\n",
            "Iteration 111, loss = 0.59443357\n",
            "Iteration 112, loss = 0.59416199\n",
            "Iteration 113, loss = 0.59395516\n",
            "Iteration 114, loss = 0.59330504\n",
            "Iteration 115, loss = 0.59423685\n",
            "Iteration 116, loss = 0.59293574\n",
            "Iteration 117, loss = 0.59455255\n",
            "Iteration 118, loss = 0.59275001\n",
            "Iteration 119, loss = 0.59300101\n",
            "Iteration 120, loss = 0.59254060\n",
            "Iteration 121, loss = 0.59266459\n",
            "Iteration 122, loss = 0.59244105\n",
            "Iteration 123, loss = 0.59234581\n",
            "Iteration 124, loss = 0.59219802\n",
            "Iteration 125, loss = 0.59209597\n",
            "Iteration 126, loss = 0.59252253\n",
            "Iteration 127, loss = 0.59329893\n",
            "Iteration 128, loss = 0.59143580\n",
            "Iteration 129, loss = 0.59159357\n",
            "Iteration 130, loss = 0.59241702\n",
            "Iteration 131, loss = 0.59130491\n",
            "Iteration 132, loss = 0.59116948\n",
            "Iteration 133, loss = 0.59140940\n",
            "Iteration 134, loss = 0.59105501\n",
            "Iteration 135, loss = 0.59127637\n",
            "Iteration 136, loss = 0.59069527\n",
            "Iteration 137, loss = 0.59058310\n",
            "Iteration 138, loss = 0.59085614\n",
            "Iteration 139, loss = 0.59126629\n",
            "Iteration 140, loss = 0.59031340\n",
            "Iteration 141, loss = 0.58999017\n",
            "Iteration 142, loss = 0.59047163\n",
            "Iteration 143, loss = 0.58984681\n",
            "Iteration 144, loss = 0.59011485\n",
            "Iteration 145, loss = 0.59132603\n",
            "Iteration 146, loss = 0.59024022\n",
            "Iteration 147, loss = 0.58963055\n",
            "Iteration 148, loss = 0.58949606\n",
            "Iteration 149, loss = 0.58938969\n",
            "Iteration 150, loss = 0.58942356\n",
            "Iteration 151, loss = 0.59042711\n",
            "Iteration 152, loss = 0.58901309\n",
            "Iteration 153, loss = 0.58901192\n",
            "Iteration 154, loss = 0.58920085\n",
            "Iteration 155, loss = 0.58898737\n",
            "Iteration 156, loss = 0.58905959\n",
            "Iteration 157, loss = 0.58869067\n",
            "Iteration 158, loss = 0.58906475\n",
            "Iteration 159, loss = 0.58883960\n",
            "Iteration 160, loss = 0.58900923\n",
            "Iteration 161, loss = 0.58804983\n",
            "Iteration 162, loss = 0.58869788\n",
            "Iteration 163, loss = 0.58920147\n",
            "Iteration 164, loss = 0.58822675\n",
            "Iteration 165, loss = 0.58879439\n",
            "Iteration 166, loss = 0.58923863\n",
            "Iteration 167, loss = 0.58842710\n",
            "Iteration 168, loss = 0.58846328\n",
            "Iteration 169, loss = 0.58822682\n",
            "Iteration 170, loss = 0.58688519\n",
            "Iteration 171, loss = 0.58868890\n",
            "Iteration 172, loss = 0.58756048\n",
            "Iteration 173, loss = 0.58812983\n",
            "Iteration 174, loss = 0.58735626\n",
            "Iteration 175, loss = 0.58811962\n",
            "Iteration 176, loss = 0.58678604\n",
            "Iteration 177, loss = 0.58685717\n",
            "Iteration 178, loss = 0.58707288\n",
            "Iteration 179, loss = 0.58629570\n",
            "Iteration 180, loss = 0.58735519\n",
            "Iteration 181, loss = 0.58682436\n",
            "Iteration 182, loss = 0.58837630\n",
            "Iteration 183, loss = 0.58596341\n",
            "Iteration 184, loss = 0.58571754\n",
            "Iteration 185, loss = 0.58638148\n",
            "Iteration 186, loss = 0.58516681\n",
            "Iteration 187, loss = 0.58529472\n",
            "Iteration 188, loss = 0.58649457\n",
            "Iteration 189, loss = 0.58564372\n",
            "Iteration 190, loss = 0.58716152\n",
            "Iteration 191, loss = 0.58495637\n",
            "Iteration 192, loss = 0.58408793\n",
            "Iteration 193, loss = 0.58413105\n",
            "Iteration 194, loss = 0.58458571\n",
            "Iteration 195, loss = 0.58580043\n",
            "Iteration 196, loss = 0.58369905\n",
            "Iteration 197, loss = 0.58309716\n",
            "Iteration 198, loss = 0.58498668\n",
            "Iteration 199, loss = 0.58325691\n",
            "Iteration 200, loss = 0.58309925\n",
            "Iteration 1, loss = 0.66258350\n",
            "Iteration 2, loss = 0.66000121\n",
            "Iteration 3, loss = 0.65959807\n",
            "Iteration 4, loss = 0.65908481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5, loss = 0.65818163\n",
            "Iteration 6, loss = 0.65854782\n",
            "Iteration 7, loss = 0.65727687\n",
            "Iteration 8, loss = 0.65613139\n",
            "Iteration 9, loss = 0.65468817\n",
            "Iteration 10, loss = 0.65417374\n",
            "Iteration 11, loss = 0.65310071\n",
            "Iteration 12, loss = 0.65233072\n",
            "Iteration 13, loss = 0.65097677\n",
            "Iteration 14, loss = 0.65033943\n",
            "Iteration 15, loss = 0.64953254\n",
            "Iteration 16, loss = 0.64790998\n",
            "Iteration 17, loss = 0.64695076\n",
            "Iteration 18, loss = 0.64590740\n",
            "Iteration 19, loss = 0.64487729\n",
            "Iteration 20, loss = 0.64317511\n",
            "Iteration 21, loss = 0.64234780\n",
            "Iteration 22, loss = 0.64111510\n",
            "Iteration 23, loss = 0.63987569\n",
            "Iteration 24, loss = 0.63850609\n",
            "Iteration 25, loss = 0.63768152\n",
            "Iteration 26, loss = 0.63618687\n",
            "Iteration 27, loss = 0.63539730\n",
            "Iteration 28, loss = 0.63397614\n",
            "Iteration 29, loss = 0.63303142\n",
            "Iteration 30, loss = 0.63177396\n",
            "Iteration 31, loss = 0.63065783\n",
            "Iteration 32, loss = 0.63014196\n",
            "Iteration 33, loss = 0.62860236\n",
            "Iteration 34, loss = 0.62793415\n",
            "Iteration 35, loss = 0.62648951\n",
            "Iteration 36, loss = 0.62622899\n",
            "Iteration 37, loss = 0.62503627\n",
            "Iteration 38, loss = 0.62362596\n",
            "Iteration 39, loss = 0.62320483\n",
            "Iteration 40, loss = 0.62208088\n",
            "Iteration 41, loss = 0.62112679\n",
            "Iteration 42, loss = 0.62052512\n",
            "Iteration 43, loss = 0.62010952\n",
            "Iteration 44, loss = 0.61915488\n",
            "Iteration 45, loss = 0.61838267\n",
            "Iteration 46, loss = 0.61812746\n",
            "Iteration 47, loss = 0.61704492\n",
            "Iteration 48, loss = 0.61650527\n",
            "Iteration 49, loss = 0.61562372\n",
            "Iteration 50, loss = 0.61521358\n",
            "Iteration 51, loss = 0.61465068\n",
            "Iteration 52, loss = 0.61467764\n",
            "Iteration 53, loss = 0.61326082\n",
            "Iteration 54, loss = 0.61299383\n",
            "Iteration 55, loss = 0.61280189\n",
            "Iteration 56, loss = 0.61202759\n",
            "Iteration 57, loss = 0.61166224\n",
            "Iteration 58, loss = 0.61129906\n",
            "Iteration 59, loss = 0.61059587\n",
            "Iteration 60, loss = 0.61005513\n",
            "Iteration 61, loss = 0.60985037\n",
            "Iteration 62, loss = 0.60941379\n",
            "Iteration 63, loss = 0.60929117\n",
            "Iteration 64, loss = 0.60867905\n",
            "Iteration 65, loss = 0.60902991\n",
            "Iteration 66, loss = 0.60805716\n",
            "Iteration 67, loss = 0.60778082\n",
            "Iteration 68, loss = 0.60783077\n",
            "Iteration 69, loss = 0.60801826\n",
            "Iteration 70, loss = 0.60740471\n",
            "Iteration 71, loss = 0.60698446\n",
            "Iteration 72, loss = 0.60661422\n",
            "Iteration 73, loss = 0.60681983\n",
            "Iteration 74, loss = 0.60608139\n",
            "Iteration 75, loss = 0.60602104\n",
            "Iteration 76, loss = 0.60567164\n",
            "Iteration 77, loss = 0.60538698\n",
            "Iteration 78, loss = 0.60553805\n",
            "Iteration 79, loss = 0.60592800\n",
            "Iteration 80, loss = 0.60479922\n",
            "Iteration 81, loss = 0.60445474\n",
            "Iteration 82, loss = 0.60492596\n",
            "Iteration 83, loss = 0.60431988\n",
            "Iteration 84, loss = 0.60401137\n",
            "Iteration 85, loss = 0.60363568\n",
            "Iteration 86, loss = 0.60375379\n",
            "Iteration 87, loss = 0.60351245\n",
            "Iteration 88, loss = 0.60350121\n",
            "Iteration 89, loss = 0.60327075\n",
            "Iteration 90, loss = 0.60365465\n",
            "Iteration 91, loss = 0.60385662\n",
            "Iteration 92, loss = 0.60294168\n",
            "Iteration 93, loss = 0.60300800\n",
            "Iteration 94, loss = 0.60269421\n",
            "Iteration 95, loss = 0.60294415\n",
            "Iteration 96, loss = 0.60230415\n",
            "Iteration 97, loss = 0.60266582\n",
            "Iteration 98, loss = 0.60300067\n",
            "Iteration 99, loss = 0.60195429\n",
            "Iteration 100, loss = 0.60245006\n",
            "Iteration 101, loss = 0.60172634\n",
            "Iteration 102, loss = 0.60168769\n",
            "Iteration 103, loss = 0.60252640\n",
            "Iteration 104, loss = 0.60154019\n",
            "Iteration 105, loss = 0.60138827\n",
            "Iteration 106, loss = 0.60161344\n",
            "Iteration 107, loss = 0.60123248\n",
            "Iteration 108, loss = 0.60161545\n",
            "Iteration 109, loss = 0.60159988\n",
            "Iteration 110, loss = 0.60080139\n",
            "Iteration 111, loss = 0.60136667\n",
            "Iteration 112, loss = 0.60024588\n",
            "Iteration 113, loss = 0.60127932\n",
            "Iteration 114, loss = 0.60081101\n",
            "Iteration 115, loss = 0.60068358\n",
            "Iteration 116, loss = 0.60023792\n",
            "Iteration 117, loss = 0.60029746\n",
            "Iteration 118, loss = 0.60008374\n",
            "Iteration 119, loss = 0.60005649\n",
            "Iteration 120, loss = 0.60037082\n",
            "Iteration 121, loss = 0.60061609\n",
            "Iteration 122, loss = 0.59994355\n",
            "Iteration 123, loss = 0.59989105\n",
            "Iteration 124, loss = 0.60063831\n",
            "Iteration 125, loss = 0.59948108\n",
            "Iteration 126, loss = 0.60019854\n",
            "Iteration 127, loss = 0.59962165\n",
            "Iteration 128, loss = 0.60042151\n",
            "Iteration 129, loss = 0.60035379\n",
            "Iteration 130, loss = 0.59991098\n",
            "Iteration 131, loss = 0.59934027\n",
            "Iteration 132, loss = 0.59965128\n",
            "Iteration 133, loss = 0.59969157\n",
            "Iteration 134, loss = 0.60069112\n",
            "Iteration 135, loss = 0.59949916\n",
            "Iteration 136, loss = 0.59944631\n",
            "Iteration 137, loss = 0.59884811\n",
            "Iteration 138, loss = 0.59931951\n",
            "Iteration 139, loss = 0.59914803\n",
            "Iteration 140, loss = 0.59880999\n",
            "Iteration 141, loss = 0.59890906\n",
            "Iteration 142, loss = 0.59846772\n",
            "Iteration 143, loss = 0.59844078\n",
            "Iteration 144, loss = 0.59862126\n",
            "Iteration 145, loss = 0.59952052\n",
            "Iteration 146, loss = 0.59851697\n",
            "Iteration 147, loss = 0.59880425\n",
            "Iteration 148, loss = 0.59977504\n",
            "Iteration 149, loss = 0.59834496\n",
            "Iteration 150, loss = 0.59859613\n",
            "Iteration 151, loss = 0.59835144\n",
            "Iteration 152, loss = 0.59803282\n",
            "Iteration 153, loss = 0.59883555\n",
            "Iteration 154, loss = 0.59899264\n",
            "Iteration 155, loss = 0.59784036\n",
            "Iteration 156, loss = 0.59799951\n",
            "Iteration 157, loss = 0.59776491\n",
            "Iteration 158, loss = 0.59820257\n",
            "Iteration 159, loss = 0.59888539\n",
            "Iteration 160, loss = 0.59818255\n",
            "Iteration 161, loss = 0.59811677\n",
            "Iteration 162, loss = 0.59724903\n",
            "Iteration 163, loss = 0.59726565\n",
            "Iteration 164, loss = 0.59748025\n",
            "Iteration 165, loss = 0.59723636\n",
            "Iteration 166, loss = 0.59817974\n",
            "Iteration 167, loss = 0.59748684\n",
            "Iteration 168, loss = 0.59757475\n",
            "Iteration 169, loss = 0.59748665\n",
            "Iteration 170, loss = 0.59738139\n",
            "Iteration 171, loss = 0.59676883\n",
            "Iteration 172, loss = 0.59720932\n",
            "Iteration 173, loss = 0.59792221\n",
            "Iteration 174, loss = 0.59781598\n",
            "Iteration 175, loss = 0.59736717\n",
            "Iteration 176, loss = 0.59705386\n",
            "Iteration 177, loss = 0.59637823\n",
            "Iteration 178, loss = 0.59695304\n",
            "Iteration 179, loss = 0.59636421\n",
            "Iteration 180, loss = 0.59652175\n",
            "Iteration 181, loss = 0.59632293\n",
            "Iteration 182, loss = 0.59618748\n",
            "Iteration 183, loss = 0.59759799\n",
            "Iteration 184, loss = 0.59689264\n",
            "Iteration 185, loss = 0.59688223\n",
            "Iteration 186, loss = 0.59641643\n",
            "Iteration 187, loss = 0.59638391\n",
            "Iteration 188, loss = 0.59628746\n",
            "Iteration 189, loss = 0.59577701\n",
            "Iteration 190, loss = 0.59607897\n",
            "Iteration 191, loss = 0.59636018\n",
            "Iteration 192, loss = 0.59563632\n",
            "Iteration 193, loss = 0.59582431\n",
            "Iteration 194, loss = 0.59555061\n",
            "Iteration 195, loss = 0.59565486\n",
            "Iteration 196, loss = 0.59544389\n",
            "Iteration 197, loss = 0.59545782\n",
            "Iteration 198, loss = 0.59628375\n",
            "Iteration 199, loss = 0.59530842\n",
            "Iteration 200, loss = 0.59523046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.68591689\n",
            "Iteration 2, loss = 0.62442434\n",
            "Iteration 3, loss = 0.66427947\n",
            "Iteration 4, loss = 0.70485928\n",
            "Iteration 5, loss = 0.61777603\n",
            "Iteration 6, loss = 0.59678129\n",
            "Iteration 7, loss = 0.57290347\n",
            "Iteration 8, loss = 0.58359727\n",
            "Iteration 9, loss = 0.53542398\n",
            "Iteration 10, loss = 0.56652654\n",
            "Iteration 11, loss = 0.53133477\n",
            "Iteration 12, loss = 0.51243388\n",
            "Iteration 13, loss = 0.54789652\n",
            "Iteration 14, loss = 0.53584770\n",
            "Iteration 15, loss = 0.62275981\n",
            "Iteration 16, loss = 0.51188872\n",
            "Iteration 17, loss = 0.48040355\n",
            "Iteration 18, loss = 0.47281633\n",
            "Iteration 19, loss = 0.45387819\n",
            "Iteration 20, loss = 0.50117305\n",
            "Iteration 21, loss = 0.48677203\n",
            "Iteration 22, loss = 0.48218627\n",
            "Iteration 23, loss = 0.47523228\n",
            "Iteration 24, loss = 0.50639700\n",
            "Iteration 25, loss = 0.49424529\n",
            "Iteration 26, loss = 0.50159964\n",
            "Iteration 27, loss = 0.47932756\n",
            "Iteration 28, loss = 0.46284905\n",
            "Iteration 29, loss = 0.45115144\n",
            "Iteration 30, loss = 0.46658864\n",
            "Iteration 31, loss = 0.45564629\n",
            "Iteration 32, loss = 0.44291151\n",
            "Iteration 33, loss = 0.53328918\n",
            "Iteration 34, loss = 0.44949498\n",
            "Iteration 35, loss = 0.45393278\n",
            "Iteration 36, loss = 0.48829278\n",
            "Iteration 37, loss = 0.45623326\n",
            "Iteration 38, loss = 0.42617185\n",
            "Iteration 39, loss = 0.44167975\n",
            "Iteration 40, loss = 0.42803755\n",
            "Iteration 41, loss = 0.42869663\n",
            "Iteration 42, loss = 0.44455720\n",
            "Iteration 43, loss = 0.42833308\n",
            "Iteration 44, loss = 0.46949482\n",
            "Iteration 45, loss = 0.43480532\n",
            "Iteration 46, loss = 0.45903593\n",
            "Iteration 47, loss = 0.46671955\n",
            "Iteration 48, loss = 0.43699183\n",
            "Iteration 49, loss = 0.43194217\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.00057317\n",
            "Iteration 2, loss = 0.67888180\n",
            "Iteration 3, loss = 0.76927865\n",
            "Iteration 4, loss = 0.71492434\n",
            "Iteration 5, loss = 0.61973670\n",
            "Iteration 6, loss = 0.72063951\n",
            "Iteration 7, loss = 0.58319430\n",
            "Iteration 8, loss = 0.60051440\n",
            "Iteration 9, loss = 0.52919755\n",
            "Iteration 10, loss = 0.67582027\n",
            "Iteration 11, loss = 0.57330220\n",
            "Iteration 12, loss = 0.51971174\n",
            "Iteration 13, loss = 0.51176396\n",
            "Iteration 14, loss = 0.50241920\n",
            "Iteration 15, loss = 0.53401842\n",
            "Iteration 16, loss = 0.52932362\n",
            "Iteration 17, loss = 0.55177463\n",
            "Iteration 18, loss = 0.56082957\n",
            "Iteration 19, loss = 0.56718286\n",
            "Iteration 20, loss = 0.51645413\n",
            "Iteration 21, loss = 0.48130841\n",
            "Iteration 22, loss = 0.47197599\n",
            "Iteration 23, loss = 0.48653183\n",
            "Iteration 24, loss = 0.47565374\n",
            "Iteration 25, loss = 0.48046359\n",
            "Iteration 26, loss = 0.45225568\n",
            "Iteration 27, loss = 0.48921995\n",
            "Iteration 28, loss = 0.44493122\n",
            "Iteration 29, loss = 0.47860877\n",
            "Iteration 30, loss = 0.47369524\n",
            "Iteration 31, loss = 0.50744446\n",
            "Iteration 32, loss = 0.48168396\n",
            "Iteration 33, loss = 0.48602544\n",
            "Iteration 34, loss = 0.45201451\n",
            "Iteration 35, loss = 0.46253367\n",
            "Iteration 36, loss = 0.43756366\n",
            "Iteration 37, loss = 0.46175133\n",
            "Iteration 38, loss = 0.44634634\n",
            "Iteration 39, loss = 0.45832727\n",
            "Iteration 40, loss = 0.46112732\n",
            "Iteration 41, loss = 0.47813881\n",
            "Iteration 42, loss = 0.46642687\n",
            "Iteration 43, loss = 0.43899959\n",
            "Iteration 44, loss = 0.43115646\n",
            "Iteration 45, loss = 0.42095996\n",
            "Iteration 46, loss = 0.44742146\n",
            "Iteration 47, loss = 0.43433700\n",
            "Iteration 48, loss = 0.45717223\n",
            "Iteration 49, loss = 0.45864467\n",
            "Iteration 50, loss = 0.43367317\n",
            "Iteration 51, loss = 0.45486317\n",
            "Iteration 52, loss = 0.43473906\n",
            "Iteration 53, loss = 0.44182535\n",
            "Iteration 54, loss = 0.43416706\n",
            "Iteration 55, loss = 0.42225527\n",
            "Iteration 56, loss = 0.44334922\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.77203927\n",
            "Iteration 2, loss = 0.60805212\n",
            "Iteration 3, loss = 0.65049971\n",
            "Iteration 4, loss = 0.58936502\n",
            "Iteration 5, loss = 0.62340124\n",
            "Iteration 6, loss = 0.65572225\n",
            "Iteration 7, loss = 0.57531671\n",
            "Iteration 8, loss = 0.52647153\n",
            "Iteration 9, loss = 0.56476943\n",
            "Iteration 10, loss = 0.64840492\n",
            "Iteration 11, loss = 0.51597832\n",
            "Iteration 12, loss = 0.49312945\n",
            "Iteration 13, loss = 0.49996092\n",
            "Iteration 14, loss = 0.63372738\n",
            "Iteration 15, loss = 0.57707510\n",
            "Iteration 16, loss = 0.58063774\n",
            "Iteration 17, loss = 0.48216326\n",
            "Iteration 18, loss = 0.58551717\n",
            "Iteration 19, loss = 0.50009953\n",
            "Iteration 20, loss = 0.49131302\n",
            "Iteration 21, loss = 0.48414824\n",
            "Iteration 22, loss = 0.49456890\n",
            "Iteration 23, loss = 0.46804419\n",
            "Iteration 24, loss = 0.48697773\n",
            "Iteration 25, loss = 0.46900709\n",
            "Iteration 26, loss = 0.53325203\n",
            "Iteration 27, loss = 0.49455374\n",
            "Iteration 28, loss = 0.48009031\n",
            "Iteration 29, loss = 0.45873217\n",
            "Iteration 30, loss = 0.48263849\n",
            "Iteration 31, loss = 0.48713475\n",
            "Iteration 32, loss = 0.46451888\n",
            "Iteration 33, loss = 0.49806724\n",
            "Iteration 34, loss = 0.46329777\n",
            "Iteration 35, loss = 0.51966307\n",
            "Iteration 36, loss = 0.66801688\n",
            "Iteration 37, loss = 0.53170275\n",
            "Iteration 38, loss = 0.48451900\n",
            "Iteration 39, loss = 0.45561416\n",
            "Iteration 40, loss = 0.48577730\n",
            "Iteration 41, loss = 0.46871809\n",
            "Iteration 42, loss = 0.46640185\n",
            "Iteration 43, loss = 0.44899298\n",
            "Iteration 44, loss = 0.44594971\n",
            "Iteration 45, loss = 0.47578159\n",
            "Iteration 46, loss = 0.47489317\n",
            "Iteration 47, loss = 0.46462376\n",
            "Iteration 48, loss = 0.48709502\n",
            "Iteration 49, loss = 0.45872443\n",
            "Iteration 50, loss = 0.44536914\n",
            "Iteration 51, loss = 0.45657447\n",
            "Iteration 52, loss = 0.46469436\n",
            "Iteration 53, loss = 0.45330069\n",
            "Iteration 54, loss = 0.47133118\n",
            "Iteration 55, loss = 0.44780909\n",
            "Iteration 56, loss = 0.44200300\n",
            "Iteration 57, loss = 0.44292105\n",
            "Iteration 58, loss = 0.45024650\n",
            "Iteration 59, loss = 0.45231275\n",
            "Iteration 60, loss = 0.44780549\n",
            "Iteration 61, loss = 0.44097993\n",
            "Iteration 62, loss = 0.45087579\n",
            "Iteration 63, loss = 0.46167099\n",
            "Iteration 64, loss = 0.43057527\n",
            "Iteration 65, loss = 0.44407321\n",
            "Iteration 66, loss = 0.45043497\n",
            "Iteration 67, loss = 0.45306101\n",
            "Iteration 68, loss = 0.44044419\n",
            "Iteration 69, loss = 0.43714309\n",
            "Iteration 70, loss = 0.46125329\n",
            "Iteration 71, loss = 0.42810082\n",
            "Iteration 72, loss = 0.45379746\n",
            "Iteration 73, loss = 0.43977302\n",
            "Iteration 74, loss = 0.45998312\n",
            "Iteration 75, loss = 0.46338432\n",
            "Iteration 76, loss = 0.43803271\n",
            "Iteration 77, loss = 0.42910821\n",
            "Iteration 78, loss = 0.43388647\n",
            "Iteration 79, loss = 0.42391594\n",
            "Iteration 80, loss = 0.43280660\n",
            "Iteration 81, loss = 0.43463139\n",
            "Iteration 82, loss = 0.43507814\n",
            "Iteration 83, loss = 0.41470433\n",
            "Iteration 84, loss = 0.47390875\n",
            "Iteration 85, loss = 0.46668898\n",
            "Iteration 86, loss = 0.43325185\n",
            "Iteration 87, loss = 0.43699714\n",
            "Iteration 88, loss = 0.43084235\n",
            "Iteration 89, loss = 0.43541518\n",
            "Iteration 90, loss = 0.44239309\n",
            "Iteration 91, loss = 0.43218692\n",
            "Iteration 92, loss = 0.41741742\n",
            "Iteration 93, loss = 0.42093525\n",
            "Iteration 94, loss = 0.41128538\n",
            "Iteration 95, loss = 0.44115485\n",
            "Iteration 96, loss = 0.43601158\n",
            "Iteration 97, loss = 0.42537683\n",
            "Iteration 98, loss = 0.41505616\n",
            "Iteration 99, loss = 0.41914483\n",
            "Iteration 100, loss = 0.42899980\n",
            "Iteration 101, loss = 0.45169883\n",
            "Iteration 102, loss = 0.41944040\n",
            "Iteration 103, loss = 0.42289957\n",
            "Iteration 104, loss = 0.40689795\n",
            "Iteration 105, loss = 0.41111194\n",
            "Iteration 106, loss = 0.42214050\n",
            "Iteration 107, loss = 0.42195932\n",
            "Iteration 108, loss = 0.41534937\n",
            "Iteration 109, loss = 0.41658851\n",
            "Iteration 110, loss = 0.43344755\n",
            "Iteration 111, loss = 0.40523217\n",
            "Iteration 112, loss = 0.45057913\n",
            "Iteration 113, loss = 0.42479826\n",
            "Iteration 114, loss = 0.41166649\n",
            "Iteration 115, loss = 0.40811788\n",
            "Iteration 116, loss = 0.40628443\n",
            "Iteration 117, loss = 0.44547666\n",
            "Iteration 118, loss = 0.43140581\n",
            "Iteration 119, loss = 0.41776500\n",
            "Iteration 120, loss = 0.41870887\n",
            "Iteration 121, loss = 0.40808708\n",
            "Iteration 122, loss = 0.41014107\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.81575529\n",
            "Iteration 2, loss = 0.65657990\n",
            "Iteration 3, loss = 0.69522965\n",
            "Iteration 4, loss = 0.64585500\n",
            "Iteration 5, loss = 0.63467863\n",
            "Iteration 6, loss = 0.58166117\n",
            "Iteration 7, loss = 0.57763433\n",
            "Iteration 8, loss = 0.52982786\n",
            "Iteration 9, loss = 0.56420626\n",
            "Iteration 10, loss = 0.50913881\n",
            "Iteration 11, loss = 0.50421801\n",
            "Iteration 12, loss = 0.50178535\n",
            "Iteration 13, loss = 0.49540601\n",
            "Iteration 14, loss = 0.55570493\n",
            "Iteration 15, loss = 0.52326770\n",
            "Iteration 16, loss = 0.48389930\n",
            "Iteration 17, loss = 0.47231557\n",
            "Iteration 18, loss = 0.52719797\n",
            "Iteration 19, loss = 0.48247616\n",
            "Iteration 20, loss = 0.48076293\n",
            "Iteration 21, loss = 0.48416109\n",
            "Iteration 22, loss = 0.50061743\n",
            "Iteration 23, loss = 0.46221979\n",
            "Iteration 24, loss = 0.46208958\n",
            "Iteration 25, loss = 0.50468485\n",
            "Iteration 26, loss = 0.45813201\n",
            "Iteration 27, loss = 0.47695964\n",
            "Iteration 28, loss = 0.53603690\n",
            "Iteration 29, loss = 0.46133406\n",
            "Iteration 30, loss = 0.53675484\n",
            "Iteration 31, loss = 0.47651938\n",
            "Iteration 32, loss = 0.49371573\n",
            "Iteration 33, loss = 0.44950946\n",
            "Iteration 34, loss = 0.47696258\n",
            "Iteration 35, loss = 0.46865916\n",
            "Iteration 36, loss = 0.51025182\n",
            "Iteration 37, loss = 0.47318827\n",
            "Iteration 38, loss = 0.47058134\n",
            "Iteration 39, loss = 0.44052198\n",
            "Iteration 40, loss = 0.44332971\n",
            "Iteration 41, loss = 0.49312333\n",
            "Iteration 42, loss = 0.45543325\n",
            "Iteration 43, loss = 0.43944689\n",
            "Iteration 44, loss = 0.44520456\n",
            "Iteration 45, loss = 0.47343644\n",
            "Iteration 46, loss = 0.43940186\n",
            "Iteration 47, loss = 0.44512894\n",
            "Iteration 48, loss = 0.45005506\n",
            "Iteration 49, loss = 0.46241103\n",
            "Iteration 50, loss = 0.48761322\n",
            "Iteration 51, loss = 0.44299932\n",
            "Iteration 52, loss = 0.43134510\n",
            "Iteration 53, loss = 0.46313132\n",
            "Iteration 54, loss = 0.43594806\n",
            "Iteration 55, loss = 0.49088731\n",
            "Iteration 56, loss = 0.44236236\n",
            "Iteration 57, loss = 0.44030530\n",
            "Iteration 58, loss = 0.42978843\n",
            "Iteration 59, loss = 0.43020852\n",
            "Iteration 60, loss = 0.41617419\n",
            "Iteration 61, loss = 0.42138848\n",
            "Iteration 62, loss = 0.42563334\n",
            "Iteration 63, loss = 0.42720010\n",
            "Iteration 64, loss = 0.44647806\n",
            "Iteration 65, loss = 0.42515918\n",
            "Iteration 66, loss = 0.41755739\n",
            "Iteration 67, loss = 0.42439998\n",
            "Iteration 68, loss = 0.43859180\n",
            "Iteration 69, loss = 0.44153507\n",
            "Iteration 70, loss = 0.43256946\n",
            "Iteration 71, loss = 0.42344022\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.89611139\n",
            "Iteration 2, loss = 0.75171000\n",
            "Iteration 3, loss = 0.62993564\n",
            "Iteration 4, loss = 0.64770193\n",
            "Iteration 5, loss = 0.80105601\n",
            "Iteration 6, loss = 0.69183365\n",
            "Iteration 7, loss = 0.59844645\n",
            "Iteration 8, loss = 0.64200098\n",
            "Iteration 9, loss = 0.71607961\n",
            "Iteration 10, loss = 0.63002612\n",
            "Iteration 11, loss = 0.58822815\n",
            "Iteration 12, loss = 0.51666990\n",
            "Iteration 13, loss = 0.64248310\n",
            "Iteration 14, loss = 0.58040565\n",
            "Iteration 15, loss = 0.59556068\n",
            "Iteration 16, loss = 0.54070055\n",
            "Iteration 17, loss = 0.52121404\n",
            "Iteration 18, loss = 0.57113501\n",
            "Iteration 19, loss = 0.48766862\n",
            "Iteration 20, loss = 0.57036206\n",
            "Iteration 21, loss = 0.49312798\n",
            "Iteration 22, loss = 0.48746816\n",
            "Iteration 23, loss = 0.48055626\n",
            "Iteration 24, loss = 0.55165293\n",
            "Iteration 25, loss = 0.54795024\n",
            "Iteration 26, loss = 0.57803085\n",
            "Iteration 27, loss = 0.49509762\n",
            "Iteration 28, loss = 0.48574339\n",
            "Iteration 29, loss = 0.46740241\n",
            "Iteration 30, loss = 0.52988993\n",
            "Iteration 31, loss = 0.50277199\n",
            "Iteration 32, loss = 0.57465290\n",
            "Iteration 33, loss = 0.47247176\n",
            "Iteration 34, loss = 0.46571024\n",
            "Iteration 35, loss = 0.47725681\n",
            "Iteration 36, loss = 0.47831156\n",
            "Iteration 37, loss = 0.45580116\n",
            "Iteration 38, loss = 0.46794552\n",
            "Iteration 39, loss = 0.48942453\n",
            "Iteration 40, loss = 0.46024251\n",
            "Iteration 41, loss = 0.47993637\n",
            "Iteration 42, loss = 0.46879273\n",
            "Iteration 43, loss = 0.44359005\n",
            "Iteration 44, loss = 0.43981965\n",
            "Iteration 45, loss = 0.45405440\n",
            "Iteration 46, loss = 0.47215497\n",
            "Iteration 47, loss = 0.46457293\n",
            "Iteration 48, loss = 0.43986600\n",
            "Iteration 49, loss = 0.46700894\n",
            "Iteration 50, loss = 0.44487612\n",
            "Iteration 51, loss = 0.51098007\n",
            "Iteration 52, loss = 0.43940785\n",
            "Iteration 53, loss = 0.50280729\n",
            "Iteration 54, loss = 0.43997565\n",
            "Iteration 55, loss = 0.43624776\n",
            "Iteration 56, loss = 0.47961242\n",
            "Iteration 57, loss = 0.43978461\n",
            "Iteration 58, loss = 0.52012682\n",
            "Iteration 59, loss = 0.45663681\n",
            "Iteration 60, loss = 0.47105837\n",
            "Iteration 61, loss = 0.43843795\n",
            "Iteration 62, loss = 0.45768759\n",
            "Iteration 63, loss = 0.47741376\n",
            "Iteration 64, loss = 0.42982849\n",
            "Iteration 65, loss = 0.49639218\n",
            "Iteration 66, loss = 0.45081293\n",
            "Iteration 67, loss = 0.48043758\n",
            "Iteration 68, loss = 0.43001724\n",
            "Iteration 69, loss = 0.42953583\n",
            "Iteration 70, loss = 0.42560097\n",
            "Iteration 71, loss = 0.42747337\n",
            "Iteration 72, loss = 0.42323246\n",
            "Iteration 73, loss = 0.43341717\n",
            "Iteration 74, loss = 0.46884241\n",
            "Iteration 75, loss = 0.44160200\n",
            "Iteration 76, loss = 0.44075528\n",
            "Iteration 77, loss = 0.44178535\n",
            "Iteration 78, loss = 0.43759333\n",
            "Iteration 79, loss = 0.45468965\n",
            "Iteration 80, loss = 0.46232138\n",
            "Iteration 81, loss = 0.47475282\n",
            "Iteration 82, loss = 0.41793754\n",
            "Iteration 83, loss = 0.46736298\n",
            "Iteration 84, loss = 0.42133641\n",
            "Iteration 85, loss = 0.40872378\n",
            "Iteration 86, loss = 0.44594868\n",
            "Iteration 87, loss = 0.41075800\n",
            "Iteration 88, loss = 0.42002527\n",
            "Iteration 89, loss = 0.41005108\n",
            "Iteration 90, loss = 0.43893789\n",
            "Iteration 91, loss = 0.41127837\n",
            "Iteration 92, loss = 0.41542832\n",
            "Iteration 93, loss = 0.40267693\n",
            "Iteration 94, loss = 0.41392517\n",
            "Iteration 95, loss = 0.40474695\n",
            "Iteration 96, loss = 0.40733005\n",
            "Iteration 97, loss = 0.41481674\n",
            "Iteration 98, loss = 0.42381713\n",
            "Iteration 99, loss = 0.40569633\n",
            "Iteration 100, loss = 0.42299175\n",
            "Iteration 101, loss = 0.45670152\n",
            "Iteration 102, loss = 0.42065593\n",
            "Iteration 103, loss = 0.39752631\n",
            "Iteration 104, loss = 0.45996361\n",
            "Iteration 105, loss = 0.42310290\n",
            "Iteration 106, loss = 0.41719473\n",
            "Iteration 107, loss = 0.40201402\n",
            "Iteration 108, loss = 0.40550720\n",
            "Iteration 109, loss = 0.40233951\n",
            "Iteration 110, loss = 0.40942021\n",
            "Iteration 111, loss = 0.39253624\n",
            "Iteration 112, loss = 0.45894966\n",
            "Iteration 113, loss = 0.40051536\n",
            "Iteration 114, loss = 0.39852531\n",
            "Iteration 115, loss = 0.39479778\n",
            "Iteration 116, loss = 0.41159789\n",
            "Iteration 117, loss = 0.42398170\n",
            "Iteration 118, loss = 0.39061447\n",
            "Iteration 119, loss = 0.42773980\n",
            "Iteration 120, loss = 0.41083060\n",
            "Iteration 121, loss = 0.38333868\n",
            "Iteration 122, loss = 0.40025717\n",
            "Iteration 123, loss = 0.40922602\n",
            "Iteration 124, loss = 0.38434213\n",
            "Iteration 125, loss = 0.40808542\n",
            "Iteration 126, loss = 0.39365792\n",
            "Iteration 127, loss = 0.39937667\n",
            "Iteration 128, loss = 0.39404162\n",
            "Iteration 129, loss = 0.39754973\n",
            "Iteration 130, loss = 0.38599409\n",
            "Iteration 131, loss = 0.41553001\n",
            "Iteration 132, loss = 0.38427188\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66207076\n",
            "Iteration 2, loss = 0.65521306\n",
            "Iteration 3, loss = 0.65329554\n",
            "Iteration 4, loss = 0.64901055\n",
            "Iteration 5, loss = 0.64567748\n",
            "Iteration 6, loss = 0.64482729\n",
            "Iteration 7, loss = 0.64011399\n",
            "Iteration 8, loss = 0.63793061\n",
            "Iteration 9, loss = 0.63397437\n",
            "Iteration 10, loss = 0.63229209\n",
            "Iteration 11, loss = 0.62960022\n",
            "Iteration 12, loss = 0.62878119\n",
            "Iteration 13, loss = 0.62505083\n",
            "Iteration 14, loss = 0.62329627\n",
            "Iteration 15, loss = 0.62069253\n",
            "Iteration 16, loss = 0.61872384\n",
            "Iteration 17, loss = 0.61829385\n",
            "Iteration 18, loss = 0.61607523\n",
            "Iteration 19, loss = 0.61483746\n",
            "Iteration 20, loss = 0.61411149\n",
            "Iteration 21, loss = 0.61349984\n",
            "Iteration 22, loss = 0.61228199\n",
            "Iteration 23, loss = 0.61270893\n",
            "Iteration 24, loss = 0.61186161\n",
            "Iteration 25, loss = 0.61089062\n",
            "Iteration 26, loss = 0.60941231\n",
            "Iteration 27, loss = 0.60989164\n",
            "Iteration 28, loss = 0.60892221\n",
            "Iteration 29, loss = 0.60865656\n",
            "Iteration 30, loss = 0.60891310\n",
            "Iteration 31, loss = 0.61036478\n",
            "Iteration 32, loss = 0.60946265\n",
            "Iteration 33, loss = 0.60857981\n",
            "Iteration 34, loss = 0.60938471\n",
            "Iteration 35, loss = 0.60838843\n",
            "Iteration 36, loss = 0.60715027\n",
            "Iteration 37, loss = 0.60832773\n",
            "Iteration 38, loss = 0.60703193\n",
            "Iteration 39, loss = 0.60822430\n",
            "Iteration 40, loss = 0.60708422\n",
            "Iteration 41, loss = 0.60653508\n",
            "Iteration 42, loss = 0.60676211\n",
            "Iteration 43, loss = 0.60720621\n",
            "Iteration 44, loss = 0.60784316\n",
            "Iteration 45, loss = 0.60840165\n",
            "Iteration 46, loss = 0.60566437\n",
            "Iteration 47, loss = 0.60568183\n",
            "Iteration 48, loss = 0.60584089\n",
            "Iteration 49, loss = 0.60742023\n",
            "Iteration 50, loss = 0.60551230\n",
            "Iteration 51, loss = 0.60565386\n",
            "Iteration 52, loss = 0.60577945\n",
            "Iteration 53, loss = 0.60712059\n",
            "Iteration 54, loss = 0.60579619\n",
            "Iteration 55, loss = 0.60456503\n",
            "Iteration 56, loss = 0.60458912\n",
            "Iteration 57, loss = 0.60776240\n",
            "Iteration 58, loss = 0.60524106\n",
            "Iteration 59, loss = 0.60396473\n",
            "Iteration 60, loss = 0.60603300\n",
            "Iteration 61, loss = 0.60394316\n",
            "Iteration 62, loss = 0.60345417\n",
            "Iteration 63, loss = 0.60306530\n",
            "Iteration 64, loss = 0.60437602\n",
            "Iteration 65, loss = 0.60401957\n",
            "Iteration 66, loss = 0.60304904\n",
            "Iteration 67, loss = 0.60350300\n",
            "Iteration 68, loss = 0.60258944\n",
            "Iteration 69, loss = 0.60428627\n",
            "Iteration 70, loss = 0.60361839\n",
            "Iteration 71, loss = 0.60255069\n",
            "Iteration 72, loss = 0.60275053\n",
            "Iteration 73, loss = 0.60308481\n",
            "Iteration 74, loss = 0.60263393\n",
            "Iteration 75, loss = 0.60267973\n",
            "Iteration 76, loss = 0.60153946\n",
            "Iteration 77, loss = 0.60087423\n",
            "Iteration 78, loss = 0.60105489\n",
            "Iteration 79, loss = 0.60119495\n",
            "Iteration 80, loss = 0.60142284\n",
            "Iteration 81, loss = 0.59989863\n",
            "Iteration 82, loss = 0.60244614\n",
            "Iteration 83, loss = 0.60096144\n",
            "Iteration 84, loss = 0.60079780\n",
            "Iteration 85, loss = 0.60027943\n",
            "Iteration 86, loss = 0.60159711\n",
            "Iteration 87, loss = 0.59894967\n",
            "Iteration 88, loss = 0.59935661\n",
            "Iteration 89, loss = 0.59949445\n",
            "Iteration 90, loss = 0.59936163\n",
            "Iteration 91, loss = 0.60151243\n",
            "Iteration 92, loss = 0.59814632\n",
            "Iteration 93, loss = 0.59812865\n",
            "Iteration 94, loss = 0.59932552\n",
            "Iteration 95, loss = 0.60068715\n",
            "Iteration 96, loss = 0.59747533\n",
            "Iteration 97, loss = 0.59796230\n",
            "Iteration 98, loss = 0.59877902\n",
            "Iteration 99, loss = 0.59891845\n",
            "Iteration 100, loss = 0.59740784\n",
            "Iteration 101, loss = 0.59643078\n",
            "Iteration 102, loss = 0.59870026\n",
            "Iteration 103, loss = 0.59822568\n",
            "Iteration 104, loss = 0.59659040\n",
            "Iteration 105, loss = 0.59719115\n",
            "Iteration 106, loss = 0.59690628\n",
            "Iteration 107, loss = 0.59736977\n",
            "Iteration 108, loss = 0.59631343\n",
            "Iteration 109, loss = 0.59639763\n",
            "Iteration 110, loss = 0.59786369\n",
            "Iteration 111, loss = 0.59477254\n",
            "Iteration 112, loss = 0.59504652\n",
            "Iteration 113, loss = 0.59582599\n",
            "Iteration 114, loss = 0.59505148\n",
            "Iteration 115, loss = 0.59484303\n",
            "Iteration 116, loss = 0.59694788\n",
            "Iteration 117, loss = 0.59732010\n",
            "Iteration 118, loss = 0.59915162\n",
            "Iteration 119, loss = 0.59535565\n",
            "Iteration 120, loss = 0.59502762\n",
            "Iteration 121, loss = 0.59339101\n",
            "Iteration 122, loss = 0.59142759\n",
            "Iteration 123, loss = 0.59475976\n",
            "Iteration 124, loss = 0.59327354\n",
            "Iteration 125, loss = 0.59347563\n",
            "Iteration 126, loss = 0.59709547\n",
            "Iteration 127, loss = 0.59442556\n",
            "Iteration 128, loss = 0.59097655\n",
            "Iteration 129, loss = 0.59077653\n",
            "Iteration 130, loss = 0.59187282\n",
            "Iteration 131, loss = 0.59254300\n",
            "Iteration 132, loss = 0.59063407\n",
            "Iteration 133, loss = 0.59104521\n",
            "Iteration 134, loss = 0.59074222\n",
            "Iteration 135, loss = 0.59442553\n",
            "Iteration 136, loss = 0.59045466\n",
            "Iteration 137, loss = 0.59076521\n",
            "Iteration 138, loss = 0.59068593\n",
            "Iteration 139, loss = 0.58741144\n",
            "Iteration 140, loss = 0.59044504\n",
            "Iteration 141, loss = 0.59036218\n",
            "Iteration 142, loss = 0.58806317\n",
            "Iteration 143, loss = 0.58771244\n",
            "Iteration 144, loss = 0.58818944\n",
            "Iteration 145, loss = 0.58754134\n",
            "Iteration 146, loss = 0.58692211\n",
            "Iteration 147, loss = 0.58714731\n",
            "Iteration 148, loss = 0.58760637\n",
            "Iteration 149, loss = 0.58611295\n",
            "Iteration 150, loss = 0.58626715\n",
            "Iteration 151, loss = 0.58511678\n",
            "Iteration 152, loss = 0.58597927\n",
            "Iteration 153, loss = 0.58491609\n",
            "Iteration 154, loss = 0.58478410\n",
            "Iteration 155, loss = 0.58486144\n",
            "Iteration 156, loss = 0.58533599\n",
            "Iteration 157, loss = 0.58408060\n",
            "Iteration 158, loss = 0.58470069\n",
            "Iteration 159, loss = 0.58519175\n",
            "Iteration 160, loss = 0.58370718\n",
            "Iteration 161, loss = 0.58462910\n",
            "Iteration 162, loss = 0.58316137\n",
            "Iteration 163, loss = 0.58183946\n",
            "Iteration 164, loss = 0.58147532\n",
            "Iteration 165, loss = 0.58114779\n",
            "Iteration 166, loss = 0.58175319\n",
            "Iteration 167, loss = 0.58080677\n",
            "Iteration 168, loss = 0.57640283\n",
            "Iteration 169, loss = 0.58227133\n",
            "Iteration 170, loss = 0.57943594\n",
            "Iteration 171, loss = 0.58115613\n",
            "Iteration 172, loss = 0.58098751\n",
            "Iteration 173, loss = 0.57976095\n",
            "Iteration 174, loss = 0.57910534\n",
            "Iteration 175, loss = 0.57790956\n",
            "Iteration 176, loss = 0.57830828\n",
            "Iteration 177, loss = 0.57798164\n",
            "Iteration 178, loss = 0.57706457\n",
            "Iteration 179, loss = 0.57517412\n",
            "Iteration 180, loss = 0.57883409\n",
            "Iteration 181, loss = 0.57395919\n",
            "Iteration 182, loss = 0.57358588\n",
            "Iteration 183, loss = 0.57535230\n",
            "Iteration 184, loss = 0.57192639\n",
            "Iteration 185, loss = 0.57424459\n",
            "Iteration 186, loss = 0.57455674\n",
            "Iteration 187, loss = 0.57105633\n",
            "Iteration 188, loss = 0.57297023\n",
            "Iteration 189, loss = 0.57448766\n",
            "Iteration 190, loss = 0.57321922\n",
            "Iteration 191, loss = 0.57413484\n",
            "Iteration 192, loss = 0.57122252\n",
            "Iteration 193, loss = 0.57019302\n",
            "Iteration 194, loss = 0.57335122\n",
            "Iteration 195, loss = 0.57150692\n",
            "Iteration 196, loss = 0.57116560\n",
            "Iteration 197, loss = 0.56754865\n",
            "Iteration 198, loss = 0.57149451\n",
            "Iteration 199, loss = 0.56804210\n",
            "Iteration 200, loss = 0.56691527\n",
            "Iteration 201, loss = 0.56760363\n",
            "Iteration 202, loss = 0.56722866\n",
            "Iteration 203, loss = 0.56780087\n",
            "Iteration 204, loss = 0.56870624\n",
            "Iteration 205, loss = 0.56405531\n",
            "Iteration 206, loss = 0.56864021\n",
            "Iteration 207, loss = 0.56506372\n",
            "Iteration 208, loss = 0.56679689\n",
            "Iteration 209, loss = 0.56374568\n",
            "Iteration 210, loss = 0.56340296\n",
            "Iteration 211, loss = 0.56142893\n",
            "Iteration 212, loss = 0.56632615\n",
            "Iteration 213, loss = 0.56455511\n",
            "Iteration 214, loss = 0.57030030\n",
            "Iteration 215, loss = 0.55895745\n",
            "Iteration 216, loss = 0.55865473\n",
            "Iteration 217, loss = 0.55961894\n",
            "Iteration 218, loss = 0.55821605\n",
            "Iteration 219, loss = 0.55763041\n",
            "Iteration 220, loss = 0.55867289\n",
            "Iteration 221, loss = 0.55650849\n",
            "Iteration 222, loss = 0.55761144\n",
            "Iteration 223, loss = 0.55457764\n",
            "Iteration 224, loss = 0.55318914\n",
            "Iteration 225, loss = 0.55338490\n",
            "Iteration 226, loss = 0.55442297\n",
            "Iteration 227, loss = 0.55299733\n",
            "Iteration 228, loss = 0.56096392\n",
            "Iteration 229, loss = 0.55370810\n",
            "Iteration 230, loss = 0.55784945\n",
            "Iteration 231, loss = 0.55091034\n",
            "Iteration 232, loss = 0.55217976\n",
            "Iteration 233, loss = 0.55431966\n",
            "Iteration 234, loss = 0.55164052\n",
            "Iteration 235, loss = 0.55033732\n",
            "Iteration 236, loss = 0.54838141\n",
            "Iteration 237, loss = 0.54505280\n",
            "Iteration 238, loss = 0.54618792\n",
            "Iteration 239, loss = 0.54995075\n",
            "Iteration 240, loss = 0.54150220\n",
            "Iteration 241, loss = 0.55155314\n",
            "Iteration 242, loss = 0.54621881\n",
            "Iteration 243, loss = 0.54220699\n",
            "Iteration 244, loss = 0.54376089\n",
            "Iteration 245, loss = 0.54150286\n",
            "Iteration 246, loss = 0.54054103\n",
            "Iteration 247, loss = 0.54027859\n",
            "Iteration 248, loss = 0.53860038\n",
            "Iteration 249, loss = 0.54176233\n",
            "Iteration 250, loss = 0.54034361\n",
            "Iteration 251, loss = 0.53628782\n",
            "Iteration 252, loss = 0.54369002\n",
            "Iteration 253, loss = 0.53627021\n",
            "Iteration 254, loss = 0.53943726\n",
            "Iteration 255, loss = 0.54404947\n",
            "Iteration 256, loss = 0.53708372\n",
            "Iteration 257, loss = 0.53325070\n",
            "Iteration 258, loss = 0.53988197\n",
            "Iteration 259, loss = 0.54005284\n",
            "Iteration 260, loss = 0.53017129\n",
            "Iteration 261, loss = 0.53747287\n",
            "Iteration 262, loss = 0.53103848\n",
            "Iteration 263, loss = 0.52802452\n",
            "Iteration 264, loss = 0.52982825\n",
            "Iteration 265, loss = 0.53108356\n",
            "Iteration 266, loss = 0.52733158\n",
            "Iteration 267, loss = 0.53730595\n",
            "Iteration 268, loss = 0.52869281\n",
            "Iteration 269, loss = 0.52714621\n",
            "Iteration 270, loss = 0.52512239\n",
            "Iteration 271, loss = 0.52998430\n",
            "Iteration 272, loss = 0.52383464\n",
            "Iteration 273, loss = 0.52416140\n",
            "Iteration 274, loss = 0.52918097\n",
            "Iteration 275, loss = 0.52391801\n",
            "Iteration 276, loss = 0.52601250\n",
            "Iteration 277, loss = 0.52096658\n",
            "Iteration 278, loss = 0.52113116\n",
            "Iteration 279, loss = 0.52028138\n",
            "Iteration 280, loss = 0.51813256\n",
            "Iteration 281, loss = 0.51441166\n",
            "Iteration 282, loss = 0.52063687\n",
            "Iteration 283, loss = 0.51073345\n",
            "Iteration 284, loss = 0.51539918\n",
            "Iteration 285, loss = 0.51553834\n",
            "Iteration 286, loss = 0.51300329\n",
            "Iteration 287, loss = 0.51473883\n",
            "Iteration 288, loss = 0.51901439\n",
            "Iteration 289, loss = 0.51044297\n",
            "Iteration 290, loss = 0.52137985\n",
            "Iteration 291, loss = 0.50930846\n",
            "Iteration 292, loss = 0.50595702\n",
            "Iteration 293, loss = 0.51388892\n",
            "Iteration 294, loss = 0.50997589\n",
            "Iteration 295, loss = 0.50689613\n",
            "Iteration 296, loss = 0.51675125\n",
            "Iteration 297, loss = 0.51780967\n",
            "Iteration 298, loss = 0.50378949\n",
            "Iteration 299, loss = 0.50674854\n",
            "Iteration 300, loss = 0.51030420\n",
            "Iteration 301, loss = 0.50345962\n",
            "Iteration 302, loss = 0.50577719\n",
            "Iteration 303, loss = 0.49498686\n",
            "Iteration 304, loss = 0.49558497\n",
            "Iteration 305, loss = 0.50205096\n",
            "Iteration 306, loss = 0.50048204\n",
            "Iteration 307, loss = 0.50162420\n",
            "Iteration 308, loss = 0.49106518\n",
            "Iteration 309, loss = 0.49437145\n",
            "Iteration 310, loss = 0.49515870\n",
            "Iteration 311, loss = 0.49818716\n",
            "Iteration 312, loss = 0.49545804\n",
            "Iteration 313, loss = 0.49227975\n",
            "Iteration 314, loss = 0.49984160\n",
            "Iteration 315, loss = 0.49172298\n",
            "Iteration 316, loss = 0.48560783\n",
            "Iteration 317, loss = 0.49795050\n",
            "Iteration 318, loss = 0.48911715\n",
            "Iteration 319, loss = 0.50093649\n",
            "Iteration 320, loss = 0.49589260\n",
            "Iteration 321, loss = 0.48757343\n",
            "Iteration 322, loss = 0.48229408\n",
            "Iteration 323, loss = 0.49466511\n",
            "Iteration 324, loss = 0.50413069\n",
            "Iteration 325, loss = 0.48602402\n",
            "Iteration 326, loss = 0.48877827\n",
            "Iteration 327, loss = 0.48326285\n",
            "Iteration 328, loss = 0.48313640\n",
            "Iteration 329, loss = 0.48607805\n",
            "Iteration 330, loss = 0.47879601\n",
            "Iteration 331, loss = 0.49035001\n",
            "Iteration 332, loss = 0.48944079\n",
            "Iteration 333, loss = 0.48838068\n",
            "Iteration 334, loss = 0.48670082\n",
            "Iteration 335, loss = 0.48039145\n",
            "Iteration 336, loss = 0.48279035\n",
            "Iteration 337, loss = 0.47857336\n",
            "Iteration 338, loss = 0.48963357\n",
            "Iteration 339, loss = 0.48408998\n",
            "Iteration 340, loss = 0.47545134\n",
            "Iteration 341, loss = 0.47576038\n",
            "Iteration 342, loss = 0.48002595\n",
            "Iteration 343, loss = 0.47820471\n",
            "Iteration 344, loss = 0.47092988\n",
            "Iteration 345, loss = 0.48897692\n",
            "Iteration 346, loss = 0.47558667\n",
            "Iteration 347, loss = 0.48248785\n",
            "Iteration 348, loss = 0.48582347\n",
            "Iteration 349, loss = 0.48094800\n",
            "Iteration 350, loss = 0.46951397\n",
            "Iteration 351, loss = 0.48050855\n",
            "Iteration 352, loss = 0.47707014\n",
            "Iteration 353, loss = 0.47319850\n",
            "Iteration 354, loss = 0.47382486\n",
            "Iteration 355, loss = 0.47719434\n",
            "Iteration 356, loss = 0.48051527\n",
            "Iteration 357, loss = 0.47034761\n",
            "Iteration 358, loss = 0.47001981\n",
            "Iteration 359, loss = 0.47359013\n",
            "Iteration 360, loss = 0.47128666\n",
            "Iteration 361, loss = 0.47376770\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67035433\n",
            "Iteration 2, loss = 0.65811188\n",
            "Iteration 3, loss = 0.65677681\n",
            "Iteration 4, loss = 0.65389713\n",
            "Iteration 5, loss = 0.64959609\n",
            "Iteration 6, loss = 0.64625175\n",
            "Iteration 7, loss = 0.64604183\n",
            "Iteration 8, loss = 0.64123365\n",
            "Iteration 9, loss = 0.63697176\n",
            "Iteration 10, loss = 0.63603368\n",
            "Iteration 11, loss = 0.63296240\n",
            "Iteration 12, loss = 0.63003751\n",
            "Iteration 13, loss = 0.62725610\n",
            "Iteration 14, loss = 0.62613985\n",
            "Iteration 15, loss = 0.62300933\n",
            "Iteration 16, loss = 0.62144850\n",
            "Iteration 17, loss = 0.61977257\n",
            "Iteration 18, loss = 0.61663986\n",
            "Iteration 19, loss = 0.61500739\n",
            "Iteration 20, loss = 0.61587769\n",
            "Iteration 21, loss = 0.61367196\n",
            "Iteration 22, loss = 0.61253999\n",
            "Iteration 23, loss = 0.61014553\n",
            "Iteration 24, loss = 0.61052934\n",
            "Iteration 25, loss = 0.60915129\n",
            "Iteration 26, loss = 0.61077245\n",
            "Iteration 27, loss = 0.60891448\n",
            "Iteration 28, loss = 0.60831355\n",
            "Iteration 29, loss = 0.60751562\n",
            "Iteration 30, loss = 0.60672913\n",
            "Iteration 31, loss = 0.60689397\n",
            "Iteration 32, loss = 0.60557222\n",
            "Iteration 33, loss = 0.60612287\n",
            "Iteration 34, loss = 0.60668029\n",
            "Iteration 35, loss = 0.60572617\n",
            "Iteration 36, loss = 0.60611747\n",
            "Iteration 37, loss = 0.60500029\n",
            "Iteration 38, loss = 0.60593564\n",
            "Iteration 39, loss = 0.60502623\n",
            "Iteration 40, loss = 0.60524676\n",
            "Iteration 41, loss = 0.60501024\n",
            "Iteration 42, loss = 0.60367914\n",
            "Iteration 43, loss = 0.60411927\n",
            "Iteration 44, loss = 0.60536301\n",
            "Iteration 45, loss = 0.60455301\n",
            "Iteration 46, loss = 0.60653457\n",
            "Iteration 47, loss = 0.60568222\n",
            "Iteration 48, loss = 0.60361745\n",
            "Iteration 49, loss = 0.60365797\n",
            "Iteration 50, loss = 0.60313712\n",
            "Iteration 51, loss = 0.60327356\n",
            "Iteration 52, loss = 0.60483694\n",
            "Iteration 53, loss = 0.60650739\n",
            "Iteration 54, loss = 0.60379859\n",
            "Iteration 55, loss = 0.60250389\n",
            "Iteration 56, loss = 0.60282737\n",
            "Iteration 57, loss = 0.60300430\n",
            "Iteration 58, loss = 0.60162419\n",
            "Iteration 59, loss = 0.60244613\n",
            "Iteration 60, loss = 0.60229811\n",
            "Iteration 61, loss = 0.60132147\n",
            "Iteration 62, loss = 0.60111372\n",
            "Iteration 63, loss = 0.60097339\n",
            "Iteration 64, loss = 0.60194821\n",
            "Iteration 65, loss = 0.60162354\n",
            "Iteration 66, loss = 0.60197653\n",
            "Iteration 67, loss = 0.60277197\n",
            "Iteration 68, loss = 0.60063193\n",
            "Iteration 69, loss = 0.60366542\n",
            "Iteration 70, loss = 0.60052109\n",
            "Iteration 71, loss = 0.60091602\n",
            "Iteration 72, loss = 0.60141550\n",
            "Iteration 73, loss = 0.60056956\n",
            "Iteration 74, loss = 0.60097617\n",
            "Iteration 75, loss = 0.60144537\n",
            "Iteration 76, loss = 0.60095255\n",
            "Iteration 77, loss = 0.59947774\n",
            "Iteration 78, loss = 0.60102604\n",
            "Iteration 79, loss = 0.60256553\n",
            "Iteration 80, loss = 0.60040084\n",
            "Iteration 81, loss = 0.59956074\n",
            "Iteration 82, loss = 0.60029432\n",
            "Iteration 83, loss = 0.59942391\n",
            "Iteration 84, loss = 0.59950899\n",
            "Iteration 85, loss = 0.59969411\n",
            "Iteration 86, loss = 0.60006246\n",
            "Iteration 87, loss = 0.59974637\n",
            "Iteration 88, loss = 0.59898963\n",
            "Iteration 89, loss = 0.59888599\n",
            "Iteration 90, loss = 0.59994799\n",
            "Iteration 91, loss = 0.60006321\n",
            "Iteration 92, loss = 0.59940447\n",
            "Iteration 93, loss = 0.59861659\n",
            "Iteration 94, loss = 0.59745247\n",
            "Iteration 95, loss = 0.59877532\n",
            "Iteration 96, loss = 0.59954752\n",
            "Iteration 97, loss = 0.59898830\n",
            "Iteration 98, loss = 0.59867755\n",
            "Iteration 99, loss = 0.59814235\n",
            "Iteration 100, loss = 0.59786080\n",
            "Iteration 101, loss = 0.60155606\n",
            "Iteration 102, loss = 0.59750150\n",
            "Iteration 103, loss = 0.59797387\n",
            "Iteration 104, loss = 0.59909750\n",
            "Iteration 105, loss = 0.59778640\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66632031\n",
            "Iteration 2, loss = 0.65540383\n",
            "Iteration 3, loss = 0.65218612\n",
            "Iteration 4, loss = 0.64958880\n",
            "Iteration 5, loss = 0.64644240\n",
            "Iteration 6, loss = 0.64343642\n",
            "Iteration 7, loss = 0.64009233\n",
            "Iteration 8, loss = 0.63888926\n",
            "Iteration 9, loss = 0.63656844\n",
            "Iteration 10, loss = 0.63420949\n",
            "Iteration 11, loss = 0.63305972\n",
            "Iteration 12, loss = 0.63045103\n",
            "Iteration 13, loss = 0.62861929\n",
            "Iteration 14, loss = 0.62954295\n",
            "Iteration 15, loss = 0.62586202\n",
            "Iteration 16, loss = 0.62440735\n",
            "Iteration 17, loss = 0.62385028\n",
            "Iteration 18, loss = 0.62162262\n",
            "Iteration 19, loss = 0.62136297\n",
            "Iteration 20, loss = 0.62057050\n",
            "Iteration 21, loss = 0.62094367\n",
            "Iteration 22, loss = 0.61976182\n",
            "Iteration 23, loss = 0.61942937\n",
            "Iteration 24, loss = 0.61948919\n",
            "Iteration 25, loss = 0.61762780\n",
            "Iteration 26, loss = 0.61857032\n",
            "Iteration 27, loss = 0.61723104\n",
            "Iteration 28, loss = 0.61742900\n",
            "Iteration 29, loss = 0.61967504\n",
            "Iteration 30, loss = 0.61732823\n",
            "Iteration 31, loss = 0.61684459\n",
            "Iteration 32, loss = 0.61589332\n",
            "Iteration 33, loss = 0.61731755\n",
            "Iteration 34, loss = 0.61607566\n",
            "Iteration 35, loss = 0.61663148\n",
            "Iteration 36, loss = 0.61602124\n",
            "Iteration 37, loss = 0.61644725\n",
            "Iteration 38, loss = 0.61630711\n",
            "Iteration 39, loss = 0.61583771\n",
            "Iteration 40, loss = 0.61650333\n",
            "Iteration 41, loss = 0.61511710\n",
            "Iteration 42, loss = 0.61606736\n",
            "Iteration 43, loss = 0.61506879\n",
            "Iteration 44, loss = 0.61567350\n",
            "Iteration 45, loss = 0.61605992\n",
            "Iteration 46, loss = 0.61579670\n",
            "Iteration 47, loss = 0.61620417\n",
            "Iteration 48, loss = 0.61318468\n",
            "Iteration 49, loss = 0.61658087\n",
            "Iteration 50, loss = 0.61368877\n",
            "Iteration 51, loss = 0.61502758\n",
            "Iteration 52, loss = 0.61467746\n",
            "Iteration 53, loss = 0.61340221\n",
            "Iteration 54, loss = 0.61341321\n",
            "Iteration 55, loss = 0.61373624\n",
            "Iteration 56, loss = 0.61349077\n",
            "Iteration 57, loss = 0.61304661\n",
            "Iteration 58, loss = 0.61374353\n",
            "Iteration 59, loss = 0.61329377\n",
            "Iteration 60, loss = 0.61515055\n",
            "Iteration 61, loss = 0.61358910\n",
            "Iteration 62, loss = 0.61278987\n",
            "Iteration 63, loss = 0.61085859\n",
            "Iteration 64, loss = 0.61363934\n",
            "Iteration 65, loss = 0.61260945\n",
            "Iteration 66, loss = 0.61196578\n",
            "Iteration 67, loss = 0.61259443\n",
            "Iteration 68, loss = 0.61249659\n",
            "Iteration 69, loss = 0.61163690\n",
            "Iteration 70, loss = 0.61119800\n",
            "Iteration 71, loss = 0.61261544\n",
            "Iteration 72, loss = 0.61181061\n",
            "Iteration 73, loss = 0.61123888\n",
            "Iteration 74, loss = 0.61158077\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66850181\n",
            "Iteration 2, loss = 0.66390610\n",
            "Iteration 3, loss = 0.65933350\n",
            "Iteration 4, loss = 0.65611891\n",
            "Iteration 5, loss = 0.65244534\n",
            "Iteration 6, loss = 0.64860972\n",
            "Iteration 7, loss = 0.64520830\n",
            "Iteration 8, loss = 0.64199298\n",
            "Iteration 9, loss = 0.64002274\n",
            "Iteration 10, loss = 0.63570465\n",
            "Iteration 11, loss = 0.63239296\n",
            "Iteration 12, loss = 0.62948526\n",
            "Iteration 13, loss = 0.62601944\n",
            "Iteration 14, loss = 0.62406894\n",
            "Iteration 15, loss = 0.62157061\n",
            "Iteration 16, loss = 0.61860411\n",
            "Iteration 17, loss = 0.61765128\n",
            "Iteration 18, loss = 0.61450918\n",
            "Iteration 19, loss = 0.61519700\n",
            "Iteration 20, loss = 0.61169087\n",
            "Iteration 21, loss = 0.61081884\n",
            "Iteration 22, loss = 0.61056438\n",
            "Iteration 23, loss = 0.60814465\n",
            "Iteration 24, loss = 0.60761733\n",
            "Iteration 25, loss = 0.60542624\n",
            "Iteration 26, loss = 0.60611488\n",
            "Iteration 27, loss = 0.60529213\n",
            "Iteration 28, loss = 0.60532828\n",
            "Iteration 29, loss = 0.60448504\n",
            "Iteration 30, loss = 0.60471298\n",
            "Iteration 31, loss = 0.60366691\n",
            "Iteration 32, loss = 0.60282237\n",
            "Iteration 33, loss = 0.60413163\n",
            "Iteration 34, loss = 0.60282262\n",
            "Iteration 35, loss = 0.60374511\n",
            "Iteration 36, loss = 0.60641868\n",
            "Iteration 37, loss = 0.60358428\n",
            "Iteration 38, loss = 0.60260628\n",
            "Iteration 39, loss = 0.60318427\n",
            "Iteration 40, loss = 0.60249257\n",
            "Iteration 41, loss = 0.60195983\n",
            "Iteration 42, loss = 0.60172368\n",
            "Iteration 43, loss = 0.60253729\n",
            "Iteration 44, loss = 0.60070173\n",
            "Iteration 45, loss = 0.60097108\n",
            "Iteration 46, loss = 0.60035052\n",
            "Iteration 47, loss = 0.60089470\n",
            "Iteration 48, loss = 0.60068222\n",
            "Iteration 49, loss = 0.60183483\n",
            "Iteration 50, loss = 0.59916909\n",
            "Iteration 51, loss = 0.59946289\n",
            "Iteration 52, loss = 0.60046662\n",
            "Iteration 53, loss = 0.59883917\n",
            "Iteration 54, loss = 0.59978533\n",
            "Iteration 55, loss = 0.59850083\n",
            "Iteration 56, loss = 0.59923732\n",
            "Iteration 57, loss = 0.59868452\n",
            "Iteration 58, loss = 0.59800202\n",
            "Iteration 59, loss = 0.59901978\n",
            "Iteration 60, loss = 0.60078807\n",
            "Iteration 61, loss = 0.59825088\n",
            "Iteration 62, loss = 0.59844800\n",
            "Iteration 63, loss = 0.59717745\n",
            "Iteration 64, loss = 0.59696954\n",
            "Iteration 65, loss = 0.59933258\n",
            "Iteration 66, loss = 0.59771700\n",
            "Iteration 67, loss = 0.59582216\n",
            "Iteration 68, loss = 0.59862430\n",
            "Iteration 69, loss = 0.59825649\n",
            "Iteration 70, loss = 0.59783246\n",
            "Iteration 71, loss = 0.59596420\n",
            "Iteration 72, loss = 0.59580398\n",
            "Iteration 73, loss = 0.59681457\n",
            "Iteration 74, loss = 0.59636303\n",
            "Iteration 75, loss = 0.59827959\n",
            "Iteration 76, loss = 0.59669757\n",
            "Iteration 77, loss = 0.59719940\n",
            "Iteration 78, loss = 0.59543019\n",
            "Iteration 79, loss = 0.59507468\n",
            "Iteration 80, loss = 0.59615985\n",
            "Iteration 81, loss = 0.59493587\n",
            "Iteration 82, loss = 0.59395132\n",
            "Iteration 83, loss = 0.59520758\n",
            "Iteration 84, loss = 0.59568194\n",
            "Iteration 85, loss = 0.59462775\n",
            "Iteration 86, loss = 0.59487558\n",
            "Iteration 87, loss = 0.59544872\n",
            "Iteration 88, loss = 0.59443353\n",
            "Iteration 89, loss = 0.59400044\n",
            "Iteration 90, loss = 0.59422356\n",
            "Iteration 91, loss = 0.59625367\n",
            "Iteration 92, loss = 0.59482438\n",
            "Iteration 93, loss = 0.59477161\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66852841\n",
            "Iteration 2, loss = 0.66504766\n",
            "Iteration 3, loss = 0.66361279\n",
            "Iteration 4, loss = 0.65738076\n",
            "Iteration 5, loss = 0.65388084\n",
            "Iteration 6, loss = 0.65034507\n",
            "Iteration 7, loss = 0.64795622\n",
            "Iteration 8, loss = 0.64654401\n",
            "Iteration 9, loss = 0.64191699\n",
            "Iteration 10, loss = 0.64281900\n",
            "Iteration 11, loss = 0.63973784\n",
            "Iteration 12, loss = 0.63641571\n",
            "Iteration 13, loss = 0.63171377\n",
            "Iteration 14, loss = 0.62958753\n",
            "Iteration 15, loss = 0.62751710\n",
            "Iteration 16, loss = 0.62960921\n",
            "Iteration 17, loss = 0.62412093\n",
            "Iteration 18, loss = 0.62105909\n",
            "Iteration 19, loss = 0.62101462\n",
            "Iteration 20, loss = 0.61751082\n",
            "Iteration 21, loss = 0.61761317\n",
            "Iteration 22, loss = 0.61594749\n",
            "Iteration 23, loss = 0.61679505\n",
            "Iteration 24, loss = 0.61528868\n",
            "Iteration 25, loss = 0.61297746\n",
            "Iteration 26, loss = 0.61308529\n",
            "Iteration 27, loss = 0.61185959\n",
            "Iteration 28, loss = 0.61264711\n",
            "Iteration 29, loss = 0.61355243\n",
            "Iteration 30, loss = 0.61177161\n",
            "Iteration 31, loss = 0.61228960\n",
            "Iteration 32, loss = 0.61193767\n",
            "Iteration 33, loss = 0.61001174\n",
            "Iteration 34, loss = 0.60982177\n",
            "Iteration 35, loss = 0.60901915\n",
            "Iteration 36, loss = 0.60887137\n",
            "Iteration 37, loss = 0.60953255\n",
            "Iteration 38, loss = 0.61013320\n",
            "Iteration 39, loss = 0.61008126\n",
            "Iteration 40, loss = 0.60899978\n",
            "Iteration 41, loss = 0.60801707\n",
            "Iteration 42, loss = 0.60900832\n",
            "Iteration 43, loss = 0.60986541\n",
            "Iteration 44, loss = 0.60924392\n",
            "Iteration 45, loss = 0.60926669\n",
            "Iteration 46, loss = 0.60882959\n",
            "Iteration 47, loss = 0.60835411\n",
            "Iteration 48, loss = 0.60727988\n",
            "Iteration 49, loss = 0.60925823\n",
            "Iteration 50, loss = 0.60982682\n",
            "Iteration 51, loss = 0.60701805\n",
            "Iteration 52, loss = 0.60689006\n",
            "Iteration 53, loss = 0.60760014\n",
            "Iteration 54, loss = 0.60668597\n",
            "Iteration 55, loss = 0.60632960\n",
            "Iteration 56, loss = 0.60599245\n",
            "Iteration 57, loss = 0.60449288\n",
            "Iteration 58, loss = 0.60647725\n",
            "Iteration 59, loss = 0.60695713\n",
            "Iteration 60, loss = 0.60752968\n",
            "Iteration 61, loss = 0.60511390\n",
            "Iteration 62, loss = 0.60534915\n",
            "Iteration 63, loss = 0.60706766\n",
            "Iteration 64, loss = 0.60618976\n",
            "Iteration 65, loss = 0.60512312\n",
            "Iteration 66, loss = 0.60529197\n",
            "Iteration 67, loss = 0.60536596\n",
            "Iteration 68, loss = 0.60462361\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69458159\n",
            "Iteration 2, loss = 0.63212967\n",
            "Iteration 3, loss = 0.61312534\n",
            "Iteration 4, loss = 0.61077183\n",
            "Iteration 5, loss = 0.60668025\n",
            "Iteration 6, loss = 0.60301244\n",
            "Iteration 7, loss = 0.61117379\n",
            "Iteration 8, loss = 0.60709594\n",
            "Iteration 9, loss = 0.59643560\n",
            "Iteration 10, loss = 0.59765785\n",
            "Iteration 11, loss = 0.59473639\n",
            "Iteration 12, loss = 0.59057463\n",
            "Iteration 13, loss = 0.59409895\n",
            "Iteration 14, loss = 0.58765590\n",
            "Iteration 15, loss = 0.58121435\n",
            "Iteration 16, loss = 0.58131534\n",
            "Iteration 17, loss = 0.58272565\n",
            "Iteration 18, loss = 0.57295918\n",
            "Iteration 19, loss = 0.56526383\n",
            "Iteration 20, loss = 0.56284251\n",
            "Iteration 21, loss = 0.55787408\n",
            "Iteration 22, loss = 0.57773353\n",
            "Iteration 23, loss = 0.54142687\n",
            "Iteration 24, loss = 0.54728107\n",
            "Iteration 25, loss = 0.53920780\n",
            "Iteration 26, loss = 0.52715327\n",
            "Iteration 27, loss = 0.51405375\n",
            "Iteration 28, loss = 0.52137629\n",
            "Iteration 29, loss = 0.52127303\n",
            "Iteration 30, loss = 0.49725710\n",
            "Iteration 31, loss = 0.48955010\n",
            "Iteration 32, loss = 0.47703053\n",
            "Iteration 33, loss = 0.48881130\n",
            "Iteration 34, loss = 0.47286553\n",
            "Iteration 35, loss = 0.47059185\n",
            "Iteration 36, loss = 0.47514390\n",
            "Iteration 37, loss = 0.46537682\n",
            "Iteration 38, loss = 0.47692516\n",
            "Iteration 39, loss = 0.45345807\n",
            "Iteration 40, loss = 0.45099865\n",
            "Iteration 41, loss = 0.45181976\n",
            "Iteration 42, loss = 0.47345581\n",
            "Iteration 43, loss = 0.45194938\n",
            "Iteration 44, loss = 0.44709673\n",
            "Iteration 45, loss = 0.44614296\n",
            "Iteration 46, loss = 0.46428833\n",
            "Iteration 47, loss = 0.45799307\n",
            "Iteration 48, loss = 0.43932648\n",
            "Iteration 49, loss = 0.44455696\n",
            "Iteration 50, loss = 0.44629400\n",
            "Iteration 51, loss = 0.45532948\n",
            "Iteration 52, loss = 0.44512215\n",
            "Iteration 53, loss = 0.44708009\n",
            "Iteration 54, loss = 0.45540289\n",
            "Iteration 55, loss = 0.44137339\n",
            "Iteration 56, loss = 0.44200161\n",
            "Iteration 57, loss = 0.44706906\n",
            "Iteration 58, loss = 0.43644748\n",
            "Iteration 59, loss = 0.44720830\n",
            "Iteration 60, loss = 0.43646570\n",
            "Iteration 61, loss = 0.45637549\n",
            "Iteration 62, loss = 0.45337072\n",
            "Iteration 63, loss = 0.44219793\n",
            "Iteration 64, loss = 0.43474826\n",
            "Iteration 65, loss = 0.43293265\n",
            "Iteration 66, loss = 0.43556732\n",
            "Iteration 67, loss = 0.43824058\n",
            "Iteration 68, loss = 0.43440684\n",
            "Iteration 69, loss = 0.43021508\n",
            "Iteration 70, loss = 0.44061364\n",
            "Iteration 71, loss = 0.44520175\n",
            "Iteration 72, loss = 0.44068267\n",
            "Iteration 73, loss = 0.45020399\n",
            "Iteration 74, loss = 0.43089096\n",
            "Iteration 75, loss = 0.43108467\n",
            "Iteration 76, loss = 0.42979520\n",
            "Iteration 77, loss = 0.43770973\n",
            "Iteration 78, loss = 0.43407319\n",
            "Iteration 79, loss = 0.43272505\n",
            "Iteration 80, loss = 0.45938995\n",
            "Iteration 81, loss = 0.43670198\n",
            "Iteration 82, loss = 0.43843787\n",
            "Iteration 83, loss = 0.42959775\n",
            "Iteration 84, loss = 0.43848606\n",
            "Iteration 85, loss = 0.43165376\n",
            "Iteration 86, loss = 0.43460270\n",
            "Iteration 87, loss = 0.42698777\n",
            "Iteration 88, loss = 0.45067352\n",
            "Iteration 89, loss = 0.43639625\n",
            "Iteration 90, loss = 0.43097573\n",
            "Iteration 91, loss = 0.42902853\n",
            "Iteration 92, loss = 0.43035617\n",
            "Iteration 93, loss = 0.42898869\n",
            "Iteration 94, loss = 0.46536783\n",
            "Iteration 95, loss = 0.44416820\n",
            "Iteration 96, loss = 0.43567311\n",
            "Iteration 97, loss = 0.43269192\n",
            "Iteration 98, loss = 0.44050229\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67313953\n",
            "Iteration 2, loss = 0.62440949\n",
            "Iteration 3, loss = 0.60884317\n",
            "Iteration 4, loss = 0.61164267\n",
            "Iteration 5, loss = 0.60219293\n",
            "Iteration 6, loss = 0.60044284\n",
            "Iteration 7, loss = 0.59735691\n",
            "Iteration 8, loss = 0.60242437\n",
            "Iteration 9, loss = 0.59789143\n",
            "Iteration 10, loss = 0.59272319\n",
            "Iteration 11, loss = 0.59570625\n",
            "Iteration 12, loss = 0.59078304\n",
            "Iteration 13, loss = 0.58164662\n",
            "Iteration 14, loss = 0.58764526\n",
            "Iteration 15, loss = 0.58151985\n",
            "Iteration 16, loss = 0.57362621\n",
            "Iteration 17, loss = 0.58919433\n",
            "Iteration 18, loss = 0.58948118\n",
            "Iteration 19, loss = 0.56552900\n",
            "Iteration 20, loss = 0.57951441\n",
            "Iteration 21, loss = 0.55014584\n",
            "Iteration 22, loss = 0.54586067\n",
            "Iteration 23, loss = 0.53656849\n",
            "Iteration 24, loss = 0.52974397\n",
            "Iteration 25, loss = 0.52288534\n",
            "Iteration 26, loss = 0.51224247\n",
            "Iteration 27, loss = 0.50597501\n",
            "Iteration 28, loss = 0.49713077\n",
            "Iteration 29, loss = 0.49361244\n",
            "Iteration 30, loss = 0.49197421\n",
            "Iteration 31, loss = 0.48522654\n",
            "Iteration 32, loss = 0.48512359\n",
            "Iteration 33, loss = 0.47948456\n",
            "Iteration 34, loss = 0.46463166\n",
            "Iteration 35, loss = 0.46292405\n",
            "Iteration 36, loss = 0.48867283\n",
            "Iteration 37, loss = 0.48406154\n",
            "Iteration 38, loss = 0.46204264\n",
            "Iteration 39, loss = 0.44959242\n",
            "Iteration 40, loss = 0.44975068\n",
            "Iteration 41, loss = 0.44649681\n",
            "Iteration 42, loss = 0.44304327\n",
            "Iteration 43, loss = 0.44655510\n",
            "Iteration 44, loss = 0.47400711\n",
            "Iteration 45, loss = 0.45126102\n",
            "Iteration 46, loss = 0.44621487\n",
            "Iteration 47, loss = 0.44640433\n",
            "Iteration 48, loss = 0.43884393\n",
            "Iteration 49, loss = 0.45049875\n",
            "Iteration 50, loss = 0.44207422\n",
            "Iteration 51, loss = 0.44544944\n",
            "Iteration 52, loss = 0.43934500\n",
            "Iteration 53, loss = 0.45368142\n",
            "Iteration 54, loss = 0.45127928\n",
            "Iteration 55, loss = 0.43424933\n",
            "Iteration 56, loss = 0.44207431\n",
            "Iteration 57, loss = 0.43513558\n",
            "Iteration 58, loss = 0.44619822\n",
            "Iteration 59, loss = 0.45152870\n",
            "Iteration 60, loss = 0.43536070\n",
            "Iteration 61, loss = 0.43570705\n",
            "Iteration 62, loss = 0.43697410\n",
            "Iteration 63, loss = 0.45772806\n",
            "Iteration 64, loss = 0.47672153\n",
            "Iteration 65, loss = 0.44281853\n",
            "Iteration 66, loss = 0.43607247\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68614965\n",
            "Iteration 2, loss = 0.64140855\n",
            "Iteration 3, loss = 0.62363575\n",
            "Iteration 4, loss = 0.61633410\n",
            "Iteration 5, loss = 0.61401982\n",
            "Iteration 6, loss = 0.61370076\n",
            "Iteration 7, loss = 0.61090895\n",
            "Iteration 8, loss = 0.60918854\n",
            "Iteration 9, loss = 0.61171707\n",
            "Iteration 10, loss = 0.61422224\n",
            "Iteration 11, loss = 0.63616609\n",
            "Iteration 12, loss = 0.60842283\n",
            "Iteration 13, loss = 0.60000930\n",
            "Iteration 14, loss = 0.60440266\n",
            "Iteration 15, loss = 0.59364975\n",
            "Iteration 16, loss = 0.59367178\n",
            "Iteration 17, loss = 0.58463527\n",
            "Iteration 18, loss = 0.57924304\n",
            "Iteration 19, loss = 0.57469923\n",
            "Iteration 20, loss = 0.57142727\n",
            "Iteration 21, loss = 0.56703676\n",
            "Iteration 22, loss = 0.56196231\n",
            "Iteration 23, loss = 0.54344239\n",
            "Iteration 24, loss = 0.53253365\n",
            "Iteration 25, loss = 0.52677885\n",
            "Iteration 26, loss = 0.52050006\n",
            "Iteration 27, loss = 0.51810435\n",
            "Iteration 28, loss = 0.50656359\n",
            "Iteration 29, loss = 0.50491177\n",
            "Iteration 30, loss = 0.49743540\n",
            "Iteration 31, loss = 0.49436041\n",
            "Iteration 32, loss = 0.48814112\n",
            "Iteration 33, loss = 0.47782134\n",
            "Iteration 34, loss = 0.49133689\n",
            "Iteration 35, loss = 0.48147265\n",
            "Iteration 36, loss = 0.46893143\n",
            "Iteration 37, loss = 0.46960959\n",
            "Iteration 38, loss = 0.46909038\n",
            "Iteration 39, loss = 0.45873690\n",
            "Iteration 40, loss = 0.48356991\n",
            "Iteration 41, loss = 0.46567937\n",
            "Iteration 42, loss = 0.47015215\n",
            "Iteration 43, loss = 0.45541318\n",
            "Iteration 44, loss = 0.46407937\n",
            "Iteration 45, loss = 0.45668074\n",
            "Iteration 46, loss = 0.46074070\n",
            "Iteration 47, loss = 0.45597165\n",
            "Iteration 48, loss = 0.45427907\n",
            "Iteration 49, loss = 0.45510594\n",
            "Iteration 50, loss = 0.45025008\n",
            "Iteration 51, loss = 0.45389159\n",
            "Iteration 52, loss = 0.45275231\n",
            "Iteration 53, loss = 0.44927935\n",
            "Iteration 54, loss = 0.44745588\n",
            "Iteration 55, loss = 0.45344434\n",
            "Iteration 56, loss = 0.45767294\n",
            "Iteration 57, loss = 0.44956297\n",
            "Iteration 58, loss = 0.44943215\n",
            "Iteration 59, loss = 0.44312977\n",
            "Iteration 60, loss = 0.44550585\n",
            "Iteration 61, loss = 0.44395346\n",
            "Iteration 62, loss = 0.44365362\n",
            "Iteration 63, loss = 0.45047965\n",
            "Iteration 64, loss = 0.48748812\n",
            "Iteration 65, loss = 0.45744277\n",
            "Iteration 66, loss = 0.45148607\n",
            "Iteration 67, loss = 0.44685801\n",
            "Iteration 68, loss = 0.45778473\n",
            "Iteration 69, loss = 0.45867704\n",
            "Iteration 70, loss = 0.44885749\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69973549\n",
            "Iteration 2, loss = 0.63792042\n",
            "Iteration 3, loss = 0.60495266\n",
            "Iteration 4, loss = 0.60184239\n",
            "Iteration 5, loss = 0.60027890\n",
            "Iteration 6, loss = 0.59920420\n",
            "Iteration 7, loss = 0.59751858\n",
            "Iteration 8, loss = 0.59356033\n",
            "Iteration 9, loss = 0.59255640\n",
            "Iteration 10, loss = 0.59255623\n",
            "Iteration 11, loss = 0.60067127\n",
            "Iteration 12, loss = 0.60855093\n",
            "Iteration 13, loss = 0.58981454\n",
            "Iteration 14, loss = 0.58276000\n",
            "Iteration 15, loss = 0.57877189\n",
            "Iteration 16, loss = 0.57790347\n",
            "Iteration 17, loss = 0.57985693\n",
            "Iteration 18, loss = 0.57727936\n",
            "Iteration 19, loss = 0.56653255\n",
            "Iteration 20, loss = 0.56221892\n",
            "Iteration 21, loss = 0.55826736\n",
            "Iteration 22, loss = 0.55728194\n",
            "Iteration 23, loss = 0.54843580\n",
            "Iteration 24, loss = 0.54249579\n",
            "Iteration 25, loss = 0.53525490\n",
            "Iteration 26, loss = 0.52583250\n",
            "Iteration 27, loss = 0.53434732\n",
            "Iteration 28, loss = 0.53296306\n",
            "Iteration 29, loss = 0.52300332\n",
            "Iteration 30, loss = 0.50953545\n",
            "Iteration 31, loss = 0.51641525\n",
            "Iteration 32, loss = 0.49363818\n",
            "Iteration 33, loss = 0.49711815\n",
            "Iteration 34, loss = 0.52326593\n",
            "Iteration 35, loss = 0.55024789\n",
            "Iteration 36, loss = 0.50192896\n",
            "Iteration 37, loss = 0.47451761\n",
            "Iteration 38, loss = 0.47162678\n",
            "Iteration 39, loss = 0.46851717\n",
            "Iteration 40, loss = 0.47147070\n",
            "Iteration 41, loss = 0.46499515\n",
            "Iteration 42, loss = 0.46387437\n",
            "Iteration 43, loss = 0.48942266\n",
            "Iteration 44, loss = 0.46918169\n",
            "Iteration 45, loss = 0.45347560\n",
            "Iteration 46, loss = 0.45720483\n",
            "Iteration 47, loss = 0.45412762\n",
            "Iteration 48, loss = 0.46836065\n",
            "Iteration 49, loss = 0.45694501\n",
            "Iteration 50, loss = 0.45346767\n",
            "Iteration 51, loss = 0.45167310\n",
            "Iteration 52, loss = 0.45398485\n",
            "Iteration 53, loss = 0.46975813\n",
            "Iteration 54, loss = 0.44833513\n",
            "Iteration 55, loss = 0.44981851\n",
            "Iteration 56, loss = 0.45127541\n",
            "Iteration 57, loss = 0.45451237\n",
            "Iteration 58, loss = 0.44512964\n",
            "Iteration 59, loss = 0.45081108\n",
            "Iteration 60, loss = 0.44538054\n",
            "Iteration 61, loss = 0.44213254\n",
            "Iteration 62, loss = 0.44596386\n",
            "Iteration 63, loss = 0.44094271\n",
            "Iteration 64, loss = 0.44311413\n",
            "Iteration 65, loss = 0.44640610\n",
            "Iteration 66, loss = 0.44209094\n",
            "Iteration 67, loss = 0.43945654\n",
            "Iteration 68, loss = 0.44427969\n",
            "Iteration 69, loss = 0.43730488\n",
            "Iteration 70, loss = 0.43948226\n",
            "Iteration 71, loss = 0.43892145\n",
            "Iteration 72, loss = 0.43654515\n",
            "Iteration 73, loss = 0.44576646\n",
            "Iteration 74, loss = 0.43548719\n",
            "Iteration 75, loss = 0.44076769\n",
            "Iteration 76, loss = 0.43801790\n",
            "Iteration 77, loss = 0.43920524\n",
            "Iteration 78, loss = 0.44568586\n",
            "Iteration 79, loss = 0.44266226\n",
            "Iteration 80, loss = 0.44420147\n",
            "Iteration 81, loss = 0.44261727\n",
            "Iteration 82, loss = 0.43615385\n",
            "Iteration 83, loss = 0.43326882\n",
            "Iteration 84, loss = 0.44423585\n",
            "Iteration 85, loss = 0.43714087\n",
            "Iteration 86, loss = 0.47516712\n",
            "Iteration 87, loss = 0.44486302\n",
            "Iteration 88, loss = 0.43739357\n",
            "Iteration 89, loss = 0.43760812\n",
            "Iteration 90, loss = 0.43979515\n",
            "Iteration 91, loss = 0.43736364\n",
            "Iteration 92, loss = 0.44174824\n",
            "Iteration 93, loss = 0.44100209\n",
            "Iteration 94, loss = 0.43198458\n",
            "Iteration 95, loss = 0.43453593\n",
            "Iteration 96, loss = 0.43622810\n",
            "Iteration 97, loss = 0.43531782\n",
            "Iteration 98, loss = 0.43752960\n",
            "Iteration 99, loss = 0.44155537\n",
            "Iteration 100, loss = 0.45342285\n",
            "Iteration 101, loss = 0.44544549\n",
            "Iteration 102, loss = 0.43023696\n",
            "Iteration 103, loss = 0.43363631\n",
            "Iteration 104, loss = 0.42744416\n",
            "Iteration 105, loss = 0.42832823\n",
            "Iteration 106, loss = 0.42887240\n",
            "Iteration 107, loss = 0.43113369\n",
            "Iteration 108, loss = 0.42635607\n",
            "Iteration 109, loss = 0.42889422\n",
            "Iteration 110, loss = 0.42934077\n",
            "Iteration 111, loss = 0.42586132\n",
            "Iteration 112, loss = 0.42516834\n",
            "Iteration 113, loss = 0.42664516\n",
            "Iteration 114, loss = 0.43053329\n",
            "Iteration 115, loss = 0.43477770\n",
            "Iteration 116, loss = 0.43252925\n",
            "Iteration 117, loss = 0.42378631\n",
            "Iteration 118, loss = 0.46357243\n",
            "Iteration 119, loss = 0.44304364\n",
            "Iteration 120, loss = 0.43963205\n",
            "Iteration 121, loss = 0.43600396\n",
            "Iteration 122, loss = 0.43931066\n",
            "Iteration 123, loss = 0.43233990\n",
            "Iteration 124, loss = 0.42529586\n",
            "Iteration 125, loss = 0.42485031\n",
            "Iteration 126, loss = 0.42553482\n",
            "Iteration 127, loss = 0.42532462\n",
            "Iteration 128, loss = 0.42818148\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68187492\n",
            "Iteration 2, loss = 0.63696441\n",
            "Iteration 3, loss = 0.61775002\n",
            "Iteration 4, loss = 0.61230166\n",
            "Iteration 5, loss = 0.61029915\n",
            "Iteration 6, loss = 0.61250649\n",
            "Iteration 7, loss = 0.61151785\n",
            "Iteration 8, loss = 0.60666962\n",
            "Iteration 9, loss = 0.59982244\n",
            "Iteration 10, loss = 0.59748231\n",
            "Iteration 11, loss = 0.59949345\n",
            "Iteration 12, loss = 0.61232942\n",
            "Iteration 13, loss = 0.61442848\n",
            "Iteration 14, loss = 0.59278172\n",
            "Iteration 15, loss = 0.59733093\n",
            "Iteration 16, loss = 0.58436271\n",
            "Iteration 17, loss = 0.58113843\n",
            "Iteration 18, loss = 0.57721475\n",
            "Iteration 19, loss = 0.57447717\n",
            "Iteration 20, loss = 0.57067257\n",
            "Iteration 21, loss = 0.56835660\n",
            "Iteration 22, loss = 0.55560897\n",
            "Iteration 23, loss = 0.55434561\n",
            "Iteration 24, loss = 0.54397054\n",
            "Iteration 25, loss = 0.54008258\n",
            "Iteration 26, loss = 0.54608462\n",
            "Iteration 27, loss = 0.53009420\n",
            "Iteration 28, loss = 0.52330565\n",
            "Iteration 29, loss = 0.51226220\n",
            "Iteration 30, loss = 0.52556336\n",
            "Iteration 31, loss = 0.50354754\n",
            "Iteration 32, loss = 0.49484541\n",
            "Iteration 33, loss = 0.49344896\n",
            "Iteration 34, loss = 0.49992680\n",
            "Iteration 35, loss = 0.48391212\n",
            "Iteration 36, loss = 0.49582064\n",
            "Iteration 37, loss = 0.49334048\n",
            "Iteration 38, loss = 0.47574472\n",
            "Iteration 39, loss = 0.47087700\n",
            "Iteration 40, loss = 0.46565701\n",
            "Iteration 41, loss = 0.48288808\n",
            "Iteration 42, loss = 0.47056389\n",
            "Iteration 43, loss = 0.46065998\n",
            "Iteration 44, loss = 0.46954267\n",
            "Iteration 45, loss = 0.47006759\n",
            "Iteration 46, loss = 0.45630813\n",
            "Iteration 47, loss = 0.45434205\n",
            "Iteration 48, loss = 0.45423688\n",
            "Iteration 49, loss = 0.45366938\n",
            "Iteration 50, loss = 0.45663349\n",
            "Iteration 51, loss = 0.45677391\n",
            "Iteration 52, loss = 0.47445168\n",
            "Iteration 53, loss = 0.45916760\n",
            "Iteration 54, loss = 0.46086771\n",
            "Iteration 55, loss = 0.45844324\n",
            "Iteration 56, loss = 0.48179794\n",
            "Iteration 57, loss = 0.46207882\n",
            "Iteration 58, loss = 0.47090575\n",
            "Iteration 59, loss = 0.45183626\n",
            "Iteration 60, loss = 0.45487868\n",
            "Iteration 61, loss = 0.46674788\n",
            "Iteration 62, loss = 0.45544801\n",
            "Iteration 63, loss = 0.44547331\n",
            "Iteration 64, loss = 0.44708748\n",
            "Iteration 65, loss = 0.44770995\n",
            "Iteration 66, loss = 0.44753648\n",
            "Iteration 67, loss = 0.45996678\n",
            "Iteration 68, loss = 0.45766262\n",
            "Iteration 69, loss = 0.47405342\n",
            "Iteration 70, loss = 0.45129218\n",
            "Iteration 71, loss = 0.45499821\n",
            "Iteration 72, loss = 0.44560870\n",
            "Iteration 73, loss = 0.44385218\n",
            "Iteration 74, loss = 0.44991172\n",
            "Iteration 75, loss = 0.44620198\n",
            "Iteration 76, loss = 0.44550285\n",
            "Iteration 77, loss = 0.44930476\n",
            "Iteration 78, loss = 0.45472969\n",
            "Iteration 79, loss = 0.45210385\n",
            "Iteration 80, loss = 0.44232017\n",
            "Iteration 81, loss = 0.44461160\n",
            "Iteration 82, loss = 0.44582026\n",
            "Iteration 83, loss = 0.45063841\n",
            "Iteration 84, loss = 0.45092203\n",
            "Iteration 85, loss = 0.44423888\n",
            "Iteration 86, loss = 0.47323787\n",
            "Iteration 87, loss = 0.45829420\n",
            "Iteration 88, loss = 0.44425000\n",
            "Iteration 89, loss = 0.44811002\n",
            "Iteration 90, loss = 0.44278334\n",
            "Iteration 91, loss = 0.44051463\n",
            "Iteration 92, loss = 0.43928891\n",
            "Iteration 93, loss = 0.45225301\n",
            "Iteration 94, loss = 0.43864560\n",
            "Iteration 95, loss = 0.44608007\n",
            "Iteration 96, loss = 0.43895223\n",
            "Iteration 97, loss = 0.43693007\n",
            "Iteration 98, loss = 0.43619256\n",
            "Iteration 99, loss = 0.43881904\n",
            "Iteration 100, loss = 0.43643321\n",
            "Iteration 101, loss = 0.43962129\n",
            "Iteration 102, loss = 0.43962364\n",
            "Iteration 103, loss = 0.43947440\n",
            "Iteration 104, loss = 0.43450013\n",
            "Iteration 105, loss = 0.44834233\n",
            "Iteration 106, loss = 0.45230620\n",
            "Iteration 107, loss = 0.44348064\n",
            "Iteration 108, loss = 0.43689386\n",
            "Iteration 109, loss = 0.44136177\n",
            "Iteration 110, loss = 0.43948638\n",
            "Iteration 111, loss = 0.43571489\n",
            "Iteration 112, loss = 0.44136159\n",
            "Iteration 113, loss = 0.43357270\n",
            "Iteration 114, loss = 0.43532610\n",
            "Iteration 115, loss = 0.43901583\n",
            "Iteration 116, loss = 0.43486031\n",
            "Iteration 117, loss = 0.43221840\n",
            "Iteration 118, loss = 0.43412033\n",
            "Iteration 119, loss = 0.43213820\n",
            "Iteration 120, loss = 0.43035204\n",
            "Iteration 121, loss = 0.44058283\n",
            "Iteration 122, loss = 0.43126616\n",
            "Iteration 123, loss = 0.42953808\n",
            "Iteration 124, loss = 0.43251744\n",
            "Iteration 125, loss = 0.44331120\n",
            "Iteration 126, loss = 0.46091903\n",
            "Iteration 127, loss = 0.44555452\n",
            "Iteration 128, loss = 0.44502831\n",
            "Iteration 129, loss = 0.45632221\n",
            "Iteration 130, loss = 0.45077274\n",
            "Iteration 131, loss = 0.44388082\n",
            "Iteration 132, loss = 0.44110243\n",
            "Iteration 133, loss = 0.43535993\n",
            "Iteration 134, loss = 0.43290357\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.78901091\n",
            "Iteration 2, loss = 0.69503083\n",
            "Iteration 3, loss = 0.63703816\n",
            "Iteration 4, loss = 0.60622480\n",
            "Iteration 5, loss = 0.68417866\n",
            "Iteration 6, loss = 0.60591709\n",
            "Iteration 7, loss = 0.59744913\n",
            "Iteration 8, loss = 0.60356881\n",
            "Iteration 9, loss = 0.60014990\n",
            "Iteration 10, loss = 0.59710701\n",
            "Iteration 11, loss = 0.60187705\n",
            "Iteration 12, loss = 0.58869420\n",
            "Iteration 13, loss = 0.58789253\n",
            "Iteration 14, loss = 0.57378250\n",
            "Iteration 15, loss = 0.57938131\n",
            "Iteration 16, loss = 0.58258759\n",
            "Iteration 17, loss = 0.57210704\n",
            "Iteration 18, loss = 0.57439989\n",
            "Iteration 19, loss = 0.56874594\n",
            "Iteration 20, loss = 0.57015626\n",
            "Iteration 21, loss = 0.56974434\n",
            "Iteration 22, loss = 0.57341856\n",
            "Iteration 23, loss = 0.56064008\n",
            "Iteration 24, loss = 0.55706229\n",
            "Iteration 25, loss = 0.57039745\n",
            "Iteration 26, loss = 0.55870452\n",
            "Iteration 27, loss = 0.56434252\n",
            "Iteration 28, loss = 0.55449674\n",
            "Iteration 29, loss = 0.55869804\n",
            "Iteration 30, loss = 0.55355738\n",
            "Iteration 31, loss = 0.54763596\n",
            "Iteration 32, loss = 0.56773519\n",
            "Iteration 33, loss = 0.54537472\n",
            "Iteration 34, loss = 0.54602053\n",
            "Iteration 35, loss = 0.55448702\n",
            "Iteration 36, loss = 0.54577057\n",
            "Iteration 37, loss = 0.53060511\n",
            "Iteration 38, loss = 0.54141640\n",
            "Iteration 39, loss = 0.53601108\n",
            "Iteration 40, loss = 0.54294199\n",
            "Iteration 41, loss = 0.52817737\n",
            "Iteration 42, loss = 0.53607183\n",
            "Iteration 43, loss = 0.53192156\n",
            "Iteration 44, loss = 0.53488995\n",
            "Iteration 45, loss = 0.52310178\n",
            "Iteration 46, loss = 0.53098331\n",
            "Iteration 47, loss = 0.52395260\n",
            "Iteration 48, loss = 0.52238455\n",
            "Iteration 49, loss = 0.52758647\n",
            "Iteration 50, loss = 0.51987893\n",
            "Iteration 51, loss = 0.51161495\n",
            "Iteration 52, loss = 0.52084988\n",
            "Iteration 53, loss = 0.53136819\n",
            "Iteration 54, loss = 0.52214362\n",
            "Iteration 55, loss = 0.50464169\n",
            "Iteration 56, loss = 0.51066167\n",
            "Iteration 57, loss = 0.50715248\n",
            "Iteration 58, loss = 0.50084984\n",
            "Iteration 59, loss = 0.50016321\n",
            "Iteration 60, loss = 0.50163017\n",
            "Iteration 61, loss = 0.51688693\n",
            "Iteration 62, loss = 0.50709288\n",
            "Iteration 63, loss = 0.49796659\n",
            "Iteration 64, loss = 0.51608860\n",
            "Iteration 65, loss = 0.49571727\n",
            "Iteration 66, loss = 0.50209281\n",
            "Iteration 67, loss = 0.50198820\n",
            "Iteration 68, loss = 0.50782997\n",
            "Iteration 69, loss = 0.49232950\n",
            "Iteration 70, loss = 0.49739785\n",
            "Iteration 71, loss = 0.50779945\n",
            "Iteration 72, loss = 0.49129014\n",
            "Iteration 73, loss = 0.49265152\n",
            "Iteration 74, loss = 0.49583464\n",
            "Iteration 75, loss = 0.49889624\n",
            "Iteration 76, loss = 0.49349436\n",
            "Iteration 77, loss = 0.51069556\n",
            "Iteration 78, loss = 0.49533818\n",
            "Iteration 79, loss = 0.49644305\n",
            "Iteration 80, loss = 0.48826420\n",
            "Iteration 81, loss = 0.49407152\n",
            "Iteration 82, loss = 0.49519734\n",
            "Iteration 83, loss = 0.48769650\n",
            "Iteration 84, loss = 0.49427903\n",
            "Iteration 85, loss = 0.49478376\n",
            "Iteration 86, loss = 0.49582018\n",
            "Iteration 87, loss = 0.49807156\n",
            "Iteration 88, loss = 0.49400523\n",
            "Iteration 89, loss = 0.50465310\n",
            "Iteration 90, loss = 0.49039985\n",
            "Iteration 91, loss = 0.51180661\n",
            "Iteration 92, loss = 0.48709975\n",
            "Iteration 93, loss = 0.50477989\n",
            "Iteration 94, loss = 0.49667582\n",
            "Iteration 95, loss = 0.49361795\n",
            "Iteration 96, loss = 0.49395955\n",
            "Iteration 97, loss = 0.51345899\n",
            "Iteration 98, loss = 0.50721084\n",
            "Iteration 99, loss = 0.49251017\n",
            "Iteration 100, loss = 0.48610571\n",
            "Iteration 101, loss = 0.48063759\n",
            "Iteration 102, loss = 0.49136026\n",
            "Iteration 103, loss = 0.47264735\n",
            "Iteration 104, loss = 0.49326284\n",
            "Iteration 105, loss = 0.48390810\n",
            "Iteration 106, loss = 0.47459770\n",
            "Iteration 107, loss = 0.48456557\n",
            "Iteration 108, loss = 0.48807487\n",
            "Iteration 109, loss = 0.48446097\n",
            "Iteration 110, loss = 0.46665787\n",
            "Iteration 111, loss = 0.48643310\n",
            "Iteration 112, loss = 0.48529491\n",
            "Iteration 113, loss = 0.47247451\n",
            "Iteration 114, loss = 0.48650141\n",
            "Iteration 115, loss = 0.49858572\n",
            "Iteration 116, loss = 0.48067035\n",
            "Iteration 117, loss = 0.53219511\n",
            "Iteration 118, loss = 0.47111503\n",
            "Iteration 119, loss = 0.48817471\n",
            "Iteration 120, loss = 0.47239419\n",
            "Iteration 121, loss = 0.47954071\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.74076332\n",
            "Iteration 2, loss = 0.62118581\n",
            "Iteration 3, loss = 0.61091123\n",
            "Iteration 4, loss = 0.60944738\n",
            "Iteration 5, loss = 0.60811296\n",
            "Iteration 6, loss = 0.60870304\n",
            "Iteration 7, loss = 0.60361244\n",
            "Iteration 8, loss = 0.60682745\n",
            "Iteration 9, loss = 0.60666034\n",
            "Iteration 10, loss = 0.59876397\n",
            "Iteration 11, loss = 0.59462811\n",
            "Iteration 12, loss = 0.58811919\n",
            "Iteration 13, loss = 0.59441936\n",
            "Iteration 14, loss = 0.58927941\n",
            "Iteration 15, loss = 0.58762107\n",
            "Iteration 16, loss = 0.58382668\n",
            "Iteration 17, loss = 0.58348036\n",
            "Iteration 18, loss = 0.57970047\n",
            "Iteration 19, loss = 0.58968052\n",
            "Iteration 20, loss = 0.58306758\n",
            "Iteration 21, loss = 0.58121012\n",
            "Iteration 22, loss = 0.58102920\n",
            "Iteration 23, loss = 0.58212130\n",
            "Iteration 24, loss = 0.57734789\n",
            "Iteration 25, loss = 0.58413153\n",
            "Iteration 26, loss = 0.57674057\n",
            "Iteration 27, loss = 0.58377823\n",
            "Iteration 28, loss = 0.57770489\n",
            "Iteration 29, loss = 0.57582759\n",
            "Iteration 30, loss = 0.57438036\n",
            "Iteration 31, loss = 0.56658743\n",
            "Iteration 32, loss = 0.57293397\n",
            "Iteration 33, loss = 0.56708000\n",
            "Iteration 34, loss = 0.56445484\n",
            "Iteration 35, loss = 0.57992362\n",
            "Iteration 36, loss = 0.56134774\n",
            "Iteration 37, loss = 0.56086929\n",
            "Iteration 38, loss = 0.56631946\n",
            "Iteration 39, loss = 0.56219543\n",
            "Iteration 40, loss = 0.56937993\n",
            "Iteration 41, loss = 0.55951683\n",
            "Iteration 42, loss = 0.55613913\n",
            "Iteration 43, loss = 0.54822682\n",
            "Iteration 44, loss = 0.55333591\n",
            "Iteration 45, loss = 0.54736838\n",
            "Iteration 46, loss = 0.54816979\n",
            "Iteration 47, loss = 0.55262357\n",
            "Iteration 48, loss = 0.54121356\n",
            "Iteration 49, loss = 0.55505920\n",
            "Iteration 50, loss = 0.54471826\n",
            "Iteration 51, loss = 0.54050446\n",
            "Iteration 52, loss = 0.53590705\n",
            "Iteration 53, loss = 0.54662427\n",
            "Iteration 54, loss = 0.54136664\n",
            "Iteration 55, loss = 0.53861074\n",
            "Iteration 56, loss = 0.52781784\n",
            "Iteration 57, loss = 0.54385406\n",
            "Iteration 58, loss = 0.53491334\n",
            "Iteration 59, loss = 0.52843427\n",
            "Iteration 60, loss = 0.53111715\n",
            "Iteration 61, loss = 0.55251887\n",
            "Iteration 62, loss = 0.53762956\n",
            "Iteration 63, loss = 0.54359061\n",
            "Iteration 64, loss = 0.52847720\n",
            "Iteration 65, loss = 0.54258063\n",
            "Iteration 66, loss = 0.51398695\n",
            "Iteration 67, loss = 0.53142056\n",
            "Iteration 68, loss = 0.52186679\n",
            "Iteration 69, loss = 0.52896548\n",
            "Iteration 70, loss = 0.52993495\n",
            "Iteration 71, loss = 0.52496214\n",
            "Iteration 72, loss = 0.51613176\n",
            "Iteration 73, loss = 0.52387471\n",
            "Iteration 74, loss = 0.51783296\n",
            "Iteration 75, loss = 0.50628387\n",
            "Iteration 76, loss = 0.52124037\n",
            "Iteration 77, loss = 0.51539751\n",
            "Iteration 78, loss = 0.50008668\n",
            "Iteration 79, loss = 0.50304190\n",
            "Iteration 80, loss = 0.52574670\n",
            "Iteration 81, loss = 0.50219359\n",
            "Iteration 82, loss = 0.50724661\n",
            "Iteration 83, loss = 0.51276858\n",
            "Iteration 84, loss = 0.52486624\n",
            "Iteration 85, loss = 0.50183539\n",
            "Iteration 86, loss = 0.50750354\n",
            "Iteration 87, loss = 0.50264086\n",
            "Iteration 88, loss = 0.50545235\n",
            "Iteration 89, loss = 0.50358917\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.88680557\n",
            "Iteration 2, loss = 0.68419467\n",
            "Iteration 3, loss = 0.64207672\n",
            "Iteration 4, loss = 0.65268323\n",
            "Iteration 5, loss = 0.60771417\n",
            "Iteration 6, loss = 0.63365861\n",
            "Iteration 7, loss = 0.61225304\n",
            "Iteration 8, loss = 0.60451130\n",
            "Iteration 9, loss = 0.60431273\n",
            "Iteration 10, loss = 0.60621120\n",
            "Iteration 11, loss = 0.59202595\n",
            "Iteration 12, loss = 0.60446168\n",
            "Iteration 13, loss = 0.59072177\n",
            "Iteration 14, loss = 0.58768560\n",
            "Iteration 15, loss = 0.59195680\n",
            "Iteration 16, loss = 0.58164975\n",
            "Iteration 17, loss = 0.58549621\n",
            "Iteration 18, loss = 0.59669001\n",
            "Iteration 19, loss = 0.58511108\n",
            "Iteration 20, loss = 0.58262825\n",
            "Iteration 21, loss = 0.57394675\n",
            "Iteration 22, loss = 0.58108337\n",
            "Iteration 23, loss = 0.59402720\n",
            "Iteration 24, loss = 0.57209592\n",
            "Iteration 25, loss = 0.56720086\n",
            "Iteration 26, loss = 0.56745333\n",
            "Iteration 27, loss = 0.56196261\n",
            "Iteration 28, loss = 0.57110475\n",
            "Iteration 29, loss = 0.55504943\n",
            "Iteration 30, loss = 0.54867546\n",
            "Iteration 31, loss = 0.55858603\n",
            "Iteration 32, loss = 0.55014248\n",
            "Iteration 33, loss = 0.54039056\n",
            "Iteration 34, loss = 0.56452466\n",
            "Iteration 35, loss = 0.54953219\n",
            "Iteration 36, loss = 0.55645649\n",
            "Iteration 37, loss = 0.54768828\n",
            "Iteration 38, loss = 0.53373032\n",
            "Iteration 39, loss = 0.56100203\n",
            "Iteration 40, loss = 0.53838024\n",
            "Iteration 41, loss = 0.54606404\n",
            "Iteration 42, loss = 0.55099180\n",
            "Iteration 43, loss = 0.54382739\n",
            "Iteration 44, loss = 0.54266901\n",
            "Iteration 45, loss = 0.51992148\n",
            "Iteration 46, loss = 0.52582420\n",
            "Iteration 47, loss = 0.53540377\n",
            "Iteration 48, loss = 0.54305149\n",
            "Iteration 49, loss = 0.53456928\n",
            "Iteration 50, loss = 0.53383861\n",
            "Iteration 51, loss = 0.53644153\n",
            "Iteration 52, loss = 0.52814716\n",
            "Iteration 53, loss = 0.53390245\n",
            "Iteration 54, loss = 0.52100427\n",
            "Iteration 55, loss = 0.52817485\n",
            "Iteration 56, loss = 0.52856231\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.06411047\n",
            "Iteration 2, loss = 0.64788370\n",
            "Iteration 3, loss = 0.65657149\n",
            "Iteration 4, loss = 0.62549579\n",
            "Iteration 5, loss = 0.62617622\n",
            "Iteration 6, loss = 0.62401885\n",
            "Iteration 7, loss = 0.61185373\n",
            "Iteration 8, loss = 0.62197085\n",
            "Iteration 9, loss = 0.59587925\n",
            "Iteration 10, loss = 0.61147541\n",
            "Iteration 11, loss = 0.60445334\n",
            "Iteration 12, loss = 0.62582371\n",
            "Iteration 13, loss = 0.59914895\n",
            "Iteration 14, loss = 0.61092001\n",
            "Iteration 15, loss = 0.60540206\n",
            "Iteration 16, loss = 0.60288565\n",
            "Iteration 17, loss = 0.59832233\n",
            "Iteration 18, loss = 0.60584245\n",
            "Iteration 19, loss = 0.60157561\n",
            "Iteration 20, loss = 0.60116764\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.04576167\n",
            "Iteration 2, loss = 0.70626065\n",
            "Iteration 3, loss = 0.66527432\n",
            "Iteration 4, loss = 0.62883315\n",
            "Iteration 5, loss = 0.61057024\n",
            "Iteration 6, loss = 0.61106229\n",
            "Iteration 7, loss = 0.59482511\n",
            "Iteration 8, loss = 0.59228271\n",
            "Iteration 9, loss = 0.59550351\n",
            "Iteration 10, loss = 0.59454528\n",
            "Iteration 11, loss = 0.59788684\n",
            "Iteration 12, loss = 0.60424847\n",
            "Iteration 13, loss = 0.59609334\n",
            "Iteration 14, loss = 0.59266533\n",
            "Iteration 15, loss = 0.59009043\n",
            "Iteration 16, loss = 0.58134506\n",
            "Iteration 17, loss = 0.57359312\n",
            "Iteration 18, loss = 0.57340632\n",
            "Iteration 19, loss = 0.57995496\n",
            "Iteration 20, loss = 0.57885009\n",
            "Iteration 21, loss = 0.58085093\n",
            "Iteration 22, loss = 0.56908600\n",
            "Iteration 23, loss = 0.56298595\n",
            "Iteration 24, loss = 0.55848204\n",
            "Iteration 25, loss = 0.56342894\n",
            "Iteration 26, loss = 0.55840751\n",
            "Iteration 27, loss = 0.55737271\n",
            "Iteration 28, loss = 0.55557113\n",
            "Iteration 29, loss = 0.54715502\n",
            "Iteration 30, loss = 0.54703622\n",
            "Iteration 31, loss = 0.54612776\n",
            "Iteration 32, loss = 0.53961703\n",
            "Iteration 33, loss = 0.55323975\n",
            "Iteration 34, loss = 0.54127800\n",
            "Iteration 35, loss = 0.54567368\n",
            "Iteration 36, loss = 0.55083595\n",
            "Iteration 37, loss = 0.54500116\n",
            "Iteration 38, loss = 0.54533049\n",
            "Iteration 39, loss = 0.56207919\n",
            "Iteration 40, loss = 0.55714059\n",
            "Iteration 41, loss = 0.53628217\n",
            "Iteration 42, loss = 0.54748003\n",
            "Iteration 43, loss = 0.52968830\n",
            "Iteration 44, loss = 0.54066854\n",
            "Iteration 45, loss = 0.53320693\n",
            "Iteration 46, loss = 0.53899149\n",
            "Iteration 47, loss = 0.52649124\n",
            "Iteration 48, loss = 0.53943540\n",
            "Iteration 49, loss = 0.53251871\n",
            "Iteration 50, loss = 0.53027955\n",
            "Iteration 51, loss = 0.51485741\n",
            "Iteration 52, loss = 0.51053838\n",
            "Iteration 53, loss = 0.51893834\n",
            "Iteration 54, loss = 0.52020874\n",
            "Iteration 55, loss = 0.52119529\n",
            "Iteration 56, loss = 0.52176141\n",
            "Iteration 57, loss = 0.52738839\n",
            "Iteration 58, loss = 0.52027421\n",
            "Iteration 59, loss = 0.51773305\n",
            "Iteration 60, loss = 0.51143006\n",
            "Iteration 61, loss = 0.52454539\n",
            "Iteration 62, loss = 0.52090250\n",
            "Iteration 63, loss = 0.52679304\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69203126\n",
            "Iteration 2, loss = 0.61879732\n",
            "Iteration 3, loss = 0.61113978\n",
            "Iteration 4, loss = 0.59753429\n",
            "Iteration 5, loss = 0.58940931\n",
            "Iteration 6, loss = 0.58491492\n",
            "Iteration 7, loss = 0.58271935\n",
            "Iteration 8, loss = 0.57444303\n",
            "Iteration 9, loss = 0.57073509\n",
            "Iteration 10, loss = 0.56751353\n",
            "Iteration 11, loss = 0.55792483\n",
            "Iteration 12, loss = 0.55336257\n",
            "Iteration 13, loss = 0.54932333\n",
            "Iteration 14, loss = 0.54161849\n",
            "Iteration 15, loss = 0.53218503\n",
            "Iteration 16, loss = 0.52702088\n",
            "Iteration 17, loss = 0.52121468\n",
            "Iteration 18, loss = 0.52577816\n",
            "Iteration 19, loss = 0.51094341\n",
            "Iteration 20, loss = 0.50069660\n",
            "Iteration 21, loss = 0.50351444\n",
            "Iteration 22, loss = 0.49039281\n",
            "Iteration 23, loss = 0.48370621\n",
            "Iteration 24, loss = 0.47614727\n",
            "Iteration 25, loss = 0.47215432\n",
            "Iteration 26, loss = 0.48056586\n",
            "Iteration 27, loss = 0.48062037\n",
            "Iteration 28, loss = 0.47641362\n",
            "Iteration 29, loss = 0.45471183\n",
            "Iteration 30, loss = 0.45453191\n",
            "Iteration 31, loss = 0.45456560\n",
            "Iteration 32, loss = 0.45028093\n",
            "Iteration 33, loss = 0.44905570\n",
            "Iteration 34, loss = 0.45155329\n",
            "Iteration 35, loss = 0.44909816\n",
            "Iteration 36, loss = 0.44106318\n",
            "Iteration 37, loss = 0.44750606\n",
            "Iteration 38, loss = 0.45623423\n",
            "Iteration 39, loss = 0.44664365\n",
            "Iteration 40, loss = 0.43543613\n",
            "Iteration 41, loss = 0.44682962\n",
            "Iteration 42, loss = 0.43585132\n",
            "Iteration 43, loss = 0.44048840\n",
            "Iteration 44, loss = 0.44656181\n",
            "Iteration 45, loss = 0.43204201\n",
            "Iteration 46, loss = 0.44118684\n",
            "Iteration 47, loss = 0.42818443\n",
            "Iteration 48, loss = 0.42632541\n",
            "Iteration 49, loss = 0.42456374\n",
            "Iteration 50, loss = 0.41913878\n",
            "Iteration 51, loss = 0.42565010\n",
            "Iteration 52, loss = 0.42565001\n",
            "Iteration 53, loss = 0.41674114\n",
            "Iteration 54, loss = 0.48403961\n",
            "Iteration 55, loss = 0.44047945\n",
            "Iteration 56, loss = 0.43253341\n",
            "Iteration 57, loss = 0.41788217\n",
            "Iteration 58, loss = 0.41763645\n",
            "Iteration 59, loss = 0.41320908\n",
            "Iteration 60, loss = 0.41605851\n",
            "Iteration 61, loss = 0.41850191\n",
            "Iteration 62, loss = 0.41233758\n",
            "Iteration 63, loss = 0.41702170\n",
            "Iteration 64, loss = 0.40497759\n",
            "Iteration 65, loss = 0.42114429\n",
            "Iteration 66, loss = 0.41226464\n",
            "Iteration 67, loss = 0.41044383\n",
            "Iteration 68, loss = 0.40796660\n",
            "Iteration 69, loss = 0.41490325\n",
            "Iteration 70, loss = 0.42244000\n",
            "Iteration 71, loss = 0.44132516\n",
            "Iteration 72, loss = 0.40891185\n",
            "Iteration 73, loss = 0.41362523\n",
            "Iteration 74, loss = 0.40487809\n",
            "Iteration 75, loss = 0.40755033\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62773331\n",
            "Iteration 2, loss = 0.60601699\n",
            "Iteration 3, loss = 0.59054806\n",
            "Iteration 4, loss = 0.59994440\n",
            "Iteration 5, loss = 0.59654204\n",
            "Iteration 6, loss = 0.58799001\n",
            "Iteration 7, loss = 0.58456863\n",
            "Iteration 8, loss = 0.57301047\n",
            "Iteration 9, loss = 0.57278057\n",
            "Iteration 10, loss = 0.56766281\n",
            "Iteration 11, loss = 0.57203025\n",
            "Iteration 12, loss = 0.56107884\n",
            "Iteration 13, loss = 0.55332571\n",
            "Iteration 14, loss = 0.55023006\n",
            "Iteration 15, loss = 0.55802435\n",
            "Iteration 16, loss = 0.54962542\n",
            "Iteration 17, loss = 0.53974772\n",
            "Iteration 18, loss = 0.52314414\n",
            "Iteration 19, loss = 0.52214225\n",
            "Iteration 20, loss = 0.51990565\n",
            "Iteration 21, loss = 0.51198308\n",
            "Iteration 22, loss = 0.50553709\n",
            "Iteration 23, loss = 0.49533132\n",
            "Iteration 24, loss = 0.48443360\n",
            "Iteration 25, loss = 0.47919147\n",
            "Iteration 26, loss = 0.47475868\n",
            "Iteration 27, loss = 0.46926253\n",
            "Iteration 28, loss = 0.47801084\n",
            "Iteration 29, loss = 0.46735197\n",
            "Iteration 30, loss = 0.47858123\n",
            "Iteration 31, loss = 0.45898867\n",
            "Iteration 32, loss = 0.45085104\n",
            "Iteration 33, loss = 0.44791193\n",
            "Iteration 34, loss = 0.44695465\n",
            "Iteration 35, loss = 0.44369836\n",
            "Iteration 36, loss = 0.44034808\n",
            "Iteration 37, loss = 0.43754407\n",
            "Iteration 38, loss = 0.45519504\n",
            "Iteration 39, loss = 0.46979381\n",
            "Iteration 40, loss = 0.44558093\n",
            "Iteration 41, loss = 0.44979245\n",
            "Iteration 42, loss = 0.43775060\n",
            "Iteration 43, loss = 0.42730228\n",
            "Iteration 44, loss = 0.43279164\n",
            "Iteration 45, loss = 0.42730858\n",
            "Iteration 46, loss = 0.42706242\n",
            "Iteration 47, loss = 0.42943653\n",
            "Iteration 48, loss = 0.42553023\n",
            "Iteration 49, loss = 0.42955292\n",
            "Iteration 50, loss = 0.42669821\n",
            "Iteration 51, loss = 0.42321191\n",
            "Iteration 52, loss = 0.42147919\n",
            "Iteration 53, loss = 0.42025735\n",
            "Iteration 54, loss = 0.42459497\n",
            "Iteration 55, loss = 0.43286357\n",
            "Iteration 56, loss = 0.42584670\n",
            "Iteration 57, loss = 0.41970968\n",
            "Iteration 58, loss = 0.41847736\n",
            "Iteration 59, loss = 0.44622412\n",
            "Iteration 60, loss = 0.42755103\n",
            "Iteration 61, loss = 0.42163265\n",
            "Iteration 62, loss = 0.41304488\n",
            "Iteration 63, loss = 0.42865253\n",
            "Iteration 64, loss = 0.42346639\n",
            "Iteration 65, loss = 0.41827739\n",
            "Iteration 66, loss = 0.42306464\n",
            "Iteration 67, loss = 0.44937084\n",
            "Iteration 68, loss = 0.42687728\n",
            "Iteration 69, loss = 0.40675836\n",
            "Iteration 70, loss = 0.40940759\n",
            "Iteration 71, loss = 0.41191604\n",
            "Iteration 72, loss = 0.40497170\n",
            "Iteration 73, loss = 0.40713317\n",
            "Iteration 74, loss = 0.40644061\n",
            "Iteration 75, loss = 0.41130398\n",
            "Iteration 76, loss = 0.43800649\n",
            "Iteration 77, loss = 0.41272640\n",
            "Iteration 78, loss = 0.40473426\n",
            "Iteration 79, loss = 0.39895090\n",
            "Iteration 80, loss = 0.40229587\n",
            "Iteration 81, loss = 0.41527674\n",
            "Iteration 82, loss = 0.40520812\n",
            "Iteration 83, loss = 0.40350480\n",
            "Iteration 84, loss = 0.40137331\n",
            "Iteration 85, loss = 0.40499040\n",
            "Iteration 86, loss = 0.40926098\n",
            "Iteration 87, loss = 0.40098432\n",
            "Iteration 88, loss = 0.39505662\n",
            "Iteration 89, loss = 0.39834047\n",
            "Iteration 90, loss = 0.39672939\n",
            "Iteration 91, loss = 0.39255886\n",
            "Iteration 92, loss = 0.39181713\n",
            "Iteration 93, loss = 0.39771911\n",
            "Iteration 94, loss = 0.40815859\n",
            "Iteration 95, loss = 0.41444169\n",
            "Iteration 96, loss = 0.40484887\n",
            "Iteration 97, loss = 0.39657523\n",
            "Iteration 98, loss = 0.39170569\n",
            "Iteration 99, loss = 0.39336245\n",
            "Iteration 100, loss = 0.39326148\n",
            "Iteration 101, loss = 0.38673602\n",
            "Iteration 102, loss = 0.41951989\n",
            "Iteration 103, loss = 0.40059652\n",
            "Iteration 104, loss = 0.39501578\n",
            "Iteration 105, loss = 0.39361834\n",
            "Iteration 106, loss = 0.39045608\n",
            "Iteration 107, loss = 0.38810975\n",
            "Iteration 108, loss = 0.38468067\n",
            "Iteration 109, loss = 0.39004058\n",
            "Iteration 110, loss = 0.40194341\n",
            "Iteration 111, loss = 0.39034059\n",
            "Iteration 112, loss = 0.40874332\n",
            "Iteration 113, loss = 0.38659923\n",
            "Iteration 114, loss = 0.38666942\n",
            "Iteration 115, loss = 0.39595994\n",
            "Iteration 116, loss = 0.38652647\n",
            "Iteration 117, loss = 0.38994899\n",
            "Iteration 118, loss = 0.39806401\n",
            "Iteration 119, loss = 0.39771884\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.73233232\n",
            "Iteration 2, loss = 0.63600697\n",
            "Iteration 3, loss = 0.62352819\n",
            "Iteration 4, loss = 0.61165843\n",
            "Iteration 5, loss = 0.60878440\n",
            "Iteration 6, loss = 0.60814990\n",
            "Iteration 7, loss = 0.60435642\n",
            "Iteration 8, loss = 0.59975716\n",
            "Iteration 9, loss = 0.59818161\n",
            "Iteration 10, loss = 0.59499656\n",
            "Iteration 11, loss = 0.59021511\n",
            "Iteration 12, loss = 0.59544098\n",
            "Iteration 13, loss = 0.58874545\n",
            "Iteration 14, loss = 0.58675710\n",
            "Iteration 15, loss = 0.58211782\n",
            "Iteration 16, loss = 0.57806739\n",
            "Iteration 17, loss = 0.56700413\n",
            "Iteration 18, loss = 0.56252392\n",
            "Iteration 19, loss = 0.56121529\n",
            "Iteration 20, loss = 0.55424237\n",
            "Iteration 21, loss = 0.54537980\n",
            "Iteration 22, loss = 0.54960996\n",
            "Iteration 23, loss = 0.53515487\n",
            "Iteration 24, loss = 0.52621632\n",
            "Iteration 25, loss = 0.51646249\n",
            "Iteration 26, loss = 0.51477225\n",
            "Iteration 27, loss = 0.49886397\n",
            "Iteration 28, loss = 0.49965278\n",
            "Iteration 29, loss = 0.49361361\n",
            "Iteration 30, loss = 0.49352337\n",
            "Iteration 31, loss = 0.48171848\n",
            "Iteration 32, loss = 0.47255196\n",
            "Iteration 33, loss = 0.46895568\n",
            "Iteration 34, loss = 0.46154454\n",
            "Iteration 35, loss = 0.46405724\n",
            "Iteration 36, loss = 0.46562339\n",
            "Iteration 37, loss = 0.45324707\n",
            "Iteration 38, loss = 0.44754984\n",
            "Iteration 39, loss = 0.44932450\n",
            "Iteration 40, loss = 0.45316561\n",
            "Iteration 41, loss = 0.47031213\n",
            "Iteration 42, loss = 0.46740260\n",
            "Iteration 43, loss = 0.45445862\n",
            "Iteration 44, loss = 0.44181153\n",
            "Iteration 45, loss = 0.44518201\n",
            "Iteration 46, loss = 0.43845050\n",
            "Iteration 47, loss = 0.43230631\n",
            "Iteration 48, loss = 0.44934614\n",
            "Iteration 49, loss = 0.45420090\n",
            "Iteration 50, loss = 0.43896169\n",
            "Iteration 51, loss = 0.43592871\n",
            "Iteration 52, loss = 0.43223982\n",
            "Iteration 53, loss = 0.43168766\n",
            "Iteration 54, loss = 0.43297091\n",
            "Iteration 55, loss = 0.42792853\n",
            "Iteration 56, loss = 0.42743465\n",
            "Iteration 57, loss = 0.43674297\n",
            "Iteration 58, loss = 0.42838942\n",
            "Iteration 59, loss = 0.43063650\n",
            "Iteration 60, loss = 0.42577191\n",
            "Iteration 61, loss = 0.42977100\n",
            "Iteration 62, loss = 0.42333916\n",
            "Iteration 63, loss = 0.42437229\n",
            "Iteration 64, loss = 0.42507000\n",
            "Iteration 65, loss = 0.42437628\n",
            "Iteration 66, loss = 0.43051978\n",
            "Iteration 67, loss = 0.42895021\n",
            "Iteration 68, loss = 0.42656868\n",
            "Iteration 69, loss = 0.41917115\n",
            "Iteration 70, loss = 0.41786685\n",
            "Iteration 71, loss = 0.42272404\n",
            "Iteration 72, loss = 0.41804319\n",
            "Iteration 73, loss = 0.41309407\n",
            "Iteration 74, loss = 0.42154304\n",
            "Iteration 75, loss = 0.41546523\n",
            "Iteration 76, loss = 0.41951868\n",
            "Iteration 77, loss = 0.41042874\n",
            "Iteration 78, loss = 0.42314076\n",
            "Iteration 79, loss = 0.44081413\n",
            "Iteration 80, loss = 0.42763234\n",
            "Iteration 81, loss = 0.42007927\n",
            "Iteration 82, loss = 0.41247171\n",
            "Iteration 83, loss = 0.40769473\n",
            "Iteration 84, loss = 0.42318722\n",
            "Iteration 85, loss = 0.43592169\n",
            "Iteration 86, loss = 0.41051635\n",
            "Iteration 87, loss = 0.41960534\n",
            "Iteration 88, loss = 0.41709720\n",
            "Iteration 89, loss = 0.40895971\n",
            "Iteration 90, loss = 0.41478635\n",
            "Iteration 91, loss = 0.40887204\n",
            "Iteration 92, loss = 0.40657712\n",
            "Iteration 93, loss = 0.41790707\n",
            "Iteration 94, loss = 0.40535748\n",
            "Iteration 95, loss = 0.40977864\n",
            "Iteration 96, loss = 0.41636648\n",
            "Iteration 97, loss = 0.40174369\n",
            "Iteration 98, loss = 0.41159801\n",
            "Iteration 99, loss = 0.41248500\n",
            "Iteration 100, loss = 0.40812166\n",
            "Iteration 101, loss = 0.40387219\n",
            "Iteration 102, loss = 0.40711611\n",
            "Iteration 103, loss = 0.40464308\n",
            "Iteration 104, loss = 0.39764577\n",
            "Iteration 105, loss = 0.40011241\n",
            "Iteration 106, loss = 0.40538712\n",
            "Iteration 107, loss = 0.40160238\n",
            "Iteration 108, loss = 0.44153500\n",
            "Iteration 109, loss = 0.42559718\n",
            "Iteration 110, loss = 0.40499674\n",
            "Iteration 111, loss = 0.39968811\n",
            "Iteration 112, loss = 0.39912633\n",
            "Iteration 113, loss = 0.39643447\n",
            "Iteration 114, loss = 0.39851695\n",
            "Iteration 115, loss = 0.41335993\n",
            "Iteration 116, loss = 0.40173679\n",
            "Iteration 117, loss = 0.40111494\n",
            "Iteration 118, loss = 0.40112226\n",
            "Iteration 119, loss = 0.39790656\n",
            "Iteration 120, loss = 0.39148996\n",
            "Iteration 121, loss = 0.39493794\n",
            "Iteration 122, loss = 0.39420932\n",
            "Iteration 123, loss = 0.40609649\n",
            "Iteration 124, loss = 0.39823884\n",
            "Iteration 125, loss = 0.39524709\n",
            "Iteration 126, loss = 0.40203071\n",
            "Iteration 127, loss = 0.39264181\n",
            "Iteration 128, loss = 0.39112822\n",
            "Iteration 129, loss = 0.38964419\n",
            "Iteration 130, loss = 0.38888237\n",
            "Iteration 131, loss = 0.39542414\n",
            "Iteration 132, loss = 0.39389703\n",
            "Iteration 133, loss = 0.39063765\n",
            "Iteration 134, loss = 0.39513597\n",
            "Iteration 135, loss = 0.39219925\n",
            "Iteration 136, loss = 0.38980235\n",
            "Iteration 137, loss = 0.39350691\n",
            "Iteration 138, loss = 0.40175893\n",
            "Iteration 139, loss = 0.39782576\n",
            "Iteration 140, loss = 0.39808095\n",
            "Iteration 141, loss = 0.39832339\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.60833975\n",
            "Iteration 2, loss = 0.59294745\n",
            "Iteration 3, loss = 0.59080779\n",
            "Iteration 4, loss = 0.58827211\n",
            "Iteration 5, loss = 0.58684324\n",
            "Iteration 6, loss = 0.57890757\n",
            "Iteration 7, loss = 0.57229319\n",
            "Iteration 8, loss = 0.57008313\n",
            "Iteration 9, loss = 0.56679024\n",
            "Iteration 10, loss = 0.56008526\n",
            "Iteration 11, loss = 0.55548130\n",
            "Iteration 12, loss = 0.55015129\n",
            "Iteration 13, loss = 0.54491156\n",
            "Iteration 14, loss = 0.53873170\n",
            "Iteration 15, loss = 0.53332135\n",
            "Iteration 16, loss = 0.52921312\n",
            "Iteration 17, loss = 0.52757983\n",
            "Iteration 18, loss = 0.52796506\n",
            "Iteration 19, loss = 0.51806275\n",
            "Iteration 20, loss = 0.52356357\n",
            "Iteration 21, loss = 0.52190330\n",
            "Iteration 22, loss = 0.52701839\n",
            "Iteration 23, loss = 0.50317464\n",
            "Iteration 24, loss = 0.49554597\n",
            "Iteration 25, loss = 0.48954014\n",
            "Iteration 26, loss = 0.48398980\n",
            "Iteration 27, loss = 0.48912182\n",
            "Iteration 28, loss = 0.47614366\n",
            "Iteration 29, loss = 0.47101872\n",
            "Iteration 30, loss = 0.46841629\n",
            "Iteration 31, loss = 0.46769705\n",
            "Iteration 32, loss = 0.46137856\n",
            "Iteration 33, loss = 0.46185418\n",
            "Iteration 34, loss = 0.46242399\n",
            "Iteration 35, loss = 0.45857608\n",
            "Iteration 36, loss = 0.45315033\n",
            "Iteration 37, loss = 0.47193301\n",
            "Iteration 38, loss = 0.45220994\n",
            "Iteration 39, loss = 0.44748284\n",
            "Iteration 40, loss = 0.44454053\n",
            "Iteration 41, loss = 0.44384948\n",
            "Iteration 42, loss = 0.44499175\n",
            "Iteration 43, loss = 0.46386858\n",
            "Iteration 44, loss = 0.44064577\n",
            "Iteration 45, loss = 0.45711112\n",
            "Iteration 46, loss = 0.44320081\n",
            "Iteration 47, loss = 0.44347790\n",
            "Iteration 48, loss = 0.45087994\n",
            "Iteration 49, loss = 0.44371958\n",
            "Iteration 50, loss = 0.42769662\n",
            "Iteration 51, loss = 0.42868366\n",
            "Iteration 52, loss = 0.43722125\n",
            "Iteration 53, loss = 0.43783471\n",
            "Iteration 54, loss = 0.43149637\n",
            "Iteration 55, loss = 0.42614227\n",
            "Iteration 56, loss = 0.42928227\n",
            "Iteration 57, loss = 0.42736940\n",
            "Iteration 58, loss = 0.42604495\n",
            "Iteration 59, loss = 0.42006629\n",
            "Iteration 60, loss = 0.42020689\n",
            "Iteration 61, loss = 0.41817820\n",
            "Iteration 62, loss = 0.42164865\n",
            "Iteration 63, loss = 0.41960129\n",
            "Iteration 64, loss = 0.42031704\n",
            "Iteration 65, loss = 0.42023676\n",
            "Iteration 66, loss = 0.43299597\n",
            "Iteration 67, loss = 0.42211058\n",
            "Iteration 68, loss = 0.42440061\n",
            "Iteration 69, loss = 0.42657708\n",
            "Iteration 70, loss = 0.41932939\n",
            "Iteration 71, loss = 0.41285218\n",
            "Iteration 72, loss = 0.41350199\n",
            "Iteration 73, loss = 0.41642911\n",
            "Iteration 74, loss = 0.41984330\n",
            "Iteration 75, loss = 0.40488512\n",
            "Iteration 76, loss = 0.42544151\n",
            "Iteration 77, loss = 0.41089012\n",
            "Iteration 78, loss = 0.43192115\n",
            "Iteration 79, loss = 0.41657903\n",
            "Iteration 80, loss = 0.40320596\n",
            "Iteration 81, loss = 0.40272180\n",
            "Iteration 82, loss = 0.40290480\n",
            "Iteration 83, loss = 0.39906798\n",
            "Iteration 84, loss = 0.39935045\n",
            "Iteration 85, loss = 0.40249537\n",
            "Iteration 86, loss = 0.40194717\n",
            "Iteration 87, loss = 0.40213980\n",
            "Iteration 88, loss = 0.40519433\n",
            "Iteration 89, loss = 0.41052248\n",
            "Iteration 90, loss = 0.40396305\n",
            "Iteration 91, loss = 0.39653854\n",
            "Iteration 92, loss = 0.39933742\n",
            "Iteration 93, loss = 0.39404981\n",
            "Iteration 94, loss = 0.40098896\n",
            "Iteration 95, loss = 0.39325933\n",
            "Iteration 96, loss = 0.40079940\n",
            "Iteration 97, loss = 0.39187754\n",
            "Iteration 98, loss = 0.39634331\n",
            "Iteration 99, loss = 0.39259652\n",
            "Iteration 100, loss = 0.39955851\n",
            "Iteration 101, loss = 0.41466862\n",
            "Iteration 102, loss = 0.43488587\n",
            "Iteration 103, loss = 0.40925672\n",
            "Iteration 104, loss = 0.39890632\n",
            "Iteration 105, loss = 0.41013710\n",
            "Iteration 106, loss = 0.40750373\n",
            "Iteration 107, loss = 0.39749806\n",
            "Iteration 108, loss = 0.38800723\n",
            "Iteration 109, loss = 0.39032742\n",
            "Iteration 110, loss = 0.38859462\n",
            "Iteration 111, loss = 0.39510643\n",
            "Iteration 112, loss = 0.38521335\n",
            "Iteration 113, loss = 0.38764406\n",
            "Iteration 114, loss = 0.38309296\n",
            "Iteration 115, loss = 0.38562118\n",
            "Iteration 116, loss = 0.39340015\n",
            "Iteration 117, loss = 0.38135699\n",
            "Iteration 118, loss = 0.38402092\n",
            "Iteration 119, loss = 0.38919283\n",
            "Iteration 120, loss = 0.38501701\n",
            "Iteration 121, loss = 0.38554261\n",
            "Iteration 122, loss = 0.38661106\n",
            "Iteration 123, loss = 0.38627863\n",
            "Iteration 124, loss = 0.38023379\n",
            "Iteration 125, loss = 0.38715327\n",
            "Iteration 126, loss = 0.38186861\n",
            "Iteration 127, loss = 0.38233869\n",
            "Iteration 128, loss = 0.37660291\n",
            "Iteration 129, loss = 0.37890084\n",
            "Iteration 130, loss = 0.39469968\n",
            "Iteration 131, loss = 0.37960985\n",
            "Iteration 132, loss = 0.37516200\n",
            "Iteration 133, loss = 0.37770491\n",
            "Iteration 134, loss = 0.38593348\n",
            "Iteration 135, loss = 0.37389565\n",
            "Iteration 136, loss = 0.38117490\n",
            "Iteration 137, loss = 0.38891124\n",
            "Iteration 138, loss = 0.38892938\n",
            "Iteration 139, loss = 0.40311233\n",
            "Iteration 140, loss = 0.41455074\n",
            "Iteration 141, loss = 0.40578160\n",
            "Iteration 142, loss = 0.40611752\n",
            "Iteration 143, loss = 0.41565238\n",
            "Iteration 144, loss = 0.37977692\n",
            "Iteration 145, loss = 0.37377269\n",
            "Iteration 146, loss = 0.37559181\n",
            "Iteration 147, loss = 0.37390737\n",
            "Iteration 148, loss = 0.37171802\n",
            "Iteration 149, loss = 0.38028886\n",
            "Iteration 150, loss = 0.37415748\n",
            "Iteration 151, loss = 0.37269263\n",
            "Iteration 152, loss = 0.37664249\n",
            "Iteration 153, loss = 0.37860483\n",
            "Iteration 154, loss = 0.37095240\n",
            "Iteration 155, loss = 0.38023964\n",
            "Iteration 156, loss = 0.37350084\n",
            "Iteration 157, loss = 0.37136810\n",
            "Iteration 158, loss = 0.38196307\n",
            "Iteration 159, loss = 0.38745927\n",
            "Iteration 160, loss = 0.37562009\n",
            "Iteration 161, loss = 0.37885659\n",
            "Iteration 162, loss = 0.36768565\n",
            "Iteration 163, loss = 0.38176579\n",
            "Iteration 164, loss = 0.37137402\n",
            "Iteration 165, loss = 0.37015658\n",
            "Iteration 166, loss = 0.37203259\n",
            "Iteration 167, loss = 0.37010454\n",
            "Iteration 168, loss = 0.36677650\n",
            "Iteration 169, loss = 0.37078723\n",
            "Iteration 170, loss = 0.36754112\n",
            "Iteration 171, loss = 0.39234572\n",
            "Iteration 172, loss = 0.37486078\n",
            "Iteration 173, loss = 0.37934822\n",
            "Iteration 174, loss = 0.37767877\n",
            "Iteration 175, loss = 0.37425460\n",
            "Iteration 176, loss = 0.36965993\n",
            "Iteration 177, loss = 0.37781320\n",
            "Iteration 178, loss = 0.37019566\n",
            "Iteration 179, loss = 0.36413939\n",
            "Iteration 180, loss = 0.36886757\n",
            "Iteration 181, loss = 0.36756132\n",
            "Iteration 182, loss = 0.37123610\n",
            "Iteration 183, loss = 0.36285990\n",
            "Iteration 184, loss = 0.36258478\n",
            "Iteration 185, loss = 0.36350224\n",
            "Iteration 186, loss = 0.36069405\n",
            "Iteration 187, loss = 0.36590597\n",
            "Iteration 188, loss = 0.36594653\n",
            "Iteration 189, loss = 0.37238787\n",
            "Iteration 190, loss = 0.38081612\n",
            "Iteration 191, loss = 0.37357991\n",
            "Iteration 192, loss = 0.35707205\n",
            "Iteration 193, loss = 0.36275686\n",
            "Iteration 194, loss = 0.36570977\n",
            "Iteration 195, loss = 0.36186218\n",
            "Iteration 196, loss = 0.37014619\n",
            "Iteration 197, loss = 0.37504385\n",
            "Iteration 198, loss = 0.37590798\n",
            "Iteration 199, loss = 0.37027115\n",
            "Iteration 200, loss = 0.36915944\n",
            "Iteration 201, loss = 0.35629073\n",
            "Iteration 202, loss = 0.35268625\n",
            "Iteration 203, loss = 0.35422413\n",
            "Iteration 204, loss = 0.35594350\n",
            "Iteration 205, loss = 0.37011965\n",
            "Iteration 206, loss = 0.36523162\n",
            "Iteration 207, loss = 0.36023214\n",
            "Iteration 208, loss = 0.35781302\n",
            "Iteration 209, loss = 0.35665230\n",
            "Iteration 210, loss = 0.35690280\n",
            "Iteration 211, loss = 0.35932574\n",
            "Iteration 212, loss = 0.35319773\n",
            "Iteration 213, loss = 0.36831271\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64911095\n",
            "Iteration 2, loss = 0.60822094\n",
            "Iteration 3, loss = 0.60492651\n",
            "Iteration 4, loss = 0.60224344\n",
            "Iteration 5, loss = 0.60052451\n",
            "Iteration 6, loss = 0.60604186\n",
            "Iteration 7, loss = 0.60101108\n",
            "Iteration 8, loss = 0.59676912\n",
            "Iteration 9, loss = 0.59764080\n",
            "Iteration 10, loss = 0.58901467\n",
            "Iteration 11, loss = 0.59010127\n",
            "Iteration 12, loss = 0.58642520\n",
            "Iteration 13, loss = 0.58246512\n",
            "Iteration 14, loss = 0.57960577\n",
            "Iteration 15, loss = 0.57537761\n",
            "Iteration 16, loss = 0.57147402\n",
            "Iteration 17, loss = 0.56808028\n",
            "Iteration 18, loss = 0.57230887\n",
            "Iteration 19, loss = 0.56291029\n",
            "Iteration 20, loss = 0.55621441\n",
            "Iteration 21, loss = 0.54884500\n",
            "Iteration 22, loss = 0.54143432\n",
            "Iteration 23, loss = 0.53837577\n",
            "Iteration 24, loss = 0.53094007\n",
            "Iteration 25, loss = 0.53313469\n",
            "Iteration 26, loss = 0.53067988\n",
            "Iteration 27, loss = 0.51595341\n",
            "Iteration 28, loss = 0.50757435\n",
            "Iteration 29, loss = 0.50294907\n",
            "Iteration 30, loss = 0.50258677\n",
            "Iteration 31, loss = 0.49891066\n",
            "Iteration 32, loss = 0.49254914\n",
            "Iteration 33, loss = 0.48489160\n",
            "Iteration 34, loss = 0.48421265\n",
            "Iteration 35, loss = 0.47945877\n",
            "Iteration 36, loss = 0.47774070\n",
            "Iteration 37, loss = 0.47301689\n",
            "Iteration 38, loss = 0.46868677\n",
            "Iteration 39, loss = 0.46579537\n",
            "Iteration 40, loss = 0.47808480\n",
            "Iteration 41, loss = 0.47000396\n",
            "Iteration 42, loss = 0.46620013\n",
            "Iteration 43, loss = 0.45657033\n",
            "Iteration 44, loss = 0.45849949\n",
            "Iteration 45, loss = 0.45514326\n",
            "Iteration 46, loss = 0.45322776\n",
            "Iteration 47, loss = 0.44722834\n",
            "Iteration 48, loss = 0.44475038\n",
            "Iteration 49, loss = 0.44133672\n",
            "Iteration 50, loss = 0.43910920\n",
            "Iteration 51, loss = 0.44671813\n",
            "Iteration 52, loss = 0.45532223\n",
            "Iteration 53, loss = 0.43825316\n",
            "Iteration 54, loss = 0.44085791\n",
            "Iteration 55, loss = 0.43769397\n",
            "Iteration 56, loss = 0.43617373\n",
            "Iteration 57, loss = 0.43577742\n",
            "Iteration 58, loss = 0.43498938\n",
            "Iteration 59, loss = 0.44620880\n",
            "Iteration 60, loss = 0.46122394\n",
            "Iteration 61, loss = 0.43249682\n",
            "Iteration 62, loss = 0.43018715\n",
            "Iteration 63, loss = 0.42272857\n",
            "Iteration 64, loss = 0.42436946\n",
            "Iteration 65, loss = 0.42537354\n",
            "Iteration 66, loss = 0.42178540\n",
            "Iteration 67, loss = 0.42058872\n",
            "Iteration 68, loss = 0.42243721\n",
            "Iteration 69, loss = 0.41976940\n",
            "Iteration 70, loss = 0.42980667\n",
            "Iteration 71, loss = 0.42174921\n",
            "Iteration 72, loss = 0.41546411\n",
            "Iteration 73, loss = 0.41587754\n",
            "Iteration 74, loss = 0.41187250\n",
            "Iteration 75, loss = 0.41245302\n",
            "Iteration 76, loss = 0.41328674\n",
            "Iteration 77, loss = 0.41950817\n",
            "Iteration 78, loss = 0.42294895\n",
            "Iteration 79, loss = 0.41797760\n",
            "Iteration 80, loss = 0.41045696\n",
            "Iteration 81, loss = 0.40941666\n",
            "Iteration 82, loss = 0.41146489\n",
            "Iteration 83, loss = 0.41000700\n",
            "Iteration 84, loss = 0.41571334\n",
            "Iteration 85, loss = 0.40910275\n",
            "Iteration 86, loss = 0.41574591\n",
            "Iteration 87, loss = 0.41085339\n",
            "Iteration 88, loss = 0.40678760\n",
            "Iteration 89, loss = 0.39901750\n",
            "Iteration 90, loss = 0.40557525\n",
            "Iteration 91, loss = 0.39831927\n",
            "Iteration 92, loss = 0.39957841\n",
            "Iteration 93, loss = 0.40325590\n",
            "Iteration 94, loss = 0.40906256\n",
            "Iteration 95, loss = 0.41210119\n",
            "Iteration 96, loss = 0.42404921\n",
            "Iteration 97, loss = 0.41296799\n",
            "Iteration 98, loss = 0.39442544\n",
            "Iteration 99, loss = 0.39822828\n",
            "Iteration 100, loss = 0.39937442\n",
            "Iteration 101, loss = 0.39359610\n",
            "Iteration 102, loss = 0.39415283\n",
            "Iteration 103, loss = 0.39283118\n",
            "Iteration 104, loss = 0.38814324\n",
            "Iteration 105, loss = 0.40067034\n",
            "Iteration 106, loss = 0.39545766\n",
            "Iteration 107, loss = 0.39750870\n",
            "Iteration 108, loss = 0.39450292\n",
            "Iteration 109, loss = 0.39721048\n",
            "Iteration 110, loss = 0.39328428\n",
            "Iteration 111, loss = 0.38842285\n",
            "Iteration 112, loss = 0.40281812\n",
            "Iteration 113, loss = 0.39581653\n",
            "Iteration 114, loss = 0.38316312\n",
            "Iteration 115, loss = 0.38564159\n",
            "Iteration 116, loss = 0.39201041\n",
            "Iteration 117, loss = 0.40675908\n",
            "Iteration 118, loss = 0.39969136\n",
            "Iteration 119, loss = 0.39689531\n",
            "Iteration 120, loss = 0.38863579\n",
            "Iteration 121, loss = 0.39089620\n",
            "Iteration 122, loss = 0.38604515\n",
            "Iteration 123, loss = 0.38334328\n",
            "Iteration 124, loss = 0.38557992\n",
            "Iteration 125, loss = 0.38506319\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66788773\n",
            "Iteration 2, loss = 0.66264281\n",
            "Iteration 3, loss = 0.65551872\n",
            "Iteration 4, loss = 0.65077532\n",
            "Iteration 5, loss = 0.64503087\n",
            "Iteration 6, loss = 0.64180584\n",
            "Iteration 7, loss = 0.63707893\n",
            "Iteration 8, loss = 0.63408806\n",
            "Iteration 9, loss = 0.62887107\n",
            "Iteration 10, loss = 0.62677915\n",
            "Iteration 11, loss = 0.62389954\n",
            "Iteration 12, loss = 0.62183705\n",
            "Iteration 13, loss = 0.61733925\n",
            "Iteration 14, loss = 0.61916354\n",
            "Iteration 15, loss = 0.61333553\n",
            "Iteration 16, loss = 0.61698694\n",
            "Iteration 17, loss = 0.61260245\n",
            "Iteration 18, loss = 0.61095939\n",
            "Iteration 19, loss = 0.61365532\n",
            "Iteration 20, loss = 0.61104999\n",
            "Iteration 21, loss = 0.60985095\n",
            "Iteration 22, loss = 0.61600840\n",
            "Iteration 23, loss = 0.61030982\n",
            "Iteration 24, loss = 0.60895067\n",
            "Iteration 25, loss = 0.61005989\n",
            "Iteration 26, loss = 0.60816800\n",
            "Iteration 27, loss = 0.60755081\n",
            "Iteration 28, loss = 0.60886405\n",
            "Iteration 29, loss = 0.60350378\n",
            "Iteration 30, loss = 0.61494771\n",
            "Iteration 31, loss = 0.60720618\n",
            "Iteration 32, loss = 0.60704341\n",
            "Iteration 33, loss = 0.60813107\n",
            "Iteration 34, loss = 0.60722481\n",
            "Iteration 35, loss = 0.60840568\n",
            "Iteration 36, loss = 0.60622492\n",
            "Iteration 37, loss = 0.60755476\n",
            "Iteration 38, loss = 0.60716312\n",
            "Iteration 39, loss = 0.60692477\n",
            "Iteration 40, loss = 0.60558403\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67348052\n",
            "Iteration 2, loss = 0.65961039\n",
            "Iteration 3, loss = 0.65310463\n",
            "Iteration 4, loss = 0.65091158\n",
            "Iteration 5, loss = 0.64506124\n",
            "Iteration 6, loss = 0.63817207\n",
            "Iteration 7, loss = 0.63408568\n",
            "Iteration 8, loss = 0.62910081\n",
            "Iteration 9, loss = 0.62816641\n",
            "Iteration 10, loss = 0.62464846\n",
            "Iteration 11, loss = 0.61779025\n",
            "Iteration 12, loss = 0.61822086\n",
            "Iteration 13, loss = 0.61974855\n",
            "Iteration 14, loss = 0.61296145\n",
            "Iteration 15, loss = 0.61383495\n",
            "Iteration 16, loss = 0.60964772\n",
            "Iteration 17, loss = 0.61052865\n",
            "Iteration 18, loss = 0.60939784\n",
            "Iteration 19, loss = 0.60802061\n",
            "Iteration 20, loss = 0.60893238\n",
            "Iteration 21, loss = 0.61307860\n",
            "Iteration 22, loss = 0.61136154\n",
            "Iteration 23, loss = 0.61155058\n",
            "Iteration 24, loss = 0.61301068\n",
            "Iteration 25, loss = 0.60858912\n",
            "Iteration 26, loss = 0.60695856\n",
            "Iteration 27, loss = 0.60507769\n",
            "Iteration 28, loss = 0.60589824\n",
            "Iteration 29, loss = 0.60448895\n",
            "Iteration 30, loss = 0.60583954\n",
            "Iteration 31, loss = 0.60353307\n",
            "Iteration 32, loss = 0.60626492\n",
            "Iteration 33, loss = 0.60680206\n",
            "Iteration 34, loss = 0.60579920\n",
            "Iteration 35, loss = 0.60779224\n",
            "Iteration 36, loss = 0.60429368\n",
            "Iteration 37, loss = 0.60419672\n",
            "Iteration 38, loss = 0.60627201\n",
            "Iteration 39, loss = 0.60412277\n",
            "Iteration 40, loss = 0.60594129\n",
            "Iteration 41, loss = 0.60713365\n",
            "Iteration 42, loss = 0.60591060\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68247941\n",
            "Iteration 2, loss = 0.66197843\n",
            "Iteration 3, loss = 0.65984198\n",
            "Iteration 4, loss = 0.65467154\n",
            "Iteration 5, loss = 0.65080912\n",
            "Iteration 6, loss = 0.64832633\n",
            "Iteration 7, loss = 0.64495617\n",
            "Iteration 8, loss = 0.64087387\n",
            "Iteration 9, loss = 0.64102906\n",
            "Iteration 10, loss = 0.63587144\n",
            "Iteration 11, loss = 0.63514628\n",
            "Iteration 12, loss = 0.63082553\n",
            "Iteration 13, loss = 0.62986006\n",
            "Iteration 14, loss = 0.62641405\n",
            "Iteration 15, loss = 0.62401609\n",
            "Iteration 16, loss = 0.62435456\n",
            "Iteration 17, loss = 0.62287468\n",
            "Iteration 18, loss = 0.62298912\n",
            "Iteration 19, loss = 0.61960431\n",
            "Iteration 20, loss = 0.61980456\n",
            "Iteration 21, loss = 0.61941743\n",
            "Iteration 22, loss = 0.62064027\n",
            "Iteration 23, loss = 0.61997024\n",
            "Iteration 24, loss = 0.62206979\n",
            "Iteration 25, loss = 0.61949571\n",
            "Iteration 26, loss = 0.61699343\n",
            "Iteration 27, loss = 0.62079159\n",
            "Iteration 28, loss = 0.61685719\n",
            "Iteration 29, loss = 0.62120581\n",
            "Iteration 30, loss = 0.61923208\n",
            "Iteration 31, loss = 0.61821011\n",
            "Iteration 32, loss = 0.61692726\n",
            "Iteration 33, loss = 0.61529181\n",
            "Iteration 34, loss = 0.61698794\n",
            "Iteration 35, loss = 0.61851624\n",
            "Iteration 36, loss = 0.61673902\n",
            "Iteration 37, loss = 0.61783966\n",
            "Iteration 38, loss = 0.61551567\n",
            "Iteration 39, loss = 0.61804683\n",
            "Iteration 40, loss = 0.61820672\n",
            "Iteration 41, loss = 0.61583238\n",
            "Iteration 42, loss = 0.61632711\n",
            "Iteration 43, loss = 0.61622684\n",
            "Iteration 44, loss = 0.61651548\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66139922\n",
            "Iteration 2, loss = 0.65296183\n",
            "Iteration 3, loss = 0.65153481\n",
            "Iteration 4, loss = 0.64165531\n",
            "Iteration 5, loss = 0.64230875\n",
            "Iteration 6, loss = 0.63017273\n",
            "Iteration 7, loss = 0.62930431\n",
            "Iteration 8, loss = 0.62593498\n",
            "Iteration 9, loss = 0.62212235\n",
            "Iteration 10, loss = 0.62082912\n",
            "Iteration 11, loss = 0.61613885\n",
            "Iteration 12, loss = 0.61403349\n",
            "Iteration 13, loss = 0.61127172\n",
            "Iteration 14, loss = 0.61086436\n",
            "Iteration 15, loss = 0.60930331\n",
            "Iteration 16, loss = 0.60727978\n",
            "Iteration 17, loss = 0.61063150\n",
            "Iteration 18, loss = 0.60630225\n",
            "Iteration 19, loss = 0.60676243\n",
            "Iteration 20, loss = 0.60512914\n",
            "Iteration 21, loss = 0.60636496\n",
            "Iteration 22, loss = 0.60654600\n",
            "Iteration 23, loss = 0.60516120\n",
            "Iteration 24, loss = 0.60728393\n",
            "Iteration 25, loss = 0.60424944\n",
            "Iteration 26, loss = 0.60499883\n",
            "Iteration 27, loss = 0.60394425\n",
            "Iteration 28, loss = 0.60415337\n",
            "Iteration 29, loss = 0.60378693\n",
            "Iteration 30, loss = 0.60698662\n",
            "Iteration 31, loss = 0.60338482\n",
            "Iteration 32, loss = 0.60449054\n",
            "Iteration 33, loss = 0.60362521\n",
            "Iteration 34, loss = 0.60477897\n",
            "Iteration 35, loss = 0.60330960\n",
            "Iteration 36, loss = 0.60367744\n",
            "Iteration 37, loss = 0.60263810\n",
            "Iteration 38, loss = 0.60147661\n",
            "Iteration 39, loss = 0.60486568\n",
            "Iteration 40, loss = 0.60147286\n",
            "Iteration 41, loss = 0.60309198\n",
            "Iteration 42, loss = 0.60277683\n",
            "Iteration 43, loss = 0.60414582\n",
            "Iteration 44, loss = 0.60394635\n",
            "Iteration 45, loss = 0.60007193\n",
            "Iteration 46, loss = 0.60253718\n",
            "Iteration 47, loss = 0.60074233\n",
            "Iteration 48, loss = 0.60163563\n",
            "Iteration 49, loss = 0.60254272\n",
            "Iteration 50, loss = 0.60055251\n",
            "Iteration 51, loss = 0.60030018\n",
            "Iteration 52, loss = 0.59997437\n",
            "Iteration 53, loss = 0.60133709\n",
            "Iteration 54, loss = 0.60079853\n",
            "Iteration 55, loss = 0.59999181\n",
            "Iteration 56, loss = 0.59923289\n",
            "Iteration 57, loss = 0.60461647\n",
            "Iteration 58, loss = 0.60355055\n",
            "Iteration 59, loss = 0.59677763\n",
            "Iteration 60, loss = 0.60828672\n",
            "Iteration 61, loss = 0.59845428\n",
            "Iteration 62, loss = 0.59924755\n",
            "Iteration 63, loss = 0.59929318\n",
            "Iteration 64, loss = 0.59477581\n",
            "Iteration 65, loss = 0.60249246\n",
            "Iteration 66, loss = 0.59768725\n",
            "Iteration 67, loss = 0.60189230\n",
            "Iteration 68, loss = 0.59984849\n",
            "Iteration 69, loss = 0.59712047\n",
            "Iteration 70, loss = 0.59894401\n",
            "Iteration 71, loss = 0.59733374\n",
            "Iteration 72, loss = 0.59970116\n",
            "Iteration 73, loss = 0.59832530\n",
            "Iteration 74, loss = 0.59812425\n",
            "Iteration 75, loss = 0.60082116\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67182102\n",
            "Iteration 2, loss = 0.65460190\n",
            "Iteration 3, loss = 0.65173140\n",
            "Iteration 4, loss = 0.64613898\n",
            "Iteration 5, loss = 0.64421244\n",
            "Iteration 6, loss = 0.63944827\n",
            "Iteration 7, loss = 0.63506467\n",
            "Iteration 8, loss = 0.63700231\n",
            "Iteration 9, loss = 0.62843255\n",
            "Iteration 10, loss = 0.62506850\n",
            "Iteration 11, loss = 0.61853910\n",
            "Iteration 12, loss = 0.62383396\n",
            "Iteration 13, loss = 0.61955896\n",
            "Iteration 14, loss = 0.61780811\n",
            "Iteration 15, loss = 0.61531902\n",
            "Iteration 16, loss = 0.61538618\n",
            "Iteration 17, loss = 0.61268867\n",
            "Iteration 18, loss = 0.61504387\n",
            "Iteration 19, loss = 0.61439795\n",
            "Iteration 20, loss = 0.61301515\n",
            "Iteration 21, loss = 0.61109690\n",
            "Iteration 22, loss = 0.61333713\n",
            "Iteration 23, loss = 0.61259518\n",
            "Iteration 24, loss = 0.61003191\n",
            "Iteration 25, loss = 0.61161939\n",
            "Iteration 26, loss = 0.61137971\n",
            "Iteration 27, loss = 0.61044567\n",
            "Iteration 28, loss = 0.61097476\n",
            "Iteration 29, loss = 0.61196710\n",
            "Iteration 30, loss = 0.61137555\n",
            "Iteration 31, loss = 0.61393995\n",
            "Iteration 32, loss = 0.61056016\n",
            "Iteration 33, loss = 0.60942879\n",
            "Iteration 34, loss = 0.60984104\n",
            "Iteration 35, loss = 0.61076245\n",
            "Iteration 36, loss = 0.60911738\n",
            "Iteration 37, loss = 0.61167275\n",
            "Iteration 38, loss = 0.61052301\n",
            "Iteration 39, loss = 0.61018631\n",
            "Iteration 40, loss = 0.61109347\n",
            "Iteration 41, loss = 0.60854367\n",
            "Iteration 42, loss = 0.60849698\n",
            "Iteration 43, loss = 0.60949495\n",
            "Iteration 44, loss = 0.61181023\n",
            "Iteration 45, loss = 0.61022088\n",
            "Iteration 46, loss = 0.60889020\n",
            "Iteration 47, loss = 0.60418697\n",
            "Iteration 48, loss = 0.61162712\n",
            "Iteration 49, loss = 0.60902196\n",
            "Iteration 50, loss = 0.60805316\n",
            "Iteration 51, loss = 0.60732420\n",
            "Iteration 52, loss = 0.60673256\n",
            "Iteration 53, loss = 0.60959490\n",
            "Iteration 54, loss = 0.60798230\n",
            "Iteration 55, loss = 0.60815699\n",
            "Iteration 56, loss = 0.60641896\n",
            "Iteration 57, loss = 0.60851509\n",
            "Iteration 58, loss = 0.60706373\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67540104\n",
            "Iteration 2, loss = 0.66258756\n",
            "Iteration 3, loss = 0.65580320\n",
            "Iteration 4, loss = 0.65363140\n",
            "Iteration 5, loss = 0.64950058\n",
            "Iteration 6, loss = 0.64637078\n",
            "Iteration 7, loss = 0.64338075\n",
            "Iteration 8, loss = 0.63994444\n",
            "Iteration 9, loss = 0.63665427\n",
            "Iteration 10, loss = 0.63300899\n",
            "Iteration 11, loss = 0.62882331\n",
            "Iteration 12, loss = 0.62608348\n",
            "Iteration 13, loss = 0.62361013\n",
            "Iteration 14, loss = 0.62085215\n",
            "Iteration 15, loss = 0.61865133\n",
            "Iteration 16, loss = 0.61522455\n",
            "Iteration 17, loss = 0.61306886\n",
            "Iteration 18, loss = 0.61038189\n",
            "Iteration 19, loss = 0.60800868\n",
            "Iteration 20, loss = 0.60619618\n",
            "Iteration 21, loss = 0.60429393\n",
            "Iteration 22, loss = 0.60300328\n",
            "Iteration 23, loss = 0.60103205\n",
            "Iteration 24, loss = 0.59941879\n",
            "Iteration 25, loss = 0.59793644\n",
            "Iteration 26, loss = 0.59643839\n",
            "Iteration 27, loss = 0.59563662\n",
            "Iteration 28, loss = 0.59596829\n",
            "Iteration 29, loss = 0.59415006\n",
            "Iteration 30, loss = 0.59119199\n",
            "Iteration 31, loss = 0.59030800\n",
            "Iteration 32, loss = 0.59045440\n",
            "Iteration 33, loss = 0.58827104\n",
            "Iteration 34, loss = 0.58651387\n",
            "Iteration 35, loss = 0.58583000\n",
            "Iteration 36, loss = 0.58398274\n",
            "Iteration 37, loss = 0.58247903\n",
            "Iteration 38, loss = 0.58133283\n",
            "Iteration 39, loss = 0.57838674\n",
            "Iteration 40, loss = 0.57731830\n",
            "Iteration 41, loss = 0.57631801\n",
            "Iteration 42, loss = 0.57344729\n",
            "Iteration 43, loss = 0.57164640\n",
            "Iteration 44, loss = 0.56960619\n",
            "Iteration 45, loss = 0.56857166\n",
            "Iteration 46, loss = 0.56745350\n",
            "Iteration 47, loss = 0.56908108\n",
            "Iteration 48, loss = 0.56291145\n",
            "Iteration 49, loss = 0.55986988\n",
            "Iteration 50, loss = 0.55771519\n",
            "Iteration 51, loss = 0.55582453\n",
            "Iteration 52, loss = 0.55573309\n",
            "Iteration 53, loss = 0.55269320\n",
            "Iteration 54, loss = 0.55217111\n",
            "Iteration 55, loss = 0.54727527\n",
            "Iteration 56, loss = 0.54659154\n",
            "Iteration 57, loss = 0.54324770\n",
            "Iteration 58, loss = 0.54310337\n",
            "Iteration 59, loss = 0.54024289\n",
            "Iteration 60, loss = 0.53714907\n",
            "Iteration 61, loss = 0.53480551\n",
            "Iteration 62, loss = 0.53257206\n",
            "Iteration 63, loss = 0.53677124\n",
            "Iteration 64, loss = 0.53112991\n",
            "Iteration 65, loss = 0.52557037\n",
            "Iteration 66, loss = 0.52202596\n",
            "Iteration 67, loss = 0.51835063\n",
            "Iteration 68, loss = 0.51707053\n",
            "Iteration 69, loss = 0.51251250\n",
            "Iteration 70, loss = 0.51209559\n",
            "Iteration 71, loss = 0.50941442\n",
            "Iteration 72, loss = 0.50611344\n",
            "Iteration 73, loss = 0.50123958\n",
            "Iteration 74, loss = 0.49788956\n",
            "Iteration 75, loss = 0.50111144\n",
            "Iteration 76, loss = 0.49326739\n",
            "Iteration 77, loss = 0.49095691\n",
            "Iteration 78, loss = 0.48909900\n",
            "Iteration 79, loss = 0.48615316\n",
            "Iteration 80, loss = 0.48887956\n",
            "Iteration 81, loss = 0.48259779\n",
            "Iteration 82, loss = 0.49400816\n",
            "Iteration 83, loss = 0.48769599\n",
            "Iteration 84, loss = 0.48790714\n",
            "Iteration 85, loss = 0.47551576\n",
            "Iteration 86, loss = 0.48052596\n",
            "Iteration 87, loss = 0.47334644\n",
            "Iteration 88, loss = 0.47333491\n",
            "Iteration 89, loss = 0.46782198\n",
            "Iteration 90, loss = 0.46694890\n",
            "Iteration 91, loss = 0.46611908\n",
            "Iteration 92, loss = 0.46489696\n",
            "Iteration 93, loss = 0.46251949\n",
            "Iteration 94, loss = 0.46136704\n",
            "Iteration 95, loss = 0.45813255\n",
            "Iteration 96, loss = 0.45929844\n",
            "Iteration 97, loss = 0.45679286\n",
            "Iteration 98, loss = 0.45407663\n",
            "Iteration 99, loss = 0.45394584\n",
            "Iteration 100, loss = 0.45291978\n",
            "Iteration 101, loss = 0.45233024\n",
            "Iteration 102, loss = 0.45281125\n",
            "Iteration 103, loss = 0.45516723\n",
            "Iteration 104, loss = 0.45197117\n",
            "Iteration 105, loss = 0.45185965\n",
            "Iteration 106, loss = 0.45391140\n",
            "Iteration 107, loss = 0.45467396\n",
            "Iteration 108, loss = 0.44855653\n",
            "Iteration 109, loss = 0.44734021\n",
            "Iteration 110, loss = 0.44629055\n",
            "Iteration 111, loss = 0.44481116\n",
            "Iteration 112, loss = 0.44617594\n",
            "Iteration 113, loss = 0.44456215\n",
            "Iteration 114, loss = 0.44340720\n",
            "Iteration 115, loss = 0.44365594\n",
            "Iteration 116, loss = 0.44196244\n",
            "Iteration 117, loss = 0.44314773\n",
            "Iteration 118, loss = 0.44073318\n",
            "Iteration 119, loss = 0.44577769\n",
            "Iteration 120, loss = 0.44514781\n",
            "Iteration 121, loss = 0.44165337\n",
            "Iteration 122, loss = 0.43985498\n",
            "Iteration 123, loss = 0.44390606\n",
            "Iteration 124, loss = 0.44169537\n",
            "Iteration 125, loss = 0.43831602\n",
            "Iteration 126, loss = 0.45148491\n",
            "Iteration 127, loss = 0.44201099\n",
            "Iteration 128, loss = 0.44256101\n",
            "Iteration 129, loss = 0.43919657\n",
            "Iteration 130, loss = 0.44015384\n",
            "Iteration 131, loss = 0.43748075\n",
            "Iteration 132, loss = 0.43797059\n",
            "Iteration 133, loss = 0.43654264\n",
            "Iteration 134, loss = 0.43597935\n",
            "Iteration 135, loss = 0.43699736\n",
            "Iteration 136, loss = 0.43727988\n",
            "Iteration 137, loss = 0.43483361\n",
            "Iteration 138, loss = 0.43722573\n",
            "Iteration 139, loss = 0.43643890\n",
            "Iteration 140, loss = 0.43750281\n",
            "Iteration 141, loss = 0.43381693\n",
            "Iteration 142, loss = 0.43265273\n",
            "Iteration 143, loss = 0.43538920\n",
            "Iteration 144, loss = 0.43923878\n",
            "Iteration 145, loss = 0.43761352\n",
            "Iteration 146, loss = 0.43830254\n",
            "Iteration 147, loss = 0.43719405\n",
            "Iteration 148, loss = 0.43536104\n",
            "Iteration 149, loss = 0.43372903\n",
            "Iteration 150, loss = 0.43087173\n",
            "Iteration 151, loss = 0.44007320\n",
            "Iteration 152, loss = 0.43270869\n",
            "Iteration 153, loss = 0.43695697\n",
            "Iteration 154, loss = 0.43256799\n",
            "Iteration 155, loss = 0.43148853\n",
            "Iteration 156, loss = 0.43021361\n",
            "Iteration 157, loss = 0.43269509\n",
            "Iteration 158, loss = 0.43059123\n",
            "Iteration 159, loss = 0.43055487\n",
            "Iteration 160, loss = 0.42999280\n",
            "Iteration 161, loss = 0.42968665\n",
            "Iteration 162, loss = 0.42968808\n",
            "Iteration 163, loss = 0.43415705\n",
            "Iteration 164, loss = 0.43926471\n",
            "Iteration 165, loss = 0.43225291\n",
            "Iteration 166, loss = 0.43700995\n",
            "Iteration 167, loss = 0.42939104\n",
            "Iteration 168, loss = 0.43481017\n",
            "Iteration 169, loss = 0.42892040\n",
            "Iteration 170, loss = 0.42889472\n",
            "Iteration 171, loss = 0.42920144\n",
            "Iteration 172, loss = 0.42917288\n",
            "Iteration 173, loss = 0.42852582\n",
            "Iteration 174, loss = 0.42793080\n",
            "Iteration 175, loss = 0.42760232\n",
            "Iteration 176, loss = 0.43063835\n",
            "Iteration 177, loss = 0.42967298\n",
            "Iteration 178, loss = 0.42770108\n",
            "Iteration 179, loss = 0.42826583\n",
            "Iteration 180, loss = 0.42593794\n",
            "Iteration 181, loss = 0.42851771\n",
            "Iteration 182, loss = 0.42523047\n",
            "Iteration 183, loss = 0.42952070\n",
            "Iteration 184, loss = 0.43214556\n",
            "Iteration 185, loss = 0.42940030\n",
            "Iteration 186, loss = 0.42777078\n",
            "Iteration 187, loss = 0.42635179\n",
            "Iteration 188, loss = 0.42627045\n",
            "Iteration 189, loss = 0.42407423\n",
            "Iteration 190, loss = 0.42485337\n",
            "Iteration 191, loss = 0.42941875\n",
            "Iteration 192, loss = 0.42411513\n",
            "Iteration 193, loss = 0.43101544\n",
            "Iteration 194, loss = 0.43047409\n",
            "Iteration 195, loss = 0.43926397\n",
            "Iteration 196, loss = 0.42954436\n",
            "Iteration 197, loss = 0.42632835\n",
            "Iteration 198, loss = 0.42808475\n",
            "Iteration 199, loss = 0.42542309\n",
            "Iteration 200, loss = 0.42620001\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68894548\n",
            "Iteration 2, loss = 0.66463513\n",
            "Iteration 3, loss = 0.65407466\n",
            "Iteration 4, loss = 0.64818907\n",
            "Iteration 5, loss = 0.64475643\n",
            "Iteration 6, loss = 0.64131988\n",
            "Iteration 7, loss = 0.63758179\n",
            "Iteration 8, loss = 0.63403147\n",
            "Iteration 9, loss = 0.62996051\n",
            "Iteration 10, loss = 0.62598077\n",
            "Iteration 11, loss = 0.62271457\n",
            "Iteration 12, loss = 0.61952085\n",
            "Iteration 13, loss = 0.61631450\n",
            "Iteration 14, loss = 0.61421486\n",
            "Iteration 15, loss = 0.61159677\n",
            "Iteration 16, loss = 0.60968732\n",
            "Iteration 17, loss = 0.60814214\n",
            "Iteration 18, loss = 0.60670931\n",
            "Iteration 19, loss = 0.60539234\n",
            "Iteration 20, loss = 0.60428908\n",
            "Iteration 21, loss = 0.60309014\n",
            "Iteration 22, loss = 0.60239392\n",
            "Iteration 23, loss = 0.60232376\n",
            "Iteration 24, loss = 0.60080876\n",
            "Iteration 25, loss = 0.60013149\n",
            "Iteration 26, loss = 0.59850410\n",
            "Iteration 27, loss = 0.59993187\n",
            "Iteration 28, loss = 0.59879765\n",
            "Iteration 29, loss = 0.59717346\n",
            "Iteration 30, loss = 0.59679876\n",
            "Iteration 31, loss = 0.59700850\n",
            "Iteration 32, loss = 0.59528933\n",
            "Iteration 33, loss = 0.59411148\n",
            "Iteration 34, loss = 0.59438726\n",
            "Iteration 35, loss = 0.59228735\n",
            "Iteration 36, loss = 0.59116627\n",
            "Iteration 37, loss = 0.59021607\n",
            "Iteration 38, loss = 0.58892515\n",
            "Iteration 39, loss = 0.58751755\n",
            "Iteration 40, loss = 0.58649545\n",
            "Iteration 41, loss = 0.58467327\n",
            "Iteration 42, loss = 0.58305116\n",
            "Iteration 43, loss = 0.58221081\n",
            "Iteration 44, loss = 0.58108512\n",
            "Iteration 45, loss = 0.57734044\n",
            "Iteration 46, loss = 0.57609381\n",
            "Iteration 47, loss = 0.57547570\n",
            "Iteration 48, loss = 0.57311653\n",
            "Iteration 49, loss = 0.57161011\n",
            "Iteration 50, loss = 0.56929035\n",
            "Iteration 51, loss = 0.56722130\n",
            "Iteration 52, loss = 0.56444927\n",
            "Iteration 53, loss = 0.56192998\n",
            "Iteration 54, loss = 0.55985082\n",
            "Iteration 55, loss = 0.55883827\n",
            "Iteration 56, loss = 0.55647096\n",
            "Iteration 57, loss = 0.55172804\n",
            "Iteration 58, loss = 0.54797309\n",
            "Iteration 59, loss = 0.54740371\n",
            "Iteration 60, loss = 0.54261010\n",
            "Iteration 61, loss = 0.53864410\n",
            "Iteration 62, loss = 0.53676605\n",
            "Iteration 63, loss = 0.53284274\n",
            "Iteration 64, loss = 0.53046401\n",
            "Iteration 65, loss = 0.52683545\n",
            "Iteration 66, loss = 0.52487117\n",
            "Iteration 67, loss = 0.52107239\n",
            "Iteration 68, loss = 0.51886648\n",
            "Iteration 69, loss = 0.51567702\n",
            "Iteration 70, loss = 0.51829108\n",
            "Iteration 71, loss = 0.51043516\n",
            "Iteration 72, loss = 0.50841576\n",
            "Iteration 73, loss = 0.50522551\n",
            "Iteration 74, loss = 0.50266325\n",
            "Iteration 75, loss = 0.49989316\n",
            "Iteration 76, loss = 0.49665224\n",
            "Iteration 77, loss = 0.50058346\n",
            "Iteration 78, loss = 0.49372437\n",
            "Iteration 79, loss = 0.49023340\n",
            "Iteration 80, loss = 0.48719647\n",
            "Iteration 81, loss = 0.48605870\n",
            "Iteration 82, loss = 0.48761484\n",
            "Iteration 83, loss = 0.48153799\n",
            "Iteration 84, loss = 0.47820912\n",
            "Iteration 85, loss = 0.47624323\n",
            "Iteration 86, loss = 0.47403724\n",
            "Iteration 87, loss = 0.47343771\n",
            "Iteration 88, loss = 0.47257217\n",
            "Iteration 89, loss = 0.46877212\n",
            "Iteration 90, loss = 0.46842880\n",
            "Iteration 91, loss = 0.47006836\n",
            "Iteration 92, loss = 0.46407652\n",
            "Iteration 93, loss = 0.46394922\n",
            "Iteration 94, loss = 0.46268096\n",
            "Iteration 95, loss = 0.46092162\n",
            "Iteration 96, loss = 0.46035915\n",
            "Iteration 97, loss = 0.46016153\n",
            "Iteration 98, loss = 0.46034718\n",
            "Iteration 99, loss = 0.46088243\n",
            "Iteration 100, loss = 0.45839390\n",
            "Iteration 101, loss = 0.45698071\n",
            "Iteration 102, loss = 0.45677012\n",
            "Iteration 103, loss = 0.45574112\n",
            "Iteration 104, loss = 0.45575899\n",
            "Iteration 105, loss = 0.45326024\n",
            "Iteration 106, loss = 0.45148496\n",
            "Iteration 107, loss = 0.45065338\n",
            "Iteration 108, loss = 0.44953930\n",
            "Iteration 109, loss = 0.45021747\n",
            "Iteration 110, loss = 0.44862063\n",
            "Iteration 111, loss = 0.45097553\n",
            "Iteration 112, loss = 0.44786004\n",
            "Iteration 113, loss = 0.44663663\n",
            "Iteration 114, loss = 0.44636285\n",
            "Iteration 115, loss = 0.44853559\n",
            "Iteration 116, loss = 0.44959754\n",
            "Iteration 117, loss = 0.44433569\n",
            "Iteration 118, loss = 0.44456944\n",
            "Iteration 119, loss = 0.44428723\n",
            "Iteration 120, loss = 0.44537609\n",
            "Iteration 121, loss = 0.44384798\n",
            "Iteration 122, loss = 0.44240225\n",
            "Iteration 123, loss = 0.44224673\n",
            "Iteration 124, loss = 0.44058932\n",
            "Iteration 125, loss = 0.44077984\n",
            "Iteration 126, loss = 0.44426456\n",
            "Iteration 127, loss = 0.43954598\n",
            "Iteration 128, loss = 0.43997348\n",
            "Iteration 129, loss = 0.43886084\n",
            "Iteration 130, loss = 0.44271883\n",
            "Iteration 131, loss = 0.43910544\n",
            "Iteration 132, loss = 0.43928086\n",
            "Iteration 133, loss = 0.43711944\n",
            "Iteration 134, loss = 0.43689219\n",
            "Iteration 135, loss = 0.44109310\n",
            "Iteration 136, loss = 0.43900480\n",
            "Iteration 137, loss = 0.43703628\n",
            "Iteration 138, loss = 0.43751740\n",
            "Iteration 139, loss = 0.43640839\n",
            "Iteration 140, loss = 0.43558374\n",
            "Iteration 141, loss = 0.43644067\n",
            "Iteration 142, loss = 0.43646065\n",
            "Iteration 143, loss = 0.44208616\n",
            "Iteration 144, loss = 0.44159332\n",
            "Iteration 145, loss = 0.44796524\n",
            "Iteration 146, loss = 0.43758741\n",
            "Iteration 147, loss = 0.43767355\n",
            "Iteration 148, loss = 0.43535296\n",
            "Iteration 149, loss = 0.43387358\n",
            "Iteration 150, loss = 0.43354151\n",
            "Iteration 151, loss = 0.43282608\n",
            "Iteration 152, loss = 0.43224715\n",
            "Iteration 153, loss = 0.43214344\n",
            "Iteration 154, loss = 0.43855658\n",
            "Iteration 155, loss = 0.43526984\n",
            "Iteration 156, loss = 0.43133690\n",
            "Iteration 157, loss = 0.43325185\n",
            "Iteration 158, loss = 0.43068238\n",
            "Iteration 159, loss = 0.43034906\n",
            "Iteration 160, loss = 0.43205874\n",
            "Iteration 161, loss = 0.43331052\n",
            "Iteration 162, loss = 0.42967012\n",
            "Iteration 163, loss = 0.43228715\n",
            "Iteration 164, loss = 0.42953910\n",
            "Iteration 165, loss = 0.42935029\n",
            "Iteration 166, loss = 0.43160752\n",
            "Iteration 167, loss = 0.43434910\n",
            "Iteration 168, loss = 0.43135291\n",
            "Iteration 169, loss = 0.42788369\n",
            "Iteration 170, loss = 0.42741398\n",
            "Iteration 171, loss = 0.42760329\n",
            "Iteration 172, loss = 0.42686155\n",
            "Iteration 173, loss = 0.42744951\n",
            "Iteration 174, loss = 0.42719171\n",
            "Iteration 175, loss = 0.42671636\n",
            "Iteration 176, loss = 0.42698137\n",
            "Iteration 177, loss = 0.42912520\n",
            "Iteration 178, loss = 0.42819935\n",
            "Iteration 179, loss = 0.42702545\n",
            "Iteration 180, loss = 0.42700219\n",
            "Iteration 181, loss = 0.42872143\n",
            "Iteration 182, loss = 0.43666756\n",
            "Iteration 183, loss = 0.43242054\n",
            "Iteration 184, loss = 0.43651622\n",
            "Iteration 185, loss = 0.43673990\n",
            "Iteration 186, loss = 0.42570334\n",
            "Iteration 187, loss = 0.42517951\n",
            "Iteration 188, loss = 0.42494445\n",
            "Iteration 189, loss = 0.42839371\n",
            "Iteration 190, loss = 0.42608544\n",
            "Iteration 191, loss = 0.42364533\n",
            "Iteration 192, loss = 0.42305902\n",
            "Iteration 193, loss = 0.42436032\n",
            "Iteration 194, loss = 0.42282742\n",
            "Iteration 195, loss = 0.42316523\n",
            "Iteration 196, loss = 0.42201762\n",
            "Iteration 197, loss = 0.42157446\n",
            "Iteration 198, loss = 0.42505441\n",
            "Iteration 199, loss = 0.42386048\n",
            "Iteration 200, loss = 0.42094224\n",
            "Iteration 1, loss = 0.87261367\n",
            "Iteration 2, loss = 0.79561675\n",
            "Iteration 3, loss = 0.74025905\n",
            "Iteration 4, loss = 0.70134462\n",
            "Iteration 5, loss = 0.67970100\n",
            "Iteration 6, loss = 0.66916943\n",
            "Iteration 7, loss = 0.66209992\n",
            "Iteration 8, loss = 0.65779211\n",
            "Iteration 9, loss = 0.65448116\n",
            "Iteration 10, loss = 0.65193410\n",
            "Iteration 11, loss = 0.64890926\n",
            "Iteration 12, loss = 0.64629644\n",
            "Iteration 13, loss = 0.64390710\n",
            "Iteration 14, loss = 0.64005346\n",
            "Iteration 15, loss = 0.63611438\n",
            "Iteration 16, loss = 0.63260292\n",
            "Iteration 17, loss = 0.62885304\n",
            "Iteration 18, loss = 0.62688719\n",
            "Iteration 19, loss = 0.62397080\n",
            "Iteration 20, loss = 0.62154738\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 21, loss = 0.61940975\n",
            "Iteration 22, loss = 0.61758774\n",
            "Iteration 23, loss = 0.61609881\n",
            "Iteration 24, loss = 0.61445309\n",
            "Iteration 25, loss = 0.61465478\n",
            "Iteration 26, loss = 0.61284495\n",
            "Iteration 27, loss = 0.61146449\n",
            "Iteration 28, loss = 0.60930564\n",
            "Iteration 29, loss = 0.60818603\n",
            "Iteration 30, loss = 0.60732584\n",
            "Iteration 31, loss = 0.60626277\n",
            "Iteration 32, loss = 0.60467837\n",
            "Iteration 33, loss = 0.60325044\n",
            "Iteration 34, loss = 0.60178787\n",
            "Iteration 35, loss = 0.60004823\n",
            "Iteration 36, loss = 0.59814823\n",
            "Iteration 37, loss = 0.59769164\n",
            "Iteration 38, loss = 0.59528446\n",
            "Iteration 39, loss = 0.59266114\n",
            "Iteration 40, loss = 0.59250571\n",
            "Iteration 41, loss = 0.59083030\n",
            "Iteration 42, loss = 0.58850423\n",
            "Iteration 43, loss = 0.58618529\n",
            "Iteration 44, loss = 0.58550179\n",
            "Iteration 45, loss = 0.58294486\n",
            "Iteration 46, loss = 0.58173801\n",
            "Iteration 47, loss = 0.57948363\n",
            "Iteration 48, loss = 0.57729271\n",
            "Iteration 49, loss = 0.57545587\n",
            "Iteration 50, loss = 0.57313119\n",
            "Iteration 51, loss = 0.57096718\n",
            "Iteration 52, loss = 0.56732275\n",
            "Iteration 53, loss = 0.56703913\n",
            "Iteration 54, loss = 0.56495643\n",
            "Iteration 55, loss = 0.55924794\n",
            "Iteration 56, loss = 0.55575328\n",
            "Iteration 57, loss = 0.55517309\n",
            "Iteration 58, loss = 0.55288485\n",
            "Iteration 59, loss = 0.54821886\n",
            "Iteration 60, loss = 0.54520640\n",
            "Iteration 61, loss = 0.54054361\n",
            "Iteration 62, loss = 0.53897333\n",
            "Iteration 63, loss = 0.53550160\n",
            "Iteration 64, loss = 0.53221629\n",
            "Iteration 65, loss = 0.52934585\n",
            "Iteration 66, loss = 0.52596041\n",
            "Iteration 67, loss = 0.52373310\n",
            "Iteration 68, loss = 0.52696932\n",
            "Iteration 69, loss = 0.51774625\n",
            "Iteration 70, loss = 0.51472129\n",
            "Iteration 71, loss = 0.51210484\n",
            "Iteration 72, loss = 0.50925298\n",
            "Iteration 73, loss = 0.50679358\n",
            "Iteration 74, loss = 0.50507303\n",
            "Iteration 75, loss = 0.50377623\n",
            "Iteration 76, loss = 0.50036490\n",
            "Iteration 77, loss = 0.50274489\n",
            "Iteration 78, loss = 0.50337663\n",
            "Iteration 79, loss = 0.49457268\n",
            "Iteration 80, loss = 0.49338881\n",
            "Iteration 81, loss = 0.49463523\n",
            "Iteration 82, loss = 0.49649901\n",
            "Iteration 83, loss = 0.49241665\n",
            "Iteration 84, loss = 0.49771445\n",
            "Iteration 85, loss = 0.48814211\n",
            "Iteration 86, loss = 0.48839642\n",
            "Iteration 87, loss = 0.49104551\n",
            "Iteration 88, loss = 0.49710226\n",
            "Iteration 89, loss = 0.48452518\n",
            "Iteration 90, loss = 0.48026825\n",
            "Iteration 91, loss = 0.48264950\n",
            "Iteration 92, loss = 0.47691745\n",
            "Iteration 93, loss = 0.47718231\n",
            "Iteration 94, loss = 0.47343768\n",
            "Iteration 95, loss = 0.47702482\n",
            "Iteration 96, loss = 0.47313879\n",
            "Iteration 97, loss = 0.47330685\n",
            "Iteration 98, loss = 0.47231907\n",
            "Iteration 99, loss = 0.47100176\n",
            "Iteration 100, loss = 0.47310550\n",
            "Iteration 101, loss = 0.47016731\n",
            "Iteration 102, loss = 0.46872849\n",
            "Iteration 103, loss = 0.46806355\n",
            "Iteration 104, loss = 0.46667176\n",
            "Iteration 105, loss = 0.46656223\n",
            "Iteration 106, loss = 0.46894608\n",
            "Iteration 107, loss = 0.46628817\n",
            "Iteration 108, loss = 0.46446351\n",
            "Iteration 109, loss = 0.46664238\n",
            "Iteration 110, loss = 0.46409894\n",
            "Iteration 111, loss = 0.46274131\n",
            "Iteration 112, loss = 0.46813146\n",
            "Iteration 113, loss = 0.46217665\n",
            "Iteration 114, loss = 0.46393533\n",
            "Iteration 115, loss = 0.46315465\n",
            "Iteration 116, loss = 0.46346443\n",
            "Iteration 117, loss = 0.46790678\n",
            "Iteration 118, loss = 0.46150762\n",
            "Iteration 119, loss = 0.46184518\n",
            "Iteration 120, loss = 0.45970039\n",
            "Iteration 121, loss = 0.46037852\n",
            "Iteration 122, loss = 0.45940334\n",
            "Iteration 123, loss = 0.46033001\n",
            "Iteration 124, loss = 0.46062431\n",
            "Iteration 125, loss = 0.45925130\n",
            "Iteration 126, loss = 0.45710264\n",
            "Iteration 127, loss = 0.45804985\n",
            "Iteration 128, loss = 0.45601159\n",
            "Iteration 129, loss = 0.45662097\n",
            "Iteration 130, loss = 0.45616607\n",
            "Iteration 131, loss = 0.45656929\n",
            "Iteration 132, loss = 0.45673258\n",
            "Iteration 133, loss = 0.46212600\n",
            "Iteration 134, loss = 0.45597756\n",
            "Iteration 135, loss = 0.45641719\n",
            "Iteration 136, loss = 0.46085733\n",
            "Iteration 137, loss = 0.45806850\n",
            "Iteration 138, loss = 0.45608325\n",
            "Iteration 139, loss = 0.45596885\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66519342\n",
            "Iteration 2, loss = 0.65986022\n",
            "Iteration 3, loss = 0.65516324\n",
            "Iteration 4, loss = 0.65027358\n",
            "Iteration 5, loss = 0.64488437\n",
            "Iteration 6, loss = 0.63971579\n",
            "Iteration 7, loss = 0.63542927\n",
            "Iteration 8, loss = 0.63051539\n",
            "Iteration 9, loss = 0.62421613\n",
            "Iteration 10, loss = 0.61925151\n",
            "Iteration 11, loss = 0.61442502\n",
            "Iteration 12, loss = 0.61062855\n",
            "Iteration 13, loss = 0.60837027\n",
            "Iteration 14, loss = 0.60628352\n",
            "Iteration 15, loss = 0.60258818\n",
            "Iteration 16, loss = 0.60029935\n",
            "Iteration 17, loss = 0.59828913\n",
            "Iteration 18, loss = 0.59659033\n",
            "Iteration 19, loss = 0.59507126\n",
            "Iteration 20, loss = 0.59404061\n",
            "Iteration 21, loss = 0.59265753\n",
            "Iteration 22, loss = 0.59078747\n",
            "Iteration 23, loss = 0.59163850\n",
            "Iteration 24, loss = 0.59002013\n",
            "Iteration 25, loss = 0.58805167\n",
            "Iteration 26, loss = 0.58763272\n",
            "Iteration 27, loss = 0.58521614\n",
            "Iteration 28, loss = 0.58322706\n",
            "Iteration 29, loss = 0.58196207\n",
            "Iteration 30, loss = 0.58045935\n",
            "Iteration 31, loss = 0.57871348\n",
            "Iteration 32, loss = 0.57759447\n",
            "Iteration 33, loss = 0.57544661\n",
            "Iteration 34, loss = 0.57335523\n",
            "Iteration 35, loss = 0.57226655\n",
            "Iteration 36, loss = 0.57091589\n",
            "Iteration 37, loss = 0.56808260\n",
            "Iteration 38, loss = 0.56696947\n",
            "Iteration 39, loss = 0.56553703\n",
            "Iteration 40, loss = 0.56310120\n",
            "Iteration 41, loss = 0.56032267\n",
            "Iteration 42, loss = 0.55837678\n",
            "Iteration 43, loss = 0.55705197\n",
            "Iteration 44, loss = 0.55325549\n",
            "Iteration 45, loss = 0.55053268\n",
            "Iteration 46, loss = 0.54806254\n",
            "Iteration 47, loss = 0.54556246\n",
            "Iteration 48, loss = 0.54338841\n",
            "Iteration 49, loss = 0.54010512\n",
            "Iteration 50, loss = 0.54126705\n",
            "Iteration 51, loss = 0.53477987\n",
            "Iteration 52, loss = 0.53103363\n",
            "Iteration 53, loss = 0.52751707\n",
            "Iteration 54, loss = 0.52539879\n",
            "Iteration 55, loss = 0.52079360\n",
            "Iteration 56, loss = 0.52019790\n",
            "Iteration 57, loss = 0.51593126\n",
            "Iteration 58, loss = 0.51370637\n",
            "Iteration 59, loss = 0.50904628\n",
            "Iteration 60, loss = 0.50777214\n",
            "Iteration 61, loss = 0.50431806\n",
            "Iteration 62, loss = 0.50205196\n",
            "Iteration 63, loss = 0.49774673\n",
            "Iteration 64, loss = 0.49485698\n",
            "Iteration 65, loss = 0.49279274\n",
            "Iteration 66, loss = 0.48849186\n",
            "Iteration 67, loss = 0.49210852\n",
            "Iteration 68, loss = 0.48449752\n",
            "Iteration 69, loss = 0.48415113\n",
            "Iteration 70, loss = 0.48051349\n",
            "Iteration 71, loss = 0.47795011\n",
            "Iteration 72, loss = 0.47719827\n",
            "Iteration 73, loss = 0.47645708\n",
            "Iteration 74, loss = 0.47138463\n",
            "Iteration 75, loss = 0.47243857\n",
            "Iteration 76, loss = 0.47241121\n",
            "Iteration 77, loss = 0.47143468\n",
            "Iteration 78, loss = 0.46802027\n",
            "Iteration 79, loss = 0.46667490\n",
            "Iteration 80, loss = 0.46485540\n",
            "Iteration 81, loss = 0.46247542\n",
            "Iteration 82, loss = 0.46821215\n",
            "Iteration 83, loss = 0.46175330\n",
            "Iteration 84, loss = 0.46020116\n",
            "Iteration 85, loss = 0.45899839\n",
            "Iteration 86, loss = 0.45757955\n",
            "Iteration 87, loss = 0.45650530\n",
            "Iteration 88, loss = 0.45594231\n",
            "Iteration 89, loss = 0.45601983\n",
            "Iteration 90, loss = 0.45616580\n",
            "Iteration 91, loss = 0.45469664\n",
            "Iteration 92, loss = 0.45263204\n",
            "Iteration 93, loss = 0.45203602\n",
            "Iteration 94, loss = 0.46629753\n",
            "Iteration 95, loss = 0.45383025\n",
            "Iteration 96, loss = 0.45254567\n",
            "Iteration 97, loss = 0.45108854\n",
            "Iteration 98, loss = 0.44891852\n",
            "Iteration 99, loss = 0.44968108\n",
            "Iteration 100, loss = 0.44889681\n",
            "Iteration 101, loss = 0.44795447\n",
            "Iteration 102, loss = 0.44666559\n",
            "Iteration 103, loss = 0.44635437\n",
            "Iteration 104, loss = 0.44452459\n",
            "Iteration 105, loss = 0.44671831\n",
            "Iteration 106, loss = 0.44457772\n",
            "Iteration 107, loss = 0.44603804\n",
            "Iteration 108, loss = 0.44980418\n",
            "Iteration 109, loss = 0.45070424\n",
            "Iteration 110, loss = 0.44806802\n",
            "Iteration 111, loss = 0.44363254\n",
            "Iteration 112, loss = 0.44413529\n",
            "Iteration 113, loss = 0.44093159\n",
            "Iteration 114, loss = 0.44250086\n",
            "Iteration 115, loss = 0.44087571\n",
            "Iteration 116, loss = 0.44048741\n",
            "Iteration 117, loss = 0.44194111\n",
            "Iteration 118, loss = 0.44096132\n",
            "Iteration 119, loss = 0.43980655\n",
            "Iteration 120, loss = 0.43885421\n",
            "Iteration 121, loss = 0.43897840\n",
            "Iteration 122, loss = 0.43954284\n",
            "Iteration 123, loss = 0.43926237\n",
            "Iteration 124, loss = 0.43663157\n",
            "Iteration 125, loss = 0.44046583\n",
            "Iteration 126, loss = 0.43612482\n",
            "Iteration 127, loss = 0.44201655\n",
            "Iteration 128, loss = 0.43910841\n",
            "Iteration 129, loss = 0.43659534\n",
            "Iteration 130, loss = 0.43517668\n",
            "Iteration 131, loss = 0.43656165\n",
            "Iteration 132, loss = 0.43490411\n",
            "Iteration 133, loss = 0.43753841\n",
            "Iteration 134, loss = 0.43625924\n",
            "Iteration 135, loss = 0.43904769\n",
            "Iteration 136, loss = 0.43639214\n",
            "Iteration 137, loss = 0.43966763\n",
            "Iteration 138, loss = 0.43755690\n",
            "Iteration 139, loss = 0.43447416\n",
            "Iteration 140, loss = 0.43696649\n",
            "Iteration 141, loss = 0.43322685\n",
            "Iteration 142, loss = 0.43563304\n",
            "Iteration 143, loss = 0.43279571\n",
            "Iteration 144, loss = 0.43345857\n",
            "Iteration 145, loss = 0.43192470\n",
            "Iteration 146, loss = 0.43097370\n",
            "Iteration 147, loss = 0.43173892\n",
            "Iteration 148, loss = 0.43139801\n",
            "Iteration 149, loss = 0.43099092\n",
            "Iteration 150, loss = 0.43149106\n",
            "Iteration 151, loss = 0.43304731\n",
            "Iteration 152, loss = 0.43239891\n",
            "Iteration 153, loss = 0.43213363\n",
            "Iteration 154, loss = 0.43727535\n",
            "Iteration 155, loss = 0.43118749\n",
            "Iteration 156, loss = 0.43349561\n",
            "Iteration 157, loss = 0.43034323\n",
            "Iteration 158, loss = 0.43352177\n",
            "Iteration 159, loss = 0.43203911\n",
            "Iteration 160, loss = 0.43192675\n",
            "Iteration 161, loss = 0.43403872\n",
            "Iteration 162, loss = 0.43076850\n",
            "Iteration 163, loss = 0.42955307\n",
            "Iteration 164, loss = 0.42897089\n",
            "Iteration 165, loss = 0.43121623\n",
            "Iteration 166, loss = 0.43141565\n",
            "Iteration 167, loss = 0.42674569\n",
            "Iteration 168, loss = 0.42658649\n",
            "Iteration 169, loss = 0.42911722\n",
            "Iteration 170, loss = 0.42762081\n",
            "Iteration 171, loss = 0.42770712\n",
            "Iteration 172, loss = 0.42713903\n",
            "Iteration 173, loss = 0.42708397\n",
            "Iteration 174, loss = 0.42704603\n",
            "Iteration 175, loss = 0.42643227\n",
            "Iteration 176, loss = 0.42754548\n",
            "Iteration 177, loss = 0.42943241\n",
            "Iteration 178, loss = 0.42927182\n",
            "Iteration 179, loss = 0.43207315\n",
            "Iteration 180, loss = 0.43497987\n",
            "Iteration 181, loss = 0.43280115\n",
            "Iteration 182, loss = 0.42852309\n",
            "Iteration 183, loss = 0.42664249\n",
            "Iteration 184, loss = 0.42583180\n",
            "Iteration 185, loss = 0.42465216\n",
            "Iteration 186, loss = 0.42668809\n",
            "Iteration 187, loss = 0.42583827\n",
            "Iteration 188, loss = 0.42420769\n",
            "Iteration 189, loss = 0.42555293\n",
            "Iteration 190, loss = 0.42680763\n",
            "Iteration 191, loss = 0.42922496\n",
            "Iteration 192, loss = 0.42209953\n",
            "Iteration 193, loss = 0.42515802\n",
            "Iteration 194, loss = 0.42854903\n",
            "Iteration 195, loss = 0.42478155\n",
            "Iteration 196, loss = 0.42438442\n",
            "Iteration 197, loss = 0.42352423\n",
            "Iteration 198, loss = 0.42214076\n",
            "Iteration 199, loss = 0.42248030\n",
            "Iteration 200, loss = 0.42248787\n",
            "Iteration 1, loss = 0.69406264\n",
            "Iteration 2, loss = 0.66773406\n",
            "Iteration 3, loss = 0.65458466\n",
            "Iteration 4, loss = 0.64768392\n",
            "Iteration 5, loss = 0.64320402\n",
            "Iteration 6, loss = 0.63994054\n",
            "Iteration 7, loss = 0.63677429\n",
            "Iteration 8, loss = 0.63391439\n",
            "Iteration 9, loss = 0.63106096\n",
            "Iteration 10, loss = 0.62868523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 11, loss = 0.62556429\n",
            "Iteration 12, loss = 0.62253156\n",
            "Iteration 13, loss = 0.61984543\n",
            "Iteration 14, loss = 0.61791653\n",
            "Iteration 15, loss = 0.61580485\n",
            "Iteration 16, loss = 0.61344616\n",
            "Iteration 17, loss = 0.61179543\n",
            "Iteration 18, loss = 0.60961764\n",
            "Iteration 19, loss = 0.60780066\n",
            "Iteration 20, loss = 0.60602463\n",
            "Iteration 21, loss = 0.60467546\n",
            "Iteration 22, loss = 0.60418486\n",
            "Iteration 23, loss = 0.60305523\n",
            "Iteration 24, loss = 0.60165770\n",
            "Iteration 25, loss = 0.60030771\n",
            "Iteration 26, loss = 0.59925306\n",
            "Iteration 27, loss = 0.59812075\n",
            "Iteration 28, loss = 0.59679111\n",
            "Iteration 29, loss = 0.59592968\n",
            "Iteration 30, loss = 0.59378722\n",
            "Iteration 31, loss = 0.59220797\n",
            "Iteration 32, loss = 0.59049814\n",
            "Iteration 33, loss = 0.58908846\n",
            "Iteration 34, loss = 0.58814366\n",
            "Iteration 35, loss = 0.58573859\n",
            "Iteration 36, loss = 0.58399950\n",
            "Iteration 37, loss = 0.58306305\n",
            "Iteration 38, loss = 0.58021281\n",
            "Iteration 39, loss = 0.57894235\n",
            "Iteration 40, loss = 0.57877022\n",
            "Iteration 41, loss = 0.57533586\n",
            "Iteration 42, loss = 0.57427973\n",
            "Iteration 43, loss = 0.57266242\n",
            "Iteration 44, loss = 0.57003728\n",
            "Iteration 45, loss = 0.56801915\n",
            "Iteration 46, loss = 0.56733962\n",
            "Iteration 47, loss = 0.56644190\n",
            "Iteration 48, loss = 0.56221499\n",
            "Iteration 49, loss = 0.56071568\n",
            "Iteration 50, loss = 0.55650703\n",
            "Iteration 51, loss = 0.55985392\n",
            "Iteration 52, loss = 0.55331567\n",
            "Iteration 53, loss = 0.55087261\n",
            "Iteration 54, loss = 0.54890550\n",
            "Iteration 55, loss = 0.54647771\n",
            "Iteration 56, loss = 0.54456040\n",
            "Iteration 57, loss = 0.54196562\n",
            "Iteration 58, loss = 0.54152853\n",
            "Iteration 59, loss = 0.53785574\n",
            "Iteration 60, loss = 0.53670309\n",
            "Iteration 61, loss = 0.53506136\n",
            "Iteration 62, loss = 0.53103516\n",
            "Iteration 63, loss = 0.52863889\n",
            "Iteration 64, loss = 0.52597772\n",
            "Iteration 65, loss = 0.52265515\n",
            "Iteration 66, loss = 0.52000118\n",
            "Iteration 67, loss = 0.51736222\n",
            "Iteration 68, loss = 0.51544855\n",
            "Iteration 69, loss = 0.51147005\n",
            "Iteration 70, loss = 0.51198789\n",
            "Iteration 71, loss = 0.50657281\n",
            "Iteration 72, loss = 0.50392569\n",
            "Iteration 73, loss = 0.50616577\n",
            "Iteration 74, loss = 0.50458881\n",
            "Iteration 75, loss = 0.50116276\n",
            "Iteration 76, loss = 0.49990158\n",
            "Iteration 77, loss = 0.49684271\n",
            "Iteration 78, loss = 0.49427689\n",
            "Iteration 79, loss = 0.49151236\n",
            "Iteration 80, loss = 0.49099520\n",
            "Iteration 81, loss = 0.48994791\n",
            "Iteration 82, loss = 0.49091090\n",
            "Iteration 83, loss = 0.48530034\n",
            "Iteration 84, loss = 0.48790333\n",
            "Iteration 85, loss = 0.48569471\n",
            "Iteration 86, loss = 0.48303613\n",
            "Iteration 87, loss = 0.48037162\n",
            "Iteration 88, loss = 0.48164375\n",
            "Iteration 89, loss = 0.47718188\n",
            "Iteration 90, loss = 0.48133251\n",
            "Iteration 91, loss = 0.47673262\n",
            "Iteration 92, loss = 0.47421144\n",
            "Iteration 93, loss = 0.47380988\n",
            "Iteration 94, loss = 0.47129902\n",
            "Iteration 95, loss = 0.47747953\n",
            "Iteration 96, loss = 0.47246843\n",
            "Iteration 97, loss = 0.47287469\n",
            "Iteration 98, loss = 0.46912377\n",
            "Iteration 99, loss = 0.46812218\n",
            "Iteration 100, loss = 0.46733514\n",
            "Iteration 101, loss = 0.46597439\n",
            "Iteration 102, loss = 0.46540460\n",
            "Iteration 103, loss = 0.46535766\n",
            "Iteration 104, loss = 0.46493043\n",
            "Iteration 105, loss = 0.46510384\n",
            "Iteration 106, loss = 0.46575261\n",
            "Iteration 107, loss = 0.46326979\n",
            "Iteration 108, loss = 0.46271972\n",
            "Iteration 109, loss = 0.46355915\n",
            "Iteration 110, loss = 0.46632741\n",
            "Iteration 111, loss = 0.46283985\n",
            "Iteration 112, loss = 0.46212593\n",
            "Iteration 113, loss = 0.46220938\n",
            "Iteration 114, loss = 0.45922015\n",
            "Iteration 115, loss = 0.46026139\n",
            "Iteration 116, loss = 0.45996734\n",
            "Iteration 117, loss = 0.46251029\n",
            "Iteration 118, loss = 0.46775587\n",
            "Iteration 119, loss = 0.45579005\n",
            "Iteration 120, loss = 0.45654582\n",
            "Iteration 121, loss = 0.45761739\n",
            "Iteration 122, loss = 0.45580094\n",
            "Iteration 123, loss = 0.45557002\n",
            "Iteration 124, loss = 0.45803241\n",
            "Iteration 125, loss = 0.45390972\n",
            "Iteration 126, loss = 0.45461817\n",
            "Iteration 127, loss = 0.45535784\n",
            "Iteration 128, loss = 0.45555568\n",
            "Iteration 129, loss = 0.45400675\n",
            "Iteration 130, loss = 0.45360422\n",
            "Iteration 131, loss = 0.45180200\n",
            "Iteration 132, loss = 0.45257949\n",
            "Iteration 133, loss = 0.46777812\n",
            "Iteration 134, loss = 0.45299801\n",
            "Iteration 135, loss = 0.45503566\n",
            "Iteration 136, loss = 0.45001354\n",
            "Iteration 137, loss = 0.45179953\n",
            "Iteration 138, loss = 0.45283516\n",
            "Iteration 139, loss = 0.45008528\n",
            "Iteration 140, loss = 0.44942285\n",
            "Iteration 141, loss = 0.45047151\n",
            "Iteration 142, loss = 0.45034411\n",
            "Iteration 143, loss = 0.45181718\n",
            "Iteration 144, loss = 0.44839241\n",
            "Iteration 145, loss = 0.44771998\n",
            "Iteration 146, loss = 0.44693692\n",
            "Iteration 147, loss = 0.44677930\n",
            "Iteration 148, loss = 0.44715427\n",
            "Iteration 149, loss = 0.44698793\n",
            "Iteration 150, loss = 0.44814232\n",
            "Iteration 151, loss = 0.44674564\n",
            "Iteration 152, loss = 0.44938394\n",
            "Iteration 153, loss = 0.44872017\n",
            "Iteration 154, loss = 0.45135920\n",
            "Iteration 155, loss = 0.44786558\n",
            "Iteration 156, loss = 0.44565839\n",
            "Iteration 157, loss = 0.44684710\n",
            "Iteration 158, loss = 0.44523157\n",
            "Iteration 159, loss = 0.44550762\n",
            "Iteration 160, loss = 0.44595552\n",
            "Iteration 161, loss = 0.45095646\n",
            "Iteration 162, loss = 0.44334982\n",
            "Iteration 163, loss = 0.44685094\n",
            "Iteration 164, loss = 0.44354778\n",
            "Iteration 165, loss = 0.44327978\n",
            "Iteration 166, loss = 0.44355831\n",
            "Iteration 167, loss = 0.44564724\n",
            "Iteration 168, loss = 0.45162080\n",
            "Iteration 169, loss = 0.44969520\n",
            "Iteration 170, loss = 0.44638475\n",
            "Iteration 171, loss = 0.44680843\n",
            "Iteration 172, loss = 0.44351237\n",
            "Iteration 173, loss = 0.44648316\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.76698831\n",
            "Iteration 2, loss = 0.60081393\n",
            "Iteration 3, loss = 0.61033114\n",
            "Iteration 4, loss = 0.58759365\n",
            "Iteration 5, loss = 0.56538785\n",
            "Iteration 6, loss = 0.60293647\n",
            "Iteration 7, loss = 0.59685868\n",
            "Iteration 8, loss = 0.56691118\n",
            "Iteration 9, loss = 0.52521180\n",
            "Iteration 10, loss = 0.50135686\n",
            "Iteration 11, loss = 0.51515069\n",
            "Iteration 12, loss = 0.51333001\n",
            "Iteration 13, loss = 0.50897635\n",
            "Iteration 14, loss = 0.56309312\n",
            "Iteration 15, loss = 0.49167008\n",
            "Iteration 16, loss = 0.48851398\n",
            "Iteration 17, loss = 0.51303483\n",
            "Iteration 18, loss = 0.47802670\n",
            "Iteration 19, loss = 0.50714905\n",
            "Iteration 20, loss = 0.47396784\n",
            "Iteration 21, loss = 0.46753481\n",
            "Iteration 22, loss = 0.45952369\n",
            "Iteration 23, loss = 0.47632684\n",
            "Iteration 24, loss = 0.45633131\n",
            "Iteration 25, loss = 0.46020678\n",
            "Iteration 26, loss = 0.47825430\n",
            "Iteration 27, loss = 0.46351815\n",
            "Iteration 28, loss = 0.47998687\n",
            "Iteration 29, loss = 0.47275180\n",
            "Iteration 30, loss = 0.47602926\n",
            "Iteration 31, loss = 0.44777045\n",
            "Iteration 32, loss = 0.48041957\n",
            "Iteration 33, loss = 0.53088286\n",
            "Iteration 34, loss = 0.45320262\n",
            "Iteration 35, loss = 0.45181765\n",
            "Iteration 36, loss = 0.46818265\n",
            "Iteration 37, loss = 0.45117683\n",
            "Iteration 38, loss = 0.47478843\n",
            "Iteration 39, loss = 0.45850714\n",
            "Iteration 40, loss = 0.48802477\n",
            "Iteration 41, loss = 0.46525217\n",
            "Iteration 42, loss = 0.44086557\n",
            "Iteration 43, loss = 0.44682464\n",
            "Iteration 44, loss = 0.44846204\n",
            "Iteration 45, loss = 0.45782685\n",
            "Iteration 46, loss = 0.43463226\n",
            "Iteration 47, loss = 0.45104805\n",
            "Iteration 48, loss = 0.45759998\n",
            "Iteration 49, loss = 0.42865545\n",
            "Iteration 50, loss = 0.44060391\n",
            "Iteration 51, loss = 0.43017837\n",
            "Iteration 52, loss = 0.45920384\n",
            "Iteration 53, loss = 0.44372899\n",
            "Iteration 54, loss = 0.48867675\n",
            "Iteration 55, loss = 0.48252988\n",
            "Iteration 56, loss = 0.45338563\n",
            "Iteration 57, loss = 0.42431315\n",
            "Iteration 58, loss = 0.43152439\n",
            "Iteration 59, loss = 0.43467079\n",
            "Iteration 60, loss = 0.46684247\n",
            "Iteration 61, loss = 0.42758297\n",
            "Iteration 62, loss = 0.43033729\n",
            "Iteration 63, loss = 0.43251026\n",
            "Iteration 64, loss = 0.42505037\n",
            "Iteration 65, loss = 0.42712727\n",
            "Iteration 66, loss = 0.47839916\n",
            "Iteration 67, loss = 0.41704324\n",
            "Iteration 68, loss = 0.44384231\n",
            "Iteration 69, loss = 0.43382083\n",
            "Iteration 70, loss = 0.42385333\n",
            "Iteration 71, loss = 0.41491604\n",
            "Iteration 72, loss = 0.44468821\n",
            "Iteration 73, loss = 0.42283197\n",
            "Iteration 74, loss = 0.44560433\n",
            "Iteration 75, loss = 0.42843946\n",
            "Iteration 76, loss = 0.44323985\n",
            "Iteration 77, loss = 0.42646896\n",
            "Iteration 78, loss = 0.43021907\n",
            "Iteration 79, loss = 0.46695604\n",
            "Iteration 80, loss = 0.41102430\n",
            "Iteration 81, loss = 0.41567067\n",
            "Iteration 82, loss = 0.45586450\n",
            "Iteration 83, loss = 0.42094870\n",
            "Iteration 84, loss = 0.45059862\n",
            "Iteration 85, loss = 0.43166782\n",
            "Iteration 86, loss = 0.42176144\n",
            "Iteration 87, loss = 0.42101449\n",
            "Iteration 88, loss = 0.41967340\n",
            "Iteration 89, loss = 0.41381742\n",
            "Iteration 90, loss = 0.40896201\n",
            "Iteration 91, loss = 0.40957484\n",
            "Iteration 92, loss = 0.41525433\n",
            "Iteration 93, loss = 0.41207411\n",
            "Iteration 94, loss = 0.41151081\n",
            "Iteration 95, loss = 0.42277688\n",
            "Iteration 96, loss = 0.42129295\n",
            "Iteration 97, loss = 0.43000645\n",
            "Iteration 98, loss = 0.42765611\n",
            "Iteration 99, loss = 0.42177103\n",
            "Iteration 100, loss = 0.40932803\n",
            "Iteration 101, loss = 0.40600311\n",
            "Iteration 102, loss = 0.40011584\n",
            "Iteration 103, loss = 0.39833777\n",
            "Iteration 104, loss = 0.41275736\n",
            "Iteration 105, loss = 0.41415498\n",
            "Iteration 106, loss = 0.44311770\n",
            "Iteration 107, loss = 0.40486590\n",
            "Iteration 108, loss = 0.42077379\n",
            "Iteration 109, loss = 0.39921326\n",
            "Iteration 110, loss = 0.40562596\n",
            "Iteration 111, loss = 0.41373810\n",
            "Iteration 112, loss = 0.39990184\n",
            "Iteration 113, loss = 0.39530275\n",
            "Iteration 114, loss = 0.44372806\n",
            "Iteration 115, loss = 0.40539325\n",
            "Iteration 116, loss = 0.41370642\n",
            "Iteration 117, loss = 0.40549925\n",
            "Iteration 118, loss = 0.40566234\n",
            "Iteration 119, loss = 0.40371921\n",
            "Iteration 120, loss = 0.41723033\n",
            "Iteration 121, loss = 0.41535895\n",
            "Iteration 122, loss = 0.40212448\n",
            "Iteration 123, loss = 0.40186044\n",
            "Iteration 124, loss = 0.39378603\n",
            "Iteration 125, loss = 0.40273353\n",
            "Iteration 126, loss = 0.39803390\n",
            "Iteration 127, loss = 0.39173199\n",
            "Iteration 128, loss = 0.38928198\n",
            "Iteration 129, loss = 0.40082887\n",
            "Iteration 130, loss = 0.41504040\n",
            "Iteration 131, loss = 0.43308714\n",
            "Iteration 132, loss = 0.40933881\n",
            "Iteration 133, loss = 0.38554333\n",
            "Iteration 134, loss = 0.39155672\n",
            "Iteration 135, loss = 0.41210657\n",
            "Iteration 136, loss = 0.38755555\n",
            "Iteration 137, loss = 0.41734986\n",
            "Iteration 138, loss = 0.48536728\n",
            "Iteration 139, loss = 0.40814722\n",
            "Iteration 140, loss = 0.39726418\n",
            "Iteration 141, loss = 0.39538024\n",
            "Iteration 142, loss = 0.38610653\n",
            "Iteration 143, loss = 0.40298238\n",
            "Iteration 144, loss = 0.38953493\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.65438316\n",
            "Iteration 2, loss = 0.63304622\n",
            "Iteration 3, loss = 0.63785047\n",
            "Iteration 4, loss = 0.62699125\n",
            "Iteration 5, loss = 0.60285712\n",
            "Iteration 6, loss = 0.57791058\n",
            "Iteration 7, loss = 0.55574204\n",
            "Iteration 8, loss = 0.58268698\n",
            "Iteration 9, loss = 0.54740437\n",
            "Iteration 10, loss = 0.61830843\n",
            "Iteration 11, loss = 0.55123062\n",
            "Iteration 12, loss = 0.54221570\n",
            "Iteration 13, loss = 0.51450683\n",
            "Iteration 14, loss = 0.52389077\n",
            "Iteration 15, loss = 0.49518751\n",
            "Iteration 16, loss = 0.48300251\n",
            "Iteration 17, loss = 0.46530296\n",
            "Iteration 18, loss = 0.45830380\n",
            "Iteration 19, loss = 0.47955107\n",
            "Iteration 20, loss = 0.47519303\n",
            "Iteration 21, loss = 0.48531637\n",
            "Iteration 22, loss = 0.46411231\n",
            "Iteration 23, loss = 0.45805153\n",
            "Iteration 24, loss = 0.53249811\n",
            "Iteration 25, loss = 0.46615058\n",
            "Iteration 26, loss = 0.44884619\n",
            "Iteration 27, loss = 0.46100883\n",
            "Iteration 28, loss = 0.45167298\n",
            "Iteration 29, loss = 0.46995746\n",
            "Iteration 30, loss = 0.46566419\n",
            "Iteration 31, loss = 0.46071842\n",
            "Iteration 32, loss = 0.44736244\n",
            "Iteration 33, loss = 0.46274450\n",
            "Iteration 34, loss = 0.44941206\n",
            "Iteration 35, loss = 0.45946619\n",
            "Iteration 36, loss = 0.49956589\n",
            "Iteration 37, loss = 0.45351548\n",
            "Iteration 38, loss = 0.43851033\n",
            "Iteration 39, loss = 0.43264432\n",
            "Iteration 40, loss = 0.44452329\n",
            "Iteration 41, loss = 0.43794575\n",
            "Iteration 42, loss = 0.43930078\n",
            "Iteration 43, loss = 0.45160743\n",
            "Iteration 44, loss = 0.43197899\n",
            "Iteration 45, loss = 0.42210748\n",
            "Iteration 46, loss = 0.42443350\n",
            "Iteration 47, loss = 0.44638732\n",
            "Iteration 48, loss = 0.44439521\n",
            "Iteration 49, loss = 0.44433183\n",
            "Iteration 50, loss = 0.42258245\n",
            "Iteration 51, loss = 0.46778681\n",
            "Iteration 52, loss = 0.44292504\n",
            "Iteration 53, loss = 0.42631265\n",
            "Iteration 54, loss = 0.43000792\n",
            "Iteration 55, loss = 0.42189929\n",
            "Iteration 56, loss = 0.42761996\n",
            "Iteration 57, loss = 0.41717023\n",
            "Iteration 58, loss = 0.42969605\n",
            "Iteration 59, loss = 0.42172486\n",
            "Iteration 60, loss = 0.46839097\n",
            "Iteration 61, loss = 0.45544635\n",
            "Iteration 62, loss = 0.42298411\n",
            "Iteration 63, loss = 0.42627928\n",
            "Iteration 64, loss = 0.44412575\n",
            "Iteration 65, loss = 0.41820607\n",
            "Iteration 66, loss = 0.43297003\n",
            "Iteration 67, loss = 0.41883717\n",
            "Iteration 68, loss = 0.42004671\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.73258400\n",
            "Iteration 2, loss = 0.64046923\n",
            "Iteration 3, loss = 0.63726904\n",
            "Iteration 4, loss = 0.66663667\n",
            "Iteration 5, loss = 0.62304918\n",
            "Iteration 6, loss = 0.64113513\n",
            "Iteration 7, loss = 0.58898942\n",
            "Iteration 8, loss = 0.60581439\n",
            "Iteration 9, loss = 0.55909205\n",
            "Iteration 10, loss = 0.55303963\n",
            "Iteration 11, loss = 0.55200436\n",
            "Iteration 12, loss = 0.52574419\n",
            "Iteration 13, loss = 0.52977779\n",
            "Iteration 14, loss = 0.51215709\n",
            "Iteration 15, loss = 0.51634626\n",
            "Iteration 16, loss = 0.50103225\n",
            "Iteration 17, loss = 0.51770598\n",
            "Iteration 18, loss = 0.50241141\n",
            "Iteration 19, loss = 0.49059911\n",
            "Iteration 20, loss = 0.54343522\n",
            "Iteration 21, loss = 0.52181154\n",
            "Iteration 22, loss = 0.47843503\n",
            "Iteration 23, loss = 0.49650728\n",
            "Iteration 24, loss = 0.47784441\n",
            "Iteration 25, loss = 0.48713430\n",
            "Iteration 26, loss = 0.49524801\n",
            "Iteration 27, loss = 0.49344537\n",
            "Iteration 28, loss = 0.52028744\n",
            "Iteration 29, loss = 0.48265617\n",
            "Iteration 30, loss = 0.48431262\n",
            "Iteration 31, loss = 0.48230130\n",
            "Iteration 32, loss = 0.47498319\n",
            "Iteration 33, loss = 0.47271460\n",
            "Iteration 34, loss = 0.49660459\n",
            "Iteration 35, loss = 0.48443033\n",
            "Iteration 36, loss = 0.48003057\n",
            "Iteration 37, loss = 0.49378025\n",
            "Iteration 38, loss = 0.46683563\n",
            "Iteration 39, loss = 0.49363943\n",
            "Iteration 40, loss = 0.47125469\n",
            "Iteration 41, loss = 0.47381466\n",
            "Iteration 42, loss = 0.45913920\n",
            "Iteration 43, loss = 0.46869034\n",
            "Iteration 44, loss = 0.46418050\n",
            "Iteration 45, loss = 0.46052824\n",
            "Iteration 46, loss = 0.50738984\n",
            "Iteration 47, loss = 0.49788014\n",
            "Iteration 48, loss = 0.47154702\n",
            "Iteration 49, loss = 0.46071163\n",
            "Iteration 50, loss = 0.45830296\n",
            "Iteration 51, loss = 0.44142821\n",
            "Iteration 52, loss = 0.44899999\n",
            "Iteration 53, loss = 0.45213086\n",
            "Iteration 54, loss = 0.48853420\n",
            "Iteration 55, loss = 0.44884454\n",
            "Iteration 56, loss = 0.47120617\n",
            "Iteration 57, loss = 0.45471871\n",
            "Iteration 58, loss = 0.47629670\n",
            "Iteration 59, loss = 0.45132206\n",
            "Iteration 60, loss = 0.46056285\n",
            "Iteration 61, loss = 0.44079443\n",
            "Iteration 62, loss = 0.45584957\n",
            "Iteration 63, loss = 0.47088810\n",
            "Iteration 64, loss = 0.43420249\n",
            "Iteration 65, loss = 0.45333939\n",
            "Iteration 66, loss = 0.45845125\n",
            "Iteration 67, loss = 0.43590948\n",
            "Iteration 68, loss = 0.44583940\n",
            "Iteration 69, loss = 0.45278083\n",
            "Iteration 70, loss = 0.45885997\n",
            "Iteration 71, loss = 0.46396417\n",
            "Iteration 72, loss = 0.43150884\n",
            "Iteration 73, loss = 0.43423767\n",
            "Iteration 74, loss = 0.43450600\n",
            "Iteration 75, loss = 0.46322684\n",
            "Iteration 76, loss = 0.44915728\n",
            "Iteration 77, loss = 0.42804780\n",
            "Iteration 78, loss = 0.43986921\n",
            "Iteration 79, loss = 0.47180779\n",
            "Iteration 80, loss = 0.45504245\n",
            "Iteration 81, loss = 0.46057114\n",
            "Iteration 82, loss = 0.45322712\n",
            "Iteration 83, loss = 0.44175941\n",
            "Iteration 84, loss = 0.43371213\n",
            "Iteration 85, loss = 0.42969117\n",
            "Iteration 86, loss = 0.43434666\n",
            "Iteration 87, loss = 0.44410545\n",
            "Iteration 88, loss = 0.44638416\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.92184201\n",
            "Iteration 2, loss = 0.63096318\n",
            "Iteration 3, loss = 0.63959691\n",
            "Iteration 4, loss = 0.60558975\n",
            "Iteration 5, loss = 0.61688588\n",
            "Iteration 6, loss = 0.57367064\n",
            "Iteration 7, loss = 0.56111886\n",
            "Iteration 8, loss = 0.55616472\n",
            "Iteration 9, loss = 0.52819547\n",
            "Iteration 10, loss = 0.53014592\n",
            "Iteration 11, loss = 0.51415647\n",
            "Iteration 12, loss = 0.49232949\n",
            "Iteration 13, loss = 0.49048612\n",
            "Iteration 14, loss = 0.48780337\n",
            "Iteration 15, loss = 0.50959077\n",
            "Iteration 16, loss = 0.47174270\n",
            "Iteration 17, loss = 0.50693246\n",
            "Iteration 18, loss = 0.47977531\n",
            "Iteration 19, loss = 0.48051943\n",
            "Iteration 20, loss = 0.49183289\n",
            "Iteration 21, loss = 0.44820870\n",
            "Iteration 22, loss = 0.45388427\n",
            "Iteration 23, loss = 0.47033402\n",
            "Iteration 24, loss = 0.46386334\n",
            "Iteration 25, loss = 0.45896869\n",
            "Iteration 26, loss = 0.48073160\n",
            "Iteration 27, loss = 0.45382633\n",
            "Iteration 28, loss = 0.43834970\n",
            "Iteration 29, loss = 0.43528805\n",
            "Iteration 30, loss = 0.44855471\n",
            "Iteration 31, loss = 0.47378158\n",
            "Iteration 32, loss = 0.49338182\n",
            "Iteration 33, loss = 0.46588342\n",
            "Iteration 34, loss = 0.47144852\n",
            "Iteration 35, loss = 0.43705704\n",
            "Iteration 36, loss = 0.45040755\n",
            "Iteration 37, loss = 0.45048345\n",
            "Iteration 38, loss = 0.47228478\n",
            "Iteration 39, loss = 0.50178533\n",
            "Iteration 40, loss = 0.45604113\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.74643156\n",
            "Iteration 2, loss = 0.64922544\n",
            "Iteration 3, loss = 0.66601158\n",
            "Iteration 4, loss = 0.62606049\n",
            "Iteration 5, loss = 0.61752240\n",
            "Iteration 6, loss = 0.58945138\n",
            "Iteration 7, loss = 0.61074133\n",
            "Iteration 8, loss = 0.60979458\n",
            "Iteration 9, loss = 0.58545206\n",
            "Iteration 10, loss = 0.59507168\n",
            "Iteration 11, loss = 0.55117219\n",
            "Iteration 12, loss = 0.54100401\n",
            "Iteration 13, loss = 0.53261982\n",
            "Iteration 14, loss = 0.55255536\n",
            "Iteration 15, loss = 0.56729958\n",
            "Iteration 16, loss = 0.53194802\n",
            "Iteration 17, loss = 0.51515458\n",
            "Iteration 18, loss = 0.51259955\n",
            "Iteration 19, loss = 0.51719231\n",
            "Iteration 20, loss = 0.51788565\n",
            "Iteration 21, loss = 0.49875046\n",
            "Iteration 22, loss = 0.49389993\n",
            "Iteration 23, loss = 0.48663625\n",
            "Iteration 24, loss = 0.49711350\n",
            "Iteration 25, loss = 0.50474275\n",
            "Iteration 26, loss = 0.48144188\n",
            "Iteration 27, loss = 0.49734148\n",
            "Iteration 28, loss = 0.47517708\n",
            "Iteration 29, loss = 0.51720383\n",
            "Iteration 30, loss = 0.47500372\n",
            "Iteration 31, loss = 0.45771541\n",
            "Iteration 32, loss = 0.56129271\n",
            "Iteration 33, loss = 0.46275693\n",
            "Iteration 34, loss = 0.54608236\n",
            "Iteration 35, loss = 0.47726482\n",
            "Iteration 36, loss = 0.45417914\n",
            "Iteration 37, loss = 0.45543845\n",
            "Iteration 38, loss = 0.46708854\n",
            "Iteration 39, loss = 0.46077119\n",
            "Iteration 40, loss = 0.45805643\n",
            "Iteration 41, loss = 0.51682117\n",
            "Iteration 42, loss = 0.45237022\n",
            "Iteration 43, loss = 0.43996648\n",
            "Iteration 44, loss = 0.47426436\n",
            "Iteration 45, loss = 0.50247904\n",
            "Iteration 46, loss = 0.46123089\n",
            "Iteration 47, loss = 0.44360669\n",
            "Iteration 48, loss = 0.45917915\n",
            "Iteration 49, loss = 0.46103060\n",
            "Iteration 50, loss = 0.43624213\n",
            "Iteration 51, loss = 0.45497417\n",
            "Iteration 52, loss = 0.46002645\n",
            "Iteration 53, loss = 0.47157432\n",
            "Iteration 54, loss = 0.46670261\n",
            "Iteration 55, loss = 0.46919051\n",
            "Iteration 56, loss = 0.45644303\n",
            "Iteration 57, loss = 0.46153764\n",
            "Iteration 58, loss = 0.46050247\n",
            "Iteration 59, loss = 0.46165638\n",
            "Iteration 60, loss = 0.43984383\n",
            "Iteration 61, loss = 0.46708390\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66158218\n",
            "Iteration 2, loss = 0.61163395\n",
            "Iteration 3, loss = 0.61022581\n",
            "Iteration 4, loss = 0.60365422\n",
            "Iteration 5, loss = 0.59650325\n",
            "Iteration 6, loss = 0.59154311\n",
            "Iteration 7, loss = 0.58871282\n",
            "Iteration 8, loss = 0.58961694\n",
            "Iteration 9, loss = 0.58453221\n",
            "Iteration 10, loss = 0.58131194\n",
            "Iteration 11, loss = 0.58544180\n",
            "Iteration 12, loss = 0.58091448\n",
            "Iteration 13, loss = 0.57850130\n",
            "Iteration 14, loss = 0.57893035\n",
            "Iteration 15, loss = 0.57490254\n",
            "Iteration 16, loss = 0.57421816\n",
            "Iteration 17, loss = 0.57616383\n",
            "Iteration 18, loss = 0.57326065\n",
            "Iteration 19, loss = 0.57114150\n",
            "Iteration 20, loss = 0.57004249\n",
            "Iteration 21, loss = 0.56898627\n",
            "Iteration 22, loss = 0.56983257\n",
            "Iteration 23, loss = 0.56839925\n",
            "Iteration 24, loss = 0.57065114\n",
            "Iteration 25, loss = 0.58030778\n",
            "Iteration 26, loss = 0.57051820\n",
            "Iteration 27, loss = 0.56354189\n",
            "Iteration 28, loss = 0.56215153\n",
            "Iteration 29, loss = 0.56236430\n",
            "Iteration 30, loss = 0.56036762\n",
            "Iteration 31, loss = 0.56037388\n",
            "Iteration 32, loss = 0.55946899\n",
            "Iteration 33, loss = 0.55698057\n",
            "Iteration 34, loss = 0.56019339\n",
            "Iteration 35, loss = 0.55483609\n",
            "Iteration 36, loss = 0.55154701\n",
            "Iteration 37, loss = 0.55388055\n",
            "Iteration 38, loss = 0.55266675\n",
            "Iteration 39, loss = 0.55076582\n",
            "Iteration 40, loss = 0.56897717\n",
            "Iteration 41, loss = 0.55398372\n",
            "Iteration 42, loss = 0.54439781\n",
            "Iteration 43, loss = 0.54586194\n",
            "Iteration 44, loss = 0.55012927\n",
            "Iteration 45, loss = 0.54973143\n",
            "Iteration 46, loss = 0.54363250\n",
            "Iteration 47, loss = 0.54335834\n",
            "Iteration 48, loss = 0.55933400\n",
            "Iteration 49, loss = 0.53996788\n",
            "Iteration 50, loss = 0.55423584\n",
            "Iteration 51, loss = 0.54058758\n",
            "Iteration 52, loss = 0.54000864\n",
            "Iteration 53, loss = 0.54016295\n",
            "Iteration 54, loss = 0.53892418\n",
            "Iteration 55, loss = 0.53177842\n",
            "Iteration 56, loss = 0.53837533\n",
            "Iteration 57, loss = 0.53487794\n",
            "Iteration 58, loss = 0.53128301\n",
            "Iteration 59, loss = 0.54680575\n",
            "Iteration 60, loss = 0.53387709\n",
            "Iteration 61, loss = 0.53586550\n",
            "Iteration 62, loss = 0.53155356\n",
            "Iteration 63, loss = 0.52697466\n",
            "Iteration 64, loss = 0.52155606\n",
            "Iteration 65, loss = 0.51921800\n",
            "Iteration 66, loss = 0.53620867\n",
            "Iteration 67, loss = 0.52303758\n",
            "Iteration 68, loss = 0.52231664\n",
            "Iteration 69, loss = 0.53311121\n",
            "Iteration 70, loss = 0.51763539\n",
            "Iteration 71, loss = 0.51984491\n",
            "Iteration 72, loss = 0.51468855\n",
            "Iteration 73, loss = 0.51942125\n",
            "Iteration 74, loss = 0.53708277\n",
            "Iteration 75, loss = 0.53529975\n",
            "Iteration 76, loss = 0.52610657\n",
            "Iteration 77, loss = 0.51123314\n",
            "Iteration 78, loss = 0.51325300\n",
            "Iteration 79, loss = 0.50733359\n",
            "Iteration 80, loss = 0.51651623\n",
            "Iteration 81, loss = 0.52016039\n",
            "Iteration 82, loss = 0.50427280\n",
            "Iteration 83, loss = 0.50571582\n",
            "Iteration 84, loss = 0.50930090\n",
            "Iteration 85, loss = 0.51117778\n",
            "Iteration 86, loss = 0.50414987\n",
            "Iteration 87, loss = 0.50000196\n",
            "Iteration 88, loss = 0.52817019\n",
            "Iteration 89, loss = 0.51148823\n",
            "Iteration 90, loss = 0.50006314\n",
            "Iteration 91, loss = 0.50923819\n",
            "Iteration 92, loss = 0.49814867\n",
            "Iteration 93, loss = 0.49417011\n",
            "Iteration 94, loss = 0.51375540\n",
            "Iteration 95, loss = 0.51653899\n",
            "Iteration 96, loss = 0.52048910\n",
            "Iteration 97, loss = 0.50837038\n",
            "Iteration 98, loss = 0.50136500\n",
            "Iteration 99, loss = 0.50913676\n",
            "Iteration 100, loss = 0.49164317\n",
            "Iteration 101, loss = 0.48967139\n",
            "Iteration 102, loss = 0.48811900\n",
            "Iteration 103, loss = 0.49748891\n",
            "Iteration 104, loss = 0.51414098\n",
            "Iteration 105, loss = 0.51222227\n",
            "Iteration 106, loss = 0.49604922\n",
            "Iteration 107, loss = 0.48686072\n",
            "Iteration 108, loss = 0.48803987\n",
            "Iteration 109, loss = 0.54513570\n",
            "Iteration 110, loss = 0.49695620\n",
            "Iteration 111, loss = 0.48575306\n",
            "Iteration 112, loss = 0.48585061\n",
            "Iteration 113, loss = 0.50507061\n",
            "Iteration 114, loss = 0.48328224\n",
            "Iteration 115, loss = 0.48104775\n",
            "Iteration 116, loss = 0.48882872\n",
            "Iteration 117, loss = 0.48282081\n",
            "Iteration 118, loss = 0.48917695\n",
            "Iteration 119, loss = 0.53404398\n",
            "Iteration 120, loss = 0.48304483\n",
            "Iteration 121, loss = 0.48389992\n",
            "Iteration 122, loss = 0.47844652\n",
            "Iteration 123, loss = 0.48751369\n",
            "Iteration 124, loss = 0.47416140\n",
            "Iteration 125, loss = 0.48913907\n",
            "Iteration 126, loss = 0.47306745\n",
            "Iteration 127, loss = 0.47503176\n",
            "Iteration 128, loss = 0.48507710\n",
            "Iteration 129, loss = 0.52270856\n",
            "Iteration 130, loss = 0.47603005\n",
            "Iteration 131, loss = 0.46868588\n",
            "Iteration 132, loss = 0.48604438\n",
            "Iteration 133, loss = 0.47574274\n",
            "Iteration 134, loss = 0.46807450\n",
            "Iteration 135, loss = 0.51885661\n",
            "Iteration 136, loss = 0.46622909\n",
            "Iteration 137, loss = 0.47868157\n",
            "Iteration 138, loss = 0.50664326\n",
            "Iteration 139, loss = 0.46798814\n",
            "Iteration 140, loss = 0.46911314\n",
            "Iteration 141, loss = 0.48902127\n",
            "Iteration 142, loss = 0.47209251\n",
            "Iteration 143, loss = 0.46519496\n",
            "Iteration 144, loss = 0.48651347\n",
            "Iteration 145, loss = 0.48602818\n",
            "Iteration 146, loss = 0.46482645\n",
            "Iteration 147, loss = 0.47829549\n",
            "Iteration 148, loss = 0.47876634\n",
            "Iteration 149, loss = 0.48209728\n",
            "Iteration 150, loss = 0.48564069\n",
            "Iteration 151, loss = 0.49128758\n",
            "Iteration 152, loss = 0.47136204\n",
            "Iteration 153, loss = 0.46502363\n",
            "Iteration 154, loss = 0.46949078\n",
            "Iteration 155, loss = 0.49879106\n",
            "Iteration 156, loss = 0.47624677\n",
            "Iteration 157, loss = 0.47014757\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62350438\n",
            "Iteration 2, loss = 0.60302810\n",
            "Iteration 3, loss = 0.60277465\n",
            "Iteration 4, loss = 0.60031078\n",
            "Iteration 5, loss = 0.59927272\n",
            "Iteration 6, loss = 0.60185442\n",
            "Iteration 7, loss = 0.59248294\n",
            "Iteration 8, loss = 0.59159779\n",
            "Iteration 9, loss = 0.59222638\n",
            "Iteration 10, loss = 0.58835373\n",
            "Iteration 11, loss = 0.58504385\n",
            "Iteration 12, loss = 0.58747735\n",
            "Iteration 13, loss = 0.58300255\n",
            "Iteration 14, loss = 0.59113055\n",
            "Iteration 15, loss = 0.58640202\n",
            "Iteration 16, loss = 0.57839233\n",
            "Iteration 17, loss = 0.57948112\n",
            "Iteration 18, loss = 0.57976813\n",
            "Iteration 19, loss = 0.57820221\n",
            "Iteration 20, loss = 0.57513808\n",
            "Iteration 21, loss = 0.57951501\n",
            "Iteration 22, loss = 0.57402072\n",
            "Iteration 23, loss = 0.57145658\n",
            "Iteration 24, loss = 0.57570221\n",
            "Iteration 25, loss = 0.57009749\n",
            "Iteration 26, loss = 0.56961983\n",
            "Iteration 27, loss = 0.56646400\n",
            "Iteration 28, loss = 0.57129228\n",
            "Iteration 29, loss = 0.56977682\n",
            "Iteration 30, loss = 0.56968498\n",
            "Iteration 31, loss = 0.56789643\n",
            "Iteration 32, loss = 0.56464709\n",
            "Iteration 33, loss = 0.57271431\n",
            "Iteration 34, loss = 0.56401683\n",
            "Iteration 35, loss = 0.56798455\n",
            "Iteration 36, loss = 0.56251185\n",
            "Iteration 37, loss = 0.57322353\n",
            "Iteration 38, loss = 0.55950483\n",
            "Iteration 39, loss = 0.56290809\n",
            "Iteration 40, loss = 0.55650854\n",
            "Iteration 41, loss = 0.56002057\n",
            "Iteration 42, loss = 0.55552963\n",
            "Iteration 43, loss = 0.55975261\n",
            "Iteration 44, loss = 0.55174049\n",
            "Iteration 45, loss = 0.55241072\n",
            "Iteration 46, loss = 0.56424861\n",
            "Iteration 47, loss = 0.55398802\n",
            "Iteration 48, loss = 0.55078892\n",
            "Iteration 49, loss = 0.55261744\n",
            "Iteration 50, loss = 0.56045937\n",
            "Iteration 51, loss = 0.54638519\n",
            "Iteration 52, loss = 0.54677126\n",
            "Iteration 53, loss = 0.54534141\n",
            "Iteration 54, loss = 0.54395637\n",
            "Iteration 55, loss = 0.53978377\n",
            "Iteration 56, loss = 0.54148503\n",
            "Iteration 57, loss = 0.54004375\n",
            "Iteration 58, loss = 0.54194758\n",
            "Iteration 59, loss = 0.54155172\n",
            "Iteration 60, loss = 0.53927253\n",
            "Iteration 61, loss = 0.53975710\n",
            "Iteration 62, loss = 0.53957373\n",
            "Iteration 63, loss = 0.55212514\n",
            "Iteration 64, loss = 0.55106057\n",
            "Iteration 65, loss = 0.54164385\n",
            "Iteration 66, loss = 0.53364343\n",
            "Iteration 67, loss = 0.53253152\n",
            "Iteration 68, loss = 0.53068741\n",
            "Iteration 69, loss = 0.52896467\n",
            "Iteration 70, loss = 0.53033336\n",
            "Iteration 71, loss = 0.53553572\n",
            "Iteration 72, loss = 0.55842373\n",
            "Iteration 73, loss = 0.53734838\n",
            "Iteration 74, loss = 0.53299665\n",
            "Iteration 75, loss = 0.53026133\n",
            "Iteration 76, loss = 0.52997723\n",
            "Iteration 77, loss = 0.52492999\n",
            "Iteration 78, loss = 0.52373527\n",
            "Iteration 79, loss = 0.52470072\n",
            "Iteration 80, loss = 0.57874788\n",
            "Iteration 81, loss = 0.54754014\n",
            "Iteration 82, loss = 0.54070020\n",
            "Iteration 83, loss = 0.53221886\n",
            "Iteration 84, loss = 0.53467878\n",
            "Iteration 85, loss = 0.53526668\n",
            "Iteration 86, loss = 0.52456179\n",
            "Iteration 87, loss = 0.52477909\n",
            "Iteration 88, loss = 0.53258595\n",
            "Iteration 89, loss = 0.53162562\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.65776740\n",
            "Iteration 2, loss = 0.61763757\n",
            "Iteration 3, loss = 0.61264569\n",
            "Iteration 4, loss = 0.60563758\n",
            "Iteration 5, loss = 0.60651554\n",
            "Iteration 6, loss = 0.60293896\n",
            "Iteration 7, loss = 0.60435453\n",
            "Iteration 8, loss = 0.59712866\n",
            "Iteration 9, loss = 0.59410704\n",
            "Iteration 10, loss = 0.59387775\n",
            "Iteration 11, loss = 0.59339211\n",
            "Iteration 12, loss = 0.59103745\n",
            "Iteration 13, loss = 0.59237942\n",
            "Iteration 14, loss = 0.59508684\n",
            "Iteration 15, loss = 0.58934355\n",
            "Iteration 16, loss = 0.58440379\n",
            "Iteration 17, loss = 0.58908670\n",
            "Iteration 18, loss = 0.58867168\n",
            "Iteration 19, loss = 0.58122684\n",
            "Iteration 20, loss = 0.58258625\n",
            "Iteration 21, loss = 0.57957322\n",
            "Iteration 22, loss = 0.58173339\n",
            "Iteration 23, loss = 0.57636881\n",
            "Iteration 24, loss = 0.57601174\n",
            "Iteration 25, loss = 0.57514879\n",
            "Iteration 26, loss = 0.57566088\n",
            "Iteration 27, loss = 0.57381887\n",
            "Iteration 28, loss = 0.57738362\n",
            "Iteration 29, loss = 0.58189579\n",
            "Iteration 30, loss = 0.57430618\n",
            "Iteration 31, loss = 0.57160626\n",
            "Iteration 32, loss = 0.56971200\n",
            "Iteration 33, loss = 0.57623759\n",
            "Iteration 34, loss = 0.56535198\n",
            "Iteration 35, loss = 0.57854013\n",
            "Iteration 36, loss = 0.56845174\n",
            "Iteration 37, loss = 0.56429093\n",
            "Iteration 38, loss = 0.57995286\n",
            "Iteration 39, loss = 0.56187483\n",
            "Iteration 40, loss = 0.56274901\n",
            "Iteration 41, loss = 0.55908815\n",
            "Iteration 42, loss = 0.55889836\n",
            "Iteration 43, loss = 0.56167667\n",
            "Iteration 44, loss = 0.55784409\n",
            "Iteration 45, loss = 0.55999280\n",
            "Iteration 46, loss = 0.55643830\n",
            "Iteration 47, loss = 0.55185231\n",
            "Iteration 48, loss = 0.55284181\n",
            "Iteration 49, loss = 0.55384930\n",
            "Iteration 50, loss = 0.54831145\n",
            "Iteration 51, loss = 0.54772629\n",
            "Iteration 52, loss = 0.54734468\n",
            "Iteration 53, loss = 0.54918526\n",
            "Iteration 54, loss = 0.54465721\n",
            "Iteration 55, loss = 0.54633253\n",
            "Iteration 56, loss = 0.54466549\n",
            "Iteration 57, loss = 0.54405150\n",
            "Iteration 58, loss = 0.54596286\n",
            "Iteration 59, loss = 0.54990260\n",
            "Iteration 60, loss = 0.54999197\n",
            "Iteration 61, loss = 0.54512859\n",
            "Iteration 62, loss = 0.54748396\n",
            "Iteration 63, loss = 0.53911204\n",
            "Iteration 64, loss = 0.53406752\n",
            "Iteration 65, loss = 0.53252573\n",
            "Iteration 66, loss = 0.53687234\n",
            "Iteration 67, loss = 0.53701886\n",
            "Iteration 68, loss = 0.55300400\n",
            "Iteration 69, loss = 0.53130337\n",
            "Iteration 70, loss = 0.52867421\n",
            "Iteration 71, loss = 0.53034407\n",
            "Iteration 72, loss = 0.53277812\n",
            "Iteration 73, loss = 0.53593866\n",
            "Iteration 74, loss = 0.52552079\n",
            "Iteration 75, loss = 0.52267994\n",
            "Iteration 76, loss = 0.52700263\n",
            "Iteration 77, loss = 0.52428887\n",
            "Iteration 78, loss = 0.52101856\n",
            "Iteration 79, loss = 0.52068353\n",
            "Iteration 80, loss = 0.54716075\n",
            "Iteration 81, loss = 0.52295587\n",
            "Iteration 82, loss = 0.51988528\n",
            "Iteration 83, loss = 0.51548209\n",
            "Iteration 84, loss = 0.51325239\n",
            "Iteration 85, loss = 0.51298021\n",
            "Iteration 86, loss = 0.51221628\n",
            "Iteration 87, loss = 0.51208869\n",
            "Iteration 88, loss = 0.51469812\n",
            "Iteration 89, loss = 0.51469527\n",
            "Iteration 90, loss = 0.50736699\n",
            "Iteration 91, loss = 0.50804629\n",
            "Iteration 92, loss = 0.53064113\n",
            "Iteration 93, loss = 0.50745928\n",
            "Iteration 94, loss = 0.50979659\n",
            "Iteration 95, loss = 0.49889313\n",
            "Iteration 96, loss = 0.51386519\n",
            "Iteration 97, loss = 0.50198904\n",
            "Iteration 98, loss = 0.50701544\n",
            "Iteration 99, loss = 0.49896644\n",
            "Iteration 100, loss = 0.50290253\n",
            "Iteration 101, loss = 0.50123044\n",
            "Iteration 102, loss = 0.50253852\n",
            "Iteration 103, loss = 0.49451834\n",
            "Iteration 104, loss = 0.50069206\n",
            "Iteration 105, loss = 0.50490727\n",
            "Iteration 106, loss = 0.50163795\n",
            "Iteration 107, loss = 0.50375821\n",
            "Iteration 108, loss = 0.49435976\n",
            "Iteration 109, loss = 0.49225465\n",
            "Iteration 110, loss = 0.50319411\n",
            "Iteration 111, loss = 0.50038650\n",
            "Iteration 112, loss = 0.50595746\n",
            "Iteration 113, loss = 0.49148278\n",
            "Iteration 114, loss = 0.51078744\n",
            "Iteration 115, loss = 0.49252096\n",
            "Iteration 116, loss = 0.50148852\n",
            "Iteration 117, loss = 0.49125359\n",
            "Iteration 118, loss = 0.50161777\n",
            "Iteration 119, loss = 0.49778501\n",
            "Iteration 120, loss = 0.49138579\n",
            "Iteration 121, loss = 0.48989297\n",
            "Iteration 122, loss = 0.49445621\n",
            "Iteration 123, loss = 0.49139237\n",
            "Iteration 124, loss = 0.49210587\n",
            "Iteration 125, loss = 0.49854819\n",
            "Iteration 126, loss = 0.48412386\n",
            "Iteration 127, loss = 0.48231065\n",
            "Iteration 128, loss = 0.50044181\n",
            "Iteration 129, loss = 0.49669434\n",
            "Iteration 130, loss = 0.49408559\n",
            "Iteration 131, loss = 0.48270593\n",
            "Iteration 132, loss = 0.48301463\n",
            "Iteration 133, loss = 0.47961510\n",
            "Iteration 134, loss = 0.47568053\n",
            "Iteration 135, loss = 0.48280370\n",
            "Iteration 136, loss = 0.47858562\n",
            "Iteration 137, loss = 0.49103058\n",
            "Iteration 138, loss = 0.48274404\n",
            "Iteration 139, loss = 0.47326824\n",
            "Iteration 140, loss = 0.47592648\n",
            "Iteration 141, loss = 0.49512748\n",
            "Iteration 142, loss = 0.47361021\n",
            "Iteration 143, loss = 0.47256204\n",
            "Iteration 144, loss = 0.47151783\n",
            "Iteration 145, loss = 0.47906705\n",
            "Iteration 146, loss = 0.46814387\n",
            "Iteration 147, loss = 0.48624332\n",
            "Iteration 148, loss = 0.46876408\n",
            "Iteration 149, loss = 0.49879769\n",
            "Iteration 150, loss = 0.49425867\n",
            "Iteration 151, loss = 0.48467018\n",
            "Iteration 152, loss = 0.49385024\n",
            "Iteration 153, loss = 0.48201289\n",
            "Iteration 154, loss = 0.46663041\n",
            "Iteration 155, loss = 0.48753569\n",
            "Iteration 156, loss = 0.51457286\n",
            "Iteration 157, loss = 0.48165498\n",
            "Iteration 158, loss = 0.46862844\n",
            "Iteration 159, loss = 0.50257646\n",
            "Iteration 160, loss = 0.49266703\n",
            "Iteration 161, loss = 0.52070910\n",
            "Iteration 162, loss = 0.49050572\n",
            "Iteration 163, loss = 0.48757556\n",
            "Iteration 164, loss = 0.47745493\n",
            "Iteration 165, loss = 0.51100575\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75054304\n",
            "Iteration 2, loss = 0.60725567\n",
            "Iteration 3, loss = 0.60257240\n",
            "Iteration 4, loss = 0.59628423\n",
            "Iteration 5, loss = 0.59373896\n",
            "Iteration 6, loss = 0.59134352\n",
            "Iteration 7, loss = 0.59277054\n",
            "Iteration 8, loss = 0.58783947\n",
            "Iteration 9, loss = 0.58610155\n",
            "Iteration 10, loss = 0.58554223\n",
            "Iteration 11, loss = 0.58524493\n",
            "Iteration 12, loss = 0.58132508\n",
            "Iteration 13, loss = 0.58215900\n",
            "Iteration 14, loss = 0.58041710\n",
            "Iteration 15, loss = 0.57585059\n",
            "Iteration 16, loss = 0.57764162\n",
            "Iteration 17, loss = 0.57696504\n",
            "Iteration 18, loss = 0.57809788\n",
            "Iteration 19, loss = 0.57897128\n",
            "Iteration 20, loss = 0.57299840\n",
            "Iteration 21, loss = 0.57096918\n",
            "Iteration 22, loss = 0.57188763\n",
            "Iteration 23, loss = 0.57142274\n",
            "Iteration 24, loss = 0.57334399\n",
            "Iteration 25, loss = 0.56952590\n",
            "Iteration 26, loss = 0.56931134\n",
            "Iteration 27, loss = 0.56898259\n",
            "Iteration 28, loss = 0.56687137\n",
            "Iteration 29, loss = 0.57122847\n",
            "Iteration 30, loss = 0.56717059\n",
            "Iteration 31, loss = 0.56538068\n",
            "Iteration 32, loss = 0.56993612\n",
            "Iteration 33, loss = 0.56580100\n",
            "Iteration 34, loss = 0.56683248\n",
            "Iteration 35, loss = 0.58115352\n",
            "Iteration 36, loss = 0.56276547\n",
            "Iteration 37, loss = 0.56201573\n",
            "Iteration 38, loss = 0.55800236\n",
            "Iteration 39, loss = 0.56628956\n",
            "Iteration 40, loss = 0.55764070\n",
            "Iteration 41, loss = 0.55409612\n",
            "Iteration 42, loss = 0.55712557\n",
            "Iteration 43, loss = 0.55443247\n",
            "Iteration 44, loss = 0.55353485\n",
            "Iteration 45, loss = 0.55596019\n",
            "Iteration 46, loss = 0.55089132\n",
            "Iteration 47, loss = 0.55126525\n",
            "Iteration 48, loss = 0.54915775\n",
            "Iteration 49, loss = 0.56351617\n",
            "Iteration 50, loss = 0.56189375\n",
            "Iteration 51, loss = 0.56224752\n",
            "Iteration 52, loss = 0.55340228\n",
            "Iteration 53, loss = 0.55128088\n",
            "Iteration 54, loss = 0.55152733\n",
            "Iteration 55, loss = 0.55139388\n",
            "Iteration 56, loss = 0.54341142\n",
            "Iteration 57, loss = 0.54280349\n",
            "Iteration 58, loss = 0.54570595\n",
            "Iteration 59, loss = 0.54088642\n",
            "Iteration 60, loss = 0.54346920\n",
            "Iteration 61, loss = 0.54391730\n",
            "Iteration 62, loss = 0.54165838\n",
            "Iteration 63, loss = 0.54134366\n",
            "Iteration 64, loss = 0.53329273\n",
            "Iteration 65, loss = 0.53223729\n",
            "Iteration 66, loss = 0.53579217\n",
            "Iteration 67, loss = 0.52977026\n",
            "Iteration 68, loss = 0.53198244\n",
            "Iteration 69, loss = 0.53844441\n",
            "Iteration 70, loss = 0.53736993\n",
            "Iteration 71, loss = 0.52818606\n",
            "Iteration 72, loss = 0.53864348\n",
            "Iteration 73, loss = 0.54153789\n",
            "Iteration 74, loss = 0.54113821\n",
            "Iteration 75, loss = 0.54616641\n",
            "Iteration 76, loss = 0.53386669\n",
            "Iteration 77, loss = 0.52664140\n",
            "Iteration 78, loss = 0.52287705\n",
            "Iteration 79, loss = 0.52563462\n",
            "Iteration 80, loss = 0.52760180\n",
            "Iteration 81, loss = 0.54396032\n",
            "Iteration 82, loss = 0.53843684\n",
            "Iteration 83, loss = 0.53340582\n",
            "Iteration 84, loss = 0.51780650\n",
            "Iteration 85, loss = 0.51525601\n",
            "Iteration 86, loss = 0.52349896\n",
            "Iteration 87, loss = 0.51626985\n",
            "Iteration 88, loss = 0.51611170\n",
            "Iteration 89, loss = 0.52046835\n",
            "Iteration 90, loss = 0.51613005\n",
            "Iteration 91, loss = 0.51083759\n",
            "Iteration 92, loss = 0.51485092\n",
            "Iteration 93, loss = 0.51018689\n",
            "Iteration 94, loss = 0.50451850\n",
            "Iteration 95, loss = 0.50547550\n",
            "Iteration 96, loss = 0.51271619\n",
            "Iteration 97, loss = 0.51055989\n",
            "Iteration 98, loss = 0.51625768\n",
            "Iteration 99, loss = 0.51206619\n",
            "Iteration 100, loss = 0.50077543\n",
            "Iteration 101, loss = 0.51254824\n",
            "Iteration 102, loss = 0.49652431\n",
            "Iteration 103, loss = 0.49842738\n",
            "Iteration 104, loss = 0.50087775\n",
            "Iteration 105, loss = 0.49469785\n",
            "Iteration 106, loss = 0.50780842\n",
            "Iteration 107, loss = 0.50243264\n",
            "Iteration 108, loss = 0.50157417\n",
            "Iteration 109, loss = 0.49222552\n",
            "Iteration 110, loss = 0.51011601\n",
            "Iteration 111, loss = 0.50482789\n",
            "Iteration 112, loss = 0.50097663\n",
            "Iteration 113, loss = 0.48807404\n",
            "Iteration 114, loss = 0.50493529\n",
            "Iteration 115, loss = 0.48835134\n",
            "Iteration 116, loss = 0.48454315\n",
            "Iteration 117, loss = 0.49375203\n",
            "Iteration 118, loss = 0.49154326\n",
            "Iteration 119, loss = 0.51341344\n",
            "Iteration 120, loss = 0.49190683\n",
            "Iteration 121, loss = 0.51556397\n",
            "Iteration 122, loss = 0.48226210\n",
            "Iteration 123, loss = 0.50491276\n",
            "Iteration 124, loss = 0.53905326\n",
            "Iteration 125, loss = 0.50697140\n",
            "Iteration 126, loss = 0.51311795\n",
            "Iteration 127, loss = 0.50375053\n",
            "Iteration 128, loss = 0.48653327\n",
            "Iteration 129, loss = 0.48141770\n",
            "Iteration 130, loss = 0.48932890\n",
            "Iteration 131, loss = 0.50336292\n",
            "Iteration 132, loss = 0.47775641\n",
            "Iteration 133, loss = 0.47818784\n",
            "Iteration 134, loss = 0.47972291\n",
            "Iteration 135, loss = 0.48437780\n",
            "Iteration 136, loss = 0.48688735\n",
            "Iteration 137, loss = 0.48670569\n",
            "Iteration 138, loss = 0.50455085\n",
            "Iteration 139, loss = 0.49240364\n",
            "Iteration 140, loss = 0.49814052\n",
            "Iteration 141, loss = 0.47433242\n",
            "Iteration 142, loss = 0.49350002\n",
            "Iteration 143, loss = 0.49522744\n",
            "Iteration 144, loss = 0.47925330\n",
            "Iteration 145, loss = 0.48259543\n",
            "Iteration 146, loss = 0.48136321\n",
            "Iteration 147, loss = 0.46910897\n",
            "Iteration 148, loss = 0.47382015\n",
            "Iteration 149, loss = 0.54082262\n",
            "Iteration 150, loss = 0.50210191\n",
            "Iteration 151, loss = 0.49725385\n",
            "Iteration 152, loss = 0.49041443\n",
            "Iteration 153, loss = 0.49144264\n",
            "Iteration 154, loss = 0.49623211\n",
            "Iteration 155, loss = 0.48995261\n",
            "Iteration 156, loss = 0.48640458\n",
            "Iteration 157, loss = 0.47485834\n",
            "Iteration 158, loss = 0.49193189\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.72579526\n",
            "Iteration 2, loss = 0.62488150\n",
            "Iteration 3, loss = 0.60857408\n",
            "Iteration 4, loss = 0.60210178\n",
            "Iteration 5, loss = 0.59935618\n",
            "Iteration 6, loss = 0.60020405\n",
            "Iteration 7, loss = 0.59675028\n",
            "Iteration 8, loss = 0.59694400\n",
            "Iteration 9, loss = 0.59737831\n",
            "Iteration 10, loss = 0.59416425\n",
            "Iteration 11, loss = 0.59157445\n",
            "Iteration 12, loss = 0.59218796\n",
            "Iteration 13, loss = 0.59034666\n",
            "Iteration 14, loss = 0.58916172\n",
            "Iteration 15, loss = 0.58781808\n",
            "Iteration 16, loss = 0.58915442\n",
            "Iteration 17, loss = 0.58580612\n",
            "Iteration 18, loss = 0.58992867\n",
            "Iteration 19, loss = 0.58900247\n",
            "Iteration 20, loss = 0.58567595\n",
            "Iteration 21, loss = 0.58277250\n",
            "Iteration 22, loss = 0.58672049\n",
            "Iteration 23, loss = 0.58663708\n",
            "Iteration 24, loss = 0.60144979\n",
            "Iteration 25, loss = 0.58097908\n",
            "Iteration 26, loss = 0.57987433\n",
            "Iteration 27, loss = 0.57884171\n",
            "Iteration 28, loss = 0.58353400\n",
            "Iteration 29, loss = 0.57735442\n",
            "Iteration 30, loss = 0.57756224\n",
            "Iteration 31, loss = 0.57718414\n",
            "Iteration 32, loss = 0.57689116\n",
            "Iteration 33, loss = 0.57633767\n",
            "Iteration 34, loss = 0.57427104\n",
            "Iteration 35, loss = 0.57343505\n",
            "Iteration 36, loss = 0.57099262\n",
            "Iteration 37, loss = 0.57072871\n",
            "Iteration 38, loss = 0.57035729\n",
            "Iteration 39, loss = 0.56873064\n",
            "Iteration 40, loss = 0.57021329\n",
            "Iteration 41, loss = 0.56987671\n",
            "Iteration 42, loss = 0.58141913\n",
            "Iteration 43, loss = 0.56786494\n",
            "Iteration 44, loss = 0.57750728\n",
            "Iteration 45, loss = 0.56811898\n",
            "Iteration 46, loss = 0.57222969\n",
            "Iteration 47, loss = 0.56878497\n",
            "Iteration 48, loss = 0.56115580\n",
            "Iteration 49, loss = 0.56208489\n",
            "Iteration 50, loss = 0.55963493\n",
            "Iteration 51, loss = 0.55986784\n",
            "Iteration 52, loss = 0.55664313\n",
            "Iteration 53, loss = 0.55731430\n",
            "Iteration 54, loss = 0.55711058\n",
            "Iteration 55, loss = 0.55633638\n",
            "Iteration 56, loss = 0.55937348\n",
            "Iteration 57, loss = 0.55723977\n",
            "Iteration 58, loss = 0.55636051\n",
            "Iteration 59, loss = 0.55571334\n",
            "Iteration 60, loss = 0.55202882\n",
            "Iteration 61, loss = 0.55369046\n",
            "Iteration 62, loss = 0.55258872\n",
            "Iteration 63, loss = 0.55774727\n",
            "Iteration 64, loss = 0.55617990\n",
            "Iteration 65, loss = 0.55119243\n",
            "Iteration 66, loss = 0.55402803\n",
            "Iteration 67, loss = 0.54530798\n",
            "Iteration 68, loss = 0.54582422\n",
            "Iteration 69, loss = 0.54761158\n",
            "Iteration 70, loss = 0.54163653\n",
            "Iteration 71, loss = 0.54478629\n",
            "Iteration 72, loss = 0.54532806\n",
            "Iteration 73, loss = 0.54857194\n",
            "Iteration 74, loss = 0.54875139\n",
            "Iteration 75, loss = 0.55040896\n",
            "Iteration 76, loss = 0.54583045\n",
            "Iteration 77, loss = 0.53893096\n",
            "Iteration 78, loss = 0.54610823\n",
            "Iteration 79, loss = 0.53614670\n",
            "Iteration 80, loss = 0.54157082\n",
            "Iteration 81, loss = 0.54491195\n",
            "Iteration 82, loss = 0.53383846\n",
            "Iteration 83, loss = 0.53081724\n",
            "Iteration 84, loss = 0.54135720\n",
            "Iteration 85, loss = 0.53448589\n",
            "Iteration 86, loss = 0.52728579\n",
            "Iteration 87, loss = 0.53577233\n",
            "Iteration 88, loss = 0.53794612\n",
            "Iteration 89, loss = 0.52862175\n",
            "Iteration 90, loss = 0.53841065\n",
            "Iteration 91, loss = 0.53677647\n",
            "Iteration 92, loss = 0.53519463\n",
            "Iteration 93, loss = 0.52582958\n",
            "Iteration 94, loss = 0.52814385\n",
            "Iteration 95, loss = 0.51995858\n",
            "Iteration 96, loss = 0.53015784\n",
            "Iteration 97, loss = 0.53324202\n",
            "Iteration 98, loss = 0.52441432\n",
            "Iteration 99, loss = 0.53175172\n",
            "Iteration 100, loss = 0.51886731\n",
            "Iteration 101, loss = 0.52244156\n",
            "Iteration 102, loss = 0.52550849\n",
            "Iteration 103, loss = 0.52163118\n",
            "Iteration 104, loss = 0.53852178\n",
            "Iteration 105, loss = 0.52934874\n",
            "Iteration 106, loss = 0.51901481\n",
            "Iteration 107, loss = 0.52875112\n",
            "Iteration 108, loss = 0.51924972\n",
            "Iteration 109, loss = 0.51273710\n",
            "Iteration 110, loss = 0.52573403\n",
            "Iteration 111, loss = 0.51093439\n",
            "Iteration 112, loss = 0.51793909\n",
            "Iteration 113, loss = 0.52023125\n",
            "Iteration 114, loss = 0.53875249\n",
            "Iteration 115, loss = 0.51107917\n",
            "Iteration 116, loss = 0.50875565\n",
            "Iteration 117, loss = 0.51348858\n",
            "Iteration 118, loss = 0.50634600\n",
            "Iteration 119, loss = 0.51520725\n",
            "Iteration 120, loss = 0.53405163\n",
            "Iteration 121, loss = 0.52003078\n",
            "Iteration 122, loss = 0.51793136\n",
            "Iteration 123, loss = 0.51493265\n",
            "Iteration 124, loss = 0.51373850\n",
            "Iteration 125, loss = 0.51552783\n",
            "Iteration 126, loss = 0.50656572\n",
            "Iteration 127, loss = 0.50963186\n",
            "Iteration 128, loss = 0.50349623\n",
            "Iteration 129, loss = 0.50070543\n",
            "Iteration 130, loss = 0.52018846\n",
            "Iteration 131, loss = 0.50417516\n",
            "Iteration 132, loss = 0.51115977\n",
            "Iteration 133, loss = 0.51962679\n",
            "Iteration 134, loss = 0.50120169\n",
            "Iteration 135, loss = 0.49985899\n",
            "Iteration 136, loss = 0.50184710\n",
            "Iteration 137, loss = 0.49494758\n",
            "Iteration 138, loss = 0.49285530\n",
            "Iteration 139, loss = 0.53296180\n",
            "Iteration 140, loss = 0.50883763\n",
            "Iteration 141, loss = 0.49015151\n",
            "Iteration 142, loss = 0.49922864\n",
            "Iteration 143, loss = 0.49701554\n",
            "Iteration 144, loss = 0.48992860\n",
            "Iteration 145, loss = 0.50412910\n",
            "Iteration 146, loss = 0.54220895\n",
            "Iteration 147, loss = 0.50930536\n",
            "Iteration 148, loss = 0.50059774\n",
            "Iteration 149, loss = 0.49585789\n",
            "Iteration 150, loss = 0.50210045\n",
            "Iteration 151, loss = 0.50138427\n",
            "Iteration 152, loss = 0.49254330\n",
            "Iteration 153, loss = 0.49177084\n",
            "Iteration 154, loss = 0.48927924\n",
            "Iteration 155, loss = 0.48697271\n",
            "Iteration 156, loss = 0.48362746\n",
            "Iteration 157, loss = 0.48687126\n",
            "Iteration 158, loss = 0.49063194\n",
            "Iteration 159, loss = 0.49217449\n",
            "Iteration 160, loss = 0.48367450\n",
            "Iteration 161, loss = 0.48291604\n",
            "Iteration 162, loss = 0.47937350\n",
            "Iteration 163, loss = 0.48078083\n",
            "Iteration 164, loss = 0.48109619\n",
            "Iteration 165, loss = 0.49520902\n",
            "Iteration 166, loss = 0.48769905\n",
            "Iteration 167, loss = 0.49185243\n",
            "Iteration 168, loss = 0.49011197\n",
            "Iteration 169, loss = 0.48185784\n",
            "Iteration 170, loss = 0.48201462\n",
            "Iteration 171, loss = 0.47819817\n",
            "Iteration 172, loss = 0.47966001\n",
            "Iteration 173, loss = 0.49987087\n",
            "Iteration 174, loss = 0.48304593\n",
            "Iteration 175, loss = 0.49450238\n",
            "Iteration 176, loss = 0.47076163\n",
            "Iteration 177, loss = 0.46562883\n",
            "Iteration 178, loss = 0.46932044\n",
            "Iteration 179, loss = 0.52993439\n",
            "Iteration 180, loss = 0.47892674\n",
            "Iteration 181, loss = 0.48224148\n",
            "Iteration 182, loss = 0.49616444\n",
            "Iteration 183, loss = 0.48225219\n",
            "Iteration 184, loss = 0.48292996\n",
            "Iteration 185, loss = 0.48390566\n",
            "Iteration 186, loss = 0.47633802\n",
            "Iteration 187, loss = 0.46714570\n",
            "Iteration 188, loss = 0.47871858\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.76323278\n",
            "Iteration 2, loss = 0.68241764\n",
            "Iteration 3, loss = 0.63908867\n",
            "Iteration 4, loss = 0.62120466\n",
            "Iteration 5, loss = 0.61286076\n",
            "Iteration 6, loss = 0.60972587\n",
            "Iteration 7, loss = 0.60651360\n",
            "Iteration 8, loss = 0.60439489\n",
            "Iteration 9, loss = 0.60205206\n",
            "Iteration 10, loss = 0.60090435\n",
            "Iteration 11, loss = 0.59940958\n",
            "Iteration 12, loss = 0.59882148\n",
            "Iteration 13, loss = 0.59703231\n",
            "Iteration 14, loss = 0.59693924\n",
            "Iteration 15, loss = 0.59444856\n",
            "Iteration 16, loss = 0.59522862\n",
            "Iteration 17, loss = 0.59363681\n",
            "Iteration 18, loss = 0.59215874\n",
            "Iteration 19, loss = 0.59091505\n",
            "Iteration 20, loss = 0.59064486\n",
            "Iteration 21, loss = 0.59531649\n",
            "Iteration 22, loss = 0.58747698\n",
            "Iteration 23, loss = 0.58820351\n",
            "Iteration 24, loss = 0.58745733\n",
            "Iteration 25, loss = 0.58617858\n",
            "Iteration 26, loss = 0.58699347\n",
            "Iteration 27, loss = 0.58782213\n",
            "Iteration 28, loss = 0.58338164\n",
            "Iteration 29, loss = 0.58302776\n",
            "Iteration 30, loss = 0.58019428\n",
            "Iteration 31, loss = 0.58316801\n",
            "Iteration 32, loss = 0.58015640\n",
            "Iteration 33, loss = 0.57738000\n",
            "Iteration 34, loss = 0.57776166\n",
            "Iteration 35, loss = 0.58142229\n",
            "Iteration 36, loss = 0.57894046\n",
            "Iteration 37, loss = 0.57034687\n",
            "Iteration 38, loss = 0.57327851\n",
            "Iteration 39, loss = 0.56692379\n",
            "Iteration 40, loss = 0.56658337\n",
            "Iteration 41, loss = 0.56684638\n",
            "Iteration 42, loss = 0.56084504\n",
            "Iteration 43, loss = 0.56264429\n",
            "Iteration 44, loss = 0.55629980\n",
            "Iteration 45, loss = 0.55988916\n",
            "Iteration 46, loss = 0.55348852\n",
            "Iteration 47, loss = 0.54954377\n",
            "Iteration 48, loss = 0.55024136\n",
            "Iteration 49, loss = 0.54389056\n",
            "Iteration 50, loss = 0.54462641\n",
            "Iteration 51, loss = 0.53913971\n",
            "Iteration 52, loss = 0.53992543\n",
            "Iteration 53, loss = 0.53900697\n",
            "Iteration 54, loss = 0.53334347\n",
            "Iteration 55, loss = 0.52790492\n",
            "Iteration 56, loss = 0.52879703\n",
            "Iteration 57, loss = 0.52031122\n",
            "Iteration 58, loss = 0.51587123\n",
            "Iteration 59, loss = 0.51388328\n",
            "Iteration 60, loss = 0.50742453\n",
            "Iteration 61, loss = 0.50252917\n",
            "Iteration 62, loss = 0.50256795\n",
            "Iteration 63, loss = 0.49893147\n",
            "Iteration 64, loss = 0.50138017\n",
            "Iteration 65, loss = 0.50555307\n",
            "Iteration 66, loss = 0.49390149\n",
            "Iteration 67, loss = 0.48738802\n",
            "Iteration 68, loss = 0.48229458\n",
            "Iteration 69, loss = 0.48086221\n",
            "Iteration 70, loss = 0.48248920\n",
            "Iteration 71, loss = 0.47937954\n",
            "Iteration 72, loss = 0.47406747\n",
            "Iteration 73, loss = 0.47229770\n",
            "Iteration 74, loss = 0.47767553\n",
            "Iteration 75, loss = 0.47070011\n",
            "Iteration 76, loss = 0.46898281\n",
            "Iteration 77, loss = 0.47019876\n",
            "Iteration 78, loss = 0.47171034\n",
            "Iteration 79, loss = 0.46793697\n",
            "Iteration 80, loss = 0.46148329\n",
            "Iteration 81, loss = 0.46618427\n",
            "Iteration 82, loss = 0.46170520\n",
            "Iteration 83, loss = 0.46712892\n",
            "Iteration 84, loss = 0.45537606\n",
            "Iteration 85, loss = 0.45770783\n",
            "Iteration 86, loss = 0.45411815\n",
            "Iteration 87, loss = 0.45373473\n",
            "Iteration 88, loss = 0.45549437\n",
            "Iteration 89, loss = 0.45594140\n",
            "Iteration 90, loss = 0.45757065\n",
            "Iteration 91, loss = 0.45231401\n",
            "Iteration 92, loss = 0.45089723\n",
            "Iteration 93, loss = 0.44828381\n",
            "Iteration 94, loss = 0.45149279\n",
            "Iteration 95, loss = 0.45059059\n",
            "Iteration 96, loss = 0.44734014\n",
            "Iteration 97, loss = 0.44675972\n",
            "Iteration 98, loss = 0.44822682\n",
            "Iteration 99, loss = 0.44548654\n",
            "Iteration 100, loss = 0.44514968\n",
            "Iteration 101, loss = 0.43771880\n",
            "Iteration 102, loss = 0.44189231\n",
            "Iteration 103, loss = 0.43700059\n",
            "Iteration 104, loss = 0.43871279\n",
            "Iteration 105, loss = 0.43989528\n",
            "Iteration 106, loss = 0.43681571\n",
            "Iteration 107, loss = 0.43395521\n",
            "Iteration 108, loss = 0.43422965\n",
            "Iteration 109, loss = 0.44887344\n",
            "Iteration 110, loss = 0.46457734\n",
            "Iteration 111, loss = 0.44226958\n",
            "Iteration 112, loss = 0.43500651\n",
            "Iteration 113, loss = 0.43029759\n",
            "Iteration 114, loss = 0.43140729\n",
            "Iteration 115, loss = 0.42718746\n",
            "Iteration 116, loss = 0.42551396\n",
            "Iteration 117, loss = 0.42681232\n",
            "Iteration 118, loss = 0.44019168\n",
            "Iteration 119, loss = 0.42298237\n",
            "Iteration 120, loss = 0.43268153\n",
            "Iteration 121, loss = 0.43186677\n",
            "Iteration 122, loss = 0.42228884\n",
            "Iteration 123, loss = 0.41832097\n",
            "Iteration 124, loss = 0.42208480\n",
            "Iteration 125, loss = 0.43910608\n",
            "Iteration 126, loss = 0.44000363\n",
            "Iteration 127, loss = 0.42425520\n",
            "Iteration 128, loss = 0.41974511\n",
            "Iteration 129, loss = 0.42412792\n",
            "Iteration 130, loss = 0.42314540\n",
            "Iteration 131, loss = 0.41790766\n",
            "Iteration 132, loss = 0.41255121\n",
            "Iteration 133, loss = 0.41130789\n",
            "Iteration 134, loss = 0.41457354\n",
            "Iteration 135, loss = 0.41516714\n",
            "Iteration 136, loss = 0.41100997\n",
            "Iteration 137, loss = 0.40815511\n",
            "Iteration 138, loss = 0.40739383\n",
            "Iteration 139, loss = 0.40595023\n",
            "Iteration 140, loss = 0.40517047\n",
            "Iteration 141, loss = 0.40753533\n",
            "Iteration 142, loss = 0.40385900\n",
            "Iteration 143, loss = 0.40724736\n",
            "Iteration 144, loss = 0.40536330\n",
            "Iteration 145, loss = 0.40366094\n",
            "Iteration 146, loss = 0.40533460\n",
            "Iteration 147, loss = 0.42688902\n",
            "Iteration 148, loss = 0.41100179\n",
            "Iteration 149, loss = 0.41883409\n",
            "Iteration 150, loss = 0.40251419\n",
            "Iteration 151, loss = 0.40404255\n",
            "Iteration 152, loss = 0.40148080\n",
            "Iteration 153, loss = 0.39863945\n",
            "Iteration 154, loss = 0.40201922\n",
            "Iteration 155, loss = 0.39903782\n",
            "Iteration 156, loss = 0.40197299\n",
            "Iteration 157, loss = 0.39895429\n",
            "Iteration 158, loss = 0.40090070\n",
            "Iteration 159, loss = 0.39944771\n",
            "Iteration 160, loss = 0.39855965\n",
            "Iteration 161, loss = 0.39622652\n",
            "Iteration 162, loss = 0.39766230\n",
            "Iteration 163, loss = 0.39821778\n",
            "Iteration 164, loss = 0.39848806\n",
            "Iteration 165, loss = 0.40113753\n",
            "Iteration 166, loss = 0.40461246\n",
            "Iteration 167, loss = 0.39389296\n",
            "Iteration 168, loss = 0.39565186\n",
            "Iteration 169, loss = 0.40209027\n",
            "Iteration 170, loss = 0.39678630\n",
            "Iteration 171, loss = 0.40125982\n",
            "Iteration 172, loss = 0.40547487\n",
            "Iteration 173, loss = 0.39421364\n",
            "Iteration 174, loss = 0.39554671\n",
            "Iteration 175, loss = 0.39682074\n",
            "Iteration 176, loss = 0.40511531\n",
            "Iteration 177, loss = 0.40195956\n",
            "Iteration 178, loss = 0.39405266\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.74661251\n",
            "Iteration 2, loss = 0.63985256\n",
            "Iteration 3, loss = 0.62565350\n",
            "Iteration 4, loss = 0.61640290\n",
            "Iteration 5, loss = 0.61063923\n",
            "Iteration 6, loss = 0.60649244\n",
            "Iteration 7, loss = 0.60218586\n",
            "Iteration 8, loss = 0.59707115\n",
            "Iteration 9, loss = 0.59260177\n",
            "Iteration 10, loss = 0.58857161\n",
            "Iteration 11, loss = 0.58609313\n",
            "Iteration 12, loss = 0.58078521\n",
            "Iteration 13, loss = 0.57656426\n",
            "Iteration 14, loss = 0.57529830\n",
            "Iteration 15, loss = 0.56867690\n",
            "Iteration 16, loss = 0.56000080\n",
            "Iteration 17, loss = 0.55355645\n",
            "Iteration 18, loss = 0.54821576\n",
            "Iteration 19, loss = 0.53874755\n",
            "Iteration 20, loss = 0.53759818\n",
            "Iteration 21, loss = 0.53458527\n",
            "Iteration 22, loss = 0.51600778\n",
            "Iteration 23, loss = 0.51940291\n",
            "Iteration 24, loss = 0.50704866\n",
            "Iteration 25, loss = 0.50666162\n",
            "Iteration 26, loss = 0.49591173\n",
            "Iteration 27, loss = 0.48934677\n",
            "Iteration 28, loss = 0.48252907\n",
            "Iteration 29, loss = 0.47645857\n",
            "Iteration 30, loss = 0.47293967\n",
            "Iteration 31, loss = 0.46862629\n",
            "Iteration 32, loss = 0.46639240\n",
            "Iteration 33, loss = 0.46381173\n",
            "Iteration 34, loss = 0.46073143\n",
            "Iteration 35, loss = 0.45776166\n",
            "Iteration 36, loss = 0.45819577\n",
            "Iteration 37, loss = 0.45118541\n",
            "Iteration 38, loss = 0.44754548\n",
            "Iteration 39, loss = 0.44975998\n",
            "Iteration 40, loss = 0.44411049\n",
            "Iteration 41, loss = 0.44543139\n",
            "Iteration 42, loss = 0.44455493\n",
            "Iteration 43, loss = 0.44105283\n",
            "Iteration 44, loss = 0.44009084\n",
            "Iteration 45, loss = 0.43796544\n",
            "Iteration 46, loss = 0.43487574\n",
            "Iteration 47, loss = 0.43518247\n",
            "Iteration 48, loss = 0.43521263\n",
            "Iteration 49, loss = 0.43479986\n",
            "Iteration 50, loss = 0.43848703\n",
            "Iteration 51, loss = 0.44573753\n",
            "Iteration 52, loss = 0.44528703\n",
            "Iteration 53, loss = 0.42932366\n",
            "Iteration 54, loss = 0.43275521\n",
            "Iteration 55, loss = 0.44152499\n",
            "Iteration 56, loss = 0.44948724\n",
            "Iteration 57, loss = 0.43524499\n",
            "Iteration 58, loss = 0.42652958\n",
            "Iteration 59, loss = 0.42647477\n",
            "Iteration 60, loss = 0.42605087\n",
            "Iteration 61, loss = 0.43153544\n",
            "Iteration 62, loss = 0.42307965\n",
            "Iteration 63, loss = 0.43057928\n",
            "Iteration 64, loss = 0.43173822\n",
            "Iteration 65, loss = 0.42390736\n",
            "Iteration 66, loss = 0.42197842\n",
            "Iteration 67, loss = 0.42070056\n",
            "Iteration 68, loss = 0.42085693\n",
            "Iteration 69, loss = 0.42171013\n",
            "Iteration 70, loss = 0.42195113\n",
            "Iteration 71, loss = 0.42683661\n",
            "Iteration 72, loss = 0.41804462\n",
            "Iteration 73, loss = 0.42017043\n",
            "Iteration 74, loss = 0.41937510\n",
            "Iteration 75, loss = 0.41702979\n",
            "Iteration 76, loss = 0.41957110\n",
            "Iteration 77, loss = 0.44935040\n",
            "Iteration 78, loss = 0.42612240\n",
            "Iteration 79, loss = 0.42768376\n",
            "Iteration 80, loss = 0.41916893\n",
            "Iteration 81, loss = 0.42249130\n",
            "Iteration 82, loss = 0.41453527\n",
            "Iteration 83, loss = 0.41533364\n",
            "Iteration 84, loss = 0.41722565\n",
            "Iteration 85, loss = 0.43163609\n",
            "Iteration 86, loss = 0.42457391\n",
            "Iteration 87, loss = 0.41547422\n",
            "Iteration 88, loss = 0.41446850\n",
            "Iteration 89, loss = 0.41692720\n",
            "Iteration 90, loss = 0.41268384\n",
            "Iteration 91, loss = 0.41220951\n",
            "Iteration 92, loss = 0.41132601\n",
            "Iteration 93, loss = 0.40982969\n",
            "Iteration 94, loss = 0.41214494\n",
            "Iteration 95, loss = 0.40965309\n",
            "Iteration 96, loss = 0.41373647\n",
            "Iteration 97, loss = 0.41589570\n",
            "Iteration 98, loss = 0.42881162\n",
            "Iteration 99, loss = 0.41363795\n",
            "Iteration 100, loss = 0.41558033\n",
            "Iteration 101, loss = 0.41510155\n",
            "Iteration 102, loss = 0.41206612\n",
            "Iteration 103, loss = 0.42132585\n",
            "Iteration 104, loss = 0.41376319\n",
            "Iteration 105, loss = 0.40880830\n",
            "Iteration 106, loss = 0.40843892\n",
            "Iteration 107, loss = 0.40883981\n",
            "Iteration 108, loss = 0.41353619\n",
            "Iteration 109, loss = 0.41027855\n",
            "Iteration 110, loss = 0.41859547\n",
            "Iteration 111, loss = 0.42889677\n",
            "Iteration 112, loss = 0.41439251\n",
            "Iteration 113, loss = 0.40765267\n",
            "Iteration 114, loss = 0.40586517\n",
            "Iteration 115, loss = 0.40824810\n",
            "Iteration 116, loss = 0.40624622\n",
            "Iteration 117, loss = 0.40367393\n",
            "Iteration 118, loss = 0.40592162\n",
            "Iteration 119, loss = 0.41160170\n",
            "Iteration 120, loss = 0.40369764\n",
            "Iteration 121, loss = 0.40750049\n",
            "Iteration 122, loss = 0.40562353\n",
            "Iteration 123, loss = 0.41071073\n",
            "Iteration 124, loss = 0.40950711\n",
            "Iteration 125, loss = 0.42682396\n",
            "Iteration 126, loss = 0.42465297\n",
            "Iteration 127, loss = 0.40453663\n",
            "Iteration 128, loss = 0.40152913\n",
            "Iteration 129, loss = 0.40516526\n",
            "Iteration 130, loss = 0.40178100\n",
            "Iteration 131, loss = 0.41143294\n",
            "Iteration 132, loss = 0.41163348\n",
            "Iteration 133, loss = 0.40920229\n",
            "Iteration 134, loss = 0.40120569\n",
            "Iteration 135, loss = 0.40066317\n",
            "Iteration 136, loss = 0.40076921\n",
            "Iteration 137, loss = 0.40463274\n",
            "Iteration 138, loss = 0.40641437\n",
            "Iteration 139, loss = 0.40681022\n",
            "Iteration 140, loss = 0.40291753\n",
            "Iteration 141, loss = 0.40081925\n",
            "Iteration 142, loss = 0.39864109\n",
            "Iteration 143, loss = 0.40063830\n",
            "Iteration 144, loss = 0.40520967\n",
            "Iteration 145, loss = 0.40069883\n",
            "Iteration 146, loss = 0.40520003\n",
            "Iteration 147, loss = 0.40275400\n",
            "Iteration 148, loss = 0.40000835\n",
            "Iteration 149, loss = 0.40736421\n",
            "Iteration 150, loss = 0.40046715\n",
            "Iteration 151, loss = 0.40947734\n",
            "Iteration 152, loss = 0.40993903\n",
            "Iteration 153, loss = 0.39878102\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62443542\n",
            "Iteration 2, loss = 0.61613945\n",
            "Iteration 3, loss = 0.61286173\n",
            "Iteration 4, loss = 0.60936086\n",
            "Iteration 5, loss = 0.60790435\n",
            "Iteration 6, loss = 0.60691563\n",
            "Iteration 7, loss = 0.60174349\n",
            "Iteration 8, loss = 0.60469759\n",
            "Iteration 9, loss = 0.59946470\n",
            "Iteration 10, loss = 0.60281261\n",
            "Iteration 11, loss = 0.59879178\n",
            "Iteration 12, loss = 0.59498345\n",
            "Iteration 13, loss = 0.59202824\n",
            "Iteration 14, loss = 0.59150484\n",
            "Iteration 15, loss = 0.58988888\n",
            "Iteration 16, loss = 0.58884387\n",
            "Iteration 17, loss = 0.58605633\n",
            "Iteration 18, loss = 0.58237487\n",
            "Iteration 19, loss = 0.58115566\n",
            "Iteration 20, loss = 0.58015492\n",
            "Iteration 21, loss = 0.57492291\n",
            "Iteration 22, loss = 0.57255521\n",
            "Iteration 23, loss = 0.57146541\n",
            "Iteration 24, loss = 0.56618974\n",
            "Iteration 25, loss = 0.56202614\n",
            "Iteration 26, loss = 0.55906808\n",
            "Iteration 27, loss = 0.55743907\n",
            "Iteration 28, loss = 0.55564193\n",
            "Iteration 29, loss = 0.54886270\n",
            "Iteration 30, loss = 0.54571807\n",
            "Iteration 31, loss = 0.54201455\n",
            "Iteration 32, loss = 0.54256589\n",
            "Iteration 33, loss = 0.53591862\n",
            "Iteration 34, loss = 0.53737368\n",
            "Iteration 35, loss = 0.53202158\n",
            "Iteration 36, loss = 0.52710198\n",
            "Iteration 37, loss = 0.52198649\n",
            "Iteration 38, loss = 0.52132558\n",
            "Iteration 39, loss = 0.52148453\n",
            "Iteration 40, loss = 0.51460291\n",
            "Iteration 41, loss = 0.51408659\n",
            "Iteration 42, loss = 0.51273961\n",
            "Iteration 43, loss = 0.50770683\n",
            "Iteration 44, loss = 0.51628069\n",
            "Iteration 45, loss = 0.49902252\n",
            "Iteration 46, loss = 0.49794403\n",
            "Iteration 47, loss = 0.49523578\n",
            "Iteration 48, loss = 0.49039655\n",
            "Iteration 49, loss = 0.48959930\n",
            "Iteration 50, loss = 0.48457241\n",
            "Iteration 51, loss = 0.48149255\n",
            "Iteration 52, loss = 0.48417763\n",
            "Iteration 53, loss = 0.47609859\n",
            "Iteration 54, loss = 0.47297227\n",
            "Iteration 55, loss = 0.47218942\n",
            "Iteration 56, loss = 0.46767209\n",
            "Iteration 57, loss = 0.46854709\n",
            "Iteration 58, loss = 0.46355454\n",
            "Iteration 59, loss = 0.46936465\n",
            "Iteration 60, loss = 0.46495717\n",
            "Iteration 61, loss = 0.45435872\n",
            "Iteration 62, loss = 0.45198517\n",
            "Iteration 63, loss = 0.45148266\n",
            "Iteration 64, loss = 0.44825799\n",
            "Iteration 65, loss = 0.44593319\n",
            "Iteration 66, loss = 0.45216406\n",
            "Iteration 67, loss = 0.44656530\n",
            "Iteration 68, loss = 0.45071773\n",
            "Iteration 69, loss = 0.44684055\n",
            "Iteration 70, loss = 0.44438178\n",
            "Iteration 71, loss = 0.44281671\n",
            "Iteration 72, loss = 0.44061510\n",
            "Iteration 73, loss = 0.44089362\n",
            "Iteration 74, loss = 0.44114032\n",
            "Iteration 75, loss = 0.43955500\n",
            "Iteration 76, loss = 0.44042215\n",
            "Iteration 77, loss = 0.43769398\n",
            "Iteration 78, loss = 0.44151222\n",
            "Iteration 79, loss = 0.43974028\n",
            "Iteration 80, loss = 0.43738773\n",
            "Iteration 81, loss = 0.43545871\n",
            "Iteration 82, loss = 0.43909231\n",
            "Iteration 83, loss = 0.43319754\n",
            "Iteration 84, loss = 0.43476316\n",
            "Iteration 85, loss = 0.43151607\n",
            "Iteration 86, loss = 0.43248968\n",
            "Iteration 87, loss = 0.43257459\n",
            "Iteration 88, loss = 0.43013593\n",
            "Iteration 89, loss = 0.43169028\n",
            "Iteration 90, loss = 0.43092687\n",
            "Iteration 91, loss = 0.43226948\n",
            "Iteration 92, loss = 0.43059404\n",
            "Iteration 93, loss = 0.42946973\n",
            "Iteration 94, loss = 0.43035593\n",
            "Iteration 95, loss = 0.42963758\n",
            "Iteration 96, loss = 0.42795282\n",
            "Iteration 97, loss = 0.42738687\n",
            "Iteration 98, loss = 0.42991715\n",
            "Iteration 99, loss = 0.42548985\n",
            "Iteration 100, loss = 0.42979538\n",
            "Iteration 101, loss = 0.42602814\n",
            "Iteration 102, loss = 0.43277706\n",
            "Iteration 103, loss = 0.42423530\n",
            "Iteration 104, loss = 0.42251282\n",
            "Iteration 105, loss = 0.42232385\n",
            "Iteration 106, loss = 0.43223025\n",
            "Iteration 107, loss = 0.42694580\n",
            "Iteration 108, loss = 0.42273597\n",
            "Iteration 109, loss = 0.42214715\n",
            "Iteration 110, loss = 0.42132717\n",
            "Iteration 111, loss = 0.42978865\n",
            "Iteration 112, loss = 0.42409067\n",
            "Iteration 113, loss = 0.42217744\n",
            "Iteration 114, loss = 0.41998241\n",
            "Iteration 115, loss = 0.42042531\n",
            "Iteration 116, loss = 0.41935111\n",
            "Iteration 117, loss = 0.42184920\n",
            "Iteration 118, loss = 0.42079301\n",
            "Iteration 119, loss = 0.41794407\n",
            "Iteration 120, loss = 0.41658598\n",
            "Iteration 121, loss = 0.41755530\n",
            "Iteration 122, loss = 0.41761363\n",
            "Iteration 123, loss = 0.41855271\n",
            "Iteration 124, loss = 0.41615889\n",
            "Iteration 125, loss = 0.42039863\n",
            "Iteration 126, loss = 0.42344765\n",
            "Iteration 127, loss = 0.42169955\n",
            "Iteration 128, loss = 0.41253760\n",
            "Iteration 129, loss = 0.42135907\n",
            "Iteration 130, loss = 0.42621197\n",
            "Iteration 131, loss = 0.43706515\n",
            "Iteration 132, loss = 0.42999386\n",
            "Iteration 133, loss = 0.42783265\n",
            "Iteration 134, loss = 0.41325204\n",
            "Iteration 135, loss = 0.41359295\n",
            "Iteration 136, loss = 0.41219637\n",
            "Iteration 137, loss = 0.41116102\n",
            "Iteration 138, loss = 0.41292580\n",
            "Iteration 139, loss = 0.40943555\n",
            "Iteration 140, loss = 0.41668474\n",
            "Iteration 141, loss = 0.42388155\n",
            "Iteration 142, loss = 0.42258587\n",
            "Iteration 143, loss = 0.40954879\n",
            "Iteration 144, loss = 0.41408936\n",
            "Iteration 145, loss = 0.41036024\n",
            "Iteration 146, loss = 0.40898448\n",
            "Iteration 147, loss = 0.41008324\n",
            "Iteration 148, loss = 0.40768872\n",
            "Iteration 149, loss = 0.40738660\n",
            "Iteration 150, loss = 0.41244331\n",
            "Iteration 151, loss = 0.41201532\n",
            "Iteration 152, loss = 0.40933730\n",
            "Iteration 153, loss = 0.40960502\n",
            "Iteration 154, loss = 0.40561342\n",
            "Iteration 155, loss = 0.41386360\n",
            "Iteration 156, loss = 0.41415643\n",
            "Iteration 157, loss = 0.40473020\n",
            "Iteration 158, loss = 0.40362002\n",
            "Iteration 159, loss = 0.40816985\n",
            "Iteration 160, loss = 0.40614365\n",
            "Iteration 161, loss = 0.40423628\n",
            "Iteration 162, loss = 0.40370545\n",
            "Iteration 163, loss = 0.40713828\n",
            "Iteration 164, loss = 0.42906826\n",
            "Iteration 165, loss = 0.41989316\n",
            "Iteration 166, loss = 0.40411892\n",
            "Iteration 167, loss = 0.40865717\n",
            "Iteration 168, loss = 0.40443705\n",
            "Iteration 169, loss = 0.40748263\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67478855\n",
            "Iteration 2, loss = 0.62593628\n",
            "Iteration 3, loss = 0.61600645\n",
            "Iteration 4, loss = 0.60488645\n",
            "Iteration 5, loss = 0.60472823\n",
            "Iteration 6, loss = 0.60479083\n",
            "Iteration 7, loss = 0.60117691\n",
            "Iteration 8, loss = 0.60339135\n",
            "Iteration 9, loss = 0.59976829\n",
            "Iteration 10, loss = 0.60070067\n",
            "Iteration 11, loss = 0.59925155\n",
            "Iteration 12, loss = 0.59747127\n",
            "Iteration 13, loss = 0.59635467\n",
            "Iteration 14, loss = 0.59513194\n",
            "Iteration 15, loss = 0.59566937\n",
            "Iteration 16, loss = 0.59408105\n",
            "Iteration 17, loss = 0.59355393\n",
            "Iteration 18, loss = 0.59748086\n",
            "Iteration 19, loss = 0.59420934\n",
            "Iteration 20, loss = 0.59076112\n",
            "Iteration 21, loss = 0.59055501\n",
            "Iteration 22, loss = 0.58972015\n",
            "Iteration 23, loss = 0.58990090\n",
            "Iteration 24, loss = 0.58888083\n",
            "Iteration 25, loss = 0.58888727\n",
            "Iteration 26, loss = 0.58571837\n",
            "Iteration 27, loss = 0.58561147\n",
            "Iteration 28, loss = 0.58457776\n",
            "Iteration 29, loss = 0.58557507\n",
            "Iteration 30, loss = 0.58408205\n",
            "Iteration 31, loss = 0.58216030\n",
            "Iteration 32, loss = 0.58016853\n",
            "Iteration 33, loss = 0.58007502\n",
            "Iteration 34, loss = 0.57787504\n",
            "Iteration 35, loss = 0.57782959\n",
            "Iteration 36, loss = 0.57829409\n",
            "Iteration 37, loss = 0.57796691\n",
            "Iteration 38, loss = 0.57823171\n",
            "Iteration 39, loss = 0.57707012\n",
            "Iteration 40, loss = 0.57839086\n",
            "Iteration 41, loss = 0.57443145\n",
            "Iteration 42, loss = 0.56992516\n",
            "Iteration 43, loss = 0.57296526\n",
            "Iteration 44, loss = 0.57290237\n",
            "Iteration 45, loss = 0.56545734\n",
            "Iteration 46, loss = 0.56133366\n",
            "Iteration 47, loss = 0.56276557\n",
            "Iteration 48, loss = 0.55521378\n",
            "Iteration 49, loss = 0.54922025\n",
            "Iteration 50, loss = 0.55022495\n",
            "Iteration 51, loss = 0.54413976\n",
            "Iteration 52, loss = 0.53980734\n",
            "Iteration 53, loss = 0.53672585\n",
            "Iteration 54, loss = 0.53575687\n",
            "Iteration 55, loss = 0.53357829\n",
            "Iteration 56, loss = 0.53061430\n",
            "Iteration 57, loss = 0.52535373\n",
            "Iteration 58, loss = 0.52753969\n",
            "Iteration 59, loss = 0.51749865\n",
            "Iteration 60, loss = 0.51390606\n",
            "Iteration 61, loss = 0.51025997\n",
            "Iteration 62, loss = 0.50715162\n",
            "Iteration 63, loss = 0.50224776\n",
            "Iteration 64, loss = 0.49884516\n",
            "Iteration 65, loss = 0.49512366\n",
            "Iteration 66, loss = 0.49747706\n",
            "Iteration 67, loss = 0.49280519\n",
            "Iteration 68, loss = 0.48418440\n",
            "Iteration 69, loss = 0.48171752\n",
            "Iteration 70, loss = 0.47759018\n",
            "Iteration 71, loss = 0.47291076\n",
            "Iteration 72, loss = 0.47161912\n",
            "Iteration 73, loss = 0.47437737\n",
            "Iteration 74, loss = 0.46349968\n",
            "Iteration 75, loss = 0.46005733\n",
            "Iteration 76, loss = 0.45958349\n",
            "Iteration 77, loss = 0.45650673\n",
            "Iteration 78, loss = 0.45674724\n",
            "Iteration 79, loss = 0.44963111\n",
            "Iteration 80, loss = 0.44663723\n",
            "Iteration 81, loss = 0.45216766\n",
            "Iteration 82, loss = 0.44491552\n",
            "Iteration 83, loss = 0.44491820\n",
            "Iteration 84, loss = 0.44331337\n",
            "Iteration 85, loss = 0.44425788\n",
            "Iteration 86, loss = 0.44474238\n",
            "Iteration 87, loss = 0.43394202\n",
            "Iteration 88, loss = 0.43311900\n",
            "Iteration 89, loss = 0.43262413\n",
            "Iteration 90, loss = 0.43228319\n",
            "Iteration 91, loss = 0.43071100\n",
            "Iteration 92, loss = 0.43356607\n",
            "Iteration 93, loss = 0.42949804\n",
            "Iteration 94, loss = 0.43090002\n",
            "Iteration 95, loss = 0.43209141\n",
            "Iteration 96, loss = 0.43746890\n",
            "Iteration 97, loss = 0.43708321\n",
            "Iteration 98, loss = 0.42263730\n",
            "Iteration 99, loss = 0.42094998\n",
            "Iteration 100, loss = 0.42366474\n",
            "Iteration 101, loss = 0.42057191\n",
            "Iteration 102, loss = 0.42720307\n",
            "Iteration 103, loss = 0.42026838\n",
            "Iteration 104, loss = 0.42155459\n",
            "Iteration 105, loss = 0.41794546\n",
            "Iteration 106, loss = 0.42464469\n",
            "Iteration 107, loss = 0.41873656\n",
            "Iteration 108, loss = 0.41946408\n",
            "Iteration 109, loss = 0.41442317\n",
            "Iteration 110, loss = 0.41659750\n",
            "Iteration 111, loss = 0.41809976\n",
            "Iteration 112, loss = 0.41400523\n",
            "Iteration 113, loss = 0.41277375\n",
            "Iteration 114, loss = 0.41337293\n",
            "Iteration 115, loss = 0.42130800\n",
            "Iteration 116, loss = 0.41362896\n",
            "Iteration 117, loss = 0.41670963\n",
            "Iteration 118, loss = 0.41124012\n",
            "Iteration 119, loss = 0.40972131\n",
            "Iteration 120, loss = 0.41014726\n",
            "Iteration 121, loss = 0.41042495\n",
            "Iteration 122, loss = 0.40733140\n",
            "Iteration 123, loss = 0.40797624\n",
            "Iteration 124, loss = 0.40790667\n",
            "Iteration 125, loss = 0.40759576\n",
            "Iteration 126, loss = 0.40949944\n",
            "Iteration 127, loss = 0.40975032\n",
            "Iteration 128, loss = 0.41008763\n",
            "Iteration 129, loss = 0.41615631\n",
            "Iteration 130, loss = 0.41177858\n",
            "Iteration 131, loss = 0.41573149\n",
            "Iteration 132, loss = 0.40796979\n",
            "Iteration 133, loss = 0.40625005\n",
            "Iteration 134, loss = 0.40640486\n",
            "Iteration 135, loss = 0.41418281\n",
            "Iteration 136, loss = 0.40574071\n",
            "Iteration 137, loss = 0.40907734\n",
            "Iteration 138, loss = 0.40040535\n",
            "Iteration 139, loss = 0.40609403\n",
            "Iteration 140, loss = 0.40420437\n",
            "Iteration 141, loss = 0.39948594\n",
            "Iteration 142, loss = 0.39822021\n",
            "Iteration 143, loss = 0.39932030\n",
            "Iteration 144, loss = 0.39867012\n",
            "Iteration 145, loss = 0.39976695\n",
            "Iteration 146, loss = 0.39952973\n",
            "Iteration 147, loss = 0.40391639\n",
            "Iteration 148, loss = 0.39864545\n",
            "Iteration 149, loss = 0.39847340\n",
            "Iteration 150, loss = 0.39775038\n",
            "Iteration 151, loss = 0.39672902\n",
            "Iteration 152, loss = 0.39859950\n",
            "Iteration 153, loss = 0.39742042\n",
            "Iteration 154, loss = 0.39569461\n",
            "Iteration 155, loss = 0.39592459\n",
            "Iteration 156, loss = 0.40123115\n",
            "Iteration 157, loss = 0.39616623\n",
            "Iteration 158, loss = 0.40060847\n",
            "Iteration 159, loss = 0.39423010\n",
            "Iteration 160, loss = 0.39368532\n",
            "Iteration 161, loss = 0.39372425\n",
            "Iteration 162, loss = 0.39448007\n",
            "Iteration 163, loss = 0.39219906\n",
            "Iteration 164, loss = 0.39359183\n",
            "Iteration 165, loss = 0.39311977\n",
            "Iteration 166, loss = 0.39295249\n",
            "Iteration 167, loss = 0.39482820\n",
            "Iteration 168, loss = 0.39147107\n",
            "Iteration 169, loss = 0.39095605\n",
            "Iteration 170, loss = 0.39094957\n",
            "Iteration 171, loss = 0.39303046\n",
            "Iteration 172, loss = 0.40630752\n",
            "Iteration 173, loss = 0.39353148\n",
            "Iteration 174, loss = 0.39089990\n",
            "Iteration 175, loss = 0.39024714\n",
            "Iteration 176, loss = 0.39883742\n",
            "Iteration 177, loss = 0.39566247\n",
            "Iteration 178, loss = 0.39258380\n",
            "Iteration 179, loss = 0.38937537\n",
            "Iteration 180, loss = 0.39129489\n",
            "Iteration 181, loss = 0.39516652\n",
            "Iteration 182, loss = 0.39099440\n",
            "Iteration 183, loss = 0.38808022\n",
            "Iteration 184, loss = 0.39331616\n",
            "Iteration 185, loss = 0.39086586\n",
            "Iteration 186, loss = 0.39102679\n",
            "Iteration 187, loss = 0.38754897\n",
            "Iteration 188, loss = 0.39154651\n",
            "Iteration 189, loss = 0.38908009\n",
            "Iteration 190, loss = 0.38856968\n",
            "Iteration 191, loss = 0.38729110\n",
            "Iteration 192, loss = 0.39386213\n",
            "Iteration 193, loss = 0.39245111\n",
            "Iteration 194, loss = 0.38696037\n",
            "Iteration 195, loss = 0.38860863\n",
            "Iteration 196, loss = 0.38762095\n",
            "Iteration 197, loss = 0.38773519\n",
            "Iteration 198, loss = 0.38614031\n",
            "Iteration 199, loss = 0.38643902\n",
            "Iteration 200, loss = 0.38774488\n",
            "Iteration 1, loss = 0.88054522\n",
            "Iteration 2, loss = 0.64958462\n",
            "Iteration 3, loss = 0.61119247\n",
            "Iteration 4, loss = 0.61144466\n",
            "Iteration 5, loss = 0.60702998\n",
            "Iteration 6, loss = 0.60593202\n",
            "Iteration 7, loss = 0.60402538\n",
            "Iteration 8, loss = 0.60150396\n",
            "Iteration 9, loss = 0.60052846\n",
            "Iteration 10, loss = 0.59977571\n",
            "Iteration 11, loss = 0.59859348\n",
            "Iteration 12, loss = 0.59651300\n",
            "Iteration 13, loss = 0.59522579\n",
            "Iteration 14, loss = 0.59577129\n",
            "Iteration 15, loss = 0.59239506\n",
            "Iteration 16, loss = 0.59119132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 17, loss = 0.58956251\n",
            "Iteration 18, loss = 0.58896853\n",
            "Iteration 19, loss = 0.58893216\n",
            "Iteration 20, loss = 0.59006314\n",
            "Iteration 21, loss = 0.58582157\n",
            "Iteration 22, loss = 0.58200785\n",
            "Iteration 23, loss = 0.58012596\n",
            "Iteration 24, loss = 0.57780204\n",
            "Iteration 25, loss = 0.57602762\n",
            "Iteration 26, loss = 0.57130769\n",
            "Iteration 27, loss = 0.56632682\n",
            "Iteration 28, loss = 0.56007458\n",
            "Iteration 29, loss = 0.55877398\n",
            "Iteration 30, loss = 0.55322103\n",
            "Iteration 31, loss = 0.54873834\n",
            "Iteration 32, loss = 0.54463050\n",
            "Iteration 33, loss = 0.54287772\n",
            "Iteration 34, loss = 0.53811763\n",
            "Iteration 35, loss = 0.53095115\n",
            "Iteration 36, loss = 0.53036458\n",
            "Iteration 37, loss = 0.52785107\n",
            "Iteration 38, loss = 0.52603805\n",
            "Iteration 39, loss = 0.51461645\n",
            "Iteration 40, loss = 0.51640915\n",
            "Iteration 41, loss = 0.50657353\n",
            "Iteration 42, loss = 0.50247042\n",
            "Iteration 43, loss = 0.49958145\n",
            "Iteration 44, loss = 0.49208797\n",
            "Iteration 45, loss = 0.49362225\n",
            "Iteration 46, loss = 0.48234592\n",
            "Iteration 47, loss = 0.47998205\n",
            "Iteration 48, loss = 0.48014479\n",
            "Iteration 49, loss = 0.48265694\n",
            "Iteration 50, loss = 0.47487406\n",
            "Iteration 51, loss = 0.47729819\n",
            "Iteration 52, loss = 0.46877914\n",
            "Iteration 53, loss = 0.46780157\n",
            "Iteration 54, loss = 0.46652971\n",
            "Iteration 55, loss = 0.46587013\n",
            "Iteration 56, loss = 0.46945336\n",
            "Iteration 57, loss = 0.45667056\n",
            "Iteration 58, loss = 0.45994722\n",
            "Iteration 59, loss = 0.45497895\n",
            "Iteration 60, loss = 0.46206392\n",
            "Iteration 61, loss = 0.45609369\n",
            "Iteration 62, loss = 0.45350419\n",
            "Iteration 63, loss = 0.44778398\n",
            "Iteration 64, loss = 0.45085426\n",
            "Iteration 65, loss = 0.45066039\n",
            "Iteration 66, loss = 0.44545260\n",
            "Iteration 67, loss = 0.45371562\n",
            "Iteration 68, loss = 0.44776276\n",
            "Iteration 69, loss = 0.44157622\n",
            "Iteration 70, loss = 0.44246121\n",
            "Iteration 71, loss = 0.43888892\n",
            "Iteration 72, loss = 0.43775443\n",
            "Iteration 73, loss = 0.43728212\n",
            "Iteration 74, loss = 0.43705575\n",
            "Iteration 75, loss = 0.43530544\n",
            "Iteration 76, loss = 0.43537538\n",
            "Iteration 77, loss = 0.43376451\n",
            "Iteration 78, loss = 0.43250684\n",
            "Iteration 79, loss = 0.43016276\n",
            "Iteration 80, loss = 0.43133940\n",
            "Iteration 81, loss = 0.43832333\n",
            "Iteration 82, loss = 0.43315086\n",
            "Iteration 83, loss = 0.43110539\n",
            "Iteration 84, loss = 0.43293970\n",
            "Iteration 85, loss = 0.43939022\n",
            "Iteration 86, loss = 0.42936882\n",
            "Iteration 87, loss = 0.42620498\n",
            "Iteration 88, loss = 0.42427286\n",
            "Iteration 89, loss = 0.42161508\n",
            "Iteration 90, loss = 0.42709318\n",
            "Iteration 91, loss = 0.42578893\n",
            "Iteration 92, loss = 0.42242371\n",
            "Iteration 93, loss = 0.42538007\n",
            "Iteration 94, loss = 0.42072463\n",
            "Iteration 95, loss = 0.42204084\n",
            "Iteration 96, loss = 0.42100256\n",
            "Iteration 97, loss = 0.43412730\n",
            "Iteration 98, loss = 0.41836132\n",
            "Iteration 99, loss = 0.42120585\n",
            "Iteration 100, loss = 0.41746910\n",
            "Iteration 101, loss = 0.41751873\n",
            "Iteration 102, loss = 0.41854718\n",
            "Iteration 103, loss = 0.41413349\n",
            "Iteration 104, loss = 0.41263218\n",
            "Iteration 105, loss = 0.41315651\n",
            "Iteration 106, loss = 0.41613442\n",
            "Iteration 107, loss = 0.41555570\n",
            "Iteration 108, loss = 0.42440874\n",
            "Iteration 109, loss = 0.41079889\n",
            "Iteration 110, loss = 0.40739121\n",
            "Iteration 111, loss = 0.41874211\n",
            "Iteration 112, loss = 0.41293965\n",
            "Iteration 113, loss = 0.40737798\n",
            "Iteration 114, loss = 0.41596579\n",
            "Iteration 115, loss = 0.41033084\n",
            "Iteration 116, loss = 0.40540830\n",
            "Iteration 117, loss = 0.40574396\n",
            "Iteration 118, loss = 0.40571325\n",
            "Iteration 119, loss = 0.40441719\n",
            "Iteration 120, loss = 0.40396334\n",
            "Iteration 121, loss = 0.40321640\n",
            "Iteration 122, loss = 0.40183584\n",
            "Iteration 123, loss = 0.40466637\n",
            "Iteration 124, loss = 0.42095167\n",
            "Iteration 125, loss = 0.41383276\n",
            "Iteration 126, loss = 0.40300462\n",
            "Iteration 127, loss = 0.40009236\n",
            "Iteration 128, loss = 0.39925627\n",
            "Iteration 129, loss = 0.40296393\n",
            "Iteration 130, loss = 0.39808035\n",
            "Iteration 131, loss = 0.40025513\n",
            "Iteration 132, loss = 0.39615445\n",
            "Iteration 133, loss = 0.39463156\n",
            "Iteration 134, loss = 0.39908794\n",
            "Iteration 135, loss = 0.39467707\n",
            "Iteration 136, loss = 0.39461773\n",
            "Iteration 137, loss = 0.40150450\n",
            "Iteration 138, loss = 0.39745235\n",
            "Iteration 139, loss = 0.40561649\n",
            "Iteration 140, loss = 0.40132306\n",
            "Iteration 141, loss = 0.40620775\n",
            "Iteration 142, loss = 0.40192588\n",
            "Iteration 143, loss = 0.39540971\n",
            "Iteration 144, loss = 0.39495552\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.63402934\n",
            "Iteration 2, loss = 0.60495846\n",
            "Iteration 3, loss = 0.60054889\n",
            "Iteration 4, loss = 0.59883893\n",
            "Iteration 5, loss = 0.59258131\n",
            "Iteration 6, loss = 0.58964271\n",
            "Iteration 7, loss = 0.59666839\n",
            "Iteration 8, loss = 0.58754357\n",
            "Iteration 9, loss = 0.58655439\n",
            "Iteration 10, loss = 0.58690802\n",
            "Iteration 11, loss = 0.58857155\n",
            "Iteration 12, loss = 0.58922437\n",
            "Iteration 13, loss = 0.57744818\n",
            "Iteration 14, loss = 0.57471960\n",
            "Iteration 15, loss = 0.56946306\n",
            "Iteration 16, loss = 0.56227909\n",
            "Iteration 17, loss = 0.57195826\n",
            "Iteration 18, loss = 0.56576112\n",
            "Iteration 19, loss = 0.56903041\n",
            "Iteration 20, loss = 0.56659288\n",
            "Iteration 21, loss = 0.56368354\n",
            "Iteration 22, loss = 0.55918876\n",
            "Iteration 23, loss = 0.56420764\n",
            "Iteration 24, loss = 0.57517699\n",
            "Iteration 25, loss = 0.55786964\n",
            "Iteration 26, loss = 0.54772109\n",
            "Iteration 27, loss = 0.54932103\n",
            "Iteration 28, loss = 0.57351591\n",
            "Iteration 29, loss = 0.55879900\n",
            "Iteration 30, loss = 0.56033673\n",
            "Iteration 31, loss = 0.54803501\n",
            "Iteration 32, loss = 0.56424803\n",
            "Iteration 33, loss = 0.54455563\n",
            "Iteration 34, loss = 0.55090140\n",
            "Iteration 35, loss = 0.55437830\n",
            "Iteration 36, loss = 0.54867874\n",
            "Iteration 37, loss = 0.53811508\n",
            "Iteration 38, loss = 0.53663568\n",
            "Iteration 39, loss = 0.53241444\n",
            "Iteration 40, loss = 0.53903887\n",
            "Iteration 41, loss = 0.54038429\n",
            "Iteration 42, loss = 0.53388566\n",
            "Iteration 43, loss = 0.53222316\n",
            "Iteration 44, loss = 0.53548371\n",
            "Iteration 45, loss = 0.52913922\n",
            "Iteration 46, loss = 0.53804620\n",
            "Iteration 47, loss = 0.52925011\n",
            "Iteration 48, loss = 0.52284389\n",
            "Iteration 49, loss = 0.51893396\n",
            "Iteration 50, loss = 0.51986927\n",
            "Iteration 51, loss = 0.53745625\n",
            "Iteration 52, loss = 0.51849993\n",
            "Iteration 53, loss = 0.52987175\n",
            "Iteration 54, loss = 0.53607994\n",
            "Iteration 55, loss = 0.54655770\n",
            "Iteration 56, loss = 0.53111983\n",
            "Iteration 57, loss = 0.52817601\n",
            "Iteration 58, loss = 0.53973297\n",
            "Iteration 59, loss = 0.51690824\n",
            "Iteration 60, loss = 0.51696208\n",
            "Iteration 61, loss = 0.53300435\n",
            "Iteration 62, loss = 0.52227483\n",
            "Iteration 63, loss = 0.51823317\n",
            "Iteration 64, loss = 0.51912938\n",
            "Iteration 65, loss = 0.52185021\n",
            "Iteration 66, loss = 0.52260251\n",
            "Iteration 67, loss = 0.53376231\n",
            "Iteration 68, loss = 0.53792378\n",
            "Iteration 69, loss = 0.52292974\n",
            "Iteration 70, loss = 0.51520873\n",
            "Iteration 71, loss = 0.51152701\n",
            "Iteration 72, loss = 0.51429724\n",
            "Iteration 73, loss = 0.51718114\n",
            "Iteration 74, loss = 0.50406478\n",
            "Iteration 75, loss = 0.52967288\n",
            "Iteration 76, loss = 0.52699626\n",
            "Iteration 77, loss = 0.51174687\n",
            "Iteration 78, loss = 0.51981005\n",
            "Iteration 79, loss = 0.51848110\n",
            "Iteration 80, loss = 0.50867509\n",
            "Iteration 81, loss = 0.51407343\n",
            "Iteration 82, loss = 0.50258670\n",
            "Iteration 83, loss = 0.50352012\n",
            "Iteration 84, loss = 0.51451752\n",
            "Iteration 85, loss = 0.49907649\n",
            "Iteration 86, loss = 0.53383260\n",
            "Iteration 87, loss = 0.50548491\n",
            "Iteration 88, loss = 0.51261708\n",
            "Iteration 89, loss = 0.50532550\n",
            "Iteration 90, loss = 0.50758903\n",
            "Iteration 91, loss = 0.53605130\n",
            "Iteration 92, loss = 0.53061806\n",
            "Iteration 93, loss = 0.51297144\n",
            "Iteration 94, loss = 0.49413247\n",
            "Iteration 95, loss = 0.50444321\n",
            "Iteration 96, loss = 0.49703650\n",
            "Iteration 97, loss = 0.50275353\n",
            "Iteration 98, loss = 0.53306996\n",
            "Iteration 99, loss = 0.51428083\n",
            "Iteration 100, loss = 0.50673695\n",
            "Iteration 101, loss = 0.50865332\n",
            "Iteration 102, loss = 0.50403088\n",
            "Iteration 103, loss = 0.49899144\n",
            "Iteration 104, loss = 0.50439844\n",
            "Iteration 105, loss = 0.51649924\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.63613995\n",
            "Iteration 2, loss = 0.59670428\n",
            "Iteration 3, loss = 0.58956236\n",
            "Iteration 4, loss = 0.58458294\n",
            "Iteration 5, loss = 0.57736395\n",
            "Iteration 6, loss = 0.60187959\n",
            "Iteration 7, loss = 0.58602601\n",
            "Iteration 8, loss = 0.57491620\n",
            "Iteration 9, loss = 0.58053789\n",
            "Iteration 10, loss = 0.57956911\n",
            "Iteration 11, loss = 0.56981525\n",
            "Iteration 12, loss = 0.57336725\n",
            "Iteration 13, loss = 0.56584231\n",
            "Iteration 14, loss = 0.56547808\n",
            "Iteration 15, loss = 0.56378238\n",
            "Iteration 16, loss = 0.57043829\n",
            "Iteration 17, loss = 0.56020700\n",
            "Iteration 18, loss = 0.56341709\n",
            "Iteration 19, loss = 0.56058562\n",
            "Iteration 20, loss = 0.55614028\n",
            "Iteration 21, loss = 0.54877536\n",
            "Iteration 22, loss = 0.55333165\n",
            "Iteration 23, loss = 0.54917307\n",
            "Iteration 24, loss = 0.55749876\n",
            "Iteration 25, loss = 0.54639513\n",
            "Iteration 26, loss = 0.54171513\n",
            "Iteration 27, loss = 0.53590071\n",
            "Iteration 28, loss = 0.54149903\n",
            "Iteration 29, loss = 0.53931215\n",
            "Iteration 30, loss = 0.53514468\n",
            "Iteration 31, loss = 0.52293224\n",
            "Iteration 32, loss = 0.53024938\n",
            "Iteration 33, loss = 0.53428687\n",
            "Iteration 34, loss = 0.53123389\n",
            "Iteration 35, loss = 0.53010347\n",
            "Iteration 36, loss = 0.53568638\n",
            "Iteration 37, loss = 0.51895434\n",
            "Iteration 38, loss = 0.54819141\n",
            "Iteration 39, loss = 0.52399361\n",
            "Iteration 40, loss = 0.52683984\n",
            "Iteration 41, loss = 0.52191587\n",
            "Iteration 42, loss = 0.50666481\n",
            "Iteration 43, loss = 0.51785363\n",
            "Iteration 44, loss = 0.51092941\n",
            "Iteration 45, loss = 0.51363354\n",
            "Iteration 46, loss = 0.51098899\n",
            "Iteration 47, loss = 0.51357667\n",
            "Iteration 48, loss = 0.52257239\n",
            "Iteration 49, loss = 0.51721838\n",
            "Iteration 50, loss = 0.49884362\n",
            "Iteration 51, loss = 0.51398787\n",
            "Iteration 52, loss = 0.51357952\n",
            "Iteration 53, loss = 0.50057080\n",
            "Iteration 54, loss = 0.49695313\n",
            "Iteration 55, loss = 0.54185081\n",
            "Iteration 56, loss = 0.50799515\n",
            "Iteration 57, loss = 0.49754226\n",
            "Iteration 58, loss = 0.49886613\n",
            "Iteration 59, loss = 0.52856105\n",
            "Iteration 60, loss = 0.51188009\n",
            "Iteration 61, loss = 0.50292454\n",
            "Iteration 62, loss = 0.51175594\n",
            "Iteration 63, loss = 0.50467642\n",
            "Iteration 64, loss = 0.50931011\n",
            "Iteration 65, loss = 0.49717154\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.63651860\n",
            "Iteration 2, loss = 0.61255482\n",
            "Iteration 3, loss = 0.61502350\n",
            "Iteration 4, loss = 0.59848890\n",
            "Iteration 5, loss = 0.60226852\n",
            "Iteration 6, loss = 0.59423821\n",
            "Iteration 7, loss = 0.59336806\n",
            "Iteration 8, loss = 0.59403287\n",
            "Iteration 9, loss = 0.58434660\n",
            "Iteration 10, loss = 0.59255616\n",
            "Iteration 11, loss = 0.58656438\n",
            "Iteration 12, loss = 0.58791197\n",
            "Iteration 13, loss = 0.58962444\n",
            "Iteration 14, loss = 0.57649628\n",
            "Iteration 15, loss = 0.57895299\n",
            "Iteration 16, loss = 0.58512618\n",
            "Iteration 17, loss = 0.57994793\n",
            "Iteration 18, loss = 0.58555267\n",
            "Iteration 19, loss = 0.56978057\n",
            "Iteration 20, loss = 0.56699859\n",
            "Iteration 21, loss = 0.57280948\n",
            "Iteration 22, loss = 0.56953901\n",
            "Iteration 23, loss = 0.56133192\n",
            "Iteration 24, loss = 0.56594674\n",
            "Iteration 25, loss = 0.57202811\n",
            "Iteration 26, loss = 0.55879247\n",
            "Iteration 27, loss = 0.56260962\n",
            "Iteration 28, loss = 0.54851685\n",
            "Iteration 29, loss = 0.55023593\n",
            "Iteration 30, loss = 0.54644650\n",
            "Iteration 31, loss = 0.54245440\n",
            "Iteration 32, loss = 0.54951535\n",
            "Iteration 33, loss = 0.53637723\n",
            "Iteration 34, loss = 0.55265321\n",
            "Iteration 35, loss = 0.55009650\n",
            "Iteration 36, loss = 0.54827625\n",
            "Iteration 37, loss = 0.53513727\n",
            "Iteration 38, loss = 0.52952771\n",
            "Iteration 39, loss = 0.52613184\n",
            "Iteration 40, loss = 0.52765887\n",
            "Iteration 41, loss = 0.58844797\n",
            "Iteration 42, loss = 0.53277454\n",
            "Iteration 43, loss = 0.54800567\n",
            "Iteration 44, loss = 0.53922338\n",
            "Iteration 45, loss = 0.51240339\n",
            "Iteration 46, loss = 0.57043178\n",
            "Iteration 47, loss = 0.54703365\n",
            "Iteration 48, loss = 0.52355801\n",
            "Iteration 49, loss = 0.51585860\n",
            "Iteration 50, loss = 0.53974808\n",
            "Iteration 51, loss = 0.55934469\n",
            "Iteration 52, loss = 0.52420685\n",
            "Iteration 53, loss = 0.52760378\n",
            "Iteration 54, loss = 0.53557182\n",
            "Iteration 55, loss = 0.53973319\n",
            "Iteration 56, loss = 0.52516542\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64523238\n",
            "Iteration 2, loss = 0.60499029\n",
            "Iteration 3, loss = 0.59622038\n",
            "Iteration 4, loss = 0.59169633\n",
            "Iteration 5, loss = 0.59171779\n",
            "Iteration 6, loss = 0.58917294\n",
            "Iteration 7, loss = 0.58930193\n",
            "Iteration 8, loss = 0.59106512\n",
            "Iteration 9, loss = 0.58755539\n",
            "Iteration 10, loss = 0.58091915\n",
            "Iteration 11, loss = 0.58390885\n",
            "Iteration 12, loss = 0.58224668\n",
            "Iteration 13, loss = 0.57850114\n",
            "Iteration 14, loss = 0.59034718\n",
            "Iteration 15, loss = 0.58949960\n",
            "Iteration 16, loss = 0.58089065\n",
            "Iteration 17, loss = 0.57541938\n",
            "Iteration 18, loss = 0.57647965\n",
            "Iteration 19, loss = 0.57043574\n",
            "Iteration 20, loss = 0.57103784\n",
            "Iteration 21, loss = 0.56576024\n",
            "Iteration 22, loss = 0.56361510\n",
            "Iteration 23, loss = 0.56327181\n",
            "Iteration 24, loss = 0.56977181\n",
            "Iteration 25, loss = 0.58551401\n",
            "Iteration 26, loss = 0.56392811\n",
            "Iteration 27, loss = 0.56700083\n",
            "Iteration 28, loss = 0.55372607\n",
            "Iteration 29, loss = 0.57203295\n",
            "Iteration 30, loss = 0.55671441\n",
            "Iteration 31, loss = 0.55642709\n",
            "Iteration 32, loss = 0.55457558\n",
            "Iteration 33, loss = 0.56362251\n",
            "Iteration 34, loss = 0.57175231\n",
            "Iteration 35, loss = 0.56488643\n",
            "Iteration 36, loss = 0.55933828\n",
            "Iteration 37, loss = 0.56859896\n",
            "Iteration 38, loss = 0.56156379\n",
            "Iteration 39, loss = 0.55379346\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62789659\n",
            "Iteration 2, loss = 0.61177826\n",
            "Iteration 3, loss = 0.60855852\n",
            "Iteration 4, loss = 0.60279265\n",
            "Iteration 5, loss = 0.60120552\n",
            "Iteration 6, loss = 0.60541371\n",
            "Iteration 7, loss = 0.60078234\n",
            "Iteration 8, loss = 0.59747260\n",
            "Iteration 9, loss = 0.59736436\n",
            "Iteration 10, loss = 0.59227002\n",
            "Iteration 11, loss = 0.59218343\n",
            "Iteration 12, loss = 0.58934684\n",
            "Iteration 13, loss = 0.58758412\n",
            "Iteration 14, loss = 0.57883283\n",
            "Iteration 15, loss = 0.58085271\n",
            "Iteration 16, loss = 0.58216934\n",
            "Iteration 17, loss = 0.57860363\n",
            "Iteration 18, loss = 0.57782433\n",
            "Iteration 19, loss = 0.57435154\n",
            "Iteration 20, loss = 0.56850181\n",
            "Iteration 21, loss = 0.57071881\n",
            "Iteration 22, loss = 0.56951895\n",
            "Iteration 23, loss = 0.56621183\n",
            "Iteration 24, loss = 0.57546036\n",
            "Iteration 25, loss = 0.57186757\n",
            "Iteration 26, loss = 0.56970882\n",
            "Iteration 27, loss = 0.56841631\n",
            "Iteration 28, loss = 0.55740691\n",
            "Iteration 29, loss = 0.57473430\n",
            "Iteration 30, loss = 0.56956943\n",
            "Iteration 31, loss = 0.56612559\n",
            "Iteration 32, loss = 0.56249437\n",
            "Iteration 33, loss = 0.56588141\n",
            "Iteration 34, loss = 0.55177949\n",
            "Iteration 35, loss = 0.56203891\n",
            "Iteration 36, loss = 0.56323948\n",
            "Iteration 37, loss = 0.54599640\n",
            "Iteration 38, loss = 0.55170961\n",
            "Iteration 39, loss = 0.55138482\n",
            "Iteration 40, loss = 0.54385976\n",
            "Iteration 41, loss = 0.54547288\n",
            "Iteration 42, loss = 0.54465691\n",
            "Iteration 43, loss = 0.55486880\n",
            "Iteration 44, loss = 0.54351885\n",
            "Iteration 45, loss = 0.53932363\n",
            "Iteration 46, loss = 0.54484378\n",
            "Iteration 47, loss = 0.53695655\n",
            "Iteration 48, loss = 0.55667879\n",
            "Iteration 49, loss = 0.52812524\n",
            "Iteration 50, loss = 0.54399034\n",
            "Iteration 51, loss = 0.54368907\n",
            "Iteration 52, loss = 0.53693472\n",
            "Iteration 53, loss = 0.54301946\n",
            "Iteration 54, loss = 0.54402492\n",
            "Iteration 55, loss = 0.54020399\n",
            "Iteration 56, loss = 0.53689130\n",
            "Iteration 57, loss = 0.54366836\n",
            "Iteration 58, loss = 0.53745582\n",
            "Iteration 59, loss = 0.52780892\n",
            "Iteration 60, loss = 0.52614859\n",
            "Iteration 61, loss = 0.52564380\n",
            "Iteration 62, loss = 0.53333702\n",
            "Iteration 63, loss = 0.53773495\n",
            "Iteration 64, loss = 0.52643456\n",
            "Iteration 65, loss = 0.53498385\n",
            "Iteration 66, loss = 0.52616363\n",
            "Iteration 67, loss = 0.52620901\n",
            "Iteration 68, loss = 0.54023969\n",
            "Iteration 69, loss = 0.52230897\n",
            "Iteration 70, loss = 0.53026419\n",
            "Iteration 71, loss = 0.52490775\n",
            "Iteration 72, loss = 0.52780214\n",
            "Iteration 73, loss = 0.53578742\n",
            "Iteration 74, loss = 0.51583296\n",
            "Iteration 75, loss = 0.51769619\n",
            "Iteration 76, loss = 0.51880704\n",
            "Iteration 77, loss = 0.52570563\n",
            "Iteration 78, loss = 0.53216948\n",
            "Iteration 79, loss = 0.51604974\n",
            "Iteration 80, loss = 0.53767186\n",
            "Iteration 81, loss = 0.51552763\n",
            "Iteration 82, loss = 0.53430951\n",
            "Iteration 83, loss = 0.54141954\n",
            "Iteration 84, loss = 0.53498029\n",
            "Iteration 85, loss = 0.52936966\n",
            "Iteration 86, loss = 0.52744317\n",
            "Iteration 87, loss = 0.50183797\n",
            "Iteration 88, loss = 0.51584151\n",
            "Iteration 89, loss = 0.50559667\n",
            "Iteration 90, loss = 0.51957804\n",
            "Iteration 91, loss = 0.50579343\n",
            "Iteration 92, loss = 0.52435955\n",
            "Iteration 93, loss = 0.51667213\n",
            "Iteration 94, loss = 0.51628556\n",
            "Iteration 95, loss = 0.50600484\n",
            "Iteration 96, loss = 0.51051211\n",
            "Iteration 97, loss = 0.51926933\n",
            "Iteration 98, loss = 0.52179506\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66078236\n",
            "Iteration 2, loss = 0.62499892\n",
            "Iteration 3, loss = 0.62348988\n",
            "Iteration 4, loss = 0.61154216\n",
            "Iteration 5, loss = 0.60425725\n",
            "Iteration 6, loss = 0.60094036\n",
            "Iteration 7, loss = 0.60245882\n",
            "Iteration 8, loss = 0.59996719\n",
            "Iteration 9, loss = 0.59724226\n",
            "Iteration 10, loss = 0.59418615\n",
            "Iteration 11, loss = 0.59197583\n",
            "Iteration 12, loss = 0.59100036\n",
            "Iteration 13, loss = 0.58709641\n",
            "Iteration 14, loss = 0.58357900\n",
            "Iteration 15, loss = 0.58082392\n",
            "Iteration 16, loss = 0.57231118\n",
            "Iteration 17, loss = 0.56996761\n",
            "Iteration 18, loss = 0.56460196\n",
            "Iteration 19, loss = 0.56278535\n",
            "Iteration 20, loss = 0.54650640\n",
            "Iteration 21, loss = 0.54526377\n",
            "Iteration 22, loss = 0.52928375\n",
            "Iteration 23, loss = 0.52312510\n",
            "Iteration 24, loss = 0.53832138\n",
            "Iteration 25, loss = 0.50671157\n",
            "Iteration 26, loss = 0.52562516\n",
            "Iteration 27, loss = 0.51521468\n",
            "Iteration 28, loss = 0.49398790\n",
            "Iteration 29, loss = 0.48347327\n",
            "Iteration 30, loss = 0.48540742\n",
            "Iteration 31, loss = 0.47809108\n",
            "Iteration 32, loss = 0.49404926\n",
            "Iteration 33, loss = 0.48487739\n",
            "Iteration 34, loss = 0.47326931\n",
            "Iteration 35, loss = 0.48372994\n",
            "Iteration 36, loss = 0.46637080\n",
            "Iteration 37, loss = 0.46968139\n",
            "Iteration 38, loss = 0.46079216\n",
            "Iteration 39, loss = 0.52073747\n",
            "Iteration 40, loss = 0.51730790\n",
            "Iteration 41, loss = 0.45753086\n",
            "Iteration 42, loss = 0.45231537\n",
            "Iteration 43, loss = 0.44965866\n",
            "Iteration 44, loss = 0.45603336\n",
            "Iteration 45, loss = 0.45336343\n",
            "Iteration 46, loss = 0.44526380\n",
            "Iteration 47, loss = 0.44416088\n",
            "Iteration 48, loss = 0.44210241\n",
            "Iteration 49, loss = 0.45300179\n",
            "Iteration 50, loss = 0.45120470\n",
            "Iteration 51, loss = 0.44931815\n",
            "Iteration 52, loss = 0.45212983\n",
            "Iteration 53, loss = 0.45756272\n",
            "Iteration 54, loss = 0.44168038\n",
            "Iteration 55, loss = 0.43760188\n",
            "Iteration 56, loss = 0.43833545\n",
            "Iteration 57, loss = 0.44891148\n",
            "Iteration 58, loss = 0.44481338\n",
            "Iteration 59, loss = 0.43784046\n",
            "Iteration 60, loss = 0.43733403\n",
            "Iteration 61, loss = 0.43691938\n",
            "Iteration 62, loss = 0.43805766\n",
            "Iteration 63, loss = 0.43944095\n",
            "Iteration 64, loss = 0.44812237\n",
            "Iteration 65, loss = 0.43396954\n",
            "Iteration 66, loss = 0.43510623\n",
            "Iteration 67, loss = 0.43048306\n",
            "Iteration 68, loss = 0.44737629\n",
            "Iteration 69, loss = 0.43296459\n",
            "Iteration 70, loss = 0.43572544\n",
            "Iteration 71, loss = 0.43225150\n",
            "Iteration 72, loss = 0.45075027\n",
            "Iteration 73, loss = 0.43180734\n",
            "Iteration 74, loss = 0.43422044\n",
            "Iteration 75, loss = 0.43148160\n",
            "Iteration 76, loss = 0.43115422\n",
            "Iteration 77, loss = 0.43178437\n",
            "Iteration 78, loss = 0.44283448\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64278466\n",
            "Iteration 2, loss = 0.61719590\n",
            "Iteration 3, loss = 0.60761911\n",
            "Iteration 4, loss = 0.60184217\n",
            "Iteration 5, loss = 0.60102345\n",
            "Iteration 6, loss = 0.60453193\n",
            "Iteration 7, loss = 0.60183158\n",
            "Iteration 8, loss = 0.61963145\n",
            "Iteration 9, loss = 0.60055740\n",
            "Iteration 10, loss = 0.58981991\n",
            "Iteration 11, loss = 0.59029169\n",
            "Iteration 12, loss = 0.58635802\n",
            "Iteration 13, loss = 0.58233277\n",
            "Iteration 14, loss = 0.57758759\n",
            "Iteration 15, loss = 0.57682322\n",
            "Iteration 16, loss = 0.57811182\n",
            "Iteration 17, loss = 0.56581572\n",
            "Iteration 18, loss = 0.57012413\n",
            "Iteration 19, loss = 0.55652362\n",
            "Iteration 20, loss = 0.55092775\n",
            "Iteration 21, loss = 0.54025140\n",
            "Iteration 22, loss = 0.56394876\n",
            "Iteration 23, loss = 0.57560405\n",
            "Iteration 24, loss = 0.53890366\n",
            "Iteration 25, loss = 0.52152416\n",
            "Iteration 26, loss = 0.53766632\n",
            "Iteration 27, loss = 0.51487713\n",
            "Iteration 28, loss = 0.52471206\n",
            "Iteration 29, loss = 0.50257422\n",
            "Iteration 30, loss = 0.49126382\n",
            "Iteration 31, loss = 0.48726557\n",
            "Iteration 32, loss = 0.47783244\n",
            "Iteration 33, loss = 0.48366983\n",
            "Iteration 34, loss = 0.49140623\n",
            "Iteration 35, loss = 0.49442951\n",
            "Iteration 36, loss = 0.50181293\n",
            "Iteration 37, loss = 0.48364050\n",
            "Iteration 38, loss = 0.46425032\n",
            "Iteration 39, loss = 0.45779192\n",
            "Iteration 40, loss = 0.46132329\n",
            "Iteration 41, loss = 0.45441302\n",
            "Iteration 42, loss = 0.45230602\n",
            "Iteration 43, loss = 0.46609297\n",
            "Iteration 44, loss = 0.45089686\n",
            "Iteration 45, loss = 0.44785189\n",
            "Iteration 46, loss = 0.45738611\n",
            "Iteration 47, loss = 0.44384911\n",
            "Iteration 48, loss = 0.44287663\n",
            "Iteration 49, loss = 0.44200563\n",
            "Iteration 50, loss = 0.44275805\n",
            "Iteration 51, loss = 0.45422095\n",
            "Iteration 52, loss = 0.44521882\n",
            "Iteration 53, loss = 0.45400770\n",
            "Iteration 54, loss = 0.45325664\n",
            "Iteration 55, loss = 0.45274786\n",
            "Iteration 56, loss = 0.47491270\n",
            "Iteration 57, loss = 0.44444067\n",
            "Iteration 58, loss = 0.44001292\n",
            "Iteration 59, loss = 0.43739645\n",
            "Iteration 60, loss = 0.43528929\n",
            "Iteration 61, loss = 0.44054536\n",
            "Iteration 62, loss = 0.43452213\n",
            "Iteration 63, loss = 0.44015492\n",
            "Iteration 64, loss = 0.44632587\n",
            "Iteration 65, loss = 0.43245246\n",
            "Iteration 66, loss = 0.43755999\n",
            "Iteration 67, loss = 0.43376290\n",
            "Iteration 68, loss = 0.43446852\n",
            "Iteration 69, loss = 0.43294338\n",
            "Iteration 70, loss = 0.43547117\n",
            "Iteration 71, loss = 0.43156053\n",
            "Iteration 72, loss = 0.43416064\n",
            "Iteration 73, loss = 0.43028051\n",
            "Iteration 74, loss = 0.42906347\n",
            "Iteration 75, loss = 0.47599978\n",
            "Iteration 76, loss = 0.45812357\n",
            "Iteration 77, loss = 0.42832793\n",
            "Iteration 78, loss = 0.43125532\n",
            "Iteration 79, loss = 0.43256960\n",
            "Iteration 80, loss = 0.42911483\n",
            "Iteration 81, loss = 0.43080219\n",
            "Iteration 82, loss = 0.43908857\n",
            "Iteration 83, loss = 0.42666296\n",
            "Iteration 84, loss = 0.42244227\n",
            "Iteration 85, loss = 0.43633085\n",
            "Iteration 86, loss = 0.42510066\n",
            "Iteration 87, loss = 0.44464910\n",
            "Iteration 88, loss = 0.42820660\n",
            "Iteration 89, loss = 0.42757674\n",
            "Iteration 90, loss = 0.42790739\n",
            "Iteration 91, loss = 0.44122800\n",
            "Iteration 92, loss = 0.42529900\n",
            "Iteration 93, loss = 0.43311884\n",
            "Iteration 94, loss = 0.44545876\n",
            "Iteration 95, loss = 0.43731298\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.65788526\n",
            "Iteration 2, loss = 0.63870264\n",
            "Iteration 3, loss = 0.61975407\n",
            "Iteration 4, loss = 0.61392944\n",
            "Iteration 5, loss = 0.61362241\n",
            "Iteration 6, loss = 0.61146121\n",
            "Iteration 7, loss = 0.61348813\n",
            "Iteration 8, loss = 0.60912551\n",
            "Iteration 9, loss = 0.60549147\n",
            "Iteration 10, loss = 0.62157205\n",
            "Iteration 11, loss = 0.60232090\n",
            "Iteration 12, loss = 0.60184054\n",
            "Iteration 13, loss = 0.59610973\n",
            "Iteration 14, loss = 0.59357368\n",
            "Iteration 15, loss = 0.59328184\n",
            "Iteration 16, loss = 0.59424324\n",
            "Iteration 17, loss = 0.58845827\n",
            "Iteration 18, loss = 0.58318872\n",
            "Iteration 19, loss = 0.57422925\n",
            "Iteration 20, loss = 0.56351910\n",
            "Iteration 21, loss = 0.56888848\n",
            "Iteration 22, loss = 0.55813930\n",
            "Iteration 23, loss = 0.55263948\n",
            "Iteration 24, loss = 0.53290771\n",
            "Iteration 25, loss = 0.52737660\n",
            "Iteration 26, loss = 0.52313992\n",
            "Iteration 27, loss = 0.51458935\n",
            "Iteration 28, loss = 0.50797806\n",
            "Iteration 29, loss = 0.52679635\n",
            "Iteration 30, loss = 0.49083267\n",
            "Iteration 31, loss = 0.49090340\n",
            "Iteration 32, loss = 0.48281077\n",
            "Iteration 33, loss = 0.47872008\n",
            "Iteration 34, loss = 0.52598186\n",
            "Iteration 35, loss = 0.49030798\n",
            "Iteration 36, loss = 0.46772513\n",
            "Iteration 37, loss = 0.47014486\n",
            "Iteration 38, loss = 0.46464664\n",
            "Iteration 39, loss = 0.45955316\n",
            "Iteration 40, loss = 0.46233940\n",
            "Iteration 41, loss = 0.46379178\n",
            "Iteration 42, loss = 0.47105962\n",
            "Iteration 43, loss = 0.45688857\n",
            "Iteration 44, loss = 0.45965630\n",
            "Iteration 45, loss = 0.45028006\n",
            "Iteration 46, loss = 0.45439797\n",
            "Iteration 47, loss = 0.45118056\n",
            "Iteration 48, loss = 0.44697052\n",
            "Iteration 49, loss = 0.44970744\n",
            "Iteration 50, loss = 0.45042780\n",
            "Iteration 51, loss = 0.44601523\n",
            "Iteration 52, loss = 0.45951722\n",
            "Iteration 53, loss = 0.48256887\n",
            "Iteration 54, loss = 0.46191824\n",
            "Iteration 55, loss = 0.44475883\n",
            "Iteration 56, loss = 0.44827244\n",
            "Iteration 57, loss = 0.47182787\n",
            "Iteration 58, loss = 0.46231024\n",
            "Iteration 59, loss = 0.44768935\n",
            "Iteration 60, loss = 0.45109642\n",
            "Iteration 61, loss = 0.45604181\n",
            "Iteration 62, loss = 0.45068808\n",
            "Iteration 63, loss = 0.45530659\n",
            "Iteration 64, loss = 0.44346205\n",
            "Iteration 65, loss = 0.43957215\n",
            "Iteration 66, loss = 0.44243109\n",
            "Iteration 67, loss = 0.44042788\n",
            "Iteration 68, loss = 0.44403686\n",
            "Iteration 69, loss = 0.44150262\n",
            "Iteration 70, loss = 0.44841499\n",
            "Iteration 71, loss = 0.44474465\n",
            "Iteration 72, loss = 0.44625587\n",
            "Iteration 73, loss = 0.45611031\n",
            "Iteration 74, loss = 0.44906898\n",
            "Iteration 75, loss = 0.43723656\n",
            "Iteration 76, loss = 0.43515136\n",
            "Iteration 77, loss = 0.43596933\n",
            "Iteration 78, loss = 0.44842223\n",
            "Iteration 79, loss = 0.45177716\n",
            "Iteration 80, loss = 0.45046297\n",
            "Iteration 81, loss = 0.45989858\n",
            "Iteration 82, loss = 0.44443248\n",
            "Iteration 83, loss = 0.44308121\n",
            "Iteration 84, loss = 0.43812808\n",
            "Iteration 85, loss = 0.44003054\n",
            "Iteration 86, loss = 0.43627449\n",
            "Iteration 87, loss = 0.43283696\n",
            "Iteration 88, loss = 0.43419819\n",
            "Iteration 89, loss = 0.44482530\n",
            "Iteration 90, loss = 0.43736346\n",
            "Iteration 91, loss = 0.43561485\n",
            "Iteration 92, loss = 0.43141767\n",
            "Iteration 93, loss = 0.43195962\n",
            "Iteration 94, loss = 0.43373381\n",
            "Iteration 95, loss = 0.43227447\n",
            "Iteration 96, loss = 0.44158730\n",
            "Iteration 97, loss = 0.44120681\n",
            "Iteration 98, loss = 0.44028246\n",
            "Iteration 99, loss = 0.44001872\n",
            "Iteration 100, loss = 0.44077812\n",
            "Iteration 101, loss = 0.44322016\n",
            "Iteration 102, loss = 0.44097291\n",
            "Iteration 103, loss = 0.44180802\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67833855\n",
            "Iteration 2, loss = 0.63003652\n",
            "Iteration 3, loss = 0.61670834\n",
            "Iteration 4, loss = 0.60038994\n",
            "Iteration 5, loss = 0.60193529\n",
            "Iteration 6, loss = 0.59730093\n",
            "Iteration 7, loss = 0.59504723\n",
            "Iteration 8, loss = 0.59701249\n",
            "Iteration 9, loss = 0.59553043\n",
            "Iteration 10, loss = 0.59321012\n",
            "Iteration 11, loss = 0.59126301\n",
            "Iteration 12, loss = 0.58823819\n",
            "Iteration 13, loss = 0.58988453\n",
            "Iteration 14, loss = 0.58284822\n",
            "Iteration 15, loss = 0.58121316\n",
            "Iteration 16, loss = 0.58284820\n",
            "Iteration 17, loss = 0.57647623\n",
            "Iteration 18, loss = 0.57676498\n",
            "Iteration 19, loss = 0.57026198\n",
            "Iteration 20, loss = 0.56458472\n",
            "Iteration 21, loss = 0.56226289\n",
            "Iteration 22, loss = 0.55449629\n",
            "Iteration 23, loss = 0.55136609\n",
            "Iteration 24, loss = 0.54604238\n",
            "Iteration 25, loss = 0.53951529\n",
            "Iteration 26, loss = 0.52904225\n",
            "Iteration 27, loss = 0.52955817\n",
            "Iteration 28, loss = 0.51762594\n",
            "Iteration 29, loss = 0.50348792\n",
            "Iteration 30, loss = 0.49550716\n",
            "Iteration 31, loss = 0.49141636\n",
            "Iteration 32, loss = 0.48957462\n",
            "Iteration 33, loss = 0.48204216\n",
            "Iteration 34, loss = 0.47594299\n",
            "Iteration 35, loss = 0.46706908\n",
            "Iteration 36, loss = 0.47966419\n",
            "Iteration 37, loss = 0.47418272\n",
            "Iteration 38, loss = 0.46134778\n",
            "Iteration 39, loss = 0.46835490\n",
            "Iteration 40, loss = 0.45858842\n",
            "Iteration 41, loss = 0.50274620\n",
            "Iteration 42, loss = 0.46272249\n",
            "Iteration 43, loss = 0.45781756\n",
            "Iteration 44, loss = 0.46761364\n",
            "Iteration 45, loss = 0.45837934\n",
            "Iteration 46, loss = 0.45357296\n",
            "Iteration 47, loss = 0.45648985\n",
            "Iteration 48, loss = 0.45513102\n",
            "Iteration 49, loss = 0.45867915\n",
            "Iteration 50, loss = 0.46736660\n",
            "Iteration 51, loss = 0.45778592\n",
            "Iteration 52, loss = 0.44791138\n",
            "Iteration 53, loss = 0.45242835\n",
            "Iteration 54, loss = 0.44940191\n",
            "Iteration 55, loss = 0.44885907\n",
            "Iteration 56, loss = 0.45414619\n",
            "Iteration 57, loss = 0.45742231\n",
            "Iteration 58, loss = 0.46710115\n",
            "Iteration 59, loss = 0.44502739\n",
            "Iteration 60, loss = 0.44460424\n",
            "Iteration 61, loss = 0.45286525\n",
            "Iteration 62, loss = 0.44432929\n",
            "Iteration 63, loss = 0.44284847\n",
            "Iteration 64, loss = 0.45249787\n",
            "Iteration 65, loss = 0.44911270\n",
            "Iteration 66, loss = 0.44146696\n",
            "Iteration 67, loss = 0.44070368\n",
            "Iteration 68, loss = 0.43880571\n",
            "Iteration 69, loss = 0.44415060\n",
            "Iteration 70, loss = 0.43868864\n",
            "Iteration 71, loss = 0.44678985\n",
            "Iteration 72, loss = 0.47328236\n",
            "Iteration 73, loss = 0.46480168\n",
            "Iteration 74, loss = 0.44482022\n",
            "Iteration 75, loss = 0.44198805\n",
            "Iteration 76, loss = 0.44703445\n",
            "Iteration 77, loss = 0.46614133\n",
            "Iteration 78, loss = 0.45130722\n",
            "Iteration 79, loss = 0.43075708\n",
            "Iteration 80, loss = 0.43641822\n",
            "Iteration 81, loss = 0.43544860\n",
            "Iteration 82, loss = 0.43406176\n",
            "Iteration 83, loss = 0.43922512\n",
            "Iteration 84, loss = 0.46811466\n",
            "Iteration 85, loss = 0.44518263\n",
            "Iteration 86, loss = 0.44965338\n",
            "Iteration 87, loss = 0.43964632\n",
            "Iteration 88, loss = 0.44299383\n",
            "Iteration 89, loss = 0.45257430\n",
            "Iteration 90, loss = 0.45077957\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66935140\n",
            "Iteration 2, loss = 0.63858474\n",
            "Iteration 3, loss = 0.61293717\n",
            "Iteration 4, loss = 0.60843233\n",
            "Iteration 5, loss = 0.60769865\n",
            "Iteration 6, loss = 0.60458544\n",
            "Iteration 7, loss = 0.60445282\n",
            "Iteration 8, loss = 0.60131257\n",
            "Iteration 9, loss = 0.59993192\n",
            "Iteration 10, loss = 0.60203246\n",
            "Iteration 11, loss = 0.59738486\n",
            "Iteration 12, loss = 0.59187495\n",
            "Iteration 13, loss = 0.59923585\n",
            "Iteration 14, loss = 0.60095093\n",
            "Iteration 15, loss = 0.59705228\n",
            "Iteration 16, loss = 0.59761920\n",
            "Iteration 17, loss = 0.59774881\n",
            "Iteration 18, loss = 0.58524109\n",
            "Iteration 19, loss = 0.57123446\n",
            "Iteration 20, loss = 0.56480669\n",
            "Iteration 21, loss = 0.56538668\n",
            "Iteration 22, loss = 0.55513653\n",
            "Iteration 23, loss = 0.55091422\n",
            "Iteration 24, loss = 0.54361068\n",
            "Iteration 25, loss = 0.54181730\n",
            "Iteration 26, loss = 0.55284635\n",
            "Iteration 27, loss = 0.53715984\n",
            "Iteration 28, loss = 0.52077354\n",
            "Iteration 29, loss = 0.51687528\n",
            "Iteration 30, loss = 0.52447525\n",
            "Iteration 31, loss = 0.50862559\n",
            "Iteration 32, loss = 0.50675319\n",
            "Iteration 33, loss = 0.49786874\n",
            "Iteration 34, loss = 0.49393945\n",
            "Iteration 35, loss = 0.48322130\n",
            "Iteration 36, loss = 0.48095350\n",
            "Iteration 37, loss = 0.47768604\n",
            "Iteration 38, loss = 0.48942331\n",
            "Iteration 39, loss = 0.47979001\n",
            "Iteration 40, loss = 0.48061931\n",
            "Iteration 41, loss = 0.46614703\n",
            "Iteration 42, loss = 0.46937967\n",
            "Iteration 43, loss = 0.46527481\n",
            "Iteration 44, loss = 0.46222139\n",
            "Iteration 45, loss = 0.45782477\n",
            "Iteration 46, loss = 0.46372819\n",
            "Iteration 47, loss = 0.46399796\n",
            "Iteration 48, loss = 0.46262384\n",
            "Iteration 49, loss = 0.45720029\n",
            "Iteration 50, loss = 0.46110658\n",
            "Iteration 51, loss = 0.46954459\n",
            "Iteration 52, loss = 0.45964648\n",
            "Iteration 53, loss = 0.45323129\n",
            "Iteration 54, loss = 0.47457362\n",
            "Iteration 55, loss = 0.46499483\n",
            "Iteration 56, loss = 0.45351454\n",
            "Iteration 57, loss = 0.45844277\n",
            "Iteration 58, loss = 0.47569204\n",
            "Iteration 59, loss = 0.48265091\n",
            "Iteration 60, loss = 0.46796258\n",
            "Iteration 61, loss = 0.45901789\n",
            "Iteration 62, loss = 0.45764970\n",
            "Iteration 63, loss = 0.44816954\n",
            "Iteration 64, loss = 0.45960519\n",
            "Iteration 65, loss = 0.44364987\n",
            "Iteration 66, loss = 0.46043854\n",
            "Iteration 67, loss = 0.45362096\n",
            "Iteration 68, loss = 0.44908410\n",
            "Iteration 69, loss = 0.44749880\n",
            "Iteration 70, loss = 0.45128570\n",
            "Iteration 71, loss = 0.45170743\n",
            "Iteration 72, loss = 0.45663142\n",
            "Iteration 73, loss = 0.44152473\n",
            "Iteration 74, loss = 0.44439003\n",
            "Iteration 75, loss = 0.44546687\n",
            "Iteration 76, loss = 0.44916617\n",
            "Iteration 77, loss = 0.44059381\n",
            "Iteration 78, loss = 0.45733352\n",
            "Iteration 79, loss = 0.44945436\n",
            "Iteration 80, loss = 0.44015343\n",
            "Iteration 81, loss = 0.44632179\n",
            "Iteration 82, loss = 0.44266312\n",
            "Iteration 83, loss = 0.44205988\n",
            "Iteration 84, loss = 0.44486875\n",
            "Iteration 85, loss = 0.44261918\n",
            "Iteration 86, loss = 0.44537275\n",
            "Iteration 87, loss = 0.43730517\n",
            "Iteration 88, loss = 0.43800177\n",
            "Iteration 89, loss = 0.44463831\n",
            "Iteration 90, loss = 0.44846637\n",
            "Iteration 91, loss = 0.43803955\n",
            "Iteration 92, loss = 0.43924815\n",
            "Iteration 93, loss = 0.43941002\n",
            "Iteration 94, loss = 0.43751570\n",
            "Iteration 95, loss = 0.43931725\n",
            "Iteration 96, loss = 0.43811072\n",
            "Iteration 97, loss = 0.43939619\n",
            "Iteration 98, loss = 0.43587068\n",
            "Iteration 99, loss = 0.43494175\n",
            "Iteration 100, loss = 0.43625170\n",
            "Iteration 101, loss = 0.43807954\n",
            "Iteration 102, loss = 0.45560448\n",
            "Iteration 103, loss = 0.43799993\n",
            "Iteration 104, loss = 0.43915906\n",
            "Iteration 105, loss = 0.43786847\n",
            "Iteration 106, loss = 0.43601386\n",
            "Iteration 107, loss = 0.44536960\n",
            "Iteration 108, loss = 0.44118308\n",
            "Iteration 109, loss = 0.45034198\n",
            "Iteration 110, loss = 0.43541725\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62308634\n",
            "Iteration 2, loss = 0.60997752\n",
            "Iteration 3, loss = 0.59785736\n",
            "Iteration 4, loss = 0.60249363\n",
            "Iteration 5, loss = 0.59568266\n",
            "Iteration 6, loss = 0.59314448\n",
            "Iteration 7, loss = 0.58745350\n",
            "Iteration 8, loss = 0.58853228\n",
            "Iteration 9, loss = 0.59054457\n",
            "Iteration 10, loss = 0.57259047\n",
            "Iteration 11, loss = 0.57951534\n",
            "Iteration 12, loss = 0.58186898\n",
            "Iteration 13, loss = 0.57668253\n",
            "Iteration 14, loss = 0.56708853\n",
            "Iteration 15, loss = 0.57529360\n",
            "Iteration 16, loss = 0.55851269\n",
            "Iteration 17, loss = 0.56506937\n",
            "Iteration 18, loss = 0.54510402\n",
            "Iteration 19, loss = 0.54691584\n",
            "Iteration 20, loss = 0.54971180\n",
            "Iteration 21, loss = 0.55586727\n",
            "Iteration 22, loss = 0.55431900\n",
            "Iteration 23, loss = 0.54154634\n",
            "Iteration 24, loss = 0.53743879\n",
            "Iteration 25, loss = 0.52456733\n",
            "Iteration 26, loss = 0.55889665\n",
            "Iteration 27, loss = 0.54534511\n",
            "Iteration 28, loss = 0.54026090\n",
            "Iteration 29, loss = 0.54688939\n",
            "Iteration 30, loss = 0.54260618\n",
            "Iteration 31, loss = 0.52716529\n",
            "Iteration 32, loss = 0.52842834\n",
            "Iteration 33, loss = 0.52421978\n",
            "Iteration 34, loss = 0.53286727\n",
            "Iteration 35, loss = 0.54103721\n",
            "Iteration 36, loss = 0.51969823\n",
            "Iteration 37, loss = 0.52611779\n",
            "Iteration 38, loss = 0.51282911\n",
            "Iteration 39, loss = 0.52983464\n",
            "Iteration 40, loss = 0.51619232\n",
            "Iteration 41, loss = 0.50898652\n",
            "Iteration 42, loss = 0.52032608\n",
            "Iteration 43, loss = 0.50699933\n",
            "Iteration 44, loss = 0.51002784\n",
            "Iteration 45, loss = 0.53004432\n",
            "Iteration 46, loss = 0.51672245\n",
            "Iteration 47, loss = 0.51205347\n",
            "Iteration 48, loss = 0.51165053\n",
            "Iteration 49, loss = 0.49796851\n",
            "Iteration 50, loss = 0.51207625\n",
            "Iteration 51, loss = 0.50739272\n",
            "Iteration 52, loss = 0.51382708\n",
            "Iteration 53, loss = 0.50095628\n",
            "Iteration 54, loss = 0.50693393\n",
            "Iteration 55, loss = 0.49928207\n",
            "Iteration 56, loss = 0.50530235\n",
            "Iteration 57, loss = 0.51290878\n",
            "Iteration 58, loss = 0.50949367\n",
            "Iteration 59, loss = 0.49767621\n",
            "Iteration 60, loss = 0.50297463\n",
            "Iteration 61, loss = 0.49764301\n",
            "Iteration 62, loss = 0.51082881\n",
            "Iteration 63, loss = 0.49996052\n",
            "Iteration 64, loss = 0.50067647\n",
            "Iteration 65, loss = 0.51395478\n",
            "Iteration 66, loss = 0.49908741\n",
            "Iteration 67, loss = 0.49472667\n",
            "Iteration 68, loss = 0.50481077\n",
            "Iteration 69, loss = 0.50326000\n",
            "Iteration 70, loss = 0.49679467\n",
            "Iteration 71, loss = 0.48957354\n",
            "Iteration 72, loss = 0.50429513\n",
            "Iteration 73, loss = 0.49875873\n",
            "Iteration 74, loss = 0.49719819\n",
            "Iteration 75, loss = 0.50115034\n",
            "Iteration 76, loss = 0.48458158\n",
            "Iteration 77, loss = 0.49298468\n",
            "Iteration 78, loss = 0.49945591\n",
            "Iteration 79, loss = 0.49766443\n",
            "Iteration 80, loss = 0.48173101\n",
            "Iteration 81, loss = 0.50316065\n",
            "Iteration 82, loss = 0.49393498\n",
            "Iteration 83, loss = 0.49970421\n",
            "Iteration 84, loss = 0.48676332\n",
            "Iteration 85, loss = 0.48846798\n",
            "Iteration 86, loss = 0.49002601\n",
            "Iteration 87, loss = 0.48026475\n",
            "Iteration 88, loss = 0.48859230\n",
            "Iteration 89, loss = 0.50153349\n",
            "Iteration 90, loss = 0.50370611\n",
            "Iteration 91, loss = 0.48426811\n",
            "Iteration 92, loss = 0.49520483\n",
            "Iteration 93, loss = 0.49986209\n",
            "Iteration 94, loss = 0.48295718\n",
            "Iteration 95, loss = 0.49038022\n",
            "Iteration 96, loss = 0.47682772\n",
            "Iteration 97, loss = 0.49225969\n",
            "Iteration 98, loss = 0.51173604\n",
            "Iteration 99, loss = 0.50728350\n",
            "Iteration 100, loss = 0.49314495\n",
            "Iteration 101, loss = 0.48904258\n",
            "Iteration 102, loss = 0.50195629\n",
            "Iteration 103, loss = 0.47775068\n",
            "Iteration 104, loss = 0.48604286\n",
            "Iteration 105, loss = 0.48281387\n",
            "Iteration 106, loss = 0.48922816\n",
            "Iteration 107, loss = 0.47167658\n",
            "Iteration 108, loss = 0.47907866\n",
            "Iteration 109, loss = 0.48866910\n",
            "Iteration 110, loss = 0.47965303\n",
            "Iteration 111, loss = 0.49380740\n",
            "Iteration 112, loss = 0.47547687\n",
            "Iteration 113, loss = 0.48921388\n",
            "Iteration 114, loss = 0.49348663\n",
            "Iteration 115, loss = 0.48190476\n",
            "Iteration 116, loss = 0.48657204\n",
            "Iteration 117, loss = 0.47890613\n",
            "Iteration 118, loss = 0.48286773\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.65268324\n",
            "Iteration 2, loss = 0.60704922\n",
            "Iteration 3, loss = 0.59955975\n",
            "Iteration 4, loss = 0.59236899\n",
            "Iteration 5, loss = 0.58835911\n",
            "Iteration 6, loss = 0.58527589\n",
            "Iteration 7, loss = 0.58644778\n",
            "Iteration 8, loss = 0.58613256\n",
            "Iteration 9, loss = 0.58987691\n",
            "Iteration 10, loss = 0.59342962\n",
            "Iteration 11, loss = 0.56758678\n",
            "Iteration 12, loss = 0.57129170\n",
            "Iteration 13, loss = 0.58776252\n",
            "Iteration 14, loss = 0.56601574\n",
            "Iteration 15, loss = 0.56857543\n",
            "Iteration 16, loss = 0.56239568\n",
            "Iteration 17, loss = 0.56063976\n",
            "Iteration 18, loss = 0.55726983\n",
            "Iteration 19, loss = 0.57415932\n",
            "Iteration 20, loss = 0.55596258\n",
            "Iteration 21, loss = 0.55732711\n",
            "Iteration 22, loss = 0.55074536\n",
            "Iteration 23, loss = 0.53973342\n",
            "Iteration 24, loss = 0.54352193\n",
            "Iteration 25, loss = 0.54455706\n",
            "Iteration 26, loss = 0.55171677\n",
            "Iteration 27, loss = 0.54798277\n",
            "Iteration 28, loss = 0.53243693\n",
            "Iteration 29, loss = 0.53738224\n",
            "Iteration 30, loss = 0.54521167\n",
            "Iteration 31, loss = 0.53990378\n",
            "Iteration 32, loss = 0.54705687\n",
            "Iteration 33, loss = 0.53736969\n",
            "Iteration 34, loss = 0.52498475\n",
            "Iteration 35, loss = 0.52666007\n",
            "Iteration 36, loss = 0.53484850\n",
            "Iteration 37, loss = 0.53811944\n",
            "Iteration 38, loss = 0.53307309\n",
            "Iteration 39, loss = 0.52633611\n",
            "Iteration 40, loss = 0.52015665\n",
            "Iteration 41, loss = 0.50973654\n",
            "Iteration 42, loss = 0.51577822\n",
            "Iteration 43, loss = 0.52030393\n",
            "Iteration 44, loss = 0.52133927\n",
            "Iteration 45, loss = 0.53349193\n",
            "Iteration 46, loss = 0.53144177\n",
            "Iteration 47, loss = 0.51043022\n",
            "Iteration 48, loss = 0.50904399\n",
            "Iteration 49, loss = 0.49667948\n",
            "Iteration 50, loss = 0.51920444\n",
            "Iteration 51, loss = 0.51160456\n",
            "Iteration 52, loss = 0.50865298\n",
            "Iteration 53, loss = 0.53686746\n",
            "Iteration 54, loss = 0.50233442\n",
            "Iteration 55, loss = 0.53013295\n",
            "Iteration 56, loss = 0.50442051\n",
            "Iteration 57, loss = 0.51935690\n",
            "Iteration 58, loss = 0.53177332\n",
            "Iteration 59, loss = 0.49908763\n",
            "Iteration 60, loss = 0.51116986\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62784480\n",
            "Iteration 2, loss = 0.62008680\n",
            "Iteration 3, loss = 0.60538682\n",
            "Iteration 4, loss = 0.61214383\n",
            "Iteration 5, loss = 0.60666314\n",
            "Iteration 6, loss = 0.59513391\n",
            "Iteration 7, loss = 0.59254204\n",
            "Iteration 8, loss = 0.58703946\n",
            "Iteration 9, loss = 0.58481814\n",
            "Iteration 10, loss = 0.58868364\n",
            "Iteration 11, loss = 0.58546678\n",
            "Iteration 12, loss = 0.57605669\n",
            "Iteration 13, loss = 0.57595861\n",
            "Iteration 14, loss = 0.57596166\n",
            "Iteration 15, loss = 0.56532445\n",
            "Iteration 16, loss = 0.56261343\n",
            "Iteration 17, loss = 0.56991561\n",
            "Iteration 18, loss = 0.56410130\n",
            "Iteration 19, loss = 0.55463684\n",
            "Iteration 20, loss = 0.56905699\n",
            "Iteration 21, loss = 0.55029789\n",
            "Iteration 22, loss = 0.54117445\n",
            "Iteration 23, loss = 0.55438370\n",
            "Iteration 24, loss = 0.53547983\n",
            "Iteration 25, loss = 0.54634115\n",
            "Iteration 26, loss = 0.53572409\n",
            "Iteration 27, loss = 0.54536132\n",
            "Iteration 28, loss = 0.53674869\n",
            "Iteration 29, loss = 0.54667304\n",
            "Iteration 30, loss = 0.52785872\n",
            "Iteration 31, loss = 0.52892094\n",
            "Iteration 32, loss = 0.53326287\n",
            "Iteration 33, loss = 0.53715275\n",
            "Iteration 34, loss = 0.54538862\n",
            "Iteration 35, loss = 0.53526274\n",
            "Iteration 36, loss = 0.53125356\n",
            "Iteration 37, loss = 0.51439555\n",
            "Iteration 38, loss = 0.51748416\n",
            "Iteration 39, loss = 0.51098430\n",
            "Iteration 40, loss = 0.52819761\n",
            "Iteration 41, loss = 0.51805595\n",
            "Iteration 42, loss = 0.52095805\n",
            "Iteration 43, loss = 0.51046112\n",
            "Iteration 44, loss = 0.52746669\n",
            "Iteration 45, loss = 0.51294224\n",
            "Iteration 46, loss = 0.50481762\n",
            "Iteration 47, loss = 0.50993258\n",
            "Iteration 48, loss = 0.51309976\n",
            "Iteration 49, loss = 0.52166720\n",
            "Iteration 50, loss = 0.51719879\n",
            "Iteration 51, loss = 0.51092498\n",
            "Iteration 52, loss = 0.51561813\n",
            "Iteration 53, loss = 0.52333156\n",
            "Iteration 54, loss = 0.52130542\n",
            "Iteration 55, loss = 0.51925483\n",
            "Iteration 56, loss = 0.53484708\n",
            "Iteration 57, loss = 0.54166708\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.63008073\n",
            "Iteration 2, loss = 0.60386936\n",
            "Iteration 3, loss = 0.59894068\n",
            "Iteration 4, loss = 0.59537008\n",
            "Iteration 5, loss = 0.59173243\n",
            "Iteration 6, loss = 0.58791870\n",
            "Iteration 7, loss = 0.58274104\n",
            "Iteration 8, loss = 0.57743001\n",
            "Iteration 9, loss = 0.59066401\n",
            "Iteration 10, loss = 0.57497125\n",
            "Iteration 11, loss = 0.59009151\n",
            "Iteration 12, loss = 0.57304722\n",
            "Iteration 13, loss = 0.58217672\n",
            "Iteration 14, loss = 0.56428689\n",
            "Iteration 15, loss = 0.56943037\n",
            "Iteration 16, loss = 0.56314695\n",
            "Iteration 17, loss = 0.57401420\n",
            "Iteration 18, loss = 0.57107943\n",
            "Iteration 19, loss = 0.57086710\n",
            "Iteration 20, loss = 0.56433967\n",
            "Iteration 21, loss = 0.55686149\n",
            "Iteration 22, loss = 0.57180439\n",
            "Iteration 23, loss = 0.54663631\n",
            "Iteration 24, loss = 0.55554443\n",
            "Iteration 25, loss = 0.54278570\n",
            "Iteration 26, loss = 0.54083228\n",
            "Iteration 27, loss = 0.55341333\n",
            "Iteration 28, loss = 0.55157826\n",
            "Iteration 29, loss = 0.54406529\n",
            "Iteration 30, loss = 0.55303208\n",
            "Iteration 31, loss = 0.53891355\n",
            "Iteration 32, loss = 0.55129632\n",
            "Iteration 33, loss = 0.54530138\n",
            "Iteration 34, loss = 0.53512963\n",
            "Iteration 35, loss = 0.55347192\n",
            "Iteration 36, loss = 0.53896105\n",
            "Iteration 37, loss = 0.54167772\n",
            "Iteration 38, loss = 0.53361180\n",
            "Iteration 39, loss = 0.54008260\n",
            "Iteration 40, loss = 0.53209676\n",
            "Iteration 41, loss = 0.52630291\n",
            "Iteration 42, loss = 0.53774172\n",
            "Iteration 43, loss = 0.52980717\n",
            "Iteration 44, loss = 0.53671132\n",
            "Iteration 45, loss = 0.51771976\n",
            "Iteration 46, loss = 0.50926331\n",
            "Iteration 47, loss = 0.52310166\n",
            "Iteration 48, loss = 0.51666251\n",
            "Iteration 49, loss = 0.53530141\n",
            "Iteration 50, loss = 0.51569525\n",
            "Iteration 51, loss = 0.53394475\n",
            "Iteration 52, loss = 0.50510161\n",
            "Iteration 53, loss = 0.51074437\n",
            "Iteration 54, loss = 0.51279720\n",
            "Iteration 55, loss = 0.50353142\n",
            "Iteration 56, loss = 0.51631945\n",
            "Iteration 57, loss = 0.50906782\n",
            "Iteration 58, loss = 0.51604687\n",
            "Iteration 59, loss = 0.52154333\n",
            "Iteration 60, loss = 0.53165462\n",
            "Iteration 61, loss = 0.52130128\n",
            "Iteration 62, loss = 0.50233548\n",
            "Iteration 63, loss = 0.49943444\n",
            "Iteration 64, loss = 0.48957681\n",
            "Iteration 65, loss = 0.50361835\n",
            "Iteration 66, loss = 0.50535083\n",
            "Iteration 67, loss = 0.51165166\n",
            "Iteration 68, loss = 0.50121313\n",
            "Iteration 69, loss = 0.50118965\n",
            "Iteration 70, loss = 0.50698575\n",
            "Iteration 71, loss = 0.51120254\n",
            "Iteration 72, loss = 0.50483058\n",
            "Iteration 73, loss = 0.50238547\n",
            "Iteration 74, loss = 0.49477701\n",
            "Iteration 75, loss = 0.51344862\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.63290840\n",
            "Iteration 2, loss = 0.61608511\n",
            "Iteration 3, loss = 0.60316356\n",
            "Iteration 4, loss = 0.61722769\n",
            "Iteration 5, loss = 0.59702836\n",
            "Iteration 6, loss = 0.59849698\n",
            "Iteration 7, loss = 0.59152526\n",
            "Iteration 8, loss = 0.58945793\n",
            "Iteration 9, loss = 0.58559025\n",
            "Iteration 10, loss = 0.58794529\n",
            "Iteration 11, loss = 0.58250003\n",
            "Iteration 12, loss = 0.58592329\n",
            "Iteration 13, loss = 0.57861757\n",
            "Iteration 14, loss = 0.59086453\n",
            "Iteration 15, loss = 0.56969703\n",
            "Iteration 16, loss = 0.58270229\n",
            "Iteration 17, loss = 0.57376540\n",
            "Iteration 18, loss = 0.57974783\n",
            "Iteration 19, loss = 0.56335385\n",
            "Iteration 20, loss = 0.58855242\n",
            "Iteration 21, loss = 0.56778990\n",
            "Iteration 22, loss = 0.56039109\n",
            "Iteration 23, loss = 0.56735691\n",
            "Iteration 24, loss = 0.56285220\n",
            "Iteration 25, loss = 0.55905694\n",
            "Iteration 26, loss = 0.55289164\n",
            "Iteration 27, loss = 0.55361076\n",
            "Iteration 28, loss = 0.54942216\n",
            "Iteration 29, loss = 0.55126029\n",
            "Iteration 30, loss = 0.55738525\n",
            "Iteration 31, loss = 0.54599474\n",
            "Iteration 32, loss = 0.54603829\n",
            "Iteration 33, loss = 0.53661805\n",
            "Iteration 34, loss = 0.55642661\n",
            "Iteration 35, loss = 0.54302677\n",
            "Iteration 36, loss = 0.53972353\n",
            "Iteration 37, loss = 0.55192376\n",
            "Iteration 38, loss = 0.53711714\n",
            "Iteration 39, loss = 0.54923598\n",
            "Iteration 40, loss = 0.52463955\n",
            "Iteration 41, loss = 0.54054782\n",
            "Iteration 42, loss = 0.55015738\n",
            "Iteration 43, loss = 0.53987622\n",
            "Iteration 44, loss = 0.55000809\n",
            "Iteration 45, loss = 0.54334152\n",
            "Iteration 46, loss = 0.54295076\n",
            "Iteration 47, loss = 0.53563546\n",
            "Iteration 48, loss = 0.52800121\n",
            "Iteration 49, loss = 0.54502344\n",
            "Iteration 50, loss = 0.52645241\n",
            "Iteration 51, loss = 0.51334760\n",
            "Iteration 52, loss = 0.51264693\n",
            "Iteration 53, loss = 0.51948744\n",
            "Iteration 54, loss = 0.51513140\n",
            "Iteration 55, loss = 0.51713569\n",
            "Iteration 56, loss = 0.51702325\n",
            "Iteration 57, loss = 0.52070467\n",
            "Iteration 58, loss = 0.52685287\n",
            "Iteration 59, loss = 0.52818039\n",
            "Iteration 60, loss = 0.53273366\n",
            "Iteration 61, loss = 0.51714174\n",
            "Iteration 62, loss = 0.52612358\n",
            "Iteration 63, loss = 0.52612811\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.78519401\n",
            "Iteration 2, loss = 0.66922719\n",
            "Iteration 3, loss = 0.62465318\n",
            "Iteration 4, loss = 0.61134957\n",
            "Iteration 5, loss = 0.60842600\n",
            "Iteration 6, loss = 0.60321573\n",
            "Iteration 7, loss = 0.60148482\n",
            "Iteration 8, loss = 0.60080650\n",
            "Iteration 9, loss = 0.59841158\n",
            "Iteration 10, loss = 0.59655263\n",
            "Iteration 11, loss = 0.59603599\n",
            "Iteration 12, loss = 0.59383686\n",
            "Iteration 13, loss = 0.59412736\n",
            "Iteration 14, loss = 0.59411270\n",
            "Iteration 15, loss = 0.59342930\n",
            "Iteration 16, loss = 0.59190232\n",
            "Iteration 17, loss = 0.59048478\n",
            "Iteration 18, loss = 0.59042588\n",
            "Iteration 19, loss = 0.59485061\n",
            "Iteration 20, loss = 0.59316856\n",
            "Iteration 21, loss = 0.59033211\n",
            "Iteration 22, loss = 0.58940926\n",
            "Iteration 23, loss = 0.58842443\n",
            "Iteration 24, loss = 0.58660619\n",
            "Iteration 25, loss = 0.58489536\n",
            "Iteration 26, loss = 0.58616074\n",
            "Iteration 27, loss = 0.58701143\n",
            "Iteration 28, loss = 0.58426414\n",
            "Iteration 29, loss = 0.59371009\n",
            "Iteration 30, loss = 0.58422540\n",
            "Iteration 31, loss = 0.58482097\n",
            "Iteration 32, loss = 0.58349252\n",
            "Iteration 33, loss = 0.58121849\n",
            "Iteration 34, loss = 0.58024982\n",
            "Iteration 35, loss = 0.57906105\n",
            "Iteration 36, loss = 0.57899215\n",
            "Iteration 37, loss = 0.57828236\n",
            "Iteration 38, loss = 0.58150480\n",
            "Iteration 39, loss = 0.57981850\n",
            "Iteration 40, loss = 0.57874321\n",
            "Iteration 41, loss = 0.57643123\n",
            "Iteration 42, loss = 0.57576483\n",
            "Iteration 43, loss = 0.57766665\n",
            "Iteration 44, loss = 0.57462147\n",
            "Iteration 45, loss = 0.57327494\n",
            "Iteration 46, loss = 0.57573104\n",
            "Iteration 47, loss = 0.57355662\n",
            "Iteration 48, loss = 0.57412953\n",
            "Iteration 49, loss = 0.57252301\n",
            "Iteration 50, loss = 0.57445459\n",
            "Iteration 51, loss = 0.57165292\n",
            "Iteration 52, loss = 0.57158489\n",
            "Iteration 53, loss = 0.56956051\n",
            "Iteration 54, loss = 0.57021234\n",
            "Iteration 55, loss = 0.56993084\n",
            "Iteration 56, loss = 0.56926778\n",
            "Iteration 57, loss = 0.57259125\n",
            "Iteration 58, loss = 0.56772271\n",
            "Iteration 59, loss = 0.57069557\n",
            "Iteration 60, loss = 0.56844974\n",
            "Iteration 61, loss = 0.56975325\n",
            "Iteration 62, loss = 0.56733851\n",
            "Iteration 63, loss = 0.56539170\n",
            "Iteration 64, loss = 0.57153398\n",
            "Iteration 65, loss = 0.56656857\n",
            "Iteration 66, loss = 0.56556016\n",
            "Iteration 67, loss = 0.56732647\n",
            "Iteration 68, loss = 0.56439582\n",
            "Iteration 69, loss = 0.56372008\n",
            "Iteration 70, loss = 0.56699436\n",
            "Iteration 71, loss = 0.56540896\n",
            "Iteration 72, loss = 0.56261965\n",
            "Iteration 73, loss = 0.56301425\n",
            "Iteration 74, loss = 0.56209118\n",
            "Iteration 75, loss = 0.56653328\n",
            "Iteration 76, loss = 0.56191150\n",
            "Iteration 77, loss = 0.56214970\n",
            "Iteration 78, loss = 0.56567399\n",
            "Iteration 79, loss = 0.55829783\n",
            "Iteration 80, loss = 0.56348496\n",
            "Iteration 81, loss = 0.55801267\n",
            "Iteration 82, loss = 0.55939783\n",
            "Iteration 83, loss = 0.55958565\n",
            "Iteration 84, loss = 0.55735803\n",
            "Iteration 85, loss = 0.55883461\n",
            "Iteration 86, loss = 0.55646315\n",
            "Iteration 87, loss = 0.55766572\n",
            "Iteration 88, loss = 0.55754485\n",
            "Iteration 89, loss = 0.55659388\n",
            "Iteration 90, loss = 0.55878878\n",
            "Iteration 91, loss = 0.55643663\n",
            "Iteration 92, loss = 0.55904295\n",
            "Iteration 93, loss = 0.56355187\n",
            "Iteration 94, loss = 0.55541502\n",
            "Iteration 95, loss = 0.55626568\n",
            "Iteration 96, loss = 0.55876188\n",
            "Iteration 97, loss = 0.55439440\n",
            "Iteration 98, loss = 0.55173001\n",
            "Iteration 99, loss = 0.55591609\n",
            "Iteration 100, loss = 0.55361131\n",
            "Iteration 101, loss = 0.55081914\n",
            "Iteration 102, loss = 0.55188527\n",
            "Iteration 103, loss = 0.54692138\n",
            "Iteration 104, loss = 0.54910234\n",
            "Iteration 105, loss = 0.54738149\n",
            "Iteration 106, loss = 0.55003550\n",
            "Iteration 107, loss = 0.55641820\n",
            "Iteration 108, loss = 0.55772513\n",
            "Iteration 109, loss = 0.55294959\n",
            "Iteration 110, loss = 0.55232051\n",
            "Iteration 111, loss = 0.55080197\n",
            "Iteration 112, loss = 0.55004086\n",
            "Iteration 113, loss = 0.55174219\n",
            "Iteration 114, loss = 0.54933813\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.88145579\n",
            "Iteration 2, loss = 0.67054655\n",
            "Iteration 3, loss = 0.63711016\n",
            "Iteration 4, loss = 0.60978361\n",
            "Iteration 5, loss = 0.60434687\n",
            "Iteration 6, loss = 0.60222687\n",
            "Iteration 7, loss = 0.59959699\n",
            "Iteration 8, loss = 0.60001412\n",
            "Iteration 9, loss = 0.59851365\n",
            "Iteration 10, loss = 0.59761973\n",
            "Iteration 11, loss = 0.59692407\n",
            "Iteration 12, loss = 0.59578193\n",
            "Iteration 13, loss = 0.59443632\n",
            "Iteration 14, loss = 0.59548080\n",
            "Iteration 15, loss = 0.59506175\n",
            "Iteration 16, loss = 0.59427563\n",
            "Iteration 17, loss = 0.59322750\n",
            "Iteration 18, loss = 0.59978335\n",
            "Iteration 19, loss = 0.59296659\n",
            "Iteration 20, loss = 0.59180143\n",
            "Iteration 21, loss = 0.59102553\n",
            "Iteration 22, loss = 0.59135924\n",
            "Iteration 23, loss = 0.59183002\n",
            "Iteration 24, loss = 0.59095261\n",
            "Iteration 25, loss = 0.59013586\n",
            "Iteration 26, loss = 0.59029487\n",
            "Iteration 27, loss = 0.58888712\n",
            "Iteration 28, loss = 0.58833149\n",
            "Iteration 29, loss = 0.58870447\n",
            "Iteration 30, loss = 0.58846807\n",
            "Iteration 31, loss = 0.58907670\n",
            "Iteration 32, loss = 0.58758616\n",
            "Iteration 33, loss = 0.58742278\n",
            "Iteration 34, loss = 0.58721367\n",
            "Iteration 35, loss = 0.58599716\n",
            "Iteration 36, loss = 0.58610473\n",
            "Iteration 37, loss = 0.58696707\n",
            "Iteration 38, loss = 0.58568941\n",
            "Iteration 39, loss = 0.58614204\n",
            "Iteration 40, loss = 0.58490962\n",
            "Iteration 41, loss = 0.58359577\n",
            "Iteration 42, loss = 0.58391540\n",
            "Iteration 43, loss = 0.58499376\n",
            "Iteration 44, loss = 0.58374604\n",
            "Iteration 45, loss = 0.58334591\n",
            "Iteration 46, loss = 0.58273840\n",
            "Iteration 47, loss = 0.58177929\n",
            "Iteration 48, loss = 0.58112866\n",
            "Iteration 49, loss = 0.58125231\n",
            "Iteration 50, loss = 0.57953084\n",
            "Iteration 51, loss = 0.57910630\n",
            "Iteration 52, loss = 0.57817848\n",
            "Iteration 53, loss = 0.57814889\n",
            "Iteration 54, loss = 0.57758061\n",
            "Iteration 55, loss = 0.58035152\n",
            "Iteration 56, loss = 0.58090035\n",
            "Iteration 57, loss = 0.58218263\n",
            "Iteration 58, loss = 0.57942212\n",
            "Iteration 59, loss = 0.57798336\n",
            "Iteration 60, loss = 0.57722491\n",
            "Iteration 61, loss = 0.57692789\n",
            "Iteration 62, loss = 0.57698057\n",
            "Iteration 63, loss = 0.57355519\n",
            "Iteration 64, loss = 0.57406145\n",
            "Iteration 65, loss = 0.57357399\n",
            "Iteration 66, loss = 0.57469780\n",
            "Iteration 67, loss = 0.57244931\n",
            "Iteration 68, loss = 0.57286898\n",
            "Iteration 69, loss = 0.57501180\n",
            "Iteration 70, loss = 0.57296565\n",
            "Iteration 71, loss = 0.57365377\n",
            "Iteration 72, loss = 0.57189212\n",
            "Iteration 73, loss = 0.57236219\n",
            "Iteration 74, loss = 0.56800207\n",
            "Iteration 75, loss = 0.56913332\n",
            "Iteration 76, loss = 0.56771256\n",
            "Iteration 77, loss = 0.56958359\n",
            "Iteration 78, loss = 0.56748802\n",
            "Iteration 79, loss = 0.56662849\n",
            "Iteration 80, loss = 0.56678750\n",
            "Iteration 81, loss = 0.57370835\n",
            "Iteration 82, loss = 0.56770027\n",
            "Iteration 83, loss = 0.57356055\n",
            "Iteration 84, loss = 0.56378326\n",
            "Iteration 85, loss = 0.56364459\n",
            "Iteration 86, loss = 0.56419600\n",
            "Iteration 87, loss = 0.56456044\n",
            "Iteration 88, loss = 0.56535820\n",
            "Iteration 89, loss = 0.56097284\n",
            "Iteration 90, loss = 0.56292333\n",
            "Iteration 91, loss = 0.56388319\n",
            "Iteration 92, loss = 0.56207230\n",
            "Iteration 93, loss = 0.56116037\n",
            "Iteration 94, loss = 0.56480901\n",
            "Iteration 95, loss = 0.56456284\n",
            "Iteration 96, loss = 0.56008052\n",
            "Iteration 97, loss = 0.56293973\n",
            "Iteration 98, loss = 0.56629584\n",
            "Iteration 99, loss = 0.56328754\n",
            "Iteration 100, loss = 0.56133984\n",
            "Iteration 101, loss = 0.55767518\n",
            "Iteration 102, loss = 0.56376791\n",
            "Iteration 103, loss = 0.55572619\n",
            "Iteration 104, loss = 0.55548955\n",
            "Iteration 105, loss = 0.55639959\n",
            "Iteration 106, loss = 0.55406037\n",
            "Iteration 107, loss = 0.55874678\n",
            "Iteration 108, loss = 0.56348001\n",
            "Iteration 109, loss = 0.55436527\n",
            "Iteration 110, loss = 0.55495220\n",
            "Iteration 111, loss = 0.55217314\n",
            "Iteration 112, loss = 0.55001147\n",
            "Iteration 113, loss = 0.55051065\n",
            "Iteration 114, loss = 0.54886547\n",
            "Iteration 115, loss = 0.55115215\n",
            "Iteration 116, loss = 0.55293490\n",
            "Iteration 117, loss = 0.54823079\n",
            "Iteration 118, loss = 0.54882566\n",
            "Iteration 119, loss = 0.54917533\n",
            "Iteration 120, loss = 0.54651691\n",
            "Iteration 121, loss = 0.54810693\n",
            "Iteration 122, loss = 0.55574312\n",
            "Iteration 123, loss = 0.55249748\n",
            "Iteration 124, loss = 0.55487076\n",
            "Iteration 125, loss = 0.54643075\n",
            "Iteration 126, loss = 0.54553999\n",
            "Iteration 127, loss = 0.54670202\n",
            "Iteration 128, loss = 0.55132164\n",
            "Iteration 129, loss = 0.54513174\n",
            "Iteration 130, loss = 0.54820096\n",
            "Iteration 131, loss = 0.54488451\n",
            "Iteration 132, loss = 0.53990197\n",
            "Iteration 133, loss = 0.54170430\n",
            "Iteration 134, loss = 0.54097968\n",
            "Iteration 135, loss = 0.53865179\n",
            "Iteration 136, loss = 0.54157452\n",
            "Iteration 137, loss = 0.54444562\n",
            "Iteration 138, loss = 0.54275430\n",
            "Iteration 139, loss = 0.53872531\n",
            "Iteration 140, loss = 0.53536397\n",
            "Iteration 141, loss = 0.53458054\n",
            "Iteration 142, loss = 0.53266625\n",
            "Iteration 143, loss = 0.53925866\n",
            "Iteration 144, loss = 0.55912724\n",
            "Iteration 145, loss = 0.55303380\n",
            "Iteration 146, loss = 0.54276560\n",
            "Iteration 147, loss = 0.54116547\n",
            "Iteration 148, loss = 0.53923426\n",
            "Iteration 149, loss = 0.54224555\n",
            "Iteration 150, loss = 0.54173080\n",
            "Iteration 151, loss = 0.55224050\n",
            "Iteration 152, loss = 0.55347399\n",
            "Iteration 153, loss = 0.54626814\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.65643568\n",
            "Iteration 2, loss = 0.63746827\n",
            "Iteration 3, loss = 0.62541470\n",
            "Iteration 4, loss = 0.61897485\n",
            "Iteration 5, loss = 0.61560287\n",
            "Iteration 6, loss = 0.61342119\n",
            "Iteration 7, loss = 0.61164514\n",
            "Iteration 8, loss = 0.60767960\n",
            "Iteration 9, loss = 0.60773752\n",
            "Iteration 10, loss = 0.60805468\n",
            "Iteration 11, loss = 0.60461061\n",
            "Iteration 12, loss = 0.60266671\n",
            "Iteration 13, loss = 0.60370210\n",
            "Iteration 14, loss = 0.60180097\n",
            "Iteration 15, loss = 0.60529435\n",
            "Iteration 16, loss = 0.59874293\n",
            "Iteration 17, loss = 0.60050513\n",
            "Iteration 18, loss = 0.59764438\n",
            "Iteration 19, loss = 0.60300098\n",
            "Iteration 20, loss = 0.59434261\n",
            "Iteration 21, loss = 0.59474253\n",
            "Iteration 22, loss = 0.59470612\n",
            "Iteration 23, loss = 0.59212598\n",
            "Iteration 24, loss = 0.59143356\n",
            "Iteration 25, loss = 0.59036915\n",
            "Iteration 26, loss = 0.59179375\n",
            "Iteration 27, loss = 0.59030207\n",
            "Iteration 28, loss = 0.58625327\n",
            "Iteration 29, loss = 0.58903212\n",
            "Iteration 30, loss = 0.58504871\n",
            "Iteration 31, loss = 0.58372473\n",
            "Iteration 32, loss = 0.59916507\n",
            "Iteration 33, loss = 0.58963285\n",
            "Iteration 34, loss = 0.58219218\n",
            "Iteration 35, loss = 0.58328991\n",
            "Iteration 36, loss = 0.57913489\n",
            "Iteration 37, loss = 0.57816192\n",
            "Iteration 38, loss = 0.57735050\n",
            "Iteration 39, loss = 0.57614406\n",
            "Iteration 40, loss = 0.57584383\n",
            "Iteration 41, loss = 0.57473557\n",
            "Iteration 42, loss = 0.58141910\n",
            "Iteration 43, loss = 0.57591932\n",
            "Iteration 44, loss = 0.57460362\n",
            "Iteration 45, loss = 0.57493335\n",
            "Iteration 46, loss = 0.56912085\n",
            "Iteration 47, loss = 0.57086649\n",
            "Iteration 48, loss = 0.56946150\n",
            "Iteration 49, loss = 0.57437380\n",
            "Iteration 50, loss = 0.56940852\n",
            "Iteration 51, loss = 0.58285365\n",
            "Iteration 52, loss = 0.56668813\n",
            "Iteration 53, loss = 0.57065663\n",
            "Iteration 54, loss = 0.57790483\n",
            "Iteration 55, loss = 0.56608390\n",
            "Iteration 56, loss = 0.56822701\n",
            "Iteration 57, loss = 0.57982278\n",
            "Iteration 58, loss = 0.57632704\n",
            "Iteration 59, loss = 0.56412319\n",
            "Iteration 60, loss = 0.57394181\n",
            "Iteration 61, loss = 0.56918532\n",
            "Iteration 62, loss = 0.56877720\n",
            "Iteration 63, loss = 0.56729672\n",
            "Iteration 64, loss = 0.56150308\n",
            "Iteration 65, loss = 0.56487744\n",
            "Iteration 66, loss = 0.56229538\n",
            "Iteration 67, loss = 0.55652484\n",
            "Iteration 68, loss = 0.56028095\n",
            "Iteration 69, loss = 0.55056440\n",
            "Iteration 70, loss = 0.56877128\n",
            "Iteration 71, loss = 0.58087132\n",
            "Iteration 72, loss = 0.57371297\n",
            "Iteration 73, loss = 0.55672983\n",
            "Iteration 74, loss = 0.55454134\n",
            "Iteration 75, loss = 0.54880800\n",
            "Iteration 76, loss = 0.55408686\n",
            "Iteration 77, loss = 0.57966806\n",
            "Iteration 78, loss = 0.56177225\n",
            "Iteration 79, loss = 0.55032940\n",
            "Iteration 80, loss = 0.55471614\n",
            "Iteration 81, loss = 0.54445103\n",
            "Iteration 82, loss = 0.59228290\n",
            "Iteration 83, loss = 0.56701700\n",
            "Iteration 84, loss = 0.55880331\n",
            "Iteration 85, loss = 0.55200417\n",
            "Iteration 86, loss = 0.55047785\n",
            "Iteration 87, loss = 0.54301635\n",
            "Iteration 88, loss = 0.54749723\n",
            "Iteration 89, loss = 0.54959834\n",
            "Iteration 90, loss = 0.53727905\n",
            "Iteration 91, loss = 0.53842788\n",
            "Iteration 92, loss = 0.56561057\n",
            "Iteration 93, loss = 0.53661175\n",
            "Iteration 94, loss = 0.52960558\n",
            "Iteration 95, loss = 0.54632863\n",
            "Iteration 96, loss = 0.53700573\n",
            "Iteration 97, loss = 0.53255525\n",
            "Iteration 98, loss = 0.55498222\n",
            "Iteration 99, loss = 0.54238224\n",
            "Iteration 100, loss = 0.54172402\n",
            "Iteration 101, loss = 0.53991987\n",
            "Iteration 102, loss = 0.53145354\n",
            "Iteration 103, loss = 0.53373450\n",
            "Iteration 104, loss = 0.53348854\n",
            "Iteration 105, loss = 0.52933568\n",
            "Iteration 106, loss = 0.54463774\n",
            "Iteration 107, loss = 0.52754998\n",
            "Iteration 108, loss = 0.52649249\n",
            "Iteration 109, loss = 0.53622973\n",
            "Iteration 110, loss = 0.52657217\n",
            "Iteration 111, loss = 0.52878365\n",
            "Iteration 112, loss = 0.51991410\n",
            "Iteration 113, loss = 0.53131412\n",
            "Iteration 114, loss = 0.54070612\n",
            "Iteration 115, loss = 0.51495654\n",
            "Iteration 116, loss = 0.52223529\n",
            "Iteration 117, loss = 0.53115124\n",
            "Iteration 118, loss = 0.52865699\n",
            "Iteration 119, loss = 0.51041319\n",
            "Iteration 120, loss = 0.60099507\n",
            "Iteration 121, loss = 0.57568918\n",
            "Iteration 122, loss = 0.56181424\n",
            "Iteration 123, loss = 0.53710873\n",
            "Iteration 124, loss = 0.54865429\n",
            "Iteration 125, loss = 0.57940499\n",
            "Iteration 126, loss = 0.55033504\n",
            "Iteration 127, loss = 0.52506074\n",
            "Iteration 128, loss = 0.52518377\n",
            "Iteration 129, loss = 0.52066756\n",
            "Iteration 130, loss = 0.54183283\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.74701135\n",
            "Iteration 2, loss = 0.61957444\n",
            "Iteration 3, loss = 0.60113460\n",
            "Iteration 4, loss = 0.59762926\n",
            "Iteration 5, loss = 0.59249193\n",
            "Iteration 6, loss = 0.59001742\n",
            "Iteration 7, loss = 0.58877012\n",
            "Iteration 8, loss = 0.58788114\n",
            "Iteration 9, loss = 0.58592340\n",
            "Iteration 10, loss = 0.58552370\n",
            "Iteration 11, loss = 0.58539774\n",
            "Iteration 12, loss = 0.58407494\n",
            "Iteration 13, loss = 0.58287013\n",
            "Iteration 14, loss = 0.58231475\n",
            "Iteration 15, loss = 0.58238907\n",
            "Iteration 16, loss = 0.58271002\n",
            "Iteration 17, loss = 0.58109857\n",
            "Iteration 18, loss = 0.58047211\n",
            "Iteration 19, loss = 0.57832200\n",
            "Iteration 20, loss = 0.57915961\n",
            "Iteration 21, loss = 0.57708029\n",
            "Iteration 22, loss = 0.57609956\n",
            "Iteration 23, loss = 0.57553464\n",
            "Iteration 24, loss = 0.57436038\n",
            "Iteration 25, loss = 0.57605107\n",
            "Iteration 26, loss = 0.57561514\n",
            "Iteration 27, loss = 0.57454554\n",
            "Iteration 28, loss = 0.57334629\n",
            "Iteration 29, loss = 0.57297432\n",
            "Iteration 30, loss = 0.57362130\n",
            "Iteration 31, loss = 0.57267710\n",
            "Iteration 32, loss = 0.57189088\n",
            "Iteration 33, loss = 0.57217135\n",
            "Iteration 34, loss = 0.57246049\n",
            "Iteration 35, loss = 0.57052003\n",
            "Iteration 36, loss = 0.56987289\n",
            "Iteration 37, loss = 0.56983990\n",
            "Iteration 38, loss = 0.57192990\n",
            "Iteration 39, loss = 0.56885128\n",
            "Iteration 40, loss = 0.57348976\n",
            "Iteration 41, loss = 0.56650713\n",
            "Iteration 42, loss = 0.56759689\n",
            "Iteration 43, loss = 0.56718018\n",
            "Iteration 44, loss = 0.56542866\n",
            "Iteration 45, loss = 0.57207837\n",
            "Iteration 46, loss = 0.56779213\n",
            "Iteration 47, loss = 0.56676434\n",
            "Iteration 48, loss = 0.56850726\n",
            "Iteration 49, loss = 0.56672708\n",
            "Iteration 50, loss = 0.56509571\n",
            "Iteration 51, loss = 0.56296659\n",
            "Iteration 52, loss = 0.56153727\n",
            "Iteration 53, loss = 0.56063008\n",
            "Iteration 54, loss = 0.56298523\n",
            "Iteration 55, loss = 0.56189366\n",
            "Iteration 56, loss = 0.55937702\n",
            "Iteration 57, loss = 0.55985240\n",
            "Iteration 58, loss = 0.56170239\n",
            "Iteration 59, loss = 0.55781878\n",
            "Iteration 60, loss = 0.55721655\n",
            "Iteration 61, loss = 0.55683611\n",
            "Iteration 62, loss = 0.55540455\n",
            "Iteration 63, loss = 0.55575370\n",
            "Iteration 64, loss = 0.55469840\n",
            "Iteration 65, loss = 0.55781794\n",
            "Iteration 66, loss = 0.55536525\n",
            "Iteration 67, loss = 0.56391243\n",
            "Iteration 68, loss = 0.55822578\n",
            "Iteration 69, loss = 0.55648664\n",
            "Iteration 70, loss = 0.55708463\n",
            "Iteration 71, loss = 0.55536480\n",
            "Iteration 72, loss = 0.55610877\n",
            "Iteration 73, loss = 0.55450146\n",
            "Iteration 74, loss = 0.55843926\n",
            "Iteration 75, loss = 0.56110168\n",
            "Iteration 76, loss = 0.55886688\n",
            "Iteration 77, loss = 0.55669181\n",
            "Iteration 78, loss = 0.55226596\n",
            "Iteration 79, loss = 0.55433015\n",
            "Iteration 80, loss = 0.55180949\n",
            "Iteration 81, loss = 0.55091800\n",
            "Iteration 82, loss = 0.55824920\n",
            "Iteration 83, loss = 0.54885805\n",
            "Iteration 84, loss = 0.55016950\n",
            "Iteration 85, loss = 0.54747884\n",
            "Iteration 86, loss = 0.54622256\n",
            "Iteration 87, loss = 0.54665557\n",
            "Iteration 88, loss = 0.55111478\n",
            "Iteration 89, loss = 0.54761640\n",
            "Iteration 90, loss = 0.54696974\n",
            "Iteration 91, loss = 0.54513157\n",
            "Iteration 92, loss = 0.54355537\n",
            "Iteration 93, loss = 0.54524270\n",
            "Iteration 94, loss = 0.54484142\n",
            "Iteration 95, loss = 0.54306985\n",
            "Iteration 96, loss = 0.54941474\n",
            "Iteration 97, loss = 0.54492863\n",
            "Iteration 98, loss = 0.54106394\n",
            "Iteration 99, loss = 0.53967472\n",
            "Iteration 100, loss = 0.53947566\n",
            "Iteration 101, loss = 0.54379314\n",
            "Iteration 102, loss = 0.54306902\n",
            "Iteration 103, loss = 0.54654728\n",
            "Iteration 104, loss = 0.53906690\n",
            "Iteration 105, loss = 0.53717490\n",
            "Iteration 106, loss = 0.54081301\n",
            "Iteration 107, loss = 0.53801372\n",
            "Iteration 108, loss = 0.53654027\n",
            "Iteration 109, loss = 0.54941913\n",
            "Iteration 110, loss = 0.53640700\n",
            "Iteration 111, loss = 0.53622335\n",
            "Iteration 112, loss = 0.53236090\n",
            "Iteration 113, loss = 0.53320911\n",
            "Iteration 114, loss = 0.54537889\n",
            "Iteration 115, loss = 0.53504270\n",
            "Iteration 116, loss = 0.54919301\n",
            "Iteration 117, loss = 0.52677060\n",
            "Iteration 118, loss = 0.53274379\n",
            "Iteration 119, loss = 0.53117896\n",
            "Iteration 120, loss = 0.52607525\n",
            "Iteration 121, loss = 0.52609891\n",
            "Iteration 122, loss = 0.52214778\n",
            "Iteration 123, loss = 0.52684096\n",
            "Iteration 124, loss = 0.52421662\n",
            "Iteration 125, loss = 0.51976130\n",
            "Iteration 126, loss = 0.55606476\n",
            "Iteration 127, loss = 0.53826101\n",
            "Iteration 128, loss = 0.52741839\n",
            "Iteration 129, loss = 0.52034910\n",
            "Iteration 130, loss = 0.52188242\n",
            "Iteration 131, loss = 0.51821611\n",
            "Iteration 132, loss = 0.51578985\n",
            "Iteration 133, loss = 0.52549141\n",
            "Iteration 134, loss = 0.51963858\n",
            "Iteration 135, loss = 0.51625977\n",
            "Iteration 136, loss = 0.51699581\n",
            "Iteration 137, loss = 0.51376464\n",
            "Iteration 138, loss = 0.51922709\n",
            "Iteration 139, loss = 0.51530852\n",
            "Iteration 140, loss = 0.51743036\n",
            "Iteration 141, loss = 0.51834798\n",
            "Iteration 142, loss = 0.52590489\n",
            "Iteration 143, loss = 0.51491353\n",
            "Iteration 144, loss = 0.51508145\n",
            "Iteration 145, loss = 0.51595896\n",
            "Iteration 146, loss = 0.51145102\n",
            "Iteration 147, loss = 0.51540711\n",
            "Iteration 148, loss = 0.52688640\n",
            "Iteration 149, loss = 0.50990352\n",
            "Iteration 150, loss = 0.51025355\n",
            "Iteration 151, loss = 0.51260115\n",
            "Iteration 152, loss = 0.50736381\n",
            "Iteration 153, loss = 0.50424616\n",
            "Iteration 154, loss = 0.51005317\n",
            "Iteration 155, loss = 0.51119027\n",
            "Iteration 156, loss = 0.53353094\n",
            "Iteration 157, loss = 0.51856683\n",
            "Iteration 158, loss = 0.50193863\n",
            "Iteration 159, loss = 0.51310371\n",
            "Iteration 160, loss = 0.51268904\n",
            "Iteration 161, loss = 0.50395601\n",
            "Iteration 162, loss = 0.50891584\n",
            "Iteration 163, loss = 0.51499333\n",
            "Iteration 164, loss = 0.50833212\n",
            "Iteration 165, loss = 0.51529406\n",
            "Iteration 166, loss = 0.51668517\n",
            "Iteration 167, loss = 0.51003288\n",
            "Iteration 168, loss = 0.50335434\n",
            "Iteration 169, loss = 0.49111556\n",
            "Iteration 170, loss = 0.49251377\n",
            "Iteration 171, loss = 0.51296145\n",
            "Iteration 172, loss = 0.49371389\n",
            "Iteration 173, loss = 0.49440425\n",
            "Iteration 174, loss = 0.49353409\n",
            "Iteration 175, loss = 0.48904266\n",
            "Iteration 176, loss = 0.49347197\n",
            "Iteration 177, loss = 0.48258597\n",
            "Iteration 178, loss = 0.51030644\n",
            "Iteration 179, loss = 0.49706979\n",
            "Iteration 180, loss = 0.49372034\n",
            "Iteration 181, loss = 0.48408878\n",
            "Iteration 182, loss = 0.49854379\n",
            "Iteration 183, loss = 0.49134866\n",
            "Iteration 184, loss = 0.57366499\n",
            "Iteration 185, loss = 0.51190249\n",
            "Iteration 186, loss = 0.50619979\n",
            "Iteration 187, loss = 0.49966457\n",
            "Iteration 188, loss = 0.51952911\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.82572860\n",
            "Iteration 2, loss = 0.63824106\n",
            "Iteration 3, loss = 0.62363961\n",
            "Iteration 4, loss = 0.61609027\n",
            "Iteration 5, loss = 0.61142625\n",
            "Iteration 6, loss = 0.60835688\n",
            "Iteration 7, loss = 0.60714796\n",
            "Iteration 8, loss = 0.60710412\n",
            "Iteration 9, loss = 0.60602009\n",
            "Iteration 10, loss = 0.60583167\n",
            "Iteration 11, loss = 0.60389548\n",
            "Iteration 12, loss = 0.60692503\n",
            "Iteration 13, loss = 0.60349152\n",
            "Iteration 14, loss = 0.60326074\n",
            "Iteration 15, loss = 0.60265577\n",
            "Iteration 16, loss = 0.60030904\n",
            "Iteration 17, loss = 0.60811469\n",
            "Iteration 18, loss = 0.59904123\n",
            "Iteration 19, loss = 0.60467552\n",
            "Iteration 20, loss = 0.60182070\n",
            "Iteration 21, loss = 0.59919705\n",
            "Iteration 22, loss = 0.59923035\n",
            "Iteration 23, loss = 0.59810155\n",
            "Iteration 24, loss = 0.59750229\n",
            "Iteration 25, loss = 0.59814694\n",
            "Iteration 26, loss = 0.60088977\n",
            "Iteration 27, loss = 0.59768181\n",
            "Iteration 28, loss = 0.60059001\n",
            "Iteration 29, loss = 0.59759156\n",
            "Iteration 30, loss = 0.59510014\n",
            "Iteration 31, loss = 0.59954412\n",
            "Iteration 32, loss = 0.59538789\n",
            "Iteration 33, loss = 0.59374711\n",
            "Iteration 34, loss = 0.59268766\n",
            "Iteration 35, loss = 0.59593495\n",
            "Iteration 36, loss = 0.59340706\n",
            "Iteration 37, loss = 0.59388907\n",
            "Iteration 38, loss = 0.59305519\n",
            "Iteration 39, loss = 0.59188074\n",
            "Iteration 40, loss = 0.59236798\n",
            "Iteration 41, loss = 0.59188933\n",
            "Iteration 42, loss = 0.59707274\n",
            "Iteration 43, loss = 0.58696302\n",
            "Iteration 44, loss = 0.59261341\n",
            "Iteration 45, loss = 0.58859971\n",
            "Iteration 46, loss = 0.58815450\n",
            "Iteration 47, loss = 0.58716415\n",
            "Iteration 48, loss = 0.58722896\n",
            "Iteration 49, loss = 0.58760383\n",
            "Iteration 50, loss = 0.58872071\n",
            "Iteration 51, loss = 0.58884388\n",
            "Iteration 52, loss = 0.58561641\n",
            "Iteration 53, loss = 0.58506765\n",
            "Iteration 54, loss = 0.58937692\n",
            "Iteration 55, loss = 0.58533841\n",
            "Iteration 56, loss = 0.58441831\n",
            "Iteration 57, loss = 0.58611196\n",
            "Iteration 58, loss = 0.58215230\n",
            "Iteration 59, loss = 0.59309239\n",
            "Iteration 60, loss = 0.58936018\n",
            "Iteration 61, loss = 0.58334570\n",
            "Iteration 62, loss = 0.58209390\n",
            "Iteration 63, loss = 0.58440338\n",
            "Iteration 64, loss = 0.58425361\n",
            "Iteration 65, loss = 0.58607842\n",
            "Iteration 66, loss = 0.59603688\n",
            "Iteration 67, loss = 0.58650214\n",
            "Iteration 68, loss = 0.58322009\n",
            "Iteration 69, loss = 0.58700650\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64815831\n",
            "Iteration 2, loss = 0.60754033\n",
            "Iteration 3, loss = 0.60498638\n",
            "Iteration 4, loss = 0.60121752\n",
            "Iteration 5, loss = 0.60322703\n",
            "Iteration 6, loss = 0.59664798\n",
            "Iteration 7, loss = 0.59546849\n",
            "Iteration 8, loss = 0.59174193\n",
            "Iteration 9, loss = 0.58953329\n",
            "Iteration 10, loss = 0.58884618\n",
            "Iteration 11, loss = 0.58776358\n",
            "Iteration 12, loss = 0.59030829\n",
            "Iteration 13, loss = 0.58439763\n",
            "Iteration 14, loss = 0.57726857\n",
            "Iteration 15, loss = 0.57644210\n",
            "Iteration 16, loss = 0.57536375\n",
            "Iteration 17, loss = 0.57341493\n",
            "Iteration 18, loss = 0.57067143\n",
            "Iteration 19, loss = 0.56853306\n",
            "Iteration 20, loss = 0.57103110\n",
            "Iteration 21, loss = 0.56963529\n",
            "Iteration 22, loss = 0.56846207\n",
            "Iteration 23, loss = 0.56518400\n",
            "Iteration 24, loss = 0.56818464\n",
            "Iteration 25, loss = 0.56228162\n",
            "Iteration 26, loss = 0.56134940\n",
            "Iteration 27, loss = 0.56102059\n",
            "Iteration 28, loss = 0.56273731\n",
            "Iteration 29, loss = 0.55885944\n",
            "Iteration 30, loss = 0.56881951\n",
            "Iteration 31, loss = 0.56017814\n",
            "Iteration 32, loss = 0.55481492\n",
            "Iteration 33, loss = 0.57095232\n",
            "Iteration 34, loss = 0.55367004\n",
            "Iteration 35, loss = 0.55173764\n",
            "Iteration 36, loss = 0.55574406\n",
            "Iteration 37, loss = 0.54940040\n",
            "Iteration 38, loss = 0.55629677\n",
            "Iteration 39, loss = 0.55202181\n",
            "Iteration 40, loss = 0.54878498\n",
            "Iteration 41, loss = 0.54617171\n",
            "Iteration 42, loss = 0.54403013\n",
            "Iteration 43, loss = 0.54180186\n",
            "Iteration 44, loss = 0.53858656\n",
            "Iteration 45, loss = 0.53935431\n",
            "Iteration 46, loss = 0.54668012\n",
            "Iteration 47, loss = 0.53797255\n",
            "Iteration 48, loss = 0.54194818\n",
            "Iteration 49, loss = 0.54075697\n",
            "Iteration 50, loss = 0.53697027\n",
            "Iteration 51, loss = 0.55525649\n",
            "Iteration 52, loss = 0.53877116\n",
            "Iteration 53, loss = 0.53271989\n",
            "Iteration 54, loss = 0.53710706\n",
            "Iteration 55, loss = 0.54893090\n",
            "Iteration 56, loss = 0.54581308\n",
            "Iteration 57, loss = 0.52682795\n",
            "Iteration 58, loss = 0.52725806\n",
            "Iteration 59, loss = 0.52536731\n",
            "Iteration 60, loss = 0.52168883\n",
            "Iteration 61, loss = 0.54056752\n",
            "Iteration 62, loss = 0.55323897\n",
            "Iteration 63, loss = 0.54320201\n",
            "Iteration 64, loss = 0.54015606\n",
            "Iteration 65, loss = 0.53791039\n",
            "Iteration 66, loss = 0.52747545\n",
            "Iteration 67, loss = 0.52917656\n",
            "Iteration 68, loss = 0.51951890\n",
            "Iteration 69, loss = 0.51799058\n",
            "Iteration 70, loss = 0.53453360\n",
            "Iteration 71, loss = 0.52473220\n",
            "Iteration 72, loss = 0.52560989\n",
            "Iteration 73, loss = 0.52316279\n",
            "Iteration 74, loss = 0.51279075\n",
            "Iteration 75, loss = 0.51606904\n",
            "Iteration 76, loss = 0.52427355\n",
            "Iteration 77, loss = 0.50809649\n",
            "Iteration 78, loss = 0.50403021\n",
            "Iteration 79, loss = 0.53967252\n",
            "Iteration 80, loss = 0.50939235\n",
            "Iteration 81, loss = 0.52381268\n",
            "Iteration 82, loss = 0.50684049\n",
            "Iteration 83, loss = 0.50909532\n",
            "Iteration 84, loss = 0.51256307\n",
            "Iteration 85, loss = 0.50196208\n",
            "Iteration 86, loss = 0.52100430\n",
            "Iteration 87, loss = 0.50258577\n",
            "Iteration 88, loss = 0.50342117\n",
            "Iteration 89, loss = 0.51372572\n",
            "Iteration 90, loss = 0.50961353\n",
            "Iteration 91, loss = 0.49915038\n",
            "Iteration 92, loss = 0.49951770\n",
            "Iteration 93, loss = 0.49412610\n",
            "Iteration 94, loss = 0.49976254\n",
            "Iteration 95, loss = 0.50605792\n",
            "Iteration 96, loss = 0.49845146\n",
            "Iteration 97, loss = 0.48575613\n",
            "Iteration 98, loss = 0.52304218\n",
            "Iteration 99, loss = 0.50009205\n",
            "Iteration 100, loss = 0.48733784\n",
            "Iteration 101, loss = 0.49680538\n",
            "Iteration 102, loss = 0.49923057\n",
            "Iteration 103, loss = 0.50218618\n",
            "Iteration 104, loss = 0.49649972\n",
            "Iteration 105, loss = 0.48535735\n",
            "Iteration 106, loss = 0.48389826\n",
            "Iteration 107, loss = 0.48104677\n",
            "Iteration 108, loss = 0.52429702\n",
            "Iteration 109, loss = 0.49344565\n",
            "Iteration 110, loss = 0.48423223\n",
            "Iteration 111, loss = 0.49017558\n",
            "Iteration 112, loss = 0.50361370\n",
            "Iteration 113, loss = 0.48196216\n",
            "Iteration 114, loss = 0.48838239\n",
            "Iteration 115, loss = 0.50109172\n",
            "Iteration 116, loss = 0.47659284\n",
            "Iteration 117, loss = 0.48204558\n",
            "Iteration 118, loss = 0.49377164\n",
            "Iteration 119, loss = 0.48087636\n",
            "Iteration 120, loss = 0.47454260\n",
            "Iteration 121, loss = 0.52208288\n",
            "Iteration 122, loss = 0.50685254\n",
            "Iteration 123, loss = 0.49929285\n",
            "Iteration 124, loss = 0.49377908\n",
            "Iteration 125, loss = 0.52444879\n",
            "Iteration 126, loss = 0.51037207\n",
            "Iteration 127, loss = 0.49628944\n",
            "Iteration 128, loss = 0.50659881\n",
            "Iteration 129, loss = 0.48811072\n",
            "Iteration 130, loss = 0.48638243\n",
            "Iteration 131, loss = 0.47332824\n",
            "Iteration 132, loss = 0.48562342\n",
            "Iteration 133, loss = 0.53881065\n",
            "Iteration 134, loss = 0.48183913\n",
            "Iteration 135, loss = 0.47397981\n",
            "Iteration 136, loss = 0.48507082\n",
            "Iteration 137, loss = 0.49309964\n",
            "Iteration 138, loss = 0.52014086\n",
            "Iteration 139, loss = 0.48643689\n",
            "Iteration 140, loss = 0.48302837\n",
            "Iteration 141, loss = 0.48082192\n",
            "Iteration 142, loss = 0.49529165\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70814946\n",
            "Iteration 2, loss = 0.63293816\n",
            "Iteration 3, loss = 0.61496787\n",
            "Iteration 4, loss = 0.60665948\n",
            "Iteration 5, loss = 0.60334820\n",
            "Iteration 6, loss = 0.59924071\n",
            "Iteration 7, loss = 0.60041610\n",
            "Iteration 8, loss = 0.59815376\n",
            "Iteration 9, loss = 0.59681051\n",
            "Iteration 10, loss = 0.59858115\n",
            "Iteration 11, loss = 0.59317474\n",
            "Iteration 12, loss = 0.59167197\n",
            "Iteration 13, loss = 0.59576534\n",
            "Iteration 14, loss = 0.58896456\n",
            "Iteration 15, loss = 0.59292921\n",
            "Iteration 16, loss = 0.58828749\n",
            "Iteration 17, loss = 0.59257408\n",
            "Iteration 18, loss = 0.58571991\n",
            "Iteration 19, loss = 0.58754343\n",
            "Iteration 20, loss = 0.58517824\n",
            "Iteration 21, loss = 0.58320643\n",
            "Iteration 22, loss = 0.58320429\n",
            "Iteration 23, loss = 0.58095977\n",
            "Iteration 24, loss = 0.58096498\n",
            "Iteration 25, loss = 0.58554261\n",
            "Iteration 26, loss = 0.57946354\n",
            "Iteration 27, loss = 0.58312320\n",
            "Iteration 28, loss = 0.57747941\n",
            "Iteration 29, loss = 0.57786955\n",
            "Iteration 30, loss = 0.57805418\n",
            "Iteration 31, loss = 0.57658279\n",
            "Iteration 32, loss = 0.57445636\n",
            "Iteration 33, loss = 0.57461713\n",
            "Iteration 34, loss = 0.57288093\n",
            "Iteration 35, loss = 0.57194589\n",
            "Iteration 36, loss = 0.57247716\n",
            "Iteration 37, loss = 0.57215456\n",
            "Iteration 38, loss = 0.57311823\n",
            "Iteration 39, loss = 0.57169926\n",
            "Iteration 40, loss = 0.56811148\n",
            "Iteration 41, loss = 0.56645775\n",
            "Iteration 42, loss = 0.56650733\n",
            "Iteration 43, loss = 0.56486828\n",
            "Iteration 44, loss = 0.56585729\n",
            "Iteration 45, loss = 0.56304564\n",
            "Iteration 46, loss = 0.57063618\n",
            "Iteration 47, loss = 0.56173606\n",
            "Iteration 48, loss = 0.56167836\n",
            "Iteration 49, loss = 0.56034886\n",
            "Iteration 50, loss = 0.56034928\n",
            "Iteration 51, loss = 0.56036769\n",
            "Iteration 52, loss = 0.55965557\n",
            "Iteration 53, loss = 0.55959330\n",
            "Iteration 54, loss = 0.55803397\n",
            "Iteration 55, loss = 0.55940363\n",
            "Iteration 56, loss = 0.56698297\n",
            "Iteration 57, loss = 0.55227905\n",
            "Iteration 58, loss = 0.55921888\n",
            "Iteration 59, loss = 0.55180076\n",
            "Iteration 60, loss = 0.55456447\n",
            "Iteration 61, loss = 0.55679964\n",
            "Iteration 62, loss = 0.55115886\n",
            "Iteration 63, loss = 0.55340133\n",
            "Iteration 64, loss = 0.55187138\n",
            "Iteration 65, loss = 0.55219312\n",
            "Iteration 66, loss = 0.55627227\n",
            "Iteration 67, loss = 0.54617114\n",
            "Iteration 68, loss = 0.54416048\n",
            "Iteration 69, loss = 0.54202947\n",
            "Iteration 70, loss = 0.54182467\n",
            "Iteration 71, loss = 0.54273647\n",
            "Iteration 72, loss = 0.54151903\n",
            "Iteration 73, loss = 0.54913820\n",
            "Iteration 74, loss = 0.54077180\n",
            "Iteration 75, loss = 0.54252121\n",
            "Iteration 76, loss = 0.53621649\n",
            "Iteration 77, loss = 0.54318533\n",
            "Iteration 78, loss = 0.53563354\n",
            "Iteration 79, loss = 0.53865102\n",
            "Iteration 80, loss = 0.53542554\n",
            "Iteration 81, loss = 0.52970144\n",
            "Iteration 82, loss = 0.53437192\n",
            "Iteration 83, loss = 0.53054381\n",
            "Iteration 84, loss = 0.53104403\n",
            "Iteration 85, loss = 0.53232004\n",
            "Iteration 86, loss = 0.52816704\n",
            "Iteration 87, loss = 0.53933081\n",
            "Iteration 88, loss = 0.52684635\n",
            "Iteration 89, loss = 0.52364841\n",
            "Iteration 90, loss = 0.52082150\n",
            "Iteration 91, loss = 0.52479719\n",
            "Iteration 92, loss = 0.52175940\n",
            "Iteration 93, loss = 0.51894306\n",
            "Iteration 94, loss = 0.52955591\n",
            "Iteration 95, loss = 0.51666036\n",
            "Iteration 96, loss = 0.52386623\n",
            "Iteration 97, loss = 0.54873642\n",
            "Iteration 98, loss = 0.51321673\n",
            "Iteration 99, loss = 0.52717287\n",
            "Iteration 100, loss = 0.52650060\n",
            "Iteration 101, loss = 0.52176357\n",
            "Iteration 102, loss = 0.51625106\n",
            "Iteration 103, loss = 0.52022607\n",
            "Iteration 104, loss = 0.51523833\n",
            "Iteration 105, loss = 0.52171544\n",
            "Iteration 106, loss = 0.52585005\n",
            "Iteration 107, loss = 0.51048006\n",
            "Iteration 108, loss = 0.50901277\n",
            "Iteration 109, loss = 0.51375262\n",
            "Iteration 110, loss = 0.50137436\n",
            "Iteration 111, loss = 0.50720593\n",
            "Iteration 112, loss = 0.53388409\n",
            "Iteration 113, loss = 0.52325419\n",
            "Iteration 114, loss = 0.51387454\n",
            "Iteration 115, loss = 0.50484583\n",
            "Iteration 116, loss = 0.55717053\n",
            "Iteration 117, loss = 0.50683083\n",
            "Iteration 118, loss = 0.50556284\n",
            "Iteration 119, loss = 0.49531147\n",
            "Iteration 120, loss = 0.49398948\n",
            "Iteration 121, loss = 0.49743942\n",
            "Iteration 122, loss = 0.50274196\n",
            "Iteration 123, loss = 0.49221022\n",
            "Iteration 124, loss = 0.49436934\n",
            "Iteration 125, loss = 0.51196659\n",
            "Iteration 126, loss = 0.53913901\n",
            "Iteration 127, loss = 0.51026418\n",
            "Iteration 128, loss = 0.50924073\n",
            "Iteration 129, loss = 0.49587597\n",
            "Iteration 130, loss = 0.49402256\n",
            "Iteration 131, loss = 0.48952656\n",
            "Iteration 132, loss = 0.49415724\n",
            "Iteration 133, loss = 0.48968961\n",
            "Iteration 134, loss = 0.49045735\n",
            "Iteration 135, loss = 0.49357127\n",
            "Iteration 136, loss = 0.47942944\n",
            "Iteration 137, loss = 0.48449869\n",
            "Iteration 138, loss = 0.48885611\n",
            "Iteration 139, loss = 0.49056443\n",
            "Iteration 140, loss = 0.48978053\n",
            "Iteration 141, loss = 0.49987404\n",
            "Iteration 142, loss = 0.48508522\n",
            "Iteration 143, loss = 0.47634344\n",
            "Iteration 144, loss = 0.48604778\n",
            "Iteration 145, loss = 0.49441694\n",
            "Iteration 146, loss = 0.48196077\n",
            "Iteration 147, loss = 0.49367762\n",
            "Iteration 148, loss = 0.47409771\n",
            "Iteration 149, loss = 0.48467210\n",
            "Iteration 150, loss = 0.48392181\n",
            "Iteration 151, loss = 0.47753946\n",
            "Iteration 152, loss = 0.50092943\n",
            "Iteration 153, loss = 0.50315541\n",
            "Iteration 154, loss = 0.47935258\n",
            "Iteration 155, loss = 0.47415435\n",
            "Iteration 156, loss = 0.47626149\n",
            "Iteration 157, loss = 0.48517442\n",
            "Iteration 158, loss = 0.48362891\n",
            "Iteration 159, loss = 0.49893678\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64753776\n",
            "Iteration 2, loss = 0.63251255\n",
            "Iteration 3, loss = 0.61173728\n",
            "Iteration 4, loss = 0.61773826\n",
            "Iteration 5, loss = 0.60699097\n",
            "Iteration 6, loss = 0.60874343\n",
            "Iteration 7, loss = 0.60260045\n",
            "Iteration 8, loss = 0.60183263\n",
            "Iteration 9, loss = 0.60686989\n",
            "Iteration 10, loss = 0.59864462\n",
            "Iteration 11, loss = 0.59794230\n",
            "Iteration 12, loss = 0.59709539\n",
            "Iteration 13, loss = 0.59538799\n",
            "Iteration 14, loss = 0.59464802\n",
            "Iteration 15, loss = 0.59367193\n",
            "Iteration 16, loss = 0.59283275\n",
            "Iteration 17, loss = 0.59642045\n",
            "Iteration 18, loss = 0.59778637\n",
            "Iteration 19, loss = 0.59166969\n",
            "Iteration 20, loss = 0.59483689\n",
            "Iteration 21, loss = 0.59406961\n",
            "Iteration 22, loss = 0.58627639\n",
            "Iteration 23, loss = 0.58929858\n",
            "Iteration 24, loss = 0.59067125\n",
            "Iteration 25, loss = 0.58911830\n",
            "Iteration 26, loss = 0.58660435\n",
            "Iteration 27, loss = 0.58594586\n",
            "Iteration 28, loss = 0.58261206\n",
            "Iteration 29, loss = 0.58623296\n",
            "Iteration 30, loss = 0.58238079\n",
            "Iteration 31, loss = 0.58249449\n",
            "Iteration 32, loss = 0.58055689\n",
            "Iteration 33, loss = 0.57831350\n",
            "Iteration 34, loss = 0.58561915\n",
            "Iteration 35, loss = 0.57832064\n",
            "Iteration 36, loss = 0.57744218\n",
            "Iteration 37, loss = 0.57471862\n",
            "Iteration 38, loss = 0.57540489\n",
            "Iteration 39, loss = 0.57325396\n",
            "Iteration 40, loss = 0.57619225\n",
            "Iteration 41, loss = 0.58333970\n",
            "Iteration 42, loss = 0.57478559\n",
            "Iteration 43, loss = 0.56841074\n",
            "Iteration 44, loss = 0.57426391\n",
            "Iteration 45, loss = 0.58073328\n",
            "Iteration 46, loss = 0.57729136\n",
            "Iteration 47, loss = 0.56838620\n",
            "Iteration 48, loss = 0.57256092\n",
            "Iteration 49, loss = 0.56545022\n",
            "Iteration 50, loss = 0.56453028\n",
            "Iteration 51, loss = 0.56982625\n",
            "Iteration 52, loss = 0.56787687\n",
            "Iteration 53, loss = 0.56554404\n",
            "Iteration 54, loss = 0.56517483\n",
            "Iteration 55, loss = 0.56266811\n",
            "Iteration 56, loss = 0.56122026\n",
            "Iteration 57, loss = 0.57161915\n",
            "Iteration 58, loss = 0.56132489\n",
            "Iteration 59, loss = 0.56338219\n",
            "Iteration 60, loss = 0.55741129\n",
            "Iteration 61, loss = 0.55548289\n",
            "Iteration 62, loss = 0.56138026\n",
            "Iteration 63, loss = 0.55845851\n",
            "Iteration 64, loss = 0.55475940\n",
            "Iteration 65, loss = 0.55141635\n",
            "Iteration 66, loss = 0.55495325\n",
            "Iteration 67, loss = 0.55309728\n",
            "Iteration 68, loss = 0.55423142\n",
            "Iteration 69, loss = 0.54442387\n",
            "Iteration 70, loss = 0.55432349\n",
            "Iteration 71, loss = 0.54332812\n",
            "Iteration 72, loss = 0.54167203\n",
            "Iteration 73, loss = 0.54613182\n",
            "Iteration 74, loss = 0.54878656\n",
            "Iteration 75, loss = 0.54115631\n",
            "Iteration 76, loss = 0.53850504\n",
            "Iteration 77, loss = 0.54658712\n",
            "Iteration 78, loss = 0.53954402\n",
            "Iteration 79, loss = 0.54439122\n",
            "Iteration 80, loss = 0.53820452\n",
            "Iteration 81, loss = 0.54193334\n",
            "Iteration 82, loss = 0.53739419\n",
            "Iteration 83, loss = 0.53318490\n",
            "Iteration 84, loss = 0.52668101\n",
            "Iteration 85, loss = 0.55362628\n",
            "Iteration 86, loss = 0.54731254\n",
            "Iteration 87, loss = 0.56499387\n",
            "Iteration 88, loss = 0.53482699\n",
            "Iteration 89, loss = 0.53561272\n",
            "Iteration 90, loss = 0.52715523\n",
            "Iteration 91, loss = 0.53360490\n",
            "Iteration 92, loss = 0.56356226\n",
            "Iteration 93, loss = 0.53477378\n",
            "Iteration 94, loss = 0.52637995\n",
            "Iteration 95, loss = 0.52229992\n",
            "Iteration 96, loss = 0.52713733\n",
            "Iteration 97, loss = 0.53335723\n",
            "Iteration 98, loss = 0.51411775\n",
            "Iteration 99, loss = 0.51851250\n",
            "Iteration 100, loss = 0.52108414\n",
            "Iteration 101, loss = 0.53464744\n",
            "Iteration 102, loss = 0.51606763\n",
            "Iteration 103, loss = 0.51175499\n",
            "Iteration 104, loss = 0.51713218\n",
            "Iteration 105, loss = 0.51872772\n",
            "Iteration 106, loss = 0.51472920\n",
            "Iteration 107, loss = 0.51158206\n",
            "Iteration 108, loss = 0.51144765\n",
            "Iteration 109, loss = 0.50675106\n",
            "Iteration 110, loss = 0.50828178\n",
            "Iteration 111, loss = 0.50699005\n",
            "Iteration 112, loss = 0.51314181\n",
            "Iteration 113, loss = 0.50647360\n",
            "Iteration 114, loss = 0.50212835\n",
            "Iteration 115, loss = 0.49369085\n",
            "Iteration 116, loss = 0.50340783\n",
            "Iteration 117, loss = 0.49962010\n",
            "Iteration 118, loss = 0.51300141\n",
            "Iteration 119, loss = 0.50640662\n",
            "Iteration 120, loss = 0.50304024\n",
            "Iteration 121, loss = 0.53916887\n",
            "Iteration 122, loss = 0.54019727\n",
            "Iteration 123, loss = 0.53867868\n",
            "Iteration 124, loss = 0.50631731\n",
            "Iteration 125, loss = 0.50259962\n",
            "Iteration 126, loss = 0.49657659\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69448336\n",
            "Iteration 2, loss = 0.62073071\n",
            "Iteration 3, loss = 0.60643740\n",
            "Iteration 4, loss = 0.59877630\n",
            "Iteration 5, loss = 0.59904362\n",
            "Iteration 6, loss = 0.59896610\n",
            "Iteration 7, loss = 0.59439845\n",
            "Iteration 8, loss = 0.59528992\n",
            "Iteration 9, loss = 0.59439426\n",
            "Iteration 10, loss = 0.59351955\n",
            "Iteration 11, loss = 0.58985995\n",
            "Iteration 12, loss = 0.59155842\n",
            "Iteration 13, loss = 0.59078531\n",
            "Iteration 14, loss = 0.59021150\n",
            "Iteration 15, loss = 0.58753543\n",
            "Iteration 16, loss = 0.58985036\n",
            "Iteration 17, loss = 0.59013969\n",
            "Iteration 18, loss = 0.58716183\n",
            "Iteration 19, loss = 0.58509359\n",
            "Iteration 20, loss = 0.58655890\n",
            "Iteration 21, loss = 0.58647862\n",
            "Iteration 22, loss = 0.58458916\n",
            "Iteration 23, loss = 0.58462382\n",
            "Iteration 24, loss = 0.58449699\n",
            "Iteration 25, loss = 0.58244578\n",
            "Iteration 26, loss = 0.58181294\n",
            "Iteration 27, loss = 0.58324872\n",
            "Iteration 28, loss = 0.58003263\n",
            "Iteration 29, loss = 0.58202396\n",
            "Iteration 30, loss = 0.58130297\n",
            "Iteration 31, loss = 0.58034425\n",
            "Iteration 32, loss = 0.58072720\n",
            "Iteration 33, loss = 0.58334516\n",
            "Iteration 34, loss = 0.57837691\n",
            "Iteration 35, loss = 0.58180477\n",
            "Iteration 36, loss = 0.57806248\n",
            "Iteration 37, loss = 0.57543755\n",
            "Iteration 38, loss = 0.57825810\n",
            "Iteration 39, loss = 0.57548563\n",
            "Iteration 40, loss = 0.57438213\n",
            "Iteration 41, loss = 0.57871562\n",
            "Iteration 42, loss = 0.57431511\n",
            "Iteration 43, loss = 0.57756446\n",
            "Iteration 44, loss = 0.57614284\n",
            "Iteration 45, loss = 0.57224785\n",
            "Iteration 46, loss = 0.57352465\n",
            "Iteration 47, loss = 0.57603435\n",
            "Iteration 48, loss = 0.58324039\n",
            "Iteration 49, loss = 0.57021504\n",
            "Iteration 50, loss = 0.57224410\n",
            "Iteration 51, loss = 0.56976397\n",
            "Iteration 52, loss = 0.57021805\n",
            "Iteration 53, loss = 0.57702514\n",
            "Iteration 54, loss = 0.57005225\n",
            "Iteration 55, loss = 0.56676503\n",
            "Iteration 56, loss = 0.57066015\n",
            "Iteration 57, loss = 0.58533018\n",
            "Iteration 58, loss = 0.56710780\n",
            "Iteration 59, loss = 0.56519299\n",
            "Iteration 60, loss = 0.56581483\n",
            "Iteration 61, loss = 0.56592803\n",
            "Iteration 62, loss = 0.56370886\n",
            "Iteration 63, loss = 0.57492286\n",
            "Iteration 64, loss = 0.56576407\n",
            "Iteration 65, loss = 0.56109885\n",
            "Iteration 66, loss = 0.55907238\n",
            "Iteration 67, loss = 0.56262214\n",
            "Iteration 68, loss = 0.56169502\n",
            "Iteration 69, loss = 0.55784063\n",
            "Iteration 70, loss = 0.55747818\n",
            "Iteration 71, loss = 0.56133852\n",
            "Iteration 72, loss = 0.56687925\n",
            "Iteration 73, loss = 0.56561117\n",
            "Iteration 74, loss = 0.55557043\n",
            "Iteration 75, loss = 0.55713540\n",
            "Iteration 76, loss = 0.55459178\n",
            "Iteration 77, loss = 0.55294803\n",
            "Iteration 78, loss = 0.55344661\n",
            "Iteration 79, loss = 0.55223790\n",
            "Iteration 80, loss = 0.55481872\n",
            "Iteration 81, loss = 0.55318617\n",
            "Iteration 82, loss = 0.55169388\n",
            "Iteration 83, loss = 0.55175401\n",
            "Iteration 84, loss = 0.55013911\n",
            "Iteration 85, loss = 0.54834687\n",
            "Iteration 86, loss = 0.54721000\n",
            "Iteration 87, loss = 0.55135240\n",
            "Iteration 88, loss = 0.54945624\n",
            "Iteration 89, loss = 0.55601245\n",
            "Iteration 90, loss = 0.54846166\n",
            "Iteration 91, loss = 0.54548309\n",
            "Iteration 92, loss = 0.54929512\n",
            "Iteration 93, loss = 0.54338282\n",
            "Iteration 94, loss = 0.54406511\n",
            "Iteration 95, loss = 0.54400016\n",
            "Iteration 96, loss = 0.54602968\n",
            "Iteration 97, loss = 0.54388594\n",
            "Iteration 98, loss = 0.53968453\n",
            "Iteration 99, loss = 0.53971112\n",
            "Iteration 100, loss = 0.54075770\n",
            "Iteration 101, loss = 0.53679559\n",
            "Iteration 102, loss = 0.53705628\n",
            "Iteration 103, loss = 0.53816188\n",
            "Iteration 104, loss = 0.53836381\n",
            "Iteration 105, loss = 0.53245085\n",
            "Iteration 106, loss = 0.53759320\n",
            "Iteration 107, loss = 0.53154650\n",
            "Iteration 108, loss = 0.53385511\n",
            "Iteration 109, loss = 0.53190353\n",
            "Iteration 110, loss = 0.54544469\n",
            "Iteration 111, loss = 0.52905559\n",
            "Iteration 112, loss = 0.54225027\n",
            "Iteration 113, loss = 0.53305201\n",
            "Iteration 114, loss = 0.52853532\n",
            "Iteration 115, loss = 0.53309217\n",
            "Iteration 116, loss = 0.52817410\n",
            "Iteration 117, loss = 0.53401617\n",
            "Iteration 118, loss = 0.53780960\n",
            "Iteration 119, loss = 0.52698594\n",
            "Iteration 120, loss = 0.52682956\n",
            "Iteration 121, loss = 0.52519993\n",
            "Iteration 122, loss = 0.52259654\n",
            "Iteration 123, loss = 0.52365458\n",
            "Iteration 124, loss = 0.51898076\n",
            "Iteration 125, loss = 0.52035474\n",
            "Iteration 126, loss = 0.51968145\n",
            "Iteration 127, loss = 0.52686596\n",
            "Iteration 128, loss = 0.52183213\n",
            "Iteration 129, loss = 0.54440687\n",
            "Iteration 130, loss = 0.51594699\n",
            "Iteration 131, loss = 0.51768536\n",
            "Iteration 132, loss = 0.52305661\n",
            "Iteration 133, loss = 0.52056213\n",
            "Iteration 134, loss = 0.51017429\n",
            "Iteration 135, loss = 0.51200794\n",
            "Iteration 136, loss = 0.50864446\n",
            "Iteration 137, loss = 0.50894549\n",
            "Iteration 138, loss = 0.52632837\n",
            "Iteration 139, loss = 0.51151753\n",
            "Iteration 140, loss = 0.50629646\n",
            "Iteration 141, loss = 0.50778607\n",
            "Iteration 142, loss = 0.51579540\n",
            "Iteration 143, loss = 0.50688416\n",
            "Iteration 144, loss = 0.51045591\n",
            "Iteration 145, loss = 0.52022963\n",
            "Iteration 146, loss = 0.51635792\n",
            "Iteration 147, loss = 0.50471920\n",
            "Iteration 148, loss = 0.51411152\n",
            "Iteration 149, loss = 0.50192323\n",
            "Iteration 150, loss = 0.50606772\n",
            "Iteration 151, loss = 0.50461634\n",
            "Iteration 152, loss = 0.50355493\n",
            "Iteration 153, loss = 0.49745208\n",
            "Iteration 154, loss = 0.49362595\n",
            "Iteration 155, loss = 0.49979457\n",
            "Iteration 156, loss = 0.49554868\n",
            "Iteration 157, loss = 0.50073222\n",
            "Iteration 158, loss = 0.49685686\n",
            "Iteration 159, loss = 0.49975916\n",
            "Iteration 160, loss = 0.50780926\n",
            "Iteration 161, loss = 0.49704651\n",
            "Iteration 162, loss = 0.49615610\n",
            "Iteration 163, loss = 0.51000413\n",
            "Iteration 164, loss = 0.51324055\n",
            "Iteration 165, loss = 0.49691645\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67800159\n",
            "Iteration 2, loss = 0.62190184\n",
            "Iteration 3, loss = 0.61701675\n",
            "Iteration 4, loss = 0.61356303\n",
            "Iteration 5, loss = 0.60350735\n",
            "Iteration 6, loss = 0.60005942\n",
            "Iteration 7, loss = 0.59609154\n",
            "Iteration 8, loss = 0.59465297\n",
            "Iteration 9, loss = 0.59782626\n",
            "Iteration 10, loss = 0.59402393\n",
            "Iteration 11, loss = 0.58989617\n",
            "Iteration 12, loss = 0.59085085\n",
            "Iteration 13, loss = 0.58933175\n",
            "Iteration 14, loss = 0.58522506\n",
            "Iteration 15, loss = 0.59104315\n",
            "Iteration 16, loss = 0.58570505\n",
            "Iteration 17, loss = 0.58414982\n",
            "Iteration 18, loss = 0.58274401\n",
            "Iteration 19, loss = 0.58475912\n",
            "Iteration 20, loss = 0.58549901\n",
            "Iteration 21, loss = 0.58403316\n",
            "Iteration 22, loss = 0.58383212\n",
            "Iteration 23, loss = 0.58231937\n",
            "Iteration 24, loss = 0.57809071\n",
            "Iteration 25, loss = 0.57934792\n",
            "Iteration 26, loss = 0.57790982\n",
            "Iteration 27, loss = 0.57695949\n",
            "Iteration 28, loss = 0.57612407\n",
            "Iteration 29, loss = 0.57766886\n",
            "Iteration 30, loss = 0.57432966\n",
            "Iteration 31, loss = 0.57386447\n",
            "Iteration 32, loss = 0.57366197\n",
            "Iteration 33, loss = 0.57225120\n",
            "Iteration 34, loss = 0.57255361\n",
            "Iteration 35, loss = 0.57021510\n",
            "Iteration 36, loss = 0.57966185\n",
            "Iteration 37, loss = 0.57667488\n",
            "Iteration 38, loss = 0.56641334\n",
            "Iteration 39, loss = 0.56577916\n",
            "Iteration 40, loss = 0.56593441\n",
            "Iteration 41, loss = 0.56574551\n",
            "Iteration 42, loss = 0.56530278\n",
            "Iteration 43, loss = 0.56729217\n",
            "Iteration 44, loss = 0.56227045\n",
            "Iteration 45, loss = 0.57332477\n",
            "Iteration 46, loss = 0.56485837\n",
            "Iteration 47, loss = 0.56410322\n",
            "Iteration 48, loss = 0.56057161\n",
            "Iteration 49, loss = 0.55909773\n",
            "Iteration 50, loss = 0.56127744\n",
            "Iteration 51, loss = 0.55590197\n",
            "Iteration 52, loss = 0.55565447\n",
            "Iteration 53, loss = 0.55658838\n",
            "Iteration 54, loss = 0.56235032\n",
            "Iteration 55, loss = 0.55149361\n",
            "Iteration 56, loss = 0.55751446\n",
            "Iteration 57, loss = 0.55407188\n",
            "Iteration 58, loss = 0.55587715\n",
            "Iteration 59, loss = 0.55046491\n",
            "Iteration 60, loss = 0.55972610\n",
            "Iteration 61, loss = 0.55469395\n",
            "Iteration 62, loss = 0.54977010\n",
            "Iteration 63, loss = 0.54992514\n",
            "Iteration 64, loss = 0.55535609\n",
            "Iteration 65, loss = 0.55797339\n",
            "Iteration 66, loss = 0.55017076\n",
            "Iteration 67, loss = 0.54893283\n",
            "Iteration 68, loss = 0.54731824\n",
            "Iteration 69, loss = 0.54685386\n",
            "Iteration 70, loss = 0.54436905\n",
            "Iteration 71, loss = 0.54367561\n",
            "Iteration 72, loss = 0.54182396\n",
            "Iteration 73, loss = 0.54940669\n",
            "Iteration 74, loss = 0.54169989\n",
            "Iteration 75, loss = 0.54360063\n",
            "Iteration 76, loss = 0.53634210\n",
            "Iteration 77, loss = 0.53534973\n",
            "Iteration 78, loss = 0.54002100\n",
            "Iteration 79, loss = 0.53404430\n",
            "Iteration 80, loss = 0.53733926\n",
            "Iteration 81, loss = 0.53088974\n",
            "Iteration 82, loss = 0.53088492\n",
            "Iteration 83, loss = 0.53623470\n",
            "Iteration 84, loss = 0.53175319\n",
            "Iteration 85, loss = 0.52881672\n",
            "Iteration 86, loss = 0.52964041\n",
            "Iteration 87, loss = 0.53799841\n",
            "Iteration 88, loss = 0.52631899\n",
            "Iteration 89, loss = 0.53050554\n",
            "Iteration 90, loss = 0.52824776\n",
            "Iteration 91, loss = 0.52650721\n",
            "Iteration 92, loss = 0.52670442\n",
            "Iteration 93, loss = 0.52422831\n",
            "Iteration 94, loss = 0.52232759\n",
            "Iteration 95, loss = 0.52123185\n",
            "Iteration 96, loss = 0.52626491\n",
            "Iteration 97, loss = 0.52777233\n",
            "Iteration 98, loss = 0.51879328\n",
            "Iteration 99, loss = 0.53935435\n",
            "Iteration 100, loss = 0.52873969\n",
            "Iteration 101, loss = 0.51844752\n",
            "Iteration 102, loss = 0.51332413\n",
            "Iteration 103, loss = 0.51357237\n",
            "Iteration 104, loss = 0.52644509\n",
            "Iteration 105, loss = 0.51166728\n",
            "Iteration 106, loss = 0.50759730\n",
            "Iteration 107, loss = 0.50803814\n",
            "Iteration 108, loss = 0.51980918\n",
            "Iteration 109, loss = 0.50600581\n",
            "Iteration 110, loss = 0.51383564\n",
            "Iteration 111, loss = 0.50423461\n",
            "Iteration 112, loss = 0.50365744\n",
            "Iteration 113, loss = 0.51218733\n",
            "Iteration 114, loss = 0.53672180\n",
            "Iteration 115, loss = 0.51012416\n",
            "Iteration 116, loss = 0.51358832\n",
            "Iteration 117, loss = 0.50538501\n",
            "Iteration 118, loss = 0.50032901\n",
            "Iteration 119, loss = 0.49750484\n",
            "Iteration 120, loss = 0.50304533\n",
            "Iteration 121, loss = 0.49724820\n",
            "Iteration 122, loss = 0.50081388\n",
            "Iteration 123, loss = 0.52076532\n",
            "Iteration 124, loss = 0.50803355\n",
            "Iteration 125, loss = 0.49579094\n",
            "Iteration 126, loss = 0.49200750\n",
            "Iteration 127, loss = 0.49564835\n",
            "Iteration 128, loss = 0.50308898\n",
            "Iteration 129, loss = 0.50135343\n",
            "Iteration 130, loss = 0.52189241\n",
            "Iteration 131, loss = 0.50197938\n",
            "Iteration 132, loss = 0.49021769\n",
            "Iteration 133, loss = 0.48943937\n",
            "Iteration 134, loss = 0.54674046\n",
            "Iteration 135, loss = 0.50146555\n",
            "Iteration 136, loss = 0.50014543\n",
            "Iteration 137, loss = 0.49232884\n",
            "Iteration 138, loss = 0.50143335\n",
            "Iteration 139, loss = 0.48808537\n",
            "Iteration 140, loss = 0.49135704\n",
            "Iteration 141, loss = 0.48161019\n",
            "Iteration 142, loss = 0.49112117\n",
            "Iteration 143, loss = 0.48649291\n",
            "Iteration 144, loss = 0.48551033\n",
            "Iteration 145, loss = 0.50070992\n",
            "Iteration 146, loss = 0.49526355\n",
            "Iteration 147, loss = 0.49131926\n",
            "Iteration 148, loss = 0.49650533\n",
            "Iteration 149, loss = 0.52520915\n",
            "Iteration 150, loss = 0.50543278\n",
            "Iteration 151, loss = 0.49814946\n",
            "Iteration 152, loss = 0.48390415\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75027860\n",
            "Iteration 2, loss = 0.72576598\n",
            "Iteration 3, loss = 0.70210017\n",
            "Iteration 4, loss = 0.68485215\n",
            "Iteration 5, loss = 0.67425433\n",
            "Iteration 6, loss = 0.66856701\n",
            "Iteration 7, loss = 0.66551663\n",
            "Iteration 8, loss = 0.66308973\n",
            "Iteration 9, loss = 0.66116211\n",
            "Iteration 10, loss = 0.66024549\n",
            "Iteration 11, loss = 0.65955016\n",
            "Iteration 12, loss = 0.65868593\n",
            "Iteration 13, loss = 0.65813194\n",
            "Iteration 14, loss = 0.65721428\n",
            "Iteration 15, loss = 0.65661207\n",
            "Iteration 16, loss = 0.65629398\n",
            "Iteration 17, loss = 0.65595138\n",
            "Iteration 18, loss = 0.65563887\n",
            "Iteration 19, loss = 0.65521208\n",
            "Iteration 20, loss = 0.65487374\n",
            "Iteration 21, loss = 0.65453524\n",
            "Iteration 22, loss = 0.65422144\n",
            "Iteration 23, loss = 0.65389147\n",
            "Iteration 24, loss = 0.65366889\n",
            "Iteration 25, loss = 0.65330317\n",
            "Iteration 26, loss = 0.65285140\n",
            "Iteration 27, loss = 0.65285765\n",
            "Iteration 28, loss = 0.65248164\n",
            "Iteration 29, loss = 0.65203461\n",
            "Iteration 30, loss = 0.65154335\n",
            "Iteration 31, loss = 0.65117296\n",
            "Iteration 32, loss = 0.65090837\n",
            "Iteration 33, loss = 0.65058518\n",
            "Iteration 34, loss = 0.65030195\n",
            "Iteration 35, loss = 0.64996488\n",
            "Iteration 36, loss = 0.64969425\n",
            "Iteration 37, loss = 0.64938705\n",
            "Iteration 38, loss = 0.64921098\n",
            "Iteration 39, loss = 0.64886990\n",
            "Iteration 40, loss = 0.64866480\n",
            "Iteration 41, loss = 0.64845143\n",
            "Iteration 42, loss = 0.64818143\n",
            "Iteration 43, loss = 0.64789150\n",
            "Iteration 44, loss = 0.64765400\n",
            "Iteration 45, loss = 0.64730066\n",
            "Iteration 46, loss = 0.64697260\n",
            "Iteration 47, loss = 0.64673697\n",
            "Iteration 48, loss = 0.64643532\n",
            "Iteration 49, loss = 0.64617685\n",
            "Iteration 50, loss = 0.64602740\n",
            "Iteration 51, loss = 0.64567892\n",
            "Iteration 52, loss = 0.64538616\n",
            "Iteration 53, loss = 0.64496427\n",
            "Iteration 54, loss = 0.64478243\n",
            "Iteration 55, loss = 0.64444932\n",
            "Iteration 56, loss = 0.64408878\n",
            "Iteration 57, loss = 0.64376550\n",
            "Iteration 58, loss = 0.64347346\n",
            "Iteration 59, loss = 0.64312407\n",
            "Iteration 60, loss = 0.64310575\n",
            "Iteration 61, loss = 0.64278520\n",
            "Iteration 62, loss = 0.64242609\n",
            "Iteration 63, loss = 0.64207443\n",
            "Iteration 64, loss = 0.64191690\n",
            "Iteration 65, loss = 0.64150953\n",
            "Iteration 66, loss = 0.64118274\n",
            "Iteration 67, loss = 0.64089904\n",
            "Iteration 68, loss = 0.64066742\n",
            "Iteration 69, loss = 0.64027072\n",
            "Iteration 70, loss = 0.63999217\n",
            "Iteration 71, loss = 0.64007170\n",
            "Iteration 72, loss = 0.63944244\n",
            "Iteration 73, loss = 0.63919015\n",
            "Iteration 74, loss = 0.63886400\n",
            "Iteration 75, loss = 0.63865373\n",
            "Iteration 76, loss = 0.63841029\n",
            "Iteration 77, loss = 0.63801646\n",
            "Iteration 78, loss = 0.63790487\n",
            "Iteration 79, loss = 0.63771341\n",
            "Iteration 80, loss = 0.63734046\n",
            "Iteration 81, loss = 0.63715076\n",
            "Iteration 82, loss = 0.63684352\n",
            "Iteration 83, loss = 0.63667549\n",
            "Iteration 84, loss = 0.63635349\n",
            "Iteration 85, loss = 0.63606006\n",
            "Iteration 86, loss = 0.63589583\n",
            "Iteration 87, loss = 0.63571125\n",
            "Iteration 88, loss = 0.63537579\n",
            "Iteration 89, loss = 0.63514514\n",
            "Iteration 90, loss = 0.63492212\n",
            "Iteration 91, loss = 0.63466601\n",
            "Iteration 92, loss = 0.63442358\n",
            "Iteration 93, loss = 0.63417427\n",
            "Iteration 94, loss = 0.63393012\n",
            "Iteration 95, loss = 0.63365146\n",
            "Iteration 96, loss = 0.63350432\n",
            "Iteration 97, loss = 0.63322654\n",
            "Iteration 98, loss = 0.63312564\n",
            "Iteration 99, loss = 0.63281096\n",
            "Iteration 100, loss = 0.63249810\n",
            "Iteration 101, loss = 0.63230104\n",
            "Iteration 102, loss = 0.63198270\n",
            "Iteration 103, loss = 0.63176512\n",
            "Iteration 104, loss = 0.63149155\n",
            "Iteration 105, loss = 0.63129250\n",
            "Iteration 106, loss = 0.63104207\n",
            "Iteration 107, loss = 0.63085826\n",
            "Iteration 108, loss = 0.63064254\n",
            "Iteration 109, loss = 0.63043368\n",
            "Iteration 110, loss = 0.63024798\n",
            "Iteration 111, loss = 0.63004679\n",
            "Iteration 112, loss = 0.62997436\n",
            "Iteration 113, loss = 0.62983811\n",
            "Iteration 114, loss = 0.62947308\n",
            "Iteration 115, loss = 0.62907721\n",
            "Iteration 116, loss = 0.62888355\n",
            "Iteration 117, loss = 0.62868395\n",
            "Iteration 118, loss = 0.62843627\n",
            "Iteration 119, loss = 0.62821826\n",
            "Iteration 120, loss = 0.62796580\n",
            "Iteration 121, loss = 0.62783522\n",
            "Iteration 122, loss = 0.62755646\n",
            "Iteration 123, loss = 0.62747831\n",
            "Iteration 124, loss = 0.62705177\n",
            "Iteration 125, loss = 0.62686866\n",
            "Iteration 126, loss = 0.62665549\n",
            "Iteration 127, loss = 0.62651658\n",
            "Iteration 128, loss = 0.62641593\n",
            "Iteration 129, loss = 0.62612841\n",
            "Iteration 130, loss = 0.62592600\n",
            "Iteration 131, loss = 0.62572583\n",
            "Iteration 132, loss = 0.62546856\n",
            "Iteration 133, loss = 0.62523835\n",
            "Iteration 134, loss = 0.62503433\n",
            "Iteration 135, loss = 0.62482455\n",
            "Iteration 136, loss = 0.62457882\n",
            "Iteration 137, loss = 0.62438562\n",
            "Iteration 138, loss = 0.62440309\n",
            "Iteration 139, loss = 0.62419448\n",
            "Iteration 140, loss = 0.62382315\n",
            "Iteration 141, loss = 0.62358584\n",
            "Iteration 142, loss = 0.62335005\n",
            "Iteration 143, loss = 0.62320053\n",
            "Iteration 144, loss = 0.62310152\n",
            "Iteration 145, loss = 0.62298411\n",
            "Iteration 146, loss = 0.62278334\n",
            "Iteration 147, loss = 0.62244118\n",
            "Iteration 148, loss = 0.62219465\n",
            "Iteration 149, loss = 0.62206390\n",
            "Iteration 150, loss = 0.62178803\n",
            "Iteration 151, loss = 0.62162242\n",
            "Iteration 152, loss = 0.62140764\n",
            "Iteration 153, loss = 0.62124857\n",
            "Iteration 154, loss = 0.62106646\n",
            "Iteration 155, loss = 0.62095739\n",
            "Iteration 156, loss = 0.62067592\n",
            "Iteration 157, loss = 0.62052012\n",
            "Iteration 158, loss = 0.62048061\n",
            "Iteration 159, loss = 0.62038832\n",
            "Iteration 160, loss = 0.62026977\n",
            "Iteration 161, loss = 0.62005868\n",
            "Iteration 162, loss = 0.61989095\n",
            "Iteration 163, loss = 0.61976024\n",
            "Iteration 164, loss = 0.61965729\n",
            "Iteration 165, loss = 0.61960763\n",
            "Iteration 166, loss = 0.61935344\n",
            "Iteration 167, loss = 0.61913404\n",
            "Iteration 168, loss = 0.61893038\n",
            "Iteration 169, loss = 0.61871569\n",
            "Iteration 170, loss = 0.61859760\n",
            "Iteration 171, loss = 0.61843181\n",
            "Iteration 172, loss = 0.61828237\n",
            "Iteration 173, loss = 0.61808201\n",
            "Iteration 174, loss = 0.61795433\n",
            "Iteration 175, loss = 0.61836326\n",
            "Iteration 176, loss = 0.61780134\n",
            "Iteration 177, loss = 0.61745764\n",
            "Iteration 178, loss = 0.61724015\n",
            "Iteration 179, loss = 0.61718889\n",
            "Iteration 180, loss = 0.61704217\n",
            "Iteration 181, loss = 0.61712153\n",
            "Iteration 182, loss = 0.61663164\n",
            "Iteration 183, loss = 0.61653040\n",
            "Iteration 184, loss = 0.61649188\n",
            "Iteration 185, loss = 0.61618342\n",
            "Iteration 186, loss = 0.61609052\n",
            "Iteration 187, loss = 0.61589856\n",
            "Iteration 188, loss = 0.61571632\n",
            "Iteration 189, loss = 0.61555577\n",
            "Iteration 190, loss = 0.61540513\n",
            "Iteration 191, loss = 0.61530665\n",
            "Iteration 192, loss = 0.61517610\n",
            "Iteration 193, loss = 0.61497799\n",
            "Iteration 194, loss = 0.61493380\n",
            "Iteration 195, loss = 0.61462408\n",
            "Iteration 196, loss = 0.61460695\n",
            "Iteration 197, loss = 0.61474488\n",
            "Iteration 198, loss = 0.61459293\n",
            "Iteration 199, loss = 0.61440816\n",
            "Iteration 200, loss = 0.61438860\n",
            "Iteration 201, loss = 0.61424900\n",
            "Iteration 202, loss = 0.61409658\n",
            "Iteration 203, loss = 0.61399217\n",
            "Iteration 204, loss = 0.61381848\n",
            "Iteration 205, loss = 0.61355937\n",
            "Iteration 206, loss = 0.61344019\n",
            "Iteration 207, loss = 0.61330283\n",
            "Iteration 208, loss = 0.61317253\n",
            "Iteration 209, loss = 0.61303835\n",
            "Iteration 210, loss = 0.61294712\n",
            "Iteration 211, loss = 0.61290067\n",
            "Iteration 212, loss = 0.61273820\n",
            "Iteration 213, loss = 0.61265575\n",
            "Iteration 214, loss = 0.61274555\n",
            "Iteration 215, loss = 0.61240359\n",
            "Iteration 216, loss = 0.61226009\n",
            "Iteration 217, loss = 0.61217750\n",
            "Iteration 218, loss = 0.61208853\n",
            "Iteration 219, loss = 0.61202411\n",
            "Iteration 220, loss = 0.61183302\n",
            "Iteration 221, loss = 0.61170179\n",
            "Iteration 222, loss = 0.61159751\n",
            "Iteration 223, loss = 0.61164903\n",
            "Iteration 224, loss = 0.61148195\n",
            "Iteration 225, loss = 0.61142694\n",
            "Iteration 226, loss = 0.61138567\n",
            "Iteration 227, loss = 0.61116030\n",
            "Iteration 228, loss = 0.61112812\n",
            "Iteration 229, loss = 0.61087825\n",
            "Iteration 230, loss = 0.61078058\n",
            "Iteration 231, loss = 0.61082851\n",
            "Iteration 232, loss = 0.61071759\n",
            "Iteration 233, loss = 0.61070058\n",
            "Iteration 234, loss = 0.61055015\n",
            "Iteration 235, loss = 0.61046048\n",
            "Iteration 236, loss = 0.61049489\n",
            "Iteration 237, loss = 0.61044368\n",
            "Iteration 238, loss = 0.61032048\n",
            "Iteration 239, loss = 0.61034009\n",
            "Iteration 240, loss = 0.61019928\n",
            "Iteration 241, loss = 0.61013929\n",
            "Iteration 242, loss = 0.61007062\n",
            "Iteration 243, loss = 0.61000914\n",
            "Iteration 244, loss = 0.61006963\n",
            "Iteration 245, loss = 0.60983617\n",
            "Iteration 246, loss = 0.60972538\n",
            "Iteration 247, loss = 0.60946634\n",
            "Iteration 248, loss = 0.60937888\n",
            "Iteration 249, loss = 0.60931650\n",
            "Iteration 250, loss = 0.60952228\n",
            "Iteration 251, loss = 0.60924362\n",
            "Iteration 252, loss = 0.60904102\n",
            "Iteration 253, loss = 0.60909989\n",
            "Iteration 254, loss = 0.60908713\n",
            "Iteration 255, loss = 0.60896628\n",
            "Iteration 256, loss = 0.60894043\n",
            "Iteration 257, loss = 0.60888908\n",
            "Iteration 258, loss = 0.60879499\n",
            "Iteration 259, loss = 0.60879584\n",
            "Iteration 260, loss = 0.60866964\n",
            "Iteration 261, loss = 0.60854486\n",
            "Iteration 262, loss = 0.60843640\n",
            "Iteration 263, loss = 0.60844622\n",
            "Iteration 264, loss = 0.60837486\n",
            "Iteration 265, loss = 0.60829395\n",
            "Iteration 266, loss = 0.60827843\n",
            "Iteration 267, loss = 0.60824429\n",
            "Iteration 268, loss = 0.60821453\n",
            "Iteration 269, loss = 0.60811041\n",
            "Iteration 270, loss = 0.60823852\n",
            "Iteration 271, loss = 0.60797551\n",
            "Iteration 272, loss = 0.60820227\n",
            "Iteration 273, loss = 0.60802617\n",
            "Iteration 274, loss = 0.60793228\n",
            "Iteration 275, loss = 0.60778444\n",
            "Iteration 276, loss = 0.60781976\n",
            "Iteration 277, loss = 0.60774421\n",
            "Iteration 278, loss = 0.60774385\n",
            "Iteration 279, loss = 0.60765007\n",
            "Iteration 280, loss = 0.60758890\n",
            "Iteration 281, loss = 0.60755550\n",
            "Iteration 282, loss = 0.60758873\n",
            "Iteration 283, loss = 0.60759084\n",
            "Iteration 284, loss = 0.60729779\n",
            "Iteration 285, loss = 0.60728005\n",
            "Iteration 286, loss = 0.60729954\n",
            "Iteration 287, loss = 0.60748344\n",
            "Iteration 288, loss = 0.60734427\n",
            "Iteration 289, loss = 0.60721436\n",
            "Iteration 290, loss = 0.60712905\n",
            "Iteration 291, loss = 0.60729805\n",
            "Iteration 292, loss = 0.60711310\n",
            "Iteration 293, loss = 0.60713150\n",
            "Iteration 294, loss = 0.60709124\n",
            "Iteration 295, loss = 0.60704307\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67168538\n",
            "Iteration 2, loss = 0.67017597\n",
            "Iteration 3, loss = 0.66862893\n",
            "Iteration 4, loss = 0.66775466\n",
            "Iteration 5, loss = 0.66721074\n",
            "Iteration 6, loss = 0.66667316\n",
            "Iteration 7, loss = 0.66626127\n",
            "Iteration 8, loss = 0.66588000\n",
            "Iteration 9, loss = 0.66568451\n",
            "Iteration 10, loss = 0.66530303\n",
            "Iteration 11, loss = 0.66473486\n",
            "Iteration 12, loss = 0.66414517\n",
            "Iteration 13, loss = 0.66406739\n",
            "Iteration 14, loss = 0.66371638\n",
            "Iteration 15, loss = 0.66322667\n",
            "Iteration 16, loss = 0.66281118\n",
            "Iteration 17, loss = 0.66246468\n",
            "Iteration 18, loss = 0.66210855\n",
            "Iteration 19, loss = 0.66174013\n",
            "Iteration 20, loss = 0.66133197\n",
            "Iteration 21, loss = 0.66088064\n",
            "Iteration 22, loss = 0.66066847\n",
            "Iteration 23, loss = 0.66027373\n",
            "Iteration 24, loss = 0.65987642\n",
            "Iteration 25, loss = 0.65956677\n",
            "Iteration 26, loss = 0.65933544\n",
            "Iteration 27, loss = 0.65899041\n",
            "Iteration 28, loss = 0.65853934\n",
            "Iteration 29, loss = 0.65823571\n",
            "Iteration 30, loss = 0.65797367\n",
            "Iteration 31, loss = 0.65777917\n",
            "Iteration 32, loss = 0.65749306\n",
            "Iteration 33, loss = 0.65717223\n",
            "Iteration 34, loss = 0.65693642\n",
            "Iteration 35, loss = 0.65673445\n",
            "Iteration 36, loss = 0.65637800\n",
            "Iteration 37, loss = 0.65609005\n",
            "Iteration 38, loss = 0.65583211\n",
            "Iteration 39, loss = 0.65574153\n",
            "Iteration 40, loss = 0.65543530\n",
            "Iteration 41, loss = 0.65525944\n",
            "Iteration 42, loss = 0.65512155\n",
            "Iteration 43, loss = 0.65476021\n",
            "Iteration 44, loss = 0.65452357\n",
            "Iteration 45, loss = 0.65428724\n",
            "Iteration 46, loss = 0.65414635\n",
            "Iteration 47, loss = 0.65381950\n",
            "Iteration 48, loss = 0.65351793\n",
            "Iteration 49, loss = 0.65344690\n",
            "Iteration 50, loss = 0.65321582\n",
            "Iteration 51, loss = 0.65284242\n",
            "Iteration 52, loss = 0.65269050\n",
            "Iteration 53, loss = 0.65236019\n",
            "Iteration 54, loss = 0.65210937\n",
            "Iteration 55, loss = 0.65197814\n",
            "Iteration 56, loss = 0.65184140\n",
            "Iteration 57, loss = 0.65154740\n",
            "Iteration 58, loss = 0.65130113\n",
            "Iteration 59, loss = 0.65130056\n",
            "Iteration 60, loss = 0.65102465\n",
            "Iteration 61, loss = 0.65082131\n",
            "Iteration 62, loss = 0.65047930\n",
            "Iteration 63, loss = 0.65026426\n",
            "Iteration 64, loss = 0.64975907\n",
            "Iteration 65, loss = 0.64942775\n",
            "Iteration 66, loss = 0.64902896\n",
            "Iteration 67, loss = 0.64854151\n",
            "Iteration 68, loss = 0.64822602\n",
            "Iteration 69, loss = 0.64781371\n",
            "Iteration 70, loss = 0.64745328\n",
            "Iteration 71, loss = 0.64708312\n",
            "Iteration 72, loss = 0.64674142\n",
            "Iteration 73, loss = 0.64639425\n",
            "Iteration 74, loss = 0.64630407\n",
            "Iteration 75, loss = 0.64614611\n",
            "Iteration 76, loss = 0.64573359\n",
            "Iteration 77, loss = 0.64538176\n",
            "Iteration 78, loss = 0.64500354\n",
            "Iteration 79, loss = 0.64467215\n",
            "Iteration 80, loss = 0.64430532\n",
            "Iteration 81, loss = 0.64396035\n",
            "Iteration 82, loss = 0.64362239\n",
            "Iteration 83, loss = 0.64327833\n",
            "Iteration 84, loss = 0.64299928\n",
            "Iteration 85, loss = 0.64266774\n",
            "Iteration 86, loss = 0.64238794\n",
            "Iteration 87, loss = 0.64212795\n",
            "Iteration 88, loss = 0.64192436\n",
            "Iteration 89, loss = 0.64169282\n",
            "Iteration 90, loss = 0.64159298\n",
            "Iteration 91, loss = 0.64119254\n",
            "Iteration 92, loss = 0.64087138\n",
            "Iteration 93, loss = 0.64067537\n",
            "Iteration 94, loss = 0.64056821\n",
            "Iteration 95, loss = 0.64034771\n",
            "Iteration 96, loss = 0.64008466\n",
            "Iteration 97, loss = 0.63981809\n",
            "Iteration 98, loss = 0.63973205\n",
            "Iteration 99, loss = 0.63945952\n",
            "Iteration 100, loss = 0.63906257\n",
            "Iteration 101, loss = 0.63878693\n",
            "Iteration 102, loss = 0.63880910\n",
            "Iteration 103, loss = 0.63839681\n",
            "Iteration 104, loss = 0.63790308\n",
            "Iteration 105, loss = 0.63766088\n",
            "Iteration 106, loss = 0.63769424\n",
            "Iteration 107, loss = 0.63738153\n",
            "Iteration 108, loss = 0.63726882\n",
            "Iteration 109, loss = 0.63695991\n",
            "Iteration 110, loss = 0.63684438\n",
            "Iteration 111, loss = 0.63652784\n",
            "Iteration 112, loss = 0.63623066\n",
            "Iteration 113, loss = 0.63589231\n",
            "Iteration 114, loss = 0.63561607\n",
            "Iteration 115, loss = 0.63527109\n",
            "Iteration 116, loss = 0.63519195\n",
            "Iteration 117, loss = 0.63481757\n",
            "Iteration 118, loss = 0.63461188\n",
            "Iteration 119, loss = 0.63434332\n",
            "Iteration 120, loss = 0.63415764\n",
            "Iteration 121, loss = 0.63410122\n",
            "Iteration 122, loss = 0.63394547\n",
            "Iteration 123, loss = 0.63355381\n",
            "Iteration 124, loss = 0.63326824\n",
            "Iteration 125, loss = 0.63308969\n",
            "Iteration 126, loss = 0.63276918\n",
            "Iteration 127, loss = 0.63243736\n",
            "Iteration 128, loss = 0.63203002\n",
            "Iteration 129, loss = 0.63174571\n",
            "Iteration 130, loss = 0.63162458\n",
            "Iteration 131, loss = 0.63129791\n",
            "Iteration 132, loss = 0.63101625\n",
            "Iteration 133, loss = 0.63073546\n",
            "Iteration 134, loss = 0.63061014\n",
            "Iteration 135, loss = 0.63037873\n",
            "Iteration 136, loss = 0.63020039\n",
            "Iteration 137, loss = 0.62997707\n",
            "Iteration 138, loss = 0.62953626\n",
            "Iteration 139, loss = 0.62928310\n",
            "Iteration 140, loss = 0.62907920\n",
            "Iteration 141, loss = 0.62891511\n",
            "Iteration 142, loss = 0.62872392\n",
            "Iteration 143, loss = 0.62834140\n",
            "Iteration 144, loss = 0.62818972\n",
            "Iteration 145, loss = 0.62782280\n",
            "Iteration 146, loss = 0.62771593\n",
            "Iteration 147, loss = 0.62746477\n",
            "Iteration 148, loss = 0.62728169\n",
            "Iteration 149, loss = 0.62708644\n",
            "Iteration 150, loss = 0.62689514\n",
            "Iteration 151, loss = 0.62685790\n",
            "Iteration 152, loss = 0.62641174\n",
            "Iteration 153, loss = 0.62621107\n",
            "Iteration 154, loss = 0.62600088\n",
            "Iteration 155, loss = 0.62569059\n",
            "Iteration 156, loss = 0.62542810\n",
            "Iteration 157, loss = 0.62519838\n",
            "Iteration 158, loss = 0.62500286\n",
            "Iteration 159, loss = 0.62486405\n",
            "Iteration 160, loss = 0.62466627\n",
            "Iteration 161, loss = 0.62434692\n",
            "Iteration 162, loss = 0.62415478\n",
            "Iteration 163, loss = 0.62390209\n",
            "Iteration 164, loss = 0.62373946\n",
            "Iteration 165, loss = 0.62350342\n",
            "Iteration 166, loss = 0.62325075\n",
            "Iteration 167, loss = 0.62331971\n",
            "Iteration 168, loss = 0.62307960\n",
            "Iteration 169, loss = 0.62267436\n",
            "Iteration 170, loss = 0.62249841\n",
            "Iteration 171, loss = 0.62216229\n",
            "Iteration 172, loss = 0.62177546\n",
            "Iteration 173, loss = 0.62165432\n",
            "Iteration 174, loss = 0.62139397\n",
            "Iteration 175, loss = 0.62116830\n",
            "Iteration 176, loss = 0.62090280\n",
            "Iteration 177, loss = 0.62063363\n",
            "Iteration 178, loss = 0.62039576\n",
            "Iteration 179, loss = 0.62024871\n",
            "Iteration 180, loss = 0.62014593\n",
            "Iteration 181, loss = 0.61989906\n",
            "Iteration 182, loss = 0.61984308\n",
            "Iteration 183, loss = 0.61965023\n",
            "Iteration 184, loss = 0.61940734\n",
            "Iteration 185, loss = 0.61943482\n",
            "Iteration 186, loss = 0.61911524\n",
            "Iteration 187, loss = 0.61896191\n",
            "Iteration 188, loss = 0.61884294\n",
            "Iteration 189, loss = 0.61858371\n",
            "Iteration 190, loss = 0.61838028\n",
            "Iteration 191, loss = 0.61812106\n",
            "Iteration 192, loss = 0.61798705\n",
            "Iteration 193, loss = 0.61788898\n",
            "Iteration 194, loss = 0.61793071\n",
            "Iteration 195, loss = 0.61774478\n",
            "Iteration 196, loss = 0.61743595\n",
            "Iteration 197, loss = 0.61706624\n",
            "Iteration 198, loss = 0.61675348\n",
            "Iteration 199, loss = 0.61659338\n",
            "Iteration 200, loss = 0.61642637\n",
            "Iteration 201, loss = 0.61647289\n",
            "Iteration 202, loss = 0.61623922\n",
            "Iteration 203, loss = 0.61589885\n",
            "Iteration 204, loss = 0.61571360\n",
            "Iteration 205, loss = 0.61554398\n",
            "Iteration 206, loss = 0.61532816\n",
            "Iteration 207, loss = 0.61534374\n",
            "Iteration 208, loss = 0.61503054\n",
            "Iteration 209, loss = 0.61510731\n",
            "Iteration 210, loss = 0.61473833\n",
            "Iteration 211, loss = 0.61473292\n",
            "Iteration 212, loss = 0.61443346\n",
            "Iteration 213, loss = 0.61437268\n",
            "Iteration 214, loss = 0.61431462\n",
            "Iteration 215, loss = 0.61413471\n",
            "Iteration 216, loss = 0.61403337\n",
            "Iteration 217, loss = 0.61377987\n",
            "Iteration 218, loss = 0.61364768\n",
            "Iteration 219, loss = 0.61349437\n",
            "Iteration 220, loss = 0.61349318\n",
            "Iteration 221, loss = 0.61326882\n",
            "Iteration 222, loss = 0.61316414\n",
            "Iteration 223, loss = 0.61298622\n",
            "Iteration 224, loss = 0.61284042\n",
            "Iteration 225, loss = 0.61289544\n",
            "Iteration 226, loss = 0.61258164\n",
            "Iteration 227, loss = 0.61240044\n",
            "Iteration 228, loss = 0.61236279\n",
            "Iteration 229, loss = 0.61227590\n",
            "Iteration 230, loss = 0.61208018\n",
            "Iteration 231, loss = 0.61202514\n",
            "Iteration 232, loss = 0.61184280\n",
            "Iteration 233, loss = 0.61168550\n",
            "Iteration 234, loss = 0.61163895\n",
            "Iteration 235, loss = 0.61153758\n",
            "Iteration 236, loss = 0.61137496\n",
            "Iteration 237, loss = 0.61134264\n",
            "Iteration 238, loss = 0.61131043\n",
            "Iteration 239, loss = 0.61083707\n",
            "Iteration 240, loss = 0.61062077\n",
            "Iteration 241, loss = 0.61053352\n",
            "Iteration 242, loss = 0.61053165\n",
            "Iteration 243, loss = 0.61067183\n",
            "Iteration 244, loss = 0.61052504\n",
            "Iteration 245, loss = 0.61026545\n",
            "Iteration 246, loss = 0.61006196\n",
            "Iteration 247, loss = 0.60994534\n",
            "Iteration 248, loss = 0.61003626\n",
            "Iteration 249, loss = 0.60979291\n",
            "Iteration 250, loss = 0.60958228\n",
            "Iteration 251, loss = 0.60961350\n",
            "Iteration 252, loss = 0.60979544\n",
            "Iteration 253, loss = 0.60952734\n",
            "Iteration 254, loss = 0.60939750\n",
            "Iteration 255, loss = 0.60936653\n",
            "Iteration 256, loss = 0.60933025\n",
            "Iteration 257, loss = 0.60915355\n",
            "Iteration 258, loss = 0.60897725\n",
            "Iteration 259, loss = 0.60890927\n",
            "Iteration 260, loss = 0.60867406\n",
            "Iteration 261, loss = 0.60852451\n",
            "Iteration 262, loss = 0.60841494\n",
            "Iteration 263, loss = 0.60832140\n",
            "Iteration 264, loss = 0.60857749\n",
            "Iteration 265, loss = 0.60821822\n",
            "Iteration 266, loss = 0.60816756\n",
            "Iteration 267, loss = 0.60815226\n",
            "Iteration 268, loss = 0.60797487\n",
            "Iteration 269, loss = 0.60780207\n",
            "Iteration 270, loss = 0.60775946\n",
            "Iteration 271, loss = 0.60753817\n",
            "Iteration 272, loss = 0.60762989\n",
            "Iteration 273, loss = 0.60747713\n",
            "Iteration 274, loss = 0.60742244\n",
            "Iteration 275, loss = 0.60741663\n",
            "Iteration 276, loss = 0.60724553\n",
            "Iteration 277, loss = 0.60715964\n",
            "Iteration 278, loss = 0.60714929\n",
            "Iteration 279, loss = 0.60711278\n",
            "Iteration 280, loss = 0.60714240\n",
            "Iteration 281, loss = 0.60688902\n",
            "Iteration 282, loss = 0.60688261\n",
            "Iteration 283, loss = 0.60675228\n",
            "Iteration 284, loss = 0.60673073\n",
            "Iteration 285, loss = 0.60662240\n",
            "Iteration 286, loss = 0.60648363\n",
            "Iteration 287, loss = 0.60642648\n",
            "Iteration 288, loss = 0.60652859\n",
            "Iteration 289, loss = 0.60640585\n",
            "Iteration 290, loss = 0.60643919\n",
            "Iteration 291, loss = 0.60640558\n",
            "Iteration 292, loss = 0.60640731\n",
            "Iteration 293, loss = 0.60631493\n",
            "Iteration 294, loss = 0.60634292\n",
            "Iteration 295, loss = 0.60578904\n",
            "Iteration 296, loss = 0.60581310\n",
            "Iteration 297, loss = 0.60575325\n",
            "Iteration 298, loss = 0.60572451\n",
            "Iteration 299, loss = 0.60552419\n",
            "Iteration 300, loss = 0.60547618\n",
            "Iteration 301, loss = 0.60549088\n",
            "Iteration 302, loss = 0.60554462\n",
            "Iteration 303, loss = 0.60533820\n",
            "Iteration 304, loss = 0.60530178\n",
            "Iteration 305, loss = 0.60534071\n",
            "Iteration 306, loss = 0.60535893\n",
            "Iteration 307, loss = 0.60516377\n",
            "Iteration 308, loss = 0.60475164\n",
            "Iteration 309, loss = 0.60494713\n",
            "Iteration 310, loss = 0.60495354\n",
            "Iteration 311, loss = 0.60492731\n",
            "Iteration 312, loss = 0.60497259\n",
            "Iteration 313, loss = 0.60484711\n",
            "Iteration 314, loss = 0.60446006\n",
            "Iteration 315, loss = 0.60437214\n",
            "Iteration 316, loss = 0.60436537\n",
            "Iteration 317, loss = 0.60424697\n",
            "Iteration 318, loss = 0.60422698\n",
            "Iteration 319, loss = 0.60410130\n",
            "Iteration 320, loss = 0.60406964\n",
            "Iteration 321, loss = 0.60407916\n",
            "Iteration 322, loss = 0.60400159\n",
            "Iteration 323, loss = 0.60394085\n",
            "Iteration 324, loss = 0.60387493\n",
            "Iteration 325, loss = 0.60389631\n",
            "Iteration 326, loss = 0.60402187\n",
            "Iteration 327, loss = 0.60373203\n",
            "Iteration 328, loss = 0.60364057\n",
            "Iteration 329, loss = 0.60358273\n",
            "Iteration 330, loss = 0.60353458\n",
            "Iteration 331, loss = 0.60371682\n",
            "Iteration 332, loss = 0.60361125\n",
            "Iteration 333, loss = 0.60339832\n",
            "Iteration 334, loss = 0.60335592\n",
            "Iteration 335, loss = 0.60313354\n",
            "Iteration 336, loss = 0.60324236\n",
            "Iteration 337, loss = 0.60312464\n",
            "Iteration 338, loss = 0.60307314\n",
            "Iteration 339, loss = 0.60300734\n",
            "Iteration 340, loss = 0.60292641\n",
            "Iteration 341, loss = 0.60306346\n",
            "Iteration 342, loss = 0.60281813\n",
            "Iteration 343, loss = 0.60279986\n",
            "Iteration 344, loss = 0.60266123\n",
            "Iteration 345, loss = 0.60280930\n",
            "Iteration 346, loss = 0.60274881\n",
            "Iteration 347, loss = 0.60296461\n",
            "Iteration 348, loss = 0.60239067\n",
            "Iteration 349, loss = 0.60238152\n",
            "Iteration 350, loss = 0.60238214\n",
            "Iteration 351, loss = 0.60236939\n",
            "Iteration 352, loss = 0.60214794\n",
            "Iteration 353, loss = 0.60211755\n",
            "Iteration 354, loss = 0.60227206\n",
            "Iteration 355, loss = 0.60220438\n",
            "Iteration 356, loss = 0.60196552\n",
            "Iteration 357, loss = 0.60197213\n",
            "Iteration 358, loss = 0.60196773\n",
            "Iteration 359, loss = 0.60187824\n",
            "Iteration 360, loss = 0.60176287\n",
            "Iteration 361, loss = 0.60163146\n",
            "Iteration 362, loss = 0.60192315\n",
            "Iteration 363, loss = 0.60159106\n",
            "Iteration 364, loss = 0.60152530\n",
            "Iteration 365, loss = 0.60149885\n",
            "Iteration 366, loss = 0.60141211\n",
            "Iteration 367, loss = 0.60137760\n",
            "Iteration 368, loss = 0.60148480\n",
            "Iteration 369, loss = 0.60126429\n",
            "Iteration 370, loss = 0.60124647\n",
            "Iteration 371, loss = 0.60101911\n",
            "Iteration 372, loss = 0.60097295\n",
            "Iteration 373, loss = 0.60105295\n",
            "Iteration 374, loss = 0.60080896\n",
            "Iteration 375, loss = 0.60086795\n",
            "Iteration 376, loss = 0.60073872\n",
            "Iteration 377, loss = 0.60076317\n",
            "Iteration 378, loss = 0.60072960\n",
            "Iteration 379, loss = 0.60060569\n",
            "Iteration 380, loss = 0.60055705\n",
            "Iteration 381, loss = 0.60041968\n",
            "Iteration 382, loss = 0.60042019\n",
            "Iteration 383, loss = 0.60109144\n",
            "Iteration 384, loss = 0.60051050\n",
            "Iteration 385, loss = 0.60054112\n",
            "Iteration 386, loss = 0.60037744\n",
            "Iteration 387, loss = 0.60020594\n",
            "Iteration 388, loss = 0.60020277\n",
            "Iteration 389, loss = 0.60016259\n",
            "Iteration 390, loss = 0.60010759\n",
            "Iteration 391, loss = 0.60003199\n",
            "Iteration 392, loss = 0.60002565\n",
            "Iteration 393, loss = 0.59988444\n",
            "Iteration 394, loss = 0.59990280\n",
            "Iteration 395, loss = 0.59975680\n",
            "Iteration 396, loss = 0.59978026\n",
            "Iteration 397, loss = 0.59979949\n",
            "Iteration 398, loss = 0.59962341\n",
            "Iteration 399, loss = 0.59964589\n",
            "Iteration 400, loss = 0.59970054\n",
            "Iteration 1, loss = 0.70132854\n",
            "Iteration 2, loss = 0.69287312\n",
            "Iteration 3, loss = 0.68275175\n",
            "Iteration 4, loss = 0.67468202\n",
            "Iteration 5, loss = 0.67035110\n",
            "Iteration 6, loss = 0.66818697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 7, loss = 0.66635457\n",
            "Iteration 8, loss = 0.66549974\n",
            "Iteration 9, loss = 0.66510182\n",
            "Iteration 10, loss = 0.66473949\n",
            "Iteration 11, loss = 0.66447079\n",
            "Iteration 12, loss = 0.66430735\n",
            "Iteration 13, loss = 0.66408644\n",
            "Iteration 14, loss = 0.66386603\n",
            "Iteration 15, loss = 0.66373528\n",
            "Iteration 16, loss = 0.66365288\n",
            "Iteration 17, loss = 0.66350917\n",
            "Iteration 18, loss = 0.66328273\n",
            "Iteration 19, loss = 0.66317509\n",
            "Iteration 20, loss = 0.66303879\n",
            "Iteration 21, loss = 0.66294421\n",
            "Iteration 22, loss = 0.66283362\n",
            "Iteration 23, loss = 0.66272062\n",
            "Iteration 24, loss = 0.66258737\n",
            "Iteration 25, loss = 0.66249816\n",
            "Iteration 26, loss = 0.66240665\n",
            "Iteration 27, loss = 0.66213396\n",
            "Iteration 28, loss = 0.66204159\n",
            "Iteration 29, loss = 0.66189876\n",
            "Iteration 30, loss = 0.66175106\n",
            "Iteration 31, loss = 0.66159835\n",
            "Iteration 32, loss = 0.66146942\n",
            "Iteration 33, loss = 0.66136178\n",
            "Iteration 34, loss = 0.66121499\n",
            "Iteration 35, loss = 0.66113034\n",
            "Iteration 36, loss = 0.66092172\n",
            "Iteration 37, loss = 0.66078342\n",
            "Iteration 38, loss = 0.66073598\n",
            "Iteration 39, loss = 0.66053407\n",
            "Iteration 40, loss = 0.66046496\n",
            "Iteration 41, loss = 0.66031700\n",
            "Iteration 42, loss = 0.66027374\n",
            "Iteration 43, loss = 0.66006662\n",
            "Iteration 44, loss = 0.65992198\n",
            "Iteration 45, loss = 0.65978218\n",
            "Iteration 46, loss = 0.65968138\n",
            "Iteration 47, loss = 0.65951588\n",
            "Iteration 48, loss = 0.65934629\n",
            "Iteration 49, loss = 0.65921934\n",
            "Iteration 50, loss = 0.65906390\n",
            "Iteration 51, loss = 0.65890425\n",
            "Iteration 52, loss = 0.65874710\n",
            "Iteration 53, loss = 0.65872471\n",
            "Iteration 54, loss = 0.65843642\n",
            "Iteration 55, loss = 0.65841507\n",
            "Iteration 56, loss = 0.65816825\n",
            "Iteration 57, loss = 0.65802158\n",
            "Iteration 58, loss = 0.65793728\n",
            "Iteration 59, loss = 0.65779550\n",
            "Iteration 60, loss = 0.65777745\n",
            "Iteration 61, loss = 0.65755624\n",
            "Iteration 62, loss = 0.65741542\n",
            "Iteration 63, loss = 0.65733910\n",
            "Iteration 64, loss = 0.65707865\n",
            "Iteration 65, loss = 0.65688566\n",
            "Iteration 66, loss = 0.65679769\n",
            "Iteration 67, loss = 0.65658186\n",
            "Iteration 68, loss = 0.65668302\n",
            "Iteration 69, loss = 0.65649656\n",
            "Iteration 70, loss = 0.65636522\n",
            "Iteration 71, loss = 0.65619259\n",
            "Iteration 72, loss = 0.65590855\n",
            "Iteration 73, loss = 0.65580049\n",
            "Iteration 74, loss = 0.65560874\n",
            "Iteration 75, loss = 0.65544087\n",
            "Iteration 76, loss = 0.65534981\n",
            "Iteration 77, loss = 0.65509137\n",
            "Iteration 78, loss = 0.65496654\n",
            "Iteration 79, loss = 0.65480427\n",
            "Iteration 80, loss = 0.65464457\n",
            "Iteration 81, loss = 0.65453548\n",
            "Iteration 82, loss = 0.65431403\n",
            "Iteration 83, loss = 0.65421578\n",
            "Iteration 84, loss = 0.65406315\n",
            "Iteration 85, loss = 0.65395645\n",
            "Iteration 86, loss = 0.65374536\n",
            "Iteration 87, loss = 0.65361253\n",
            "Iteration 88, loss = 0.65342622\n",
            "Iteration 89, loss = 0.65331127\n",
            "Iteration 90, loss = 0.65313779\n",
            "Iteration 91, loss = 0.65301795\n",
            "Iteration 92, loss = 0.65288137\n",
            "Iteration 93, loss = 0.65262228\n",
            "Iteration 94, loss = 0.65257727\n",
            "Iteration 95, loss = 0.65242847\n",
            "Iteration 96, loss = 0.65220480\n",
            "Iteration 97, loss = 0.65206935\n",
            "Iteration 98, loss = 0.65200090\n",
            "Iteration 99, loss = 0.65190208\n",
            "Iteration 100, loss = 0.65180363\n",
            "Iteration 101, loss = 0.65171716\n",
            "Iteration 102, loss = 0.65143911\n",
            "Iteration 103, loss = 0.65112668\n",
            "Iteration 104, loss = 0.65092235\n",
            "Iteration 105, loss = 0.65077039\n",
            "Iteration 106, loss = 0.65065655\n",
            "Iteration 107, loss = 0.65045308\n",
            "Iteration 108, loss = 0.65043977\n",
            "Iteration 109, loss = 0.65017580\n",
            "Iteration 110, loss = 0.65017679\n",
            "Iteration 111, loss = 0.64974793\n",
            "Iteration 112, loss = 0.64960645\n",
            "Iteration 113, loss = 0.64945790\n",
            "Iteration 114, loss = 0.64937097\n",
            "Iteration 115, loss = 0.64921676\n",
            "Iteration 116, loss = 0.64902660\n",
            "Iteration 117, loss = 0.64877528\n",
            "Iteration 118, loss = 0.64849805\n",
            "Iteration 119, loss = 0.64832811\n",
            "Iteration 120, loss = 0.64817537\n",
            "Iteration 121, loss = 0.64800494\n",
            "Iteration 122, loss = 0.64772569\n",
            "Iteration 123, loss = 0.64755692\n",
            "Iteration 124, loss = 0.64735220\n",
            "Iteration 125, loss = 0.64716433\n",
            "Iteration 126, loss = 0.64705562\n",
            "Iteration 127, loss = 0.64693868\n",
            "Iteration 128, loss = 0.64679402\n",
            "Iteration 129, loss = 0.64662976\n",
            "Iteration 130, loss = 0.64626933\n",
            "Iteration 131, loss = 0.64634874\n",
            "Iteration 132, loss = 0.64589344\n",
            "Iteration 133, loss = 0.64587682\n",
            "Iteration 134, loss = 0.64566536\n",
            "Iteration 135, loss = 0.64542465\n",
            "Iteration 136, loss = 0.64526689\n",
            "Iteration 137, loss = 0.64514253\n",
            "Iteration 138, loss = 0.64505451\n",
            "Iteration 139, loss = 0.64470673\n",
            "Iteration 140, loss = 0.64443785\n",
            "Iteration 141, loss = 0.64425744\n",
            "Iteration 142, loss = 0.64412906\n",
            "Iteration 143, loss = 0.64394041\n",
            "Iteration 144, loss = 0.64378231\n",
            "Iteration 145, loss = 0.64366418\n",
            "Iteration 146, loss = 0.64354023\n",
            "Iteration 147, loss = 0.64326370\n",
            "Iteration 148, loss = 0.64312018\n",
            "Iteration 149, loss = 0.64277016\n",
            "Iteration 150, loss = 0.64262566\n",
            "Iteration 151, loss = 0.64251927\n",
            "Iteration 152, loss = 0.64236177\n",
            "Iteration 153, loss = 0.64218173\n",
            "Iteration 154, loss = 0.64200055\n",
            "Iteration 155, loss = 0.64177105\n",
            "Iteration 156, loss = 0.64189082\n",
            "Iteration 157, loss = 0.64156525\n",
            "Iteration 158, loss = 0.64131399\n",
            "Iteration 159, loss = 0.64124377\n",
            "Iteration 160, loss = 0.64106916\n",
            "Iteration 161, loss = 0.64071383\n",
            "Iteration 162, loss = 0.64056043\n",
            "Iteration 163, loss = 0.64045584\n",
            "Iteration 164, loss = 0.64013580\n",
            "Iteration 165, loss = 0.64001319\n",
            "Iteration 166, loss = 0.63993873\n",
            "Iteration 167, loss = 0.63971520\n",
            "Iteration 168, loss = 0.63934839\n",
            "Iteration 169, loss = 0.63919949\n",
            "Iteration 170, loss = 0.63925772\n",
            "Iteration 171, loss = 0.63894400\n",
            "Iteration 172, loss = 0.63866005\n",
            "Iteration 173, loss = 0.63841888\n",
            "Iteration 174, loss = 0.63848251\n",
            "Iteration 175, loss = 0.63790680\n",
            "Iteration 176, loss = 0.63772628\n",
            "Iteration 177, loss = 0.63767687\n",
            "Iteration 178, loss = 0.63747965\n",
            "Iteration 179, loss = 0.63725637\n",
            "Iteration 180, loss = 0.63700980\n",
            "Iteration 181, loss = 0.63685611\n",
            "Iteration 182, loss = 0.63661726\n",
            "Iteration 183, loss = 0.63660550\n",
            "Iteration 184, loss = 0.63629942\n",
            "Iteration 185, loss = 0.63613893\n",
            "Iteration 186, loss = 0.63600236\n",
            "Iteration 187, loss = 0.63581303\n",
            "Iteration 188, loss = 0.63563070\n",
            "Iteration 189, loss = 0.63552122\n",
            "Iteration 190, loss = 0.63526910\n",
            "Iteration 191, loss = 0.63515355\n",
            "Iteration 192, loss = 0.63513435\n",
            "Iteration 193, loss = 0.63517763\n",
            "Iteration 194, loss = 0.63494554\n",
            "Iteration 195, loss = 0.63467597\n",
            "Iteration 196, loss = 0.63453732\n",
            "Iteration 197, loss = 0.63428936\n",
            "Iteration 198, loss = 0.63397471\n",
            "Iteration 199, loss = 0.63390067\n",
            "Iteration 200, loss = 0.63386395\n",
            "Iteration 201, loss = 0.63358919\n",
            "Iteration 202, loss = 0.63352074\n",
            "Iteration 203, loss = 0.63327216\n",
            "Iteration 204, loss = 0.63280869\n",
            "Iteration 205, loss = 0.63267615\n",
            "Iteration 206, loss = 0.63259686\n",
            "Iteration 207, loss = 0.63240636\n",
            "Iteration 208, loss = 0.63217178\n",
            "Iteration 209, loss = 0.63216408\n",
            "Iteration 210, loss = 0.63183617\n",
            "Iteration 211, loss = 0.63168334\n",
            "Iteration 212, loss = 0.63157100\n",
            "Iteration 213, loss = 0.63134643\n",
            "Iteration 214, loss = 0.63121179\n",
            "Iteration 215, loss = 0.63112284\n",
            "Iteration 216, loss = 0.63090712\n",
            "Iteration 217, loss = 0.63114202\n",
            "Iteration 218, loss = 0.63069114\n",
            "Iteration 219, loss = 0.63061179\n",
            "Iteration 220, loss = 0.63036296\n",
            "Iteration 221, loss = 0.63019727\n",
            "Iteration 222, loss = 0.63000866\n",
            "Iteration 223, loss = 0.62989670\n",
            "Iteration 224, loss = 0.62965001\n",
            "Iteration 225, loss = 0.62952927\n",
            "Iteration 226, loss = 0.62937425\n",
            "Iteration 227, loss = 0.62944611\n",
            "Iteration 228, loss = 0.62917044\n",
            "Iteration 229, loss = 0.62899652\n",
            "Iteration 230, loss = 0.62881025\n",
            "Iteration 231, loss = 0.62873845\n",
            "Iteration 232, loss = 0.62850708\n",
            "Iteration 233, loss = 0.62843285\n",
            "Iteration 234, loss = 0.62813023\n",
            "Iteration 235, loss = 0.62794358\n",
            "Iteration 236, loss = 0.62786284\n",
            "Iteration 237, loss = 0.62773277\n",
            "Iteration 238, loss = 0.62759361\n",
            "Iteration 239, loss = 0.62742948\n",
            "Iteration 240, loss = 0.62725977\n",
            "Iteration 241, loss = 0.62710645\n",
            "Iteration 242, loss = 0.62692039\n",
            "Iteration 243, loss = 0.62672293\n",
            "Iteration 244, loss = 0.62662464\n",
            "Iteration 245, loss = 0.62650646\n",
            "Iteration 246, loss = 0.62646562\n",
            "Iteration 247, loss = 0.62637940\n",
            "Iteration 248, loss = 0.62623441\n",
            "Iteration 249, loss = 0.62615619\n",
            "Iteration 250, loss = 0.62602326\n",
            "Iteration 251, loss = 0.62591770\n",
            "Iteration 252, loss = 0.62574207\n",
            "Iteration 253, loss = 0.62577175\n",
            "Iteration 254, loss = 0.62553926\n",
            "Iteration 255, loss = 0.62550665\n",
            "Iteration 256, loss = 0.62548868\n",
            "Iteration 257, loss = 0.62535783\n",
            "Iteration 258, loss = 0.62529811\n",
            "Iteration 259, loss = 0.62505963\n",
            "Iteration 260, loss = 0.62492605\n",
            "Iteration 261, loss = 0.62484884\n",
            "Iteration 262, loss = 0.62473553\n",
            "Iteration 263, loss = 0.62472644\n",
            "Iteration 264, loss = 0.62454158\n",
            "Iteration 265, loss = 0.62448235\n",
            "Iteration 266, loss = 0.62421118\n",
            "Iteration 267, loss = 0.62415711\n",
            "Iteration 268, loss = 0.62403350\n",
            "Iteration 269, loss = 0.62390459\n",
            "Iteration 270, loss = 0.62376084\n",
            "Iteration 271, loss = 0.62366829\n",
            "Iteration 272, loss = 0.62375447\n",
            "Iteration 273, loss = 0.62357468\n",
            "Iteration 274, loss = 0.62341191\n",
            "Iteration 275, loss = 0.62327645\n",
            "Iteration 276, loss = 0.62331013\n",
            "Iteration 277, loss = 0.62322763\n",
            "Iteration 278, loss = 0.62298331\n",
            "Iteration 279, loss = 0.62279025\n",
            "Iteration 280, loss = 0.62280703\n",
            "Iteration 281, loss = 0.62267981\n",
            "Iteration 282, loss = 0.62261408\n",
            "Iteration 283, loss = 0.62258676\n",
            "Iteration 284, loss = 0.62256702\n",
            "Iteration 285, loss = 0.62254104\n",
            "Iteration 286, loss = 0.62242704\n",
            "Iteration 287, loss = 0.62238605\n",
            "Iteration 288, loss = 0.62225836\n",
            "Iteration 289, loss = 0.62218893\n",
            "Iteration 290, loss = 0.62212260\n",
            "Iteration 291, loss = 0.62211048\n",
            "Iteration 292, loss = 0.62225366\n",
            "Iteration 293, loss = 0.62169048\n",
            "Iteration 294, loss = 0.62173168\n",
            "Iteration 295, loss = 0.62152076\n",
            "Iteration 296, loss = 0.62148407\n",
            "Iteration 297, loss = 0.62145377\n",
            "Iteration 298, loss = 0.62127881\n",
            "Iteration 299, loss = 0.62123278\n",
            "Iteration 300, loss = 0.62125042\n",
            "Iteration 301, loss = 0.62122590\n",
            "Iteration 302, loss = 0.62124720\n",
            "Iteration 303, loss = 0.62129385\n",
            "Iteration 304, loss = 0.62110986\n",
            "Iteration 305, loss = 0.62099492\n",
            "Iteration 306, loss = 0.62079388\n",
            "Iteration 307, loss = 0.62046041\n",
            "Iteration 308, loss = 0.62038145\n",
            "Iteration 309, loss = 0.62040724\n",
            "Iteration 310, loss = 0.62043587\n",
            "Iteration 311, loss = 0.62026730\n",
            "Iteration 312, loss = 0.62013747\n",
            "Iteration 313, loss = 0.62002927\n",
            "Iteration 314, loss = 0.61992819\n",
            "Iteration 315, loss = 0.61977238\n",
            "Iteration 316, loss = 0.61951189\n",
            "Iteration 317, loss = 0.61955723\n",
            "Iteration 318, loss = 0.61943665\n",
            "Iteration 319, loss = 0.61960136\n",
            "Iteration 320, loss = 0.61960769\n",
            "Iteration 321, loss = 0.61939754\n",
            "Iteration 322, loss = 0.61930479\n",
            "Iteration 323, loss = 0.61928351\n",
            "Iteration 324, loss = 0.61903477\n",
            "Iteration 325, loss = 0.61908308\n",
            "Iteration 326, loss = 0.61898803\n",
            "Iteration 327, loss = 0.61883371\n",
            "Iteration 328, loss = 0.61899367\n",
            "Iteration 329, loss = 0.61879530\n",
            "Iteration 330, loss = 0.61874035\n",
            "Iteration 331, loss = 0.61878521\n",
            "Iteration 332, loss = 0.61872070\n",
            "Iteration 333, loss = 0.61863605\n",
            "Iteration 334, loss = 0.61867904\n",
            "Iteration 335, loss = 0.61875168\n",
            "Iteration 336, loss = 0.61888614\n",
            "Iteration 337, loss = 0.61863766\n",
            "Iteration 338, loss = 0.61841590\n",
            "Iteration 339, loss = 0.61835586\n",
            "Iteration 340, loss = 0.61815602\n",
            "Iteration 341, loss = 0.61826877\n",
            "Iteration 342, loss = 0.61819784\n",
            "Iteration 343, loss = 0.61806355\n",
            "Iteration 344, loss = 0.61799036\n",
            "Iteration 345, loss = 0.61803137\n",
            "Iteration 346, loss = 0.61809641\n",
            "Iteration 347, loss = 0.61792190\n",
            "Iteration 348, loss = 0.61754043\n",
            "Iteration 349, loss = 0.61760191\n",
            "Iteration 350, loss = 0.61796848\n",
            "Iteration 351, loss = 0.61783696\n",
            "Iteration 352, loss = 0.61760118\n",
            "Iteration 353, loss = 0.61745849\n",
            "Iteration 354, loss = 0.61773746\n",
            "Iteration 355, loss = 0.61767030\n",
            "Iteration 356, loss = 0.61740584\n",
            "Iteration 357, loss = 0.61729551\n",
            "Iteration 358, loss = 0.61732608\n",
            "Iteration 359, loss = 0.61720565\n",
            "Iteration 360, loss = 0.61722434\n",
            "Iteration 361, loss = 0.61717582\n",
            "Iteration 362, loss = 0.61708414\n",
            "Iteration 363, loss = 0.61700484\n",
            "Iteration 364, loss = 0.61701479\n",
            "Iteration 365, loss = 0.61706324\n",
            "Iteration 366, loss = 0.61697229\n",
            "Iteration 367, loss = 0.61695851\n",
            "Iteration 368, loss = 0.61677616\n",
            "Iteration 369, loss = 0.61676745\n",
            "Iteration 370, loss = 0.61669517\n",
            "Iteration 371, loss = 0.61668134\n",
            "Iteration 372, loss = 0.61681565\n",
            "Iteration 373, loss = 0.61670174\n",
            "Iteration 374, loss = 0.61660845\n",
            "Iteration 375, loss = 0.61648420\n",
            "Iteration 376, loss = 0.61651100\n",
            "Iteration 377, loss = 0.61636725\n",
            "Iteration 378, loss = 0.61632511\n",
            "Iteration 379, loss = 0.61629938\n",
            "Iteration 380, loss = 0.61626256\n",
            "Iteration 381, loss = 0.61622399\n",
            "Iteration 382, loss = 0.61620910\n",
            "Iteration 383, loss = 0.61613454\n",
            "Iteration 384, loss = 0.61610028\n",
            "Iteration 385, loss = 0.61622042\n",
            "Iteration 386, loss = 0.61605492\n",
            "Iteration 387, loss = 0.61611385\n",
            "Iteration 388, loss = 0.61601366\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68420352\n",
            "Iteration 2, loss = 0.67847284\n",
            "Iteration 3, loss = 0.67292781\n",
            "Iteration 4, loss = 0.66869451\n",
            "Iteration 5, loss = 0.66531771\n",
            "Iteration 6, loss = 0.66394837\n",
            "Iteration 7, loss = 0.66280221\n",
            "Iteration 8, loss = 0.66242169\n",
            "Iteration 9, loss = 0.66215024\n",
            "Iteration 10, loss = 0.66185887\n",
            "Iteration 11, loss = 0.66162768\n",
            "Iteration 12, loss = 0.66150632\n",
            "Iteration 13, loss = 0.66126878\n",
            "Iteration 14, loss = 0.66110438\n",
            "Iteration 15, loss = 0.66094439\n",
            "Iteration 16, loss = 0.66087460\n",
            "Iteration 17, loss = 0.66074023\n",
            "Iteration 18, loss = 0.66049099\n",
            "Iteration 19, loss = 0.66009899\n",
            "Iteration 20, loss = 0.65993560\n",
            "Iteration 21, loss = 0.65977588\n",
            "Iteration 22, loss = 0.65966790\n",
            "Iteration 23, loss = 0.65947364\n",
            "Iteration 24, loss = 0.65931345\n",
            "Iteration 25, loss = 0.65916587\n",
            "Iteration 26, loss = 0.65890270\n",
            "Iteration 27, loss = 0.65876162\n",
            "Iteration 28, loss = 0.65879335\n",
            "Iteration 29, loss = 0.65840227\n",
            "Iteration 30, loss = 0.65813379\n",
            "Iteration 31, loss = 0.65808768\n",
            "Iteration 32, loss = 0.65797059\n",
            "Iteration 33, loss = 0.65786738\n",
            "Iteration 34, loss = 0.65764206\n",
            "Iteration 35, loss = 0.65749619\n",
            "Iteration 36, loss = 0.65733639\n",
            "Iteration 37, loss = 0.65715266\n",
            "Iteration 38, loss = 0.65701005\n",
            "Iteration 39, loss = 0.65669387\n",
            "Iteration 40, loss = 0.65661396\n",
            "Iteration 41, loss = 0.65617622\n",
            "Iteration 42, loss = 0.65592747\n",
            "Iteration 43, loss = 0.65582819\n",
            "Iteration 44, loss = 0.65552609\n",
            "Iteration 45, loss = 0.65531606\n",
            "Iteration 46, loss = 0.65512430\n",
            "Iteration 47, loss = 0.65485879\n",
            "Iteration 48, loss = 0.65473066\n",
            "Iteration 49, loss = 0.65430103\n",
            "Iteration 50, loss = 0.65413206\n",
            "Iteration 51, loss = 0.65383230\n",
            "Iteration 52, loss = 0.65364503\n",
            "Iteration 53, loss = 0.65334486\n",
            "Iteration 54, loss = 0.65314937\n",
            "Iteration 55, loss = 0.65293921\n",
            "Iteration 56, loss = 0.65272097\n",
            "Iteration 57, loss = 0.65253392\n",
            "Iteration 58, loss = 0.65229254\n",
            "Iteration 59, loss = 0.65214908\n",
            "Iteration 60, loss = 0.65197725\n",
            "Iteration 61, loss = 0.65176533\n",
            "Iteration 62, loss = 0.65151703\n",
            "Iteration 63, loss = 0.65126798\n",
            "Iteration 64, loss = 0.65098332\n",
            "Iteration 65, loss = 0.65077266\n",
            "Iteration 66, loss = 0.65059923\n",
            "Iteration 67, loss = 0.65040745\n",
            "Iteration 68, loss = 0.65018734\n",
            "Iteration 69, loss = 0.65001708\n",
            "Iteration 70, loss = 0.64979301\n",
            "Iteration 71, loss = 0.64960889\n",
            "Iteration 72, loss = 0.64934277\n",
            "Iteration 73, loss = 0.64909782\n",
            "Iteration 74, loss = 0.64888504\n",
            "Iteration 75, loss = 0.64883403\n",
            "Iteration 76, loss = 0.64847047\n",
            "Iteration 77, loss = 0.64832049\n",
            "Iteration 78, loss = 0.64808252\n",
            "Iteration 79, loss = 0.64795860\n",
            "Iteration 80, loss = 0.64761816\n",
            "Iteration 81, loss = 0.64757551\n",
            "Iteration 82, loss = 0.64752191\n",
            "Iteration 83, loss = 0.64730102\n",
            "Iteration 84, loss = 0.64706368\n",
            "Iteration 85, loss = 0.64672353\n",
            "Iteration 86, loss = 0.64611218\n",
            "Iteration 87, loss = 0.64589665\n",
            "Iteration 88, loss = 0.64573520\n",
            "Iteration 89, loss = 0.64543558\n",
            "Iteration 90, loss = 0.64511084\n",
            "Iteration 91, loss = 0.64493163\n",
            "Iteration 92, loss = 0.64463672\n",
            "Iteration 93, loss = 0.64430895\n",
            "Iteration 94, loss = 0.64419094\n",
            "Iteration 95, loss = 0.64394698\n",
            "Iteration 96, loss = 0.64376186\n",
            "Iteration 97, loss = 0.64352631\n",
            "Iteration 98, loss = 0.64343036\n",
            "Iteration 99, loss = 0.64316110\n",
            "Iteration 100, loss = 0.64268274\n",
            "Iteration 101, loss = 0.64272672\n",
            "Iteration 102, loss = 0.64208095\n",
            "Iteration 103, loss = 0.64186983\n",
            "Iteration 104, loss = 0.64168521\n",
            "Iteration 105, loss = 0.64139261\n",
            "Iteration 106, loss = 0.64108290\n",
            "Iteration 107, loss = 0.64087146\n",
            "Iteration 108, loss = 0.64064567\n",
            "Iteration 109, loss = 0.64030848\n",
            "Iteration 110, loss = 0.64038229\n",
            "Iteration 111, loss = 0.63991350\n",
            "Iteration 112, loss = 0.63967879\n",
            "Iteration 113, loss = 0.63949198\n",
            "Iteration 114, loss = 0.63925390\n",
            "Iteration 115, loss = 0.63905471\n",
            "Iteration 116, loss = 0.63879634\n",
            "Iteration 117, loss = 0.63847895\n",
            "Iteration 118, loss = 0.63826484\n",
            "Iteration 119, loss = 0.63799246\n",
            "Iteration 120, loss = 0.63769168\n",
            "Iteration 121, loss = 0.63737749\n",
            "Iteration 122, loss = 0.63725741\n",
            "Iteration 123, loss = 0.63695093\n",
            "Iteration 124, loss = 0.63679600\n",
            "Iteration 125, loss = 0.63669114\n",
            "Iteration 126, loss = 0.63645812\n",
            "Iteration 127, loss = 0.63610597\n",
            "Iteration 128, loss = 0.63592179\n",
            "Iteration 129, loss = 0.63575672\n",
            "Iteration 130, loss = 0.63549942\n",
            "Iteration 131, loss = 0.63522436\n",
            "Iteration 132, loss = 0.63481043\n",
            "Iteration 133, loss = 0.63458783\n",
            "Iteration 134, loss = 0.63432880\n",
            "Iteration 135, loss = 0.63406070\n",
            "Iteration 136, loss = 0.63378946\n",
            "Iteration 137, loss = 0.63349806\n",
            "Iteration 138, loss = 0.63330062\n",
            "Iteration 139, loss = 0.63305720\n",
            "Iteration 140, loss = 0.63277605\n",
            "Iteration 141, loss = 0.63263710\n",
            "Iteration 142, loss = 0.63244090\n",
            "Iteration 143, loss = 0.63224488\n",
            "Iteration 144, loss = 0.63205793\n",
            "Iteration 145, loss = 0.63177909\n",
            "Iteration 146, loss = 0.63143538\n",
            "Iteration 147, loss = 0.63117097\n",
            "Iteration 148, loss = 0.63095101\n",
            "Iteration 149, loss = 0.63077164\n",
            "Iteration 150, loss = 0.63050425\n",
            "Iteration 151, loss = 0.63032071\n",
            "Iteration 152, loss = 0.63000006\n",
            "Iteration 153, loss = 0.62976115\n",
            "Iteration 154, loss = 0.62958221\n",
            "Iteration 155, loss = 0.62934897\n",
            "Iteration 156, loss = 0.62907907\n",
            "Iteration 157, loss = 0.62883700\n",
            "Iteration 158, loss = 0.62863039\n",
            "Iteration 159, loss = 0.62843416\n",
            "Iteration 160, loss = 0.62813701\n",
            "Iteration 161, loss = 0.62796598\n",
            "Iteration 162, loss = 0.62768320\n",
            "Iteration 163, loss = 0.62750649\n",
            "Iteration 164, loss = 0.62725313\n",
            "Iteration 165, loss = 0.62707881\n",
            "Iteration 166, loss = 0.62677325\n",
            "Iteration 167, loss = 0.62656218\n",
            "Iteration 168, loss = 0.62640922\n",
            "Iteration 169, loss = 0.62612163\n",
            "Iteration 170, loss = 0.62586593\n",
            "Iteration 171, loss = 0.62565812\n",
            "Iteration 172, loss = 0.62539985\n",
            "Iteration 173, loss = 0.62514670\n",
            "Iteration 174, loss = 0.62490140\n",
            "Iteration 175, loss = 0.62465551\n",
            "Iteration 176, loss = 0.62458363\n",
            "Iteration 177, loss = 0.62425450\n",
            "Iteration 178, loss = 0.62399524\n",
            "Iteration 179, loss = 0.62377019\n",
            "Iteration 180, loss = 0.62346150\n",
            "Iteration 181, loss = 0.62335474\n",
            "Iteration 182, loss = 0.62314579\n",
            "Iteration 183, loss = 0.62308020\n",
            "Iteration 184, loss = 0.62282100\n",
            "Iteration 185, loss = 0.62261170\n",
            "Iteration 186, loss = 0.62229450\n",
            "Iteration 187, loss = 0.62217217\n",
            "Iteration 188, loss = 0.62199887\n",
            "Iteration 189, loss = 0.62176196\n",
            "Iteration 190, loss = 0.62161369\n",
            "Iteration 191, loss = 0.62151223\n",
            "Iteration 192, loss = 0.62129711\n",
            "Iteration 193, loss = 0.62101165\n",
            "Iteration 194, loss = 0.62100012\n",
            "Iteration 195, loss = 0.62073422\n",
            "Iteration 196, loss = 0.62052405\n",
            "Iteration 197, loss = 0.62030449\n",
            "Iteration 198, loss = 0.62017892\n",
            "Iteration 199, loss = 0.61982381\n",
            "Iteration 200, loss = 0.61963092\n",
            "Iteration 201, loss = 0.61945952\n",
            "Iteration 202, loss = 0.61928803\n",
            "Iteration 203, loss = 0.61904582\n",
            "Iteration 204, loss = 0.61883815\n",
            "Iteration 205, loss = 0.61868309\n",
            "Iteration 206, loss = 0.61841818\n",
            "Iteration 207, loss = 0.61830839\n",
            "Iteration 208, loss = 0.61807590\n",
            "Iteration 209, loss = 0.61781777\n",
            "Iteration 210, loss = 0.61767772\n",
            "Iteration 211, loss = 0.61757904\n",
            "Iteration 212, loss = 0.61734180\n",
            "Iteration 213, loss = 0.61717335\n",
            "Iteration 214, loss = 0.61698846\n",
            "Iteration 215, loss = 0.61677967\n",
            "Iteration 216, loss = 0.61670593\n",
            "Iteration 217, loss = 0.61646683\n",
            "Iteration 218, loss = 0.61635867\n",
            "Iteration 219, loss = 0.61616897\n",
            "Iteration 220, loss = 0.61607080\n",
            "Iteration 221, loss = 0.61601851\n",
            "Iteration 222, loss = 0.61577579\n",
            "Iteration 223, loss = 0.61563855\n",
            "Iteration 224, loss = 0.61538446\n",
            "Iteration 225, loss = 0.61515588\n",
            "Iteration 226, loss = 0.61496867\n",
            "Iteration 227, loss = 0.61486335\n",
            "Iteration 228, loss = 0.61482061\n",
            "Iteration 229, loss = 0.61493681\n",
            "Iteration 230, loss = 0.61470602\n",
            "Iteration 231, loss = 0.61454498\n",
            "Iteration 232, loss = 0.61430221\n",
            "Iteration 233, loss = 0.61413058\n",
            "Iteration 234, loss = 0.61389248\n",
            "Iteration 235, loss = 0.61379480\n",
            "Iteration 236, loss = 0.61367320\n",
            "Iteration 237, loss = 0.61356842\n",
            "Iteration 238, loss = 0.61390043\n",
            "Iteration 239, loss = 0.61374271\n",
            "Iteration 240, loss = 0.61332659\n",
            "Iteration 241, loss = 0.61304152\n",
            "Iteration 242, loss = 0.61280281\n",
            "Iteration 243, loss = 0.61269963\n",
            "Iteration 244, loss = 0.61253660\n",
            "Iteration 245, loss = 0.61230584\n",
            "Iteration 246, loss = 0.61205049\n",
            "Iteration 247, loss = 0.61189947\n",
            "Iteration 248, loss = 0.61183646\n",
            "Iteration 249, loss = 0.61163682\n",
            "Iteration 250, loss = 0.61168390\n",
            "Iteration 251, loss = 0.61140425\n",
            "Iteration 252, loss = 0.61129848\n",
            "Iteration 253, loss = 0.61119761\n",
            "Iteration 254, loss = 0.61104957\n",
            "Iteration 255, loss = 0.61091275\n",
            "Iteration 256, loss = 0.61088557\n",
            "Iteration 257, loss = 0.61065064\n",
            "Iteration 258, loss = 0.61060457\n",
            "Iteration 259, loss = 0.61063125\n",
            "Iteration 260, loss = 0.61029878\n",
            "Iteration 261, loss = 0.61028925\n",
            "Iteration 262, loss = 0.61011075\n",
            "Iteration 263, loss = 0.60993098\n",
            "Iteration 264, loss = 0.60981436\n",
            "Iteration 265, loss = 0.60968366\n",
            "Iteration 266, loss = 0.60961262\n",
            "Iteration 267, loss = 0.60938456\n",
            "Iteration 268, loss = 0.60928750\n",
            "Iteration 269, loss = 0.60932812\n",
            "Iteration 270, loss = 0.60914178\n",
            "Iteration 271, loss = 0.60898993\n",
            "Iteration 272, loss = 0.60884473\n",
            "Iteration 273, loss = 0.60876143\n",
            "Iteration 274, loss = 0.60865040\n",
            "Iteration 275, loss = 0.60866611\n",
            "Iteration 276, loss = 0.60842723\n",
            "Iteration 277, loss = 0.60849088\n",
            "Iteration 278, loss = 0.60826339\n",
            "Iteration 279, loss = 0.60803936\n",
            "Iteration 280, loss = 0.60797656\n",
            "Iteration 281, loss = 0.60781541\n",
            "Iteration 282, loss = 0.60777949\n",
            "Iteration 283, loss = 0.60766847\n",
            "Iteration 284, loss = 0.60767911\n",
            "Iteration 285, loss = 0.60761317\n",
            "Iteration 286, loss = 0.60742047\n",
            "Iteration 287, loss = 0.60730691\n",
            "Iteration 288, loss = 0.60709319\n",
            "Iteration 289, loss = 0.60706973\n",
            "Iteration 290, loss = 0.60709403\n",
            "Iteration 291, loss = 0.60689887\n",
            "Iteration 292, loss = 0.60681091\n",
            "Iteration 293, loss = 0.60663078\n",
            "Iteration 294, loss = 0.60659607\n",
            "Iteration 295, loss = 0.60642085\n",
            "Iteration 296, loss = 0.60641506\n",
            "Iteration 297, loss = 0.60630253\n",
            "Iteration 298, loss = 0.60610633\n",
            "Iteration 299, loss = 0.60602002\n",
            "Iteration 300, loss = 0.60596435\n",
            "Iteration 301, loss = 0.60588892\n",
            "Iteration 302, loss = 0.60585835\n",
            "Iteration 303, loss = 0.60566379\n",
            "Iteration 304, loss = 0.60554956\n",
            "Iteration 305, loss = 0.60547487\n",
            "Iteration 306, loss = 0.60550040\n",
            "Iteration 307, loss = 0.60533238\n",
            "Iteration 308, loss = 0.60524229\n",
            "Iteration 309, loss = 0.60526897\n",
            "Iteration 310, loss = 0.60520600\n",
            "Iteration 311, loss = 0.60486366\n",
            "Iteration 312, loss = 0.60489802\n",
            "Iteration 313, loss = 0.60480601\n",
            "Iteration 314, loss = 0.60475882\n",
            "Iteration 315, loss = 0.60465556\n",
            "Iteration 316, loss = 0.60455889\n",
            "Iteration 317, loss = 0.60447245\n",
            "Iteration 318, loss = 0.60438188\n",
            "Iteration 319, loss = 0.60421665\n",
            "Iteration 320, loss = 0.60419956\n",
            "Iteration 321, loss = 0.60442975\n",
            "Iteration 322, loss = 0.60440689\n",
            "Iteration 323, loss = 0.60413307\n",
            "Iteration 324, loss = 0.60406493\n",
            "Iteration 325, loss = 0.60394715\n",
            "Iteration 326, loss = 0.60394166\n",
            "Iteration 327, loss = 0.60384067\n",
            "Iteration 328, loss = 0.60360237\n",
            "Iteration 329, loss = 0.60361666\n",
            "Iteration 330, loss = 0.60353956\n",
            "Iteration 331, loss = 0.60353205\n",
            "Iteration 332, loss = 0.60345332\n",
            "Iteration 333, loss = 0.60343407\n",
            "Iteration 334, loss = 0.60341276\n",
            "Iteration 335, loss = 0.60328332\n",
            "Iteration 336, loss = 0.60317485\n",
            "Iteration 337, loss = 0.60309460\n",
            "Iteration 338, loss = 0.60315192\n",
            "Iteration 339, loss = 0.60318131\n",
            "Iteration 340, loss = 0.60284505\n",
            "Iteration 341, loss = 0.60309542\n",
            "Iteration 342, loss = 0.60284139\n",
            "Iteration 343, loss = 0.60276276\n",
            "Iteration 344, loss = 0.60264870\n",
            "Iteration 345, loss = 0.60254526\n",
            "Iteration 346, loss = 0.60246218\n",
            "Iteration 347, loss = 0.60243792\n",
            "Iteration 348, loss = 0.60240832\n",
            "Iteration 349, loss = 0.60232155\n",
            "Iteration 350, loss = 0.60229039\n",
            "Iteration 351, loss = 0.60240205\n",
            "Iteration 352, loss = 0.60213418\n",
            "Iteration 353, loss = 0.60213186\n",
            "Iteration 354, loss = 0.60205985\n",
            "Iteration 355, loss = 0.60195709\n",
            "Iteration 356, loss = 0.60183259\n",
            "Iteration 357, loss = 0.60203718\n",
            "Iteration 358, loss = 0.60166356\n",
            "Iteration 359, loss = 0.60175174\n",
            "Iteration 360, loss = 0.60158857\n",
            "Iteration 361, loss = 0.60152792\n",
            "Iteration 362, loss = 0.60141258\n",
            "Iteration 363, loss = 0.60142627\n",
            "Iteration 364, loss = 0.60125543\n",
            "Iteration 365, loss = 0.60117876\n",
            "Iteration 366, loss = 0.60130561\n",
            "Iteration 367, loss = 0.60123560\n",
            "Iteration 368, loss = 0.60105826\n",
            "Iteration 369, loss = 0.60101532\n",
            "Iteration 370, loss = 0.60096247\n",
            "Iteration 371, loss = 0.60094364\n",
            "Iteration 372, loss = 0.60086177\n",
            "Iteration 373, loss = 0.60088761\n",
            "Iteration 374, loss = 0.60095408\n",
            "Iteration 375, loss = 0.60055900\n",
            "Iteration 376, loss = 0.60059816\n",
            "Iteration 377, loss = 0.60055526\n",
            "Iteration 378, loss = 0.60048655\n",
            "Iteration 379, loss = 0.60042274\n",
            "Iteration 380, loss = 0.60027453\n",
            "Iteration 381, loss = 0.60067980\n",
            "Iteration 382, loss = 0.60076485\n",
            "Iteration 383, loss = 0.60028657\n",
            "Iteration 384, loss = 0.60022841\n",
            "Iteration 385, loss = 0.60018415\n",
            "Iteration 386, loss = 0.60013880\n",
            "Iteration 387, loss = 0.60016084\n",
            "Iteration 388, loss = 0.60015709\n",
            "Iteration 389, loss = 0.60012565\n",
            "Iteration 390, loss = 0.60018636\n",
            "Iteration 391, loss = 0.59993224\n",
            "Iteration 392, loss = 0.59996560\n",
            "Iteration 393, loss = 0.59982358\n",
            "Iteration 394, loss = 0.59993543\n",
            "Iteration 395, loss = 0.60000946\n",
            "Iteration 396, loss = 0.59974093\n",
            "Iteration 397, loss = 0.59965874\n",
            "Iteration 398, loss = 0.59958121\n",
            "Iteration 399, loss = 0.59974482\n",
            "Iteration 400, loss = 0.59952140\n",
            "Iteration 1, loss = 0.67587295\n",
            "Iteration 2, loss = 0.67305842\n",
            "Iteration 3, loss = 0.67004449\n",
            "Iteration 4, loss = 0.66780834\n",
            "Iteration 5, loss = 0.66642446\n",
            "Iteration 6, loss = 0.66470395\n",
            "Iteration 7, loss = 0.66334422\n",
            "Iteration 8, loss = 0.66231400\n",
            "Iteration 9, loss = 0.66215377\n",
            "Iteration 10, loss = 0.66193498\n",
            "Iteration 11, loss = 0.66183503\n",
            "Iteration 12, loss = 0.66158358\n",
            "Iteration 13, loss = 0.66138753\n",
            "Iteration 14, loss = 0.66112215\n",
            "Iteration 15, loss = 0.66094597\n",
            "Iteration 16, loss = 0.66076580\n",
            "Iteration 17, loss = 0.66061379\n",
            "Iteration 18, loss = 0.66046738\n",
            "Iteration 19, loss = 0.66032112\n",
            "Iteration 20, loss = 0.66011434\n",
            "Iteration 21, loss = 0.65991652\n",
            "Iteration 22, loss = 0.65971585\n",
            "Iteration 23, loss = 0.65959020\n",
            "Iteration 24, loss = 0.65928935\n",
            "Iteration 25, loss = 0.65910290\n",
            "Iteration 26, loss = 0.65876060\n",
            "Iteration 27, loss = 0.65855391\n",
            "Iteration 28, loss = 0.65847409\n",
            "Iteration 29, loss = 0.65822054\n",
            "Iteration 30, loss = 0.65804935\n",
            "Iteration 31, loss = 0.65782504\n",
            "Iteration 32, loss = 0.65767854\n",
            "Iteration 33, loss = 0.65750863\n",
            "Iteration 34, loss = 0.65727708\n",
            "Iteration 35, loss = 0.65704813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration 19, loss = 0.58104079\n",
            "Iteration 20, loss = 0.58733166\n",
            "Iteration 21, loss = 0.58202925\n",
            "Iteration 22, loss = 0.57891691\n",
            "Iteration 23, loss = 0.58088126\n",
            "Iteration 24, loss = 0.57155022\n",
            "Iteration 25, loss = 0.57363248\n",
            "Iteration 26, loss = 0.57104864\n",
            "Iteration 27, loss = 0.55542142\n",
            "Iteration 28, loss = 0.58305794\n",
            "Iteration 29, loss = 0.56340858\n",
            "Iteration 30, loss = 0.57252506\n",
            "Iteration 31, loss = 0.56046026\n",
            "Iteration 32, loss = 0.56273807\n",
            "Iteration 33, loss = 0.55215712\n",
            "Iteration 34, loss = 0.55477202\n",
            "Iteration 35, loss = 0.55555791\n",
            "Iteration 36, loss = 0.55176925\n",
            "Iteration 37, loss = 0.54923849\n",
            "Iteration 38, loss = 0.54888218\n",
            "Iteration 39, loss = 0.54203062\n",
            "Iteration 40, loss = 0.55949394\n",
            "Iteration 41, loss = 0.54796884\n",
            "Iteration 42, loss = 0.53558031\n",
            "Iteration 43, loss = 0.54303244\n",
            "Iteration 44, loss = 0.54395865\n",
            "Iteration 45, loss = 0.54030930\n",
            "Iteration 46, loss = 0.53417927\n",
            "Iteration 47, loss = 0.51930720\n",
            "Iteration 48, loss = 0.54026207\n",
            "Iteration 49, loss = 0.53381430\n",
            "Iteration 50, loss = 0.54244424\n",
            "Iteration 51, loss = 0.54115236\n",
            "Iteration 52, loss = 0.52159492\n",
            "Iteration 53, loss = 0.52710316\n",
            "Iteration 54, loss = 0.52741995\n",
            "Iteration 55, loss = 0.54407138\n",
            "Iteration 56, loss = 0.52520334\n",
            "Iteration 57, loss = 0.51616669\n",
            "Iteration 58, loss = 0.52068532\n",
            "Iteration 59, loss = 0.50942016\n",
            "Iteration 60, loss = 0.53145553\n",
            "Iteration 61, loss = 0.52772296\n",
            "Iteration 62, loss = 0.51872925\n",
            "Iteration 63, loss = 0.52931199\n",
            "Iteration 64, loss = 0.52648627\n",
            "Iteration 65, loss = 0.51845447\n",
            "Iteration 66, loss = 0.53449834\n",
            "Iteration 67, loss = 0.51552417\n",
            "Iteration 68, loss = 0.53574621\n",
            "Iteration 69, loss = 0.54288876\n",
            "Iteration 70, loss = 0.53639555\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.60703420\n",
            "Iteration 2, loss = 0.60461340\n",
            "Iteration 3, loss = 0.59736247\n",
            "Iteration 4, loss = 0.59492886\n",
            "Iteration 5, loss = 0.59295662\n",
            "Iteration 6, loss = 0.59056378\n",
            "Iteration 7, loss = 0.58433033\n",
            "Iteration 8, loss = 0.58230767\n",
            "Iteration 9, loss = 0.58280286\n",
            "Iteration 10, loss = 0.58268655\n",
            "Iteration 11, loss = 0.58390143\n",
            "Iteration 12, loss = 0.58066458\n",
            "Iteration 13, loss = 0.57104334\n",
            "Iteration 14, loss = 0.57179729\n",
            "Iteration 15, loss = 0.57515660\n",
            "Iteration 16, loss = 0.56297401\n",
            "Iteration 17, loss = 0.56521943\n",
            "Iteration 18, loss = 0.55793655\n",
            "Iteration 19, loss = 0.56563422\n",
            "Iteration 20, loss = 0.56457663\n",
            "Iteration 21, loss = 0.56305000\n",
            "Iteration 22, loss = 0.55662757\n",
            "Iteration 23, loss = 0.56191660\n",
            "Iteration 24, loss = 0.55418162\n",
            "Iteration 25, loss = 0.55326494\n",
            "Iteration 26, loss = 0.54840764\n",
            "Iteration 27, loss = 0.55355512\n",
            "Iteration 28, loss = 0.54104551\n",
            "Iteration 29, loss = 0.54800436\n",
            "Iteration 30, loss = 0.54486871\n",
            "Iteration 31, loss = 0.53307138\n",
            "Iteration 32, loss = 0.53279509\n",
            "Iteration 33, loss = 0.53267716\n",
            "Iteration 34, loss = 0.53246022\n",
            "Iteration 35, loss = 0.52592431\n",
            "Iteration 36, loss = 0.52963937\n",
            "Iteration 37, loss = 0.53272977\n",
            "Iteration 38, loss = 0.53738586\n",
            "Iteration 39, loss = 0.52700490\n",
            "Iteration 40, loss = 0.52583971\n",
            "Iteration 41, loss = 0.52769560\n",
            "Iteration 42, loss = 0.52216062\n",
            "Iteration 43, loss = 0.55184955\n",
            "Iteration 44, loss = 0.55476396\n",
            "Iteration 45, loss = 0.52815229\n",
            "Iteration 46, loss = 0.55067371\n",
            "Iteration 47, loss = 0.54057587\n",
            "Iteration 48, loss = 0.51458391\n",
            "Iteration 49, loss = 0.53250408\n",
            "Iteration 50, loss = 0.51199038\n",
            "Iteration 51, loss = 0.53105917\n",
            "Iteration 52, loss = 0.51287011\n",
            "Iteration 53, loss = 0.53646680\n",
            "Iteration 54, loss = 0.52115641\n",
            "Iteration 55, loss = 0.52134983\n",
            "Iteration 56, loss = 0.52476581\n",
            "Iteration 57, loss = 0.50990798\n",
            "Iteration 58, loss = 0.50945830\n",
            "Iteration 59, loss = 0.51381500\n",
            "Iteration 60, loss = 0.51346929\n",
            "Iteration 61, loss = 0.51274293\n",
            "Iteration 62, loss = 0.51112462\n",
            "Iteration 63, loss = 0.50882808\n",
            "Iteration 64, loss = 0.52032097\n",
            "Iteration 65, loss = 0.52825248\n",
            "Iteration 66, loss = 0.52401594\n",
            "Iteration 67, loss = 0.50420097\n",
            "Iteration 68, loss = 0.52334519\n",
            "Iteration 69, loss = 0.50829946\n",
            "Iteration 70, loss = 0.52155036\n",
            "Iteration 71, loss = 0.50565444\n",
            "Iteration 72, loss = 0.53866588\n",
            "Iteration 73, loss = 0.53673649\n",
            "Iteration 74, loss = 0.52370894\n",
            "Iteration 75, loss = 0.51246694\n",
            "Iteration 76, loss = 0.50797720\n",
            "Iteration 77, loss = 0.51259938\n",
            "Iteration 78, loss = 0.53114937\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61588062\n",
            "Iteration 2, loss = 0.60724094\n",
            "Iteration 3, loss = 0.60508335\n",
            "Iteration 4, loss = 0.60312435\n",
            "Iteration 5, loss = 0.59703946\n",
            "Iteration 6, loss = 0.59437390\n",
            "Iteration 7, loss = 0.59560137\n",
            "Iteration 8, loss = 0.58979720\n",
            "Iteration 9, loss = 0.58159420\n",
            "Iteration 10, loss = 0.58842625\n",
            "Iteration 11, loss = 0.58696453\n",
            "Iteration 12, loss = 0.58275666\n",
            "Iteration 13, loss = 0.58558789\n",
            "Iteration 14, loss = 0.57193512\n",
            "Iteration 15, loss = 0.57917487\n",
            "Iteration 16, loss = 0.56818682\n",
            "Iteration 17, loss = 0.56861599\n",
            "Iteration 18, loss = 0.57275757\n",
            "Iteration 19, loss = 0.56000794\n",
            "Iteration 20, loss = 0.56292674\n",
            "Iteration 21, loss = 0.57064937\n",
            "Iteration 22, loss = 0.56520715\n",
            "Iteration 23, loss = 0.57364650\n",
            "Iteration 24, loss = 0.55554996\n",
            "Iteration 25, loss = 0.55479535\n",
            "Iteration 26, loss = 0.56423538\n",
            "Iteration 27, loss = 0.55746146\n",
            "Iteration 28, loss = 0.55729332\n",
            "Iteration 29, loss = 0.54908471\n",
            "Iteration 30, loss = 0.55542813\n",
            "Iteration 31, loss = 0.54916707\n",
            "Iteration 32, loss = 0.54285868\n",
            "Iteration 33, loss = 0.55874140\n",
            "Iteration 34, loss = 0.54322015\n",
            "Iteration 35, loss = 0.53911859\n",
            "Iteration 36, loss = 0.53346569\n",
            "Iteration 37, loss = 0.52734253\n",
            "Iteration 38, loss = 0.53487665\n",
            "Iteration 39, loss = 0.53221706\n",
            "Iteration 40, loss = 0.53076615\n",
            "Iteration 41, loss = 0.53774380\n",
            "Iteration 42, loss = 0.54086790\n",
            "Iteration 43, loss = 0.54337732\n",
            "Iteration 44, loss = 0.55043234\n",
            "Iteration 45, loss = 0.54499675\n",
            "Iteration 46, loss = 0.52876169\n",
            "Iteration 47, loss = 0.53580948\n",
            "Iteration 48, loss = 0.55111661\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.00211049\n",
            "Iteration 2, loss = 0.73710303\n",
            "Iteration 3, loss = 0.76970758\n",
            "Iteration 4, loss = 0.62250365\n",
            "Iteration 5, loss = 0.60996734\n",
            "Iteration 6, loss = 0.61128501\n",
            "Iteration 7, loss = 0.62031837\n",
            "Iteration 8, loss = 0.61628809\n",
            "Iteration 9, loss = 0.63218811\n",
            "Iteration 10, loss = 0.59621595\n",
            "Iteration 11, loss = 0.59804116\n",
            "Iteration 12, loss = 0.58809944\n",
            "Iteration 13, loss = 0.59181170\n",
            "Iteration 14, loss = 0.58633828\n",
            "Iteration 15, loss = 0.58590376\n",
            "Iteration 16, loss = 0.58368165\n",
            "Iteration 17, loss = 0.57699044\n",
            "Iteration 18, loss = 0.56786969\n",
            "Iteration 19, loss = 0.56946855\n",
            "Iteration 20, loss = 0.56281957\n",
            "Iteration 21, loss = 0.57594113\n",
            "Iteration 22, loss = 0.56412470\n",
            "Iteration 23, loss = 0.56892795\n",
            "Iteration 24, loss = 0.58443829\n",
            "Iteration 25, loss = 0.56019472\n",
            "Iteration 26, loss = 0.56089733\n",
            "Iteration 27, loss = 0.55666924\n",
            "Iteration 28, loss = 0.55470524\n",
            "Iteration 29, loss = 0.57193289\n",
            "Iteration 30, loss = 0.55376385\n",
            "Iteration 31, loss = 0.55165473\n",
            "Iteration 32, loss = 0.54422918\n",
            "Iteration 33, loss = 0.54493087\n",
            "Iteration 34, loss = 0.55003998\n",
            "Iteration 35, loss = 0.54071257\n",
            "Iteration 36, loss = 0.54874591\n",
            "Iteration 37, loss = 0.54090505\n",
            "Iteration 38, loss = 0.52713226\n",
            "Iteration 39, loss = 0.55613634\n",
            "Iteration 40, loss = 0.55048595\n",
            "Iteration 41, loss = 0.52722119\n",
            "Iteration 42, loss = 0.54717669\n",
            "Iteration 43, loss = 0.56529698\n",
            "Iteration 44, loss = 0.54252715\n",
            "Iteration 45, loss = 0.53900767\n",
            "Iteration 46, loss = 0.52880183\n",
            "Iteration 47, loss = 0.52084851\n",
            "Iteration 48, loss = 0.52150888\n",
            "Iteration 49, loss = 0.52846954\n",
            "Iteration 50, loss = 0.52562504\n",
            "Iteration 51, loss = 0.52351895\n",
            "Iteration 52, loss = 0.51728831\n",
            "Iteration 53, loss = 0.51045259\n",
            "Iteration 54, loss = 0.51813149\n",
            "Iteration 55, loss = 0.52829600\n",
            "Iteration 56, loss = 0.52357653\n",
            "Iteration 57, loss = 0.53981338\n",
            "Iteration 58, loss = 0.52296869\n",
            "Iteration 59, loss = 0.51238963\n",
            "Iteration 60, loss = 0.51915682\n",
            "Iteration 61, loss = 0.50524764\n",
            "Iteration 62, loss = 0.50480729\n",
            "Iteration 63, loss = 0.51194645\n",
            "Iteration 64, loss = 0.48678998\n",
            "Iteration 65, loss = 0.50875401\n",
            "Iteration 66, loss = 0.52420812\n",
            "Iteration 67, loss = 0.50340009\n",
            "Iteration 68, loss = 0.48171402\n",
            "Iteration 69, loss = 0.50153927\n",
            "Iteration 70, loss = 0.51434138\n",
            "Iteration 71, loss = 0.49155331\n",
            "Iteration 72, loss = 0.50504771\n",
            "Iteration 73, loss = 0.52506367\n",
            "Iteration 74, loss = 0.50110138\n",
            "Iteration 75, loss = 0.48721949\n",
            "Iteration 76, loss = 0.48290530\n",
            "Iteration 77, loss = 0.48361431\n",
            "Iteration 78, loss = 0.49833680\n",
            "Iteration 79, loss = 0.53272904\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.94482421\n",
            "Iteration 2, loss = 0.82868259\n",
            "Iteration 3, loss = 0.66297950\n",
            "Iteration 4, loss = 0.61555299\n",
            "Iteration 5, loss = 0.61560239\n",
            "Iteration 6, loss = 0.58958398\n",
            "Iteration 7, loss = 0.59295832\n",
            "Iteration 8, loss = 0.58438901\n",
            "Iteration 9, loss = 0.58912998\n",
            "Iteration 10, loss = 0.57187369\n",
            "Iteration 11, loss = 0.57955058\n",
            "Iteration 12, loss = 0.57031245\n",
            "Iteration 13, loss = 0.57214387\n",
            "Iteration 14, loss = 0.56855086\n",
            "Iteration 15, loss = 0.56301718\n",
            "Iteration 16, loss = 0.57042310\n",
            "Iteration 17, loss = 0.57583902\n",
            "Iteration 18, loss = 0.59090839\n",
            "Iteration 19, loss = 0.58105791\n",
            "Iteration 20, loss = 0.55896289\n",
            "Iteration 21, loss = 0.56935292\n",
            "Iteration 22, loss = 0.56892452\n",
            "Iteration 23, loss = 0.58378217\n",
            "Iteration 24, loss = 0.56649954\n",
            "Iteration 25, loss = 0.57284460\n",
            "Iteration 26, loss = 0.55312442\n",
            "Iteration 27, loss = 0.55902284\n",
            "Iteration 28, loss = 0.56483927\n",
            "Iteration 29, loss = 0.56497060\n",
            "Iteration 30, loss = 0.56541214\n",
            "Iteration 31, loss = 0.55685769\n",
            "Iteration 32, loss = 0.55259523\n",
            "Iteration 33, loss = 0.56298271\n",
            "Iteration 34, loss = 0.54914078\n",
            "Iteration 35, loss = 0.55005966\n",
            "Iteration 36, loss = 0.54425373\n",
            "Iteration 37, loss = 0.54445890\n",
            "Iteration 38, loss = 0.55759098\n",
            "Iteration 39, loss = 0.54455557\n",
            "Iteration 40, loss = 0.55788684\n",
            "Iteration 41, loss = 0.54376502\n",
            "Iteration 42, loss = 0.55233817\n",
            "Iteration 43, loss = 0.53693434\n",
            "Iteration 44, loss = 0.54198072\n",
            "Iteration 45, loss = 0.53905091\n",
            "Iteration 46, loss = 0.53608705\n",
            "Iteration 47, loss = 0.55860557\n",
            "Iteration 48, loss = 0.53617872\n",
            "Iteration 49, loss = 0.52587236\n",
            "Iteration 50, loss = 0.52889869\n",
            "Iteration 51, loss = 0.53884039\n",
            "Iteration 52, loss = 0.53117562\n",
            "Iteration 53, loss = 0.52559058\n",
            "Iteration 54, loss = 0.51988966\n",
            "Iteration 55, loss = 0.53620106\n",
            "Iteration 56, loss = 0.52661497\n",
            "Iteration 57, loss = 0.50772815\n",
            "Iteration 58, loss = 0.50194328\n",
            "Iteration 59, loss = 0.52141798\n",
            "Iteration 60, loss = 0.53756027\n",
            "Iteration 61, loss = 0.53770508\n",
            "Iteration 62, loss = 0.52947071\n",
            "Iteration 63, loss = 0.51379513\n",
            "Iteration 64, loss = 0.51846989\n",
            "Iteration 65, loss = 0.52362217\n",
            "Iteration 66, loss = 0.52487350\n",
            "Iteration 67, loss = 0.51323542\n",
            "Iteration 68, loss = 0.53108648\n",
            "Iteration 69, loss = 0.51447755\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.34223064\n",
            "Iteration 2, loss = 0.66588790\n",
            "Iteration 3, loss = 0.67782336\n",
            "Iteration 4, loss = 0.63885695\n",
            "Iteration 5, loss = 0.61896109\n",
            "Iteration 6, loss = 0.61061030\n",
            "Iteration 7, loss = 0.61295707\n",
            "Iteration 8, loss = 0.62285234\n",
            "Iteration 9, loss = 0.63547523\n",
            "Iteration 10, loss = 0.60202529\n",
            "Iteration 11, loss = 0.61721796\n",
            "Iteration 12, loss = 0.60143366\n",
            "Iteration 13, loss = 0.59987747\n",
            "Iteration 14, loss = 0.59731075\n",
            "Iteration 15, loss = 0.59183369\n",
            "Iteration 16, loss = 0.59576927\n",
            "Iteration 17, loss = 0.58391225\n",
            "Iteration 18, loss = 0.59000115\n",
            "Iteration 19, loss = 0.59208086\n",
            "Iteration 20, loss = 0.58305450\n",
            "Iteration 21, loss = 0.57852179\n",
            "Iteration 22, loss = 0.59429047\n",
            "Iteration 23, loss = 0.58751551\n",
            "Iteration 24, loss = 0.58065101\n",
            "Iteration 25, loss = 0.57663366\n",
            "Iteration 26, loss = 0.58045575\n",
            "Iteration 27, loss = 0.58000712\n",
            "Iteration 28, loss = 0.57082528\n",
            "Iteration 29, loss = 0.57438344\n",
            "Iteration 30, loss = 0.57647687\n",
            "Iteration 31, loss = 0.56628436\n",
            "Iteration 32, loss = 0.57786520\n",
            "Iteration 33, loss = 0.56806736\n",
            "Iteration 34, loss = 0.60869958\n",
            "Iteration 35, loss = 0.59180922\n",
            "Iteration 36, loss = 0.58669977\n",
            "Iteration 37, loss = 0.57795816\n",
            "Iteration 38, loss = 0.57488914\n",
            "Iteration 39, loss = 0.56984264\n",
            "Iteration 40, loss = 0.57191377\n",
            "Iteration 41, loss = 0.58286021\n",
            "Iteration 42, loss = 0.56117881\n",
            "Iteration 43, loss = 0.55276440\n",
            "Iteration 44, loss = 0.55209286\n",
            "Iteration 45, loss = 0.55435804\n",
            "Iteration 46, loss = 0.55227493\n",
            "Iteration 47, loss = 0.55756100\n",
            "Iteration 48, loss = 0.54765197\n",
            "Iteration 49, loss = 0.54940275\n",
            "Iteration 50, loss = 0.54837770\n",
            "Iteration 51, loss = 0.54022028\n",
            "Iteration 52, loss = 0.55123585\n",
            "Iteration 53, loss = 0.53730146\n",
            "Iteration 54, loss = 0.52932146\n",
            "Iteration 55, loss = 0.52752652\n",
            "Iteration 56, loss = 0.53845087\n",
            "Iteration 57, loss = 0.51999129\n",
            "Iteration 58, loss = 0.52736960\n",
            "Iteration 59, loss = 0.52400650\n",
            "Iteration 60, loss = 0.52233486\n",
            "Iteration 61, loss = 0.52288231\n",
            "Iteration 62, loss = 0.50435201\n",
            "Iteration 63, loss = 0.53205815\n",
            "Iteration 64, loss = 0.52187909\n",
            "Iteration 65, loss = 0.52356286\n",
            "Iteration 66, loss = 0.51811182\n",
            "Iteration 67, loss = 0.51418166\n",
            "Iteration 68, loss = 0.52608984\n",
            "Iteration 69, loss = 0.52737679\n",
            "Iteration 70, loss = 0.51490084\n",
            "Iteration 71, loss = 0.51264006\n",
            "Iteration 72, loss = 0.50243550\n",
            "Iteration 73, loss = 0.52158052\n",
            "Iteration 74, loss = 0.51583804\n",
            "Iteration 75, loss = 0.51717435\n",
            "Iteration 76, loss = 0.52629088\n",
            "Iteration 77, loss = 0.51122759\n",
            "Iteration 78, loss = 0.51106383\n",
            "Iteration 79, loss = 0.50774724\n",
            "Iteration 80, loss = 0.50715538\n",
            "Iteration 81, loss = 0.50550029\n",
            "Iteration 82, loss = 0.49931019\n",
            "Iteration 83, loss = 0.49963735\n",
            "Iteration 84, loss = 0.49599147\n",
            "Iteration 85, loss = 0.52022644\n",
            "Iteration 86, loss = 0.50768870\n",
            "Iteration 87, loss = 0.50048820\n",
            "Iteration 88, loss = 0.49654385\n",
            "Iteration 89, loss = 0.49593523\n",
            "Iteration 90, loss = 0.50076764\n",
            "Iteration 91, loss = 0.51443202\n",
            "Iteration 92, loss = 0.51962896\n",
            "Iteration 93, loss = 0.52264085\n",
            "Iteration 94, loss = 0.49512647\n",
            "Iteration 95, loss = 0.52076431\n",
            "Iteration 96, loss = 0.54980571\n",
            "Iteration 97, loss = 0.50226186\n",
            "Iteration 98, loss = 0.50388784\n",
            "Iteration 99, loss = 0.48328803\n",
            "Iteration 100, loss = 0.50655511\n",
            "Iteration 101, loss = 0.49056987\n",
            "Iteration 102, loss = 0.49611386\n",
            "Iteration 103, loss = 0.50539937\n",
            "Iteration 104, loss = 0.48867240\n",
            "Iteration 105, loss = 0.49319533\n",
            "Iteration 106, loss = 0.48267171\n",
            "Iteration 107, loss = 0.50303562\n",
            "Iteration 108, loss = 0.48467964\n",
            "Iteration 109, loss = 0.48954229\n",
            "Iteration 110, loss = 0.49056588\n",
            "Iteration 111, loss = 0.48767521\n",
            "Iteration 112, loss = 0.49559427\n",
            "Iteration 113, loss = 0.48977903\n",
            "Iteration 114, loss = 0.48867600\n",
            "Iteration 115, loss = 0.51287385\n",
            "Iteration 116, loss = 0.49787150\n",
            "Iteration 117, loss = 0.49090600\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.14757689\n",
            "Iteration 2, loss = 0.63043983\n",
            "Iteration 3, loss = 0.60809428\n",
            "Iteration 4, loss = 0.61279949\n",
            "Iteration 5, loss = 0.61376325\n",
            "Iteration 6, loss = 0.60737736\n",
            "Iteration 7, loss = 0.60803188\n",
            "Iteration 8, loss = 0.60368812\n",
            "Iteration 9, loss = 0.60111641\n",
            "Iteration 10, loss = 0.60537732\n",
            "Iteration 11, loss = 0.59834238\n",
            "Iteration 12, loss = 0.59652352\n",
            "Iteration 13, loss = 0.60380481\n",
            "Iteration 14, loss = 0.59999174\n",
            "Iteration 15, loss = 0.60024666\n",
            "Iteration 16, loss = 0.59966355\n",
            "Iteration 17, loss = 0.59534813\n",
            "Iteration 18, loss = 0.59925792\n",
            "Iteration 19, loss = 0.59554565\n",
            "Iteration 20, loss = 0.60644507\n",
            "Iteration 21, loss = 0.59265065\n",
            "Iteration 22, loss = 0.59608326\n",
            "Iteration 23, loss = 0.60104140\n",
            "Iteration 24, loss = 0.59790109\n",
            "Iteration 25, loss = 0.59454542\n",
            "Iteration 26, loss = 0.59057143\n",
            "Iteration 27, loss = 0.59129831\n",
            "Iteration 28, loss = 0.58956502\n",
            "Iteration 29, loss = 0.58995585\n",
            "Iteration 30, loss = 0.58488448\n",
            "Iteration 31, loss = 0.59092218\n",
            "Iteration 32, loss = 0.58770503\n",
            "Iteration 33, loss = 0.58741980\n",
            "Iteration 34, loss = 0.58890785\n",
            "Iteration 35, loss = 0.58235048\n",
            "Iteration 36, loss = 0.58560760\n",
            "Iteration 37, loss = 0.58182973\n",
            "Iteration 38, loss = 0.57970382\n",
            "Iteration 39, loss = 0.58949345\n",
            "Iteration 40, loss = 0.57911987\n",
            "Iteration 41, loss = 0.58239567\n",
            "Iteration 42, loss = 0.58531275\n",
            "Iteration 43, loss = 0.58000913\n",
            "Iteration 44, loss = 0.57907006\n",
            "Iteration 45, loss = 0.57561971\n",
            "Iteration 46, loss = 0.57661645\n",
            "Iteration 47, loss = 0.58011056\n",
            "Iteration 48, loss = 0.57216362\n",
            "Iteration 49, loss = 0.58206545\n",
            "Iteration 50, loss = 0.57313491\n",
            "Iteration 51, loss = 0.57277173\n",
            "Iteration 52, loss = 0.57669901\n",
            "Iteration 53, loss = 0.56907744\n",
            "Iteration 54, loss = 0.56767529\n",
            "Iteration 55, loss = 0.56572693\n",
            "Iteration 56, loss = 0.56619082\n",
            "Iteration 57, loss = 0.56757141\n",
            "Iteration 58, loss = 0.56846215\n",
            "Iteration 59, loss = 0.55814849\n",
            "Iteration 60, loss = 0.56999328\n",
            "Iteration 61, loss = 0.56069229\n",
            "Iteration 62, loss = 0.55917708\n",
            "Iteration 63, loss = 0.55615852\n",
            "Iteration 64, loss = 0.55758700\n",
            "Iteration 65, loss = 0.55283127\n",
            "Iteration 66, loss = 0.56094233\n",
            "Iteration 67, loss = 0.55605914\n",
            "Iteration 68, loss = 0.55590656\n",
            "Iteration 69, loss = 0.55215683\n",
            "Iteration 70, loss = 0.55448775\n",
            "Iteration 71, loss = 0.55967866\n",
            "Iteration 72, loss = 0.54465060\n",
            "Iteration 73, loss = 0.54718757\n",
            "Iteration 74, loss = 0.55294187\n",
            "Iteration 75, loss = 0.55153950\n",
            "Iteration 76, loss = 0.56054026\n",
            "Iteration 77, loss = 0.53220509\n",
            "Iteration 78, loss = 0.55488087\n",
            "Iteration 79, loss = 0.54183004\n",
            "Iteration 80, loss = 0.54915671\n",
            "Iteration 81, loss = 0.53548695\n",
            "Iteration 82, loss = 0.52852274\n",
            "Iteration 83, loss = 0.53507090\n",
            "Iteration 84, loss = 0.53701515\n",
            "Iteration 85, loss = 0.53874298\n",
            "Iteration 86, loss = 0.54930429\n",
            "Iteration 87, loss = 0.53587215\n",
            "Iteration 88, loss = 0.53238541\n",
            "Iteration 89, loss = 0.54042856\n",
            "Iteration 90, loss = 0.54772501\n",
            "Iteration 91, loss = 0.53975385\n",
            "Iteration 92, loss = 0.53716068\n",
            "Iteration 93, loss = 0.54087165\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.18167193\n",
            "Iteration 2, loss = 0.66001450\n",
            "Iteration 3, loss = 0.68732091\n",
            "Iteration 4, loss = 0.62939809\n",
            "Iteration 5, loss = 0.61801658\n",
            "Iteration 6, loss = 0.60612769\n",
            "Iteration 7, loss = 0.60561949\n",
            "Iteration 8, loss = 0.60511244\n",
            "Iteration 9, loss = 0.60341484\n",
            "Iteration 10, loss = 0.59143327\n",
            "Iteration 11, loss = 0.59324198\n",
            "Iteration 12, loss = 0.58248640\n",
            "Iteration 13, loss = 0.59398077\n",
            "Iteration 14, loss = 0.58261258\n",
            "Iteration 15, loss = 0.57549702\n",
            "Iteration 16, loss = 0.58879594\n",
            "Iteration 17, loss = 0.57979157\n",
            "Iteration 18, loss = 0.56892606\n",
            "Iteration 19, loss = 0.58418800\n",
            "Iteration 20, loss = 0.56707673\n",
            "Iteration 21, loss = 0.58206309\n",
            "Iteration 22, loss = 0.57479493\n",
            "Iteration 23, loss = 0.55538395\n",
            "Iteration 24, loss = 0.56904925\n",
            "Iteration 25, loss = 0.56587082\n",
            "Iteration 26, loss = 0.55540318\n",
            "Iteration 27, loss = 0.55924861\n",
            "Iteration 28, loss = 0.55197608\n",
            "Iteration 29, loss = 0.55269949\n",
            "Iteration 30, loss = 0.54224483\n",
            "Iteration 31, loss = 0.55450545\n",
            "Iteration 32, loss = 0.55305172\n",
            "Iteration 33, loss = 0.54844875\n",
            "Iteration 34, loss = 0.55446773\n",
            "Iteration 35, loss = 0.53359127\n",
            "Iteration 36, loss = 0.56488808\n",
            "Iteration 37, loss = 0.53389892\n",
            "Iteration 38, loss = 0.53583817\n",
            "Iteration 39, loss = 0.54207951\n",
            "Iteration 40, loss = 0.54067535\n",
            "Iteration 41, loss = 0.54581162\n",
            "Iteration 42, loss = 0.57291830\n",
            "Iteration 43, loss = 0.54886867\n",
            "Iteration 44, loss = 0.53196798\n",
            "Iteration 45, loss = 0.54082779\n",
            "Iteration 46, loss = 0.53561716\n",
            "Iteration 47, loss = 0.54882341\n",
            "Iteration 48, loss = 0.53658887\n",
            "Iteration 49, loss = 0.51862532\n",
            "Iteration 50, loss = 0.52748617\n",
            "Iteration 51, loss = 0.54035394\n",
            "Iteration 52, loss = 0.52303871\n",
            "Iteration 53, loss = 0.52242024\n",
            "Iteration 54, loss = 0.51890890\n",
            "Iteration 55, loss = 0.53399930\n",
            "Iteration 56, loss = 0.52813847\n",
            "Iteration 57, loss = 0.51756914\n",
            "Iteration 58, loss = 0.51751810\n",
            "Iteration 59, loss = 0.52481022\n",
            "Iteration 60, loss = 0.53219656\n",
            "Iteration 61, loss = 0.50174826\n",
            "Iteration 62, loss = 0.52462236\n",
            "Iteration 63, loss = 0.51655038\n",
            "Iteration 64, loss = 0.52918591\n",
            "Iteration 65, loss = 0.52019450\n",
            "Iteration 66, loss = 0.54428123\n",
            "Iteration 67, loss = 0.52545568\n",
            "Iteration 68, loss = 0.50262391\n",
            "Iteration 69, loss = 0.50464227\n",
            "Iteration 70, loss = 0.51901229\n",
            "Iteration 71, loss = 0.51103483\n",
            "Iteration 72, loss = 0.51143767\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.72492323\n",
            "Iteration 2, loss = 0.66626806\n",
            "Iteration 3, loss = 0.66273746\n",
            "Iteration 4, loss = 0.65930635\n",
            "Iteration 5, loss = 0.65953999\n",
            "Iteration 6, loss = 0.65771261\n",
            "Iteration 7, loss = 0.65646900\n",
            "Iteration 8, loss = 0.65610195\n",
            "Iteration 9, loss = 0.65414514\n",
            "Iteration 10, loss = 0.65341184\n",
            "Iteration 11, loss = 0.65239042\n",
            "Iteration 12, loss = 0.65121389\n",
            "Iteration 13, loss = 0.65109761\n",
            "Iteration 14, loss = 0.65060029\n",
            "Iteration 15, loss = 0.65289332\n",
            "Iteration 16, loss = 0.64962683\n",
            "Iteration 17, loss = 0.64722247\n",
            "Iteration 18, loss = 0.64666619\n",
            "Iteration 19, loss = 0.64782755\n",
            "Iteration 20, loss = 0.64522184\n",
            "Iteration 21, loss = 0.64433198\n",
            "Iteration 22, loss = 0.64485379\n",
            "Iteration 23, loss = 0.64256427\n",
            "Iteration 24, loss = 0.64225697\n",
            "Iteration 25, loss = 0.64080666\n",
            "Iteration 26, loss = 0.63987135\n",
            "Iteration 27, loss = 0.63982437\n",
            "Iteration 28, loss = 0.64058847\n",
            "Iteration 29, loss = 0.63856384\n",
            "Iteration 30, loss = 0.63610136\n",
            "Iteration 31, loss = 0.63985718\n",
            "Iteration 32, loss = 0.63815544\n",
            "Iteration 33, loss = 0.63371299\n",
            "Iteration 34, loss = 0.63888955\n",
            "Iteration 35, loss = 0.63439790\n",
            "Iteration 36, loss = 0.63363206\n",
            "Iteration 37, loss = 0.63213575\n",
            "Iteration 38, loss = 0.63157473\n",
            "Iteration 39, loss = 0.63188588\n",
            "Iteration 40, loss = 0.63050258\n",
            "Iteration 41, loss = 0.62959482\n",
            "Iteration 42, loss = 0.62864097\n",
            "Iteration 43, loss = 0.62833485\n",
            "Iteration 44, loss = 0.62740787\n",
            "Iteration 45, loss = 0.62862533\n",
            "Iteration 46, loss = 0.62685543\n",
            "Iteration 47, loss = 0.62665213\n",
            "Iteration 48, loss = 0.62565188\n",
            "Iteration 49, loss = 0.62485597\n",
            "Iteration 50, loss = 0.62400318\n",
            "Iteration 51, loss = 0.62358905\n",
            "Iteration 52, loss = 0.62256268\n",
            "Iteration 53, loss = 0.62628762\n",
            "Iteration 54, loss = 0.62524303\n",
            "Iteration 55, loss = 0.62166617\n",
            "Iteration 56, loss = 0.62281505\n",
            "Iteration 57, loss = 0.62132851\n",
            "Iteration 58, loss = 0.61955396\n",
            "Iteration 59, loss = 0.62002094\n",
            "Iteration 60, loss = 0.61906822\n",
            "Iteration 61, loss = 0.61913730\n",
            "Iteration 62, loss = 0.61969105\n",
            "Iteration 63, loss = 0.61628948\n",
            "Iteration 64, loss = 0.61774772\n",
            "Iteration 65, loss = 0.61700888\n",
            "Iteration 66, loss = 0.61779940\n",
            "Iteration 67, loss = 0.61713484\n",
            "Iteration 68, loss = 0.61598600\n",
            "Iteration 69, loss = 0.61570280\n",
            "Iteration 70, loss = 0.61693042\n",
            "Iteration 71, loss = 0.61508711\n",
            "Iteration 72, loss = 0.61606090\n",
            "Iteration 73, loss = 0.61454153\n",
            "Iteration 74, loss = 0.61399824\n",
            "Iteration 75, loss = 0.61413590\n",
            "Iteration 76, loss = 0.61328863\n",
            "Iteration 77, loss = 0.61409995\n",
            "Iteration 78, loss = 0.61380479\n",
            "Iteration 79, loss = 0.61260568\n",
            "Iteration 80, loss = 0.61237440\n",
            "Iteration 81, loss = 0.61232305\n",
            "Iteration 82, loss = 0.61168304\n",
            "Iteration 83, loss = 0.61216957\n",
            "Iteration 84, loss = 0.61131260\n",
            "Iteration 85, loss = 0.61113509\n",
            "Iteration 86, loss = 0.61102259\n",
            "Iteration 87, loss = 0.61122940\n",
            "Iteration 88, loss = 0.61107824\n",
            "Iteration 89, loss = 0.61088405\n",
            "Iteration 90, loss = 0.61042928\n",
            "Iteration 91, loss = 0.61085700\n",
            "Iteration 92, loss = 0.61034920\n",
            "Iteration 93, loss = 0.61056955\n",
            "Iteration 94, loss = 0.61035948\n",
            "Iteration 95, loss = 0.61024538\n",
            "Iteration 96, loss = 0.60905144\n",
            "Iteration 97, loss = 0.60894867\n",
            "Iteration 98, loss = 0.60902500\n",
            "Iteration 99, loss = 0.60911193\n",
            "Iteration 100, loss = 0.60877370\n",
            "Iteration 101, loss = 0.60862094\n",
            "Iteration 102, loss = 0.60842985\n",
            "Iteration 103, loss = 0.60880006\n",
            "Iteration 104, loss = 0.60832238\n",
            "Iteration 105, loss = 0.60842493\n",
            "Iteration 106, loss = 0.60850580\n",
            "Iteration 107, loss = 0.60786373\n",
            "Iteration 108, loss = 0.60779604\n",
            "Iteration 109, loss = 0.60837058\n",
            "Iteration 110, loss = 0.60792990\n",
            "Iteration 111, loss = 0.60753156\n",
            "Iteration 112, loss = 0.60738410\n",
            "Iteration 113, loss = 0.60804402\n",
            "Iteration 114, loss = 0.60697093\n",
            "Iteration 115, loss = 0.60968366\n",
            "Iteration 116, loss = 0.60730869\n",
            "Iteration 117, loss = 0.60676384\n",
            "Iteration 118, loss = 0.60726519\n",
            "Iteration 119, loss = 0.60775795\n",
            "Iteration 120, loss = 0.60828209\n",
            "Iteration 121, loss = 0.60711415\n",
            "Iteration 122, loss = 0.60971873\n",
            "Iteration 123, loss = 0.60763596\n",
            "Iteration 124, loss = 0.60661671\n",
            "Iteration 125, loss = 0.60708636\n",
            "Iteration 126, loss = 0.60670967\n",
            "Iteration 127, loss = 0.60679381\n",
            "Iteration 128, loss = 0.60735050\n",
            "Iteration 129, loss = 0.60664163\n",
            "Iteration 130, loss = 0.60608368\n",
            "Iteration 131, loss = 0.60648021\n",
            "Iteration 132, loss = 0.60847178\n",
            "Iteration 133, loss = 0.60790975\n",
            "Iteration 134, loss = 0.60703112\n",
            "Iteration 135, loss = 0.60615703\n",
            "Iteration 136, loss = 0.60598119\n",
            "Iteration 137, loss = 0.60647071\n",
            "Iteration 138, loss = 0.60594401\n",
            "Iteration 139, loss = 0.60629089\n",
            "Iteration 140, loss = 0.60598758\n",
            "Iteration 141, loss = 0.60602197\n",
            "Iteration 142, loss = 0.60607693\n",
            "Iteration 143, loss = 0.60580511\n",
            "Iteration 144, loss = 0.60621062\n",
            "Iteration 145, loss = 0.60614127\n",
            "Iteration 146, loss = 0.60585291\n",
            "Iteration 147, loss = 0.60575730\n",
            "Iteration 148, loss = 0.60628897\n",
            "Iteration 149, loss = 0.60644909\n",
            "Iteration 150, loss = 0.60670903\n",
            "Iteration 151, loss = 0.60546915\n",
            "Iteration 152, loss = 0.60634115\n",
            "Iteration 153, loss = 0.60489013\n",
            "Iteration 154, loss = 0.60626336\n",
            "Iteration 155, loss = 0.60702030\n",
            "Iteration 156, loss = 0.60553844\n",
            "Iteration 157, loss = 0.60526550\n",
            "Iteration 158, loss = 0.60554272\n",
            "Iteration 159, loss = 0.60498776\n",
            "Iteration 160, loss = 0.60577256\n",
            "Iteration 161, loss = 0.60619017\n",
            "Iteration 162, loss = 0.60583005\n",
            "Iteration 163, loss = 0.60660769\n",
            "Iteration 164, loss = 0.60686772\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66609040\n",
            "Iteration 2, loss = 0.66333548\n",
            "Iteration 3, loss = 0.66266917\n",
            "Iteration 4, loss = 0.66423084\n",
            "Iteration 5, loss = 0.66243610\n",
            "Iteration 6, loss = 0.65887143\n",
            "Iteration 7, loss = 0.65741259\n",
            "Iteration 8, loss = 0.65611090\n",
            "Iteration 9, loss = 0.65604111\n",
            "Iteration 10, loss = 0.65393504\n",
            "Iteration 11, loss = 0.65409207\n",
            "Iteration 12, loss = 0.65282230\n",
            "Iteration 13, loss = 0.65158412\n",
            "Iteration 14, loss = 0.65110565\n",
            "Iteration 15, loss = 0.65055263\n",
            "Iteration 16, loss = 0.65072979\n",
            "Iteration 17, loss = 0.64780685\n",
            "Iteration 18, loss = 0.64652510\n",
            "Iteration 19, loss = 0.64559304\n",
            "Iteration 20, loss = 0.64460764\n",
            "Iteration 21, loss = 0.64519239\n",
            "Iteration 22, loss = 0.64312821\n",
            "Iteration 23, loss = 0.64256396\n",
            "Iteration 24, loss = 0.64225998\n",
            "Iteration 25, loss = 0.64123294\n",
            "Iteration 26, loss = 0.63978259\n",
            "Iteration 27, loss = 0.63852204\n",
            "Iteration 28, loss = 0.63802376\n",
            "Iteration 29, loss = 0.63650305\n",
            "Iteration 30, loss = 0.63649976\n",
            "Iteration 31, loss = 0.63508573\n",
            "Iteration 32, loss = 0.63487843\n",
            "Iteration 33, loss = 0.63364295\n",
            "Iteration 34, loss = 0.63352251\n",
            "Iteration 35, loss = 0.63231350\n",
            "Iteration 36, loss = 0.63109057\n",
            "Iteration 37, loss = 0.63061164\n",
            "Iteration 38, loss = 0.63089418\n",
            "Iteration 39, loss = 0.63005492\n",
            "Iteration 40, loss = 0.62804540\n",
            "Iteration 41, loss = 0.62760895\n",
            "Iteration 42, loss = 0.62664868\n",
            "Iteration 43, loss = 0.62679252\n",
            "Iteration 44, loss = 0.62495214\n",
            "Iteration 45, loss = 0.62504578\n",
            "Iteration 46, loss = 0.62439618\n",
            "Iteration 47, loss = 0.62372101\n",
            "Iteration 48, loss = 0.62287564\n",
            "Iteration 49, loss = 0.62228435\n",
            "Iteration 50, loss = 0.62233435\n",
            "Iteration 51, loss = 0.62028078\n",
            "Iteration 52, loss = 0.62061866\n",
            "Iteration 53, loss = 0.62019906\n",
            "Iteration 54, loss = 0.62019548\n",
            "Iteration 55, loss = 0.62130351\n",
            "Iteration 56, loss = 0.61993024\n",
            "Iteration 57, loss = 0.61873263\n",
            "Iteration 58, loss = 0.61764624\n",
            "Iteration 59, loss = 0.61696422\n",
            "Iteration 60, loss = 0.61673349\n",
            "Iteration 61, loss = 0.61635240\n",
            "Iteration 62, loss = 0.61655380\n",
            "Iteration 63, loss = 0.61566824\n",
            "Iteration 64, loss = 0.61523988\n",
            "Iteration 65, loss = 0.61574240\n",
            "Iteration 66, loss = 0.61371738\n",
            "Iteration 67, loss = 0.61699064\n",
            "Iteration 68, loss = 0.61441001\n",
            "Iteration 69, loss = 0.61482201\n",
            "Iteration 70, loss = 0.61401033\n",
            "Iteration 71, loss = 0.61334648\n",
            "Iteration 72, loss = 0.61408218\n",
            "Iteration 73, loss = 0.61326210\n",
            "Iteration 74, loss = 0.61424002\n",
            "Iteration 75, loss = 0.61373483\n",
            "Iteration 76, loss = 0.61116531\n",
            "Iteration 77, loss = 0.61073345\n",
            "Iteration 78, loss = 0.61087119\n",
            "Iteration 79, loss = 0.61031742\n",
            "Iteration 80, loss = 0.60990155\n",
            "Iteration 81, loss = 0.60951156\n",
            "Iteration 82, loss = 0.60930263\n",
            "Iteration 83, loss = 0.61048465\n",
            "Iteration 84, loss = 0.61013989\n",
            "Iteration 85, loss = 0.60968327\n",
            "Iteration 86, loss = 0.60884999\n",
            "Iteration 87, loss = 0.60842533\n",
            "Iteration 88, loss = 0.60861841\n",
            "Iteration 89, loss = 0.60867121\n",
            "Iteration 90, loss = 0.60888145\n",
            "Iteration 91, loss = 0.60776044\n",
            "Iteration 92, loss = 0.60805552\n",
            "Iteration 93, loss = 0.60852327\n",
            "Iteration 94, loss = 0.60787016\n",
            "Iteration 95, loss = 0.60772327\n",
            "Iteration 96, loss = 0.60720654\n",
            "Iteration 97, loss = 0.60699011\n",
            "Iteration 98, loss = 0.60687171\n",
            "Iteration 99, loss = 0.60678763\n",
            "Iteration 100, loss = 0.60795656\n",
            "Iteration 101, loss = 0.60680260\n",
            "Iteration 102, loss = 0.60634719\n",
            "Iteration 103, loss = 0.60654712\n",
            "Iteration 104, loss = 0.60752046\n",
            "Iteration 105, loss = 0.60799438\n",
            "Iteration 106, loss = 0.60736659\n",
            "Iteration 107, loss = 0.60627275\n",
            "Iteration 108, loss = 0.60555277\n",
            "Iteration 109, loss = 0.60567468\n",
            "Iteration 110, loss = 0.60581729\n",
            "Iteration 111, loss = 0.60652065\n",
            "Iteration 112, loss = 0.60593425\n",
            "Iteration 113, loss = 0.60623468\n",
            "Iteration 114, loss = 0.60565675\n",
            "Iteration 115, loss = 0.60625777\n",
            "Iteration 116, loss = 0.60541618\n",
            "Iteration 117, loss = 0.60473724\n",
            "Iteration 118, loss = 0.60553260\n",
            "Iteration 119, loss = 0.60550205\n",
            "Iteration 120, loss = 0.60591456\n",
            "Iteration 121, loss = 0.60511142\n",
            "Iteration 122, loss = 0.60484614\n",
            "Iteration 123, loss = 0.60499652\n",
            "Iteration 124, loss = 0.60616274\n",
            "Iteration 125, loss = 0.60544716\n",
            "Iteration 126, loss = 0.60451244\n",
            "Iteration 127, loss = 0.60488432\n",
            "Iteration 128, loss = 0.60491260\n",
            "Iteration 129, loss = 0.60556683\n",
            "Iteration 130, loss = 0.60461384\n",
            "Iteration 131, loss = 0.60467421\n",
            "Iteration 132, loss = 0.60456094\n",
            "Iteration 133, loss = 0.60501798\n",
            "Iteration 134, loss = 0.60472310\n",
            "Iteration 135, loss = 0.60426590\n",
            "Iteration 136, loss = 0.60400125\n",
            "Iteration 137, loss = 0.60399187\n",
            "Iteration 138, loss = 0.60456479\n",
            "Iteration 139, loss = 0.60442044\n",
            "Iteration 140, loss = 0.60414328\n",
            "Iteration 141, loss = 0.60479104\n",
            "Iteration 142, loss = 0.60376127\n",
            "Iteration 143, loss = 0.60420744\n",
            "Iteration 144, loss = 0.60390972\n",
            "Iteration 145, loss = 0.60358536\n",
            "Iteration 146, loss = 0.60610499\n",
            "Iteration 147, loss = 0.60404406\n",
            "Iteration 148, loss = 0.60373477\n",
            "Iteration 149, loss = 0.60437648\n",
            "Iteration 150, loss = 0.60301952\n",
            "Iteration 151, loss = 0.60357204\n",
            "Iteration 152, loss = 0.60339689\n",
            "Iteration 153, loss = 0.60431666\n",
            "Iteration 154, loss = 0.60310837\n",
            "Iteration 155, loss = 0.60340727\n",
            "Iteration 156, loss = 0.60380119\n",
            "Iteration 157, loss = 0.60301643\n",
            "Iteration 158, loss = 0.60347264\n",
            "Iteration 159, loss = 0.60411606\n",
            "Iteration 160, loss = 0.60314789\n",
            "Iteration 161, loss = 0.60395207\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69625788\n",
            "Iteration 2, loss = 0.66714922\n",
            "Iteration 3, loss = 0.66468724\n",
            "Iteration 4, loss = 0.66255386\n",
            "Iteration 5, loss = 0.66158745\n",
            "Iteration 6, loss = 0.66060964\n",
            "Iteration 7, loss = 0.65967357\n",
            "Iteration 8, loss = 0.65922878\n",
            "Iteration 9, loss = 0.65884381\n",
            "Iteration 10, loss = 0.65705005\n",
            "Iteration 11, loss = 0.65696475\n",
            "Iteration 12, loss = 0.65638124\n",
            "Iteration 13, loss = 0.65506923\n",
            "Iteration 14, loss = 0.65597012\n",
            "Iteration 15, loss = 0.65419181\n",
            "Iteration 16, loss = 0.65378283\n",
            "Iteration 17, loss = 0.65279879\n",
            "Iteration 18, loss = 0.65270539\n",
            "Iteration 19, loss = 0.65127709\n",
            "Iteration 20, loss = 0.65060015\n",
            "Iteration 21, loss = 0.64940392\n",
            "Iteration 22, loss = 0.64883943\n",
            "Iteration 23, loss = 0.64807344\n",
            "Iteration 24, loss = 0.64780168\n",
            "Iteration 25, loss = 0.64718231\n",
            "Iteration 26, loss = 0.64614265\n",
            "Iteration 27, loss = 0.64525912\n",
            "Iteration 28, loss = 0.64483908\n",
            "Iteration 29, loss = 0.64419172\n",
            "Iteration 30, loss = 0.64433624\n",
            "Iteration 31, loss = 0.64296366\n",
            "Iteration 32, loss = 0.64221824\n",
            "Iteration 33, loss = 0.64222250\n",
            "Iteration 34, loss = 0.64123563\n",
            "Iteration 35, loss = 0.64077107\n",
            "Iteration 36, loss = 0.63971098\n",
            "Iteration 37, loss = 0.63951047\n",
            "Iteration 38, loss = 0.63987296\n",
            "Iteration 39, loss = 0.63898363\n",
            "Iteration 40, loss = 0.63719485\n",
            "Iteration 41, loss = 0.63696588\n",
            "Iteration 42, loss = 0.63623251\n",
            "Iteration 43, loss = 0.63585069\n",
            "Iteration 44, loss = 0.63605034\n",
            "Iteration 45, loss = 0.63543911\n",
            "Iteration 46, loss = 0.63417858\n",
            "Iteration 47, loss = 0.63620345\n",
            "Iteration 48, loss = 0.63379029\n",
            "Iteration 49, loss = 0.63337392\n",
            "Iteration 50, loss = 0.63231443\n",
            "Iteration 51, loss = 0.63138670\n",
            "Iteration 52, loss = 0.63139958\n",
            "Iteration 53, loss = 0.63176284\n",
            "Iteration 54, loss = 0.63041891\n",
            "Iteration 55, loss = 0.63001167\n",
            "Iteration 56, loss = 0.62987379\n",
            "Iteration 57, loss = 0.62865464\n",
            "Iteration 58, loss = 0.62898465\n",
            "Iteration 59, loss = 0.62911880\n",
            "Iteration 60, loss = 0.62798901\n",
            "Iteration 61, loss = 0.62818703\n",
            "Iteration 62, loss = 0.62687882\n",
            "Iteration 63, loss = 0.62674690\n",
            "Iteration 64, loss = 0.62654991\n",
            "Iteration 65, loss = 0.62613444\n",
            "Iteration 66, loss = 0.62606309\n",
            "Iteration 67, loss = 0.62555039\n",
            "Iteration 68, loss = 0.62519640\n",
            "Iteration 69, loss = 0.62490135\n",
            "Iteration 70, loss = 0.62471526\n",
            "Iteration 71, loss = 0.62418683\n",
            "Iteration 72, loss = 0.62410214\n",
            "Iteration 73, loss = 0.62513399\n",
            "Iteration 74, loss = 0.62410115\n",
            "Iteration 75, loss = 0.62321344\n",
            "Iteration 76, loss = 0.62349638\n",
            "Iteration 77, loss = 0.62294439\n",
            "Iteration 78, loss = 0.62307859\n",
            "Iteration 79, loss = 0.62217292\n",
            "Iteration 80, loss = 0.62200638\n",
            "Iteration 81, loss = 0.62168438\n",
            "Iteration 82, loss = 0.62127444\n",
            "Iteration 83, loss = 0.62105618\n",
            "Iteration 84, loss = 0.62192603\n",
            "Iteration 85, loss = 0.62098851\n",
            "Iteration 86, loss = 0.62064913\n",
            "Iteration 87, loss = 0.62153943\n",
            "Iteration 88, loss = 0.61977006\n",
            "Iteration 89, loss = 0.61997286\n",
            "Iteration 90, loss = 0.61993589\n",
            "Iteration 91, loss = 0.61983495\n",
            "Iteration 92, loss = 0.62015296\n",
            "Iteration 93, loss = 0.62269510\n",
            "Iteration 94, loss = 0.62124543\n",
            "Iteration 95, loss = 0.61893057\n",
            "Iteration 96, loss = 0.61938381\n",
            "Iteration 97, loss = 0.61867125\n",
            "Iteration 98, loss = 0.61962943\n",
            "Iteration 99, loss = 0.62163610\n",
            "Iteration 100, loss = 0.61997212\n",
            "Iteration 101, loss = 0.61860355\n",
            "Iteration 102, loss = 0.61899156\n",
            "Iteration 103, loss = 0.61954113\n",
            "Iteration 104, loss = 0.61864303\n",
            "Iteration 105, loss = 0.61800085\n",
            "Iteration 106, loss = 0.61780242\n",
            "Iteration 107, loss = 0.62006447\n",
            "Iteration 108, loss = 0.61877934\n",
            "Iteration 109, loss = 0.61792385\n",
            "Iteration 110, loss = 0.61757352\n",
            "Iteration 111, loss = 0.61815432\n",
            "Iteration 112, loss = 0.61684956\n",
            "Iteration 113, loss = 0.61807816\n",
            "Iteration 114, loss = 0.61697489\n",
            "Iteration 115, loss = 0.61765065\n",
            "Iteration 116, loss = 0.61739424\n",
            "Iteration 117, loss = 0.61659942\n",
            "Iteration 118, loss = 0.61792379\n",
            "Iteration 119, loss = 0.61769296\n",
            "Iteration 120, loss = 0.61678819\n",
            "Iteration 121, loss = 0.61711653\n",
            "Iteration 122, loss = 0.61711368\n",
            "Iteration 123, loss = 0.61777320\n",
            "Iteration 124, loss = 0.61799788\n",
            "Iteration 125, loss = 0.61721563\n",
            "Iteration 126, loss = 0.61693736\n",
            "Iteration 127, loss = 0.61707704\n",
            "Iteration 128, loss = 0.61713235\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66720443\n",
            "Iteration 2, loss = 0.66647878\n",
            "Iteration 3, loss = 0.66603499\n",
            "Iteration 4, loss = 0.66368159\n",
            "Iteration 5, loss = 0.66177960\n",
            "Iteration 6, loss = 0.66137741\n",
            "Iteration 7, loss = 0.66031068\n",
            "Iteration 8, loss = 0.65817961\n",
            "Iteration 9, loss = 0.65788147\n",
            "Iteration 10, loss = 0.65677284\n",
            "Iteration 11, loss = 0.65596659\n",
            "Iteration 12, loss = 0.65399363\n",
            "Iteration 13, loss = 0.65374908\n",
            "Iteration 14, loss = 0.65220272\n",
            "Iteration 15, loss = 0.65133806\n",
            "Iteration 16, loss = 0.65077105\n",
            "Iteration 17, loss = 0.64900985\n",
            "Iteration 18, loss = 0.64782341\n",
            "Iteration 19, loss = 0.64705818\n",
            "Iteration 20, loss = 0.64632014\n",
            "Iteration 21, loss = 0.64562024\n",
            "Iteration 22, loss = 0.64436309\n",
            "Iteration 23, loss = 0.64330376\n",
            "Iteration 24, loss = 0.64285057\n",
            "Iteration 25, loss = 0.64216723\n",
            "Iteration 26, loss = 0.64218413\n",
            "Iteration 27, loss = 0.64024946\n",
            "Iteration 28, loss = 0.63895924\n",
            "Iteration 29, loss = 0.63811928\n",
            "Iteration 30, loss = 0.63835508\n",
            "Iteration 31, loss = 0.63733869\n",
            "Iteration 32, loss = 0.63596393\n",
            "Iteration 33, loss = 0.63440617\n",
            "Iteration 34, loss = 0.63499899\n",
            "Iteration 35, loss = 0.63335003\n",
            "Iteration 36, loss = 0.63255030\n",
            "Iteration 37, loss = 0.63148815\n",
            "Iteration 38, loss = 0.63094219\n",
            "Iteration 39, loss = 0.63010071\n",
            "Iteration 40, loss = 0.62928784\n",
            "Iteration 41, loss = 0.62845459\n",
            "Iteration 42, loss = 0.62773162\n",
            "Iteration 43, loss = 0.62713023\n",
            "Iteration 44, loss = 0.62765934\n",
            "Iteration 45, loss = 0.62583807\n",
            "Iteration 46, loss = 0.62528906\n",
            "Iteration 47, loss = 0.62432285\n",
            "Iteration 48, loss = 0.62356359\n",
            "Iteration 49, loss = 0.62291516\n",
            "Iteration 50, loss = 0.62235719\n",
            "Iteration 51, loss = 0.62211842\n",
            "Iteration 52, loss = 0.62071151\n",
            "Iteration 53, loss = 0.62081008\n",
            "Iteration 54, loss = 0.61975928\n",
            "Iteration 55, loss = 0.61898861\n",
            "Iteration 56, loss = 0.61794797\n",
            "Iteration 57, loss = 0.61860374\n",
            "Iteration 58, loss = 0.61779113\n",
            "Iteration 59, loss = 0.61712904\n",
            "Iteration 60, loss = 0.61649935\n",
            "Iteration 61, loss = 0.61602615\n",
            "Iteration 62, loss = 0.61523915\n",
            "Iteration 63, loss = 0.61592714\n",
            "Iteration 64, loss = 0.61554062\n",
            "Iteration 65, loss = 0.61455530\n",
            "Iteration 66, loss = 0.61410274\n",
            "Iteration 67, loss = 0.61436557\n",
            "Iteration 68, loss = 0.61278200\n",
            "Iteration 69, loss = 0.61341620\n",
            "Iteration 70, loss = 0.61200278\n",
            "Iteration 71, loss = 0.61217091\n",
            "Iteration 72, loss = 0.61160172\n",
            "Iteration 73, loss = 0.61076112\n",
            "Iteration 74, loss = 0.61066032\n",
            "Iteration 75, loss = 0.61037200\n",
            "Iteration 76, loss = 0.60977542\n",
            "Iteration 77, loss = 0.60970475\n",
            "Iteration 78, loss = 0.60978113\n",
            "Iteration 79, loss = 0.60922679\n",
            "Iteration 80, loss = 0.60884388\n",
            "Iteration 81, loss = 0.60919328\n",
            "Iteration 82, loss = 0.60794971\n",
            "Iteration 83, loss = 0.60779219\n",
            "Iteration 84, loss = 0.61060114\n",
            "Iteration 85, loss = 0.60808643\n",
            "Iteration 86, loss = 0.60729024\n",
            "Iteration 87, loss = 0.60783450\n",
            "Iteration 88, loss = 0.60685014\n",
            "Iteration 89, loss = 0.60649103\n",
            "Iteration 90, loss = 0.60698862\n",
            "Iteration 91, loss = 0.60650774\n",
            "Iteration 92, loss = 0.60564223\n",
            "Iteration 93, loss = 0.60539161\n",
            "Iteration 94, loss = 0.60557041\n",
            "Iteration 95, loss = 0.60547584\n",
            "Iteration 96, loss = 0.60559516\n",
            "Iteration 97, loss = 0.60482110\n",
            "Iteration 98, loss = 0.60648814\n",
            "Iteration 99, loss = 0.60360871\n",
            "Iteration 100, loss = 0.60529662\n",
            "Iteration 101, loss = 0.60461008\n",
            "Iteration 102, loss = 0.60506198\n",
            "Iteration 103, loss = 0.60526099\n",
            "Iteration 104, loss = 0.60384683\n",
            "Iteration 105, loss = 0.60629922\n",
            "Iteration 106, loss = 0.60420257\n",
            "Iteration 107, loss = 0.60346073\n",
            "Iteration 108, loss = 0.60295117\n",
            "Iteration 109, loss = 0.60311582\n",
            "Iteration 110, loss = 0.60269231\n",
            "Iteration 111, loss = 0.60404491\n",
            "Iteration 112, loss = 0.60278977\n",
            "Iteration 113, loss = 0.60263651\n",
            "Iteration 114, loss = 0.60292901\n",
            "Iteration 115, loss = 0.60262715\n",
            "Iteration 116, loss = 0.60247812\n",
            "Iteration 117, loss = 0.60211067\n",
            "Iteration 118, loss = 0.60267652\n",
            "Iteration 119, loss = 0.60326385\n",
            "Iteration 120, loss = 0.60303517\n",
            "Iteration 121, loss = 0.60238420\n",
            "Iteration 122, loss = 0.60196209\n",
            "Iteration 123, loss = 0.60185065\n",
            "Iteration 124, loss = 0.60160839\n",
            "Iteration 125, loss = 0.60194061\n",
            "Iteration 126, loss = 0.60325630\n",
            "Iteration 127, loss = 0.60305636\n",
            "Iteration 128, loss = 0.60194049\n",
            "Iteration 129, loss = 0.60183647\n",
            "Iteration 130, loss = 0.60178612\n",
            "Iteration 131, loss = 0.60173182\n",
            "Iteration 132, loss = 0.60173091\n",
            "Iteration 133, loss = 0.60129736\n",
            "Iteration 134, loss = 0.60292955\n",
            "Iteration 135, loss = 0.60447166\n",
            "Iteration 136, loss = 0.60212094\n",
            "Iteration 137, loss = 0.60101593\n",
            "Iteration 138, loss = 0.60182768\n",
            "Iteration 139, loss = 0.60271358\n",
            "Iteration 140, loss = 0.60145762\n",
            "Iteration 141, loss = 0.60125509\n",
            "Iteration 142, loss = 0.60203626\n",
            "Iteration 143, loss = 0.60319475\n",
            "Iteration 144, loss = 0.60317601\n",
            "Iteration 145, loss = 0.59995765\n",
            "Iteration 146, loss = 0.60134494\n",
            "Iteration 147, loss = 0.60294871\n",
            "Iteration 148, loss = 0.60219598\n",
            "Iteration 149, loss = 0.60096838\n",
            "Iteration 150, loss = 0.60096028\n",
            "Iteration 151, loss = 0.60156518\n",
            "Iteration 152, loss = 0.60094879\n",
            "Iteration 153, loss = 0.60053298\n",
            "Iteration 154, loss = 0.60097716\n",
            "Iteration 155, loss = 0.60066031\n",
            "Iteration 156, loss = 0.60056945\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68573742\n",
            "Iteration 2, loss = 0.66811238\n",
            "Iteration 3, loss = 0.66498978\n",
            "Iteration 4, loss = 0.66301889\n",
            "Iteration 5, loss = 0.66217481\n",
            "Iteration 6, loss = 0.66155992\n",
            "Iteration 7, loss = 0.65985078\n",
            "Iteration 8, loss = 0.65851902\n",
            "Iteration 9, loss = 0.65874084\n",
            "Iteration 10, loss = 0.65698479\n",
            "Iteration 11, loss = 0.65552046\n",
            "Iteration 12, loss = 0.65359378\n",
            "Iteration 13, loss = 0.65323579\n",
            "Iteration 14, loss = 0.65142281\n",
            "Iteration 15, loss = 0.65252995\n",
            "Iteration 16, loss = 0.64996629\n",
            "Iteration 17, loss = 0.64916722\n",
            "Iteration 18, loss = 0.64858250\n",
            "Iteration 19, loss = 0.64727731\n",
            "Iteration 20, loss = 0.64624592\n",
            "Iteration 21, loss = 0.64461191\n",
            "Iteration 22, loss = 0.64515642\n",
            "Iteration 23, loss = 0.64366646\n",
            "Iteration 24, loss = 0.64217397\n",
            "Iteration 25, loss = 0.64186197\n",
            "Iteration 26, loss = 0.64071200\n",
            "Iteration 27, loss = 0.63990401\n",
            "Iteration 28, loss = 0.63899853\n",
            "Iteration 29, loss = 0.63903167\n",
            "Iteration 30, loss = 0.63838232\n",
            "Iteration 31, loss = 0.63772085\n",
            "Iteration 32, loss = 0.63601595\n",
            "Iteration 33, loss = 0.63518880\n",
            "Iteration 34, loss = 0.63478281\n",
            "Iteration 35, loss = 0.63378683\n",
            "Iteration 36, loss = 0.63290834\n",
            "Iteration 37, loss = 0.63192429\n",
            "Iteration 38, loss = 0.63125019\n",
            "Iteration 39, loss = 0.63139101\n",
            "Iteration 40, loss = 0.63052792\n",
            "Iteration 41, loss = 0.62939644\n",
            "Iteration 42, loss = 0.62888745\n",
            "Iteration 43, loss = 0.62869415\n",
            "Iteration 44, loss = 0.62708813\n",
            "Iteration 45, loss = 0.62654162\n",
            "Iteration 46, loss = 0.62617439\n",
            "Iteration 47, loss = 0.62623928\n",
            "Iteration 48, loss = 0.62495519\n",
            "Iteration 49, loss = 0.62397757\n",
            "Iteration 50, loss = 0.62356194\n",
            "Iteration 51, loss = 0.62312044\n",
            "Iteration 52, loss = 0.62321408\n",
            "Iteration 53, loss = 0.62246545\n",
            "Iteration 54, loss = 0.62302106\n",
            "Iteration 55, loss = 0.62030228\n",
            "Iteration 56, loss = 0.62110882\n",
            "Iteration 57, loss = 0.62089788\n",
            "Iteration 58, loss = 0.61936523\n",
            "Iteration 59, loss = 0.61891797\n",
            "Iteration 60, loss = 0.62001191\n",
            "Iteration 61, loss = 0.61942516\n",
            "Iteration 62, loss = 0.61877410\n",
            "Iteration 63, loss = 0.61785731\n",
            "Iteration 64, loss = 0.61806996\n",
            "Iteration 65, loss = 0.61735605\n",
            "Iteration 66, loss = 0.61766742\n",
            "Iteration 67, loss = 0.61551862\n",
            "Iteration 68, loss = 0.61555285\n",
            "Iteration 69, loss = 0.61514429\n",
            "Iteration 70, loss = 0.61497049\n",
            "Iteration 71, loss = 0.61469417\n",
            "Iteration 72, loss = 0.61473832\n",
            "Iteration 73, loss = 0.61390108\n",
            "Iteration 74, loss = 0.61559862\n",
            "Iteration 75, loss = 0.61369499\n",
            "Iteration 76, loss = 0.61345893\n",
            "Iteration 77, loss = 0.61316990\n",
            "Iteration 78, loss = 0.61281351\n",
            "Iteration 79, loss = 0.61286439\n",
            "Iteration 80, loss = 0.61332661\n",
            "Iteration 81, loss = 0.61265641\n",
            "Iteration 82, loss = 0.61243467\n",
            "Iteration 83, loss = 0.61200201\n",
            "Iteration 84, loss = 0.61202378\n",
            "Iteration 85, loss = 0.61172552\n",
            "Iteration 86, loss = 0.61143178\n",
            "Iteration 87, loss = 0.61209995\n",
            "Iteration 88, loss = 0.61125959\n",
            "Iteration 89, loss = 0.61090796\n",
            "Iteration 90, loss = 0.61090673\n",
            "Iteration 91, loss = 0.61026214\n",
            "Iteration 92, loss = 0.61098760\n",
            "Iteration 93, loss = 0.61023082\n",
            "Iteration 94, loss = 0.61031560\n",
            "Iteration 95, loss = 0.61039624\n",
            "Iteration 96, loss = 0.61018884\n",
            "Iteration 97, loss = 0.60958189\n",
            "Iteration 98, loss = 0.60978964\n",
            "Iteration 99, loss = 0.60927130\n",
            "Iteration 100, loss = 0.60997325\n",
            "Iteration 101, loss = 0.60987822\n",
            "Iteration 102, loss = 0.60998252\n",
            "Iteration 103, loss = 0.60969940\n",
            "Iteration 104, loss = 0.61094201\n",
            "Iteration 105, loss = 0.60909665\n",
            "Iteration 106, loss = 0.60966154\n",
            "Iteration 107, loss = 0.60962439\n",
            "Iteration 108, loss = 0.60884488\n",
            "Iteration 109, loss = 0.60907284\n",
            "Iteration 110, loss = 0.60996516\n",
            "Iteration 111, loss = 0.60880398\n",
            "Iteration 112, loss = 0.60851637\n",
            "Iteration 113, loss = 0.60895906\n",
            "Iteration 114, loss = 0.60903188\n",
            "Iteration 115, loss = 0.60863325\n",
            "Iteration 116, loss = 0.60961421\n",
            "Iteration 117, loss = 0.60906854\n",
            "Iteration 118, loss = 0.61104666\n",
            "Iteration 119, loss = 0.60889205\n",
            "Iteration 120, loss = 0.61065810\n",
            "Iteration 121, loss = 0.61018090\n",
            "Iteration 122, loss = 0.60833293\n",
            "Iteration 123, loss = 0.60801811\n",
            "Iteration 124, loss = 0.60814508\n",
            "Iteration 125, loss = 0.60800359\n",
            "Iteration 126, loss = 0.60792436\n",
            "Iteration 127, loss = 0.60789837\n",
            "Iteration 128, loss = 0.60926930\n",
            "Iteration 129, loss = 0.61146719\n",
            "Iteration 130, loss = 0.60796809\n",
            "Iteration 131, loss = 0.60803181\n",
            "Iteration 132, loss = 0.60889799\n",
            "Iteration 133, loss = 0.60762538\n",
            "Iteration 134, loss = 0.60751383\n",
            "Iteration 135, loss = 0.60734614\n",
            "Iteration 136, loss = 0.60744906\n",
            "Iteration 137, loss = 0.60830468\n",
            "Iteration 138, loss = 0.60783276\n",
            "Iteration 139, loss = 0.60694096\n",
            "Iteration 140, loss = 0.60814326\n",
            "Iteration 141, loss = 0.60808499\n",
            "Iteration 142, loss = 0.60713874\n",
            "Iteration 143, loss = 0.60737709\n",
            "Iteration 144, loss = 0.60746370\n",
            "Iteration 145, loss = 0.60738702\n",
            "Iteration 146, loss = 0.60784105\n",
            "Iteration 147, loss = 0.60728987\n",
            "Iteration 148, loss = 0.60756824\n",
            "Iteration 149, loss = 0.60780060\n",
            "Iteration 150, loss = 0.60757319\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.63899711\n",
            "Iteration 2, loss = 0.60618121\n",
            "Iteration 3, loss = 0.60504476\n",
            "Iteration 4, loss = 0.59462099\n",
            "Iteration 5, loss = 0.58802701\n",
            "Iteration 6, loss = 0.58138704\n",
            "Iteration 7, loss = 0.56825902\n",
            "Iteration 8, loss = 0.55611443\n",
            "Iteration 9, loss = 0.54218867\n",
            "Iteration 10, loss = 0.52212682\n",
            "Iteration 11, loss = 0.50103235\n",
            "Iteration 12, loss = 0.49762217\n",
            "Iteration 13, loss = 0.48245789\n",
            "Iteration 14, loss = 0.46495797\n",
            "Iteration 15, loss = 0.47611537\n",
            "Iteration 16, loss = 0.45238377\n",
            "Iteration 17, loss = 0.45445365\n",
            "Iteration 18, loss = 0.45115076\n",
            "Iteration 19, loss = 0.44929926\n",
            "Iteration 20, loss = 0.44993468\n",
            "Iteration 21, loss = 0.44716619\n",
            "Iteration 22, loss = 0.44046632\n",
            "Iteration 23, loss = 0.44451767\n",
            "Iteration 24, loss = 0.45321120\n",
            "Iteration 25, loss = 0.44362372\n",
            "Iteration 26, loss = 0.44068147\n",
            "Iteration 27, loss = 0.43874441\n",
            "Iteration 28, loss = 0.43803969\n",
            "Iteration 29, loss = 0.43848801\n",
            "Iteration 30, loss = 0.43025815\n",
            "Iteration 31, loss = 0.44693784\n",
            "Iteration 32, loss = 0.45236628\n",
            "Iteration 33, loss = 0.43584451\n",
            "Iteration 34, loss = 0.43394855\n",
            "Iteration 35, loss = 0.43468664\n",
            "Iteration 36, loss = 0.44339179\n",
            "Iteration 37, loss = 0.43611440\n",
            "Iteration 38, loss = 0.43016556\n",
            "Iteration 39, loss = 0.45218041\n",
            "Iteration 40, loss = 0.43231601\n",
            "Iteration 41, loss = 0.42861656\n",
            "Iteration 42, loss = 0.42704401\n",
            "Iteration 43, loss = 0.44068696\n",
            "Iteration 44, loss = 0.42780467\n",
            "Iteration 45, loss = 0.43037963\n",
            "Iteration 46, loss = 0.42618437\n",
            "Iteration 47, loss = 0.43821016\n",
            "Iteration 48, loss = 0.42505140\n",
            "Iteration 49, loss = 0.42693178\n",
            "Iteration 50, loss = 0.42526781\n",
            "Iteration 51, loss = 0.42013786\n",
            "Iteration 52, loss = 0.41797118\n",
            "Iteration 53, loss = 0.42465036\n",
            "Iteration 54, loss = 0.42376887\n",
            "Iteration 55, loss = 0.42331666\n",
            "Iteration 56, loss = 0.42743786\n",
            "Iteration 57, loss = 0.42355647\n",
            "Iteration 58, loss = 0.42087775\n",
            "Iteration 59, loss = 0.42695261\n",
            "Iteration 60, loss = 0.42083060\n",
            "Iteration 61, loss = 0.42579003\n",
            "Iteration 62, loss = 0.42106171\n",
            "Iteration 63, loss = 0.42090643\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64881794\n",
            "Iteration 2, loss = 0.60816489\n",
            "Iteration 3, loss = 0.60027968\n",
            "Iteration 4, loss = 0.59275716\n",
            "Iteration 5, loss = 0.59078060\n",
            "Iteration 6, loss = 0.57809433\n",
            "Iteration 7, loss = 0.57142185\n",
            "Iteration 8, loss = 0.56184090\n",
            "Iteration 9, loss = 0.53345488\n",
            "Iteration 10, loss = 0.52394714\n",
            "Iteration 11, loss = 0.51024348\n",
            "Iteration 12, loss = 0.50092405\n",
            "Iteration 13, loss = 0.49609998\n",
            "Iteration 14, loss = 0.47118224\n",
            "Iteration 15, loss = 0.47033966\n",
            "Iteration 16, loss = 0.47127192\n",
            "Iteration 17, loss = 0.47468727\n",
            "Iteration 18, loss = 0.46791150\n",
            "Iteration 19, loss = 0.46100505\n",
            "Iteration 20, loss = 0.45918701\n",
            "Iteration 21, loss = 0.45014646\n",
            "Iteration 22, loss = 0.46985243\n",
            "Iteration 23, loss = 0.44911414\n",
            "Iteration 24, loss = 0.45068020\n",
            "Iteration 25, loss = 0.44712503\n",
            "Iteration 26, loss = 0.44657718\n",
            "Iteration 27, loss = 0.44147363\n",
            "Iteration 28, loss = 0.45223304\n",
            "Iteration 29, loss = 0.44343612\n",
            "Iteration 30, loss = 0.45001671\n",
            "Iteration 31, loss = 0.44896418\n",
            "Iteration 32, loss = 0.44656112\n",
            "Iteration 33, loss = 0.44521111\n",
            "Iteration 34, loss = 0.47503939\n",
            "Iteration 35, loss = 0.45076407\n",
            "Iteration 36, loss = 0.44706715\n",
            "Iteration 37, loss = 0.44379605\n",
            "Iteration 38, loss = 0.43780429\n",
            "Iteration 39, loss = 0.45208627\n",
            "Iteration 40, loss = 0.43568817\n",
            "Iteration 41, loss = 0.43995763\n",
            "Iteration 42, loss = 0.43557621\n",
            "Iteration 43, loss = 0.43072599\n",
            "Iteration 44, loss = 0.43377527\n",
            "Iteration 45, loss = 0.43336375\n",
            "Iteration 46, loss = 0.43759196\n",
            "Iteration 47, loss = 0.43203462\n",
            "Iteration 48, loss = 0.43917395\n",
            "Iteration 49, loss = 0.43974768\n",
            "Iteration 50, loss = 0.45000499\n",
            "Iteration 51, loss = 0.43539958\n",
            "Iteration 52, loss = 0.42636484\n",
            "Iteration 53, loss = 0.42611281\n",
            "Iteration 54, loss = 0.42674567\n",
            "Iteration 55, loss = 0.43057719\n",
            "Iteration 56, loss = 0.42971842\n",
            "Iteration 57, loss = 0.43099436\n",
            "Iteration 58, loss = 0.42171466\n",
            "Iteration 59, loss = 0.42418196\n",
            "Iteration 60, loss = 0.42866834\n",
            "Iteration 61, loss = 0.42298196\n",
            "Iteration 62, loss = 0.42241286\n",
            "Iteration 63, loss = 0.42002911\n",
            "Iteration 64, loss = 0.42332846\n",
            "Iteration 65, loss = 0.42236558\n",
            "Iteration 66, loss = 0.42518276\n",
            "Iteration 67, loss = 0.41775552\n",
            "Iteration 68, loss = 0.42136635\n",
            "Iteration 69, loss = 0.42056140\n",
            "Iteration 70, loss = 0.41785713\n",
            "Iteration 71, loss = 0.41998235\n",
            "Iteration 72, loss = 0.42220427\n",
            "Iteration 73, loss = 0.41398983\n",
            "Iteration 74, loss = 0.41519874\n",
            "Iteration 75, loss = 0.44300814\n",
            "Iteration 76, loss = 0.42194124\n",
            "Iteration 77, loss = 0.41435385\n",
            "Iteration 78, loss = 0.42584871\n",
            "Iteration 79, loss = 0.41982721\n",
            "Iteration 80, loss = 0.41602959\n",
            "Iteration 81, loss = 0.41755081\n",
            "Iteration 82, loss = 0.41445224\n",
            "Iteration 83, loss = 0.42188678\n",
            "Iteration 84, loss = 0.41799038\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64941601\n",
            "Iteration 2, loss = 0.61787593\n",
            "Iteration 3, loss = 0.61479471\n",
            "Iteration 4, loss = 0.61300031\n",
            "Iteration 5, loss = 0.60178302\n",
            "Iteration 6, loss = 0.59311250\n",
            "Iteration 7, loss = 0.58160504\n",
            "Iteration 8, loss = 0.57603770\n",
            "Iteration 9, loss = 0.54028481\n",
            "Iteration 10, loss = 0.52498347\n",
            "Iteration 11, loss = 0.50301550\n",
            "Iteration 12, loss = 0.50137656\n",
            "Iteration 13, loss = 0.48380598\n",
            "Iteration 14, loss = 0.49905681\n",
            "Iteration 15, loss = 0.48638134\n",
            "Iteration 16, loss = 0.47484383\n",
            "Iteration 17, loss = 0.48225152\n",
            "Iteration 18, loss = 0.46774538\n",
            "Iteration 19, loss = 0.46406090\n",
            "Iteration 20, loss = 0.46354554\n",
            "Iteration 21, loss = 0.46379472\n",
            "Iteration 22, loss = 0.45469051\n",
            "Iteration 23, loss = 0.46124021\n",
            "Iteration 24, loss = 0.45778724\n",
            "Iteration 25, loss = 0.45674332\n",
            "Iteration 26, loss = 0.45448890\n",
            "Iteration 27, loss = 0.45199122\n",
            "Iteration 28, loss = 0.45315153\n",
            "Iteration 29, loss = 0.46179808\n",
            "Iteration 30, loss = 0.45231537\n",
            "Iteration 31, loss = 0.44624133\n",
            "Iteration 32, loss = 0.45178846\n",
            "Iteration 33, loss = 0.45312890\n",
            "Iteration 34, loss = 0.45314541\n",
            "Iteration 35, loss = 0.45241939\n",
            "Iteration 36, loss = 0.44737028\n",
            "Iteration 37, loss = 0.45028679\n",
            "Iteration 38, loss = 0.45047913\n",
            "Iteration 39, loss = 0.44892573\n",
            "Iteration 40, loss = 0.45101364\n",
            "Iteration 41, loss = 0.45016276\n",
            "Iteration 42, loss = 0.45141722\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64784948\n",
            "Iteration 2, loss = 0.61030581\n",
            "Iteration 3, loss = 0.60701043\n",
            "Iteration 4, loss = 0.59827067\n",
            "Iteration 5, loss = 0.58950769\n",
            "Iteration 6, loss = 0.58316397\n",
            "Iteration 7, loss = 0.57859893\n",
            "Iteration 8, loss = 0.57043613\n",
            "Iteration 9, loss = 0.56269610\n",
            "Iteration 10, loss = 0.54526698\n",
            "Iteration 11, loss = 0.53629792\n",
            "Iteration 12, loss = 0.50950991\n",
            "Iteration 13, loss = 0.49674878\n",
            "Iteration 14, loss = 0.49600465\n",
            "Iteration 15, loss = 0.47901642\n",
            "Iteration 16, loss = 0.48001512\n",
            "Iteration 17, loss = 0.46657277\n",
            "Iteration 18, loss = 0.47044421\n",
            "Iteration 19, loss = 0.47005797\n",
            "Iteration 20, loss = 0.46299673\n",
            "Iteration 21, loss = 0.48008765\n",
            "Iteration 22, loss = 0.45653873\n",
            "Iteration 23, loss = 0.44891611\n",
            "Iteration 24, loss = 0.45793667\n",
            "Iteration 25, loss = 0.44979128\n",
            "Iteration 26, loss = 0.45164343\n",
            "Iteration 27, loss = 0.44952894\n",
            "Iteration 28, loss = 0.44840798\n",
            "Iteration 29, loss = 0.45498322\n",
            "Iteration 30, loss = 0.44744049\n",
            "Iteration 31, loss = 0.45207078\n",
            "Iteration 32, loss = 0.44239929\n",
            "Iteration 33, loss = 0.44094734\n",
            "Iteration 34, loss = 0.44824657\n",
            "Iteration 35, loss = 0.45276078\n",
            "Iteration 36, loss = 0.43641033\n",
            "Iteration 37, loss = 0.44338677\n",
            "Iteration 38, loss = 0.44048470\n",
            "Iteration 39, loss = 0.44099011\n",
            "Iteration 40, loss = 0.43340339\n",
            "Iteration 41, loss = 0.43506831\n",
            "Iteration 42, loss = 0.43147408\n",
            "Iteration 43, loss = 0.43309682\n",
            "Iteration 44, loss = 0.42964621\n",
            "Iteration 45, loss = 0.42883044\n",
            "Iteration 46, loss = 0.42843422\n",
            "Iteration 47, loss = 0.43864506\n",
            "Iteration 48, loss = 0.42992051\n",
            "Iteration 49, loss = 0.43409559\n",
            "Iteration 50, loss = 0.42918955\n",
            "Iteration 51, loss = 0.43364079\n",
            "Iteration 52, loss = 0.42689365\n",
            "Iteration 53, loss = 0.42617462\n",
            "Iteration 54, loss = 0.42810521\n",
            "Iteration 55, loss = 0.43023363\n",
            "Iteration 56, loss = 0.42594294\n",
            "Iteration 57, loss = 0.42736947\n",
            "Iteration 58, loss = 0.42497846\n",
            "Iteration 59, loss = 0.42536545\n",
            "Iteration 60, loss = 0.42573205\n",
            "Iteration 61, loss = 0.42593938\n",
            "Iteration 62, loss = 0.42773904\n",
            "Iteration 63, loss = 0.43614994\n",
            "Iteration 64, loss = 0.42545255\n",
            "Iteration 65, loss = 0.41784744\n",
            "Iteration 66, loss = 0.42266948\n",
            "Iteration 67, loss = 0.42835312\n",
            "Iteration 68, loss = 0.42040044\n",
            "Iteration 69, loss = 0.42951366\n",
            "Iteration 70, loss = 0.41837727\n",
            "Iteration 71, loss = 0.41722126\n",
            "Iteration 72, loss = 0.41445514\n",
            "Iteration 73, loss = 0.41822163\n",
            "Iteration 74, loss = 0.42153968\n",
            "Iteration 75, loss = 0.41787529\n",
            "Iteration 76, loss = 0.42181036\n",
            "Iteration 77, loss = 0.42051847\n",
            "Iteration 78, loss = 0.43292415\n",
            "Iteration 79, loss = 0.41790054\n",
            "Iteration 80, loss = 0.41540180\n",
            "Iteration 81, loss = 0.42257087\n",
            "Iteration 82, loss = 0.43100098\n",
            "Iteration 83, loss = 0.42118342\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64519054\n",
            "Iteration 2, loss = 0.61690991\n",
            "Iteration 3, loss = 0.60226827\n",
            "Iteration 4, loss = 0.60296134\n",
            "Iteration 5, loss = 0.59748389\n",
            "Iteration 6, loss = 0.58575039\n",
            "Iteration 7, loss = 0.57738961\n",
            "Iteration 8, loss = 0.56648187\n",
            "Iteration 9, loss = 0.54950888\n",
            "Iteration 10, loss = 0.53706939\n",
            "Iteration 11, loss = 0.52073579\n",
            "Iteration 12, loss = 0.50729039\n",
            "Iteration 13, loss = 0.49733460\n",
            "Iteration 14, loss = 0.49657096\n",
            "Iteration 15, loss = 0.47935576\n",
            "Iteration 16, loss = 0.48911318\n",
            "Iteration 17, loss = 0.47627343\n",
            "Iteration 18, loss = 0.46500532\n",
            "Iteration 19, loss = 0.46853948\n",
            "Iteration 20, loss = 0.47648251\n",
            "Iteration 21, loss = 0.45919108\n",
            "Iteration 22, loss = 0.46314875\n",
            "Iteration 23, loss = 0.47660992\n",
            "Iteration 24, loss = 0.45753574\n",
            "Iteration 25, loss = 0.46207999\n",
            "Iteration 26, loss = 0.46676320\n",
            "Iteration 27, loss = 0.46145457\n",
            "Iteration 28, loss = 0.45208334\n",
            "Iteration 29, loss = 0.45440018\n",
            "Iteration 30, loss = 0.44565347\n",
            "Iteration 31, loss = 0.45347101\n",
            "Iteration 32, loss = 0.46129832\n",
            "Iteration 33, loss = 0.44710115\n",
            "Iteration 34, loss = 0.44350576\n",
            "Iteration 35, loss = 0.44732478\n",
            "Iteration 36, loss = 0.45491658\n",
            "Iteration 37, loss = 0.44183543\n",
            "Iteration 38, loss = 0.45291928\n",
            "Iteration 39, loss = 0.44756533\n",
            "Iteration 40, loss = 0.44369723\n",
            "Iteration 41, loss = 0.44928213\n",
            "Iteration 42, loss = 0.44401294\n",
            "Iteration 43, loss = 0.44622882\n",
            "Iteration 44, loss = 0.44007010\n",
            "Iteration 45, loss = 0.44008925\n",
            "Iteration 46, loss = 0.43826012\n",
            "Iteration 47, loss = 0.43939306\n",
            "Iteration 48, loss = 0.43527780\n",
            "Iteration 49, loss = 0.44914751\n",
            "Iteration 50, loss = 0.44272598\n",
            "Iteration 51, loss = 0.44498879\n",
            "Iteration 52, loss = 0.43495683\n",
            "Iteration 53, loss = 0.43428532\n",
            "Iteration 54, loss = 0.45192245\n",
            "Iteration 55, loss = 0.43611984\n",
            "Iteration 56, loss = 0.43761863\n",
            "Iteration 57, loss = 0.43730986\n",
            "Iteration 58, loss = 0.43059473\n",
            "Iteration 59, loss = 0.43339440\n",
            "Iteration 60, loss = 0.43775866\n",
            "Iteration 61, loss = 0.42943312\n",
            "Iteration 62, loss = 0.43135056\n",
            "Iteration 63, loss = 0.43061761\n",
            "Iteration 64, loss = 0.43346086\n",
            "Iteration 65, loss = 0.43657438\n",
            "Iteration 66, loss = 0.43618062\n",
            "Iteration 67, loss = 0.43912924\n",
            "Iteration 68, loss = 0.43177844\n",
            "Iteration 69, loss = 0.42437293\n",
            "Iteration 70, loss = 0.42504136\n",
            "Iteration 71, loss = 0.42384023\n",
            "Iteration 72, loss = 0.42867322\n",
            "Iteration 73, loss = 0.42131285\n",
            "Iteration 74, loss = 0.42488452\n",
            "Iteration 75, loss = 0.42396329\n",
            "Iteration 76, loss = 0.42069438\n",
            "Iteration 77, loss = 0.41848649\n",
            "Iteration 78, loss = 0.41877883\n",
            "Iteration 79, loss = 0.41759768\n",
            "Iteration 80, loss = 0.42434912\n",
            "Iteration 81, loss = 0.41684282\n",
            "Iteration 82, loss = 0.41707160\n",
            "Iteration 83, loss = 0.42111499\n",
            "Iteration 84, loss = 0.41691637\n",
            "Iteration 85, loss = 0.41992370\n",
            "Iteration 86, loss = 0.42463714\n",
            "Iteration 87, loss = 0.42013279\n",
            "Iteration 88, loss = 0.41438934\n",
            "Iteration 89, loss = 0.41413320\n",
            "Iteration 90, loss = 0.41719321\n",
            "Iteration 91, loss = 0.41331231\n",
            "Iteration 92, loss = 0.41545882\n",
            "Iteration 93, loss = 0.41947827\n",
            "Iteration 94, loss = 0.41929845\n",
            "Iteration 95, loss = 0.42219766\n",
            "Iteration 96, loss = 0.41195455\n",
            "Iteration 97, loss = 0.42290838\n",
            "Iteration 98, loss = 0.40619863\n",
            "Iteration 99, loss = 0.40795286\n",
            "Iteration 100, loss = 0.41836573\n",
            "Iteration 101, loss = 0.41121028\n",
            "Iteration 102, loss = 0.41306297\n",
            "Iteration 103, loss = 0.40991432\n",
            "Iteration 104, loss = 0.41031279\n",
            "Iteration 105, loss = 0.41212685\n",
            "Iteration 106, loss = 0.40479166\n",
            "Iteration 107, loss = 0.40683808\n",
            "Iteration 108, loss = 0.41907895\n",
            "Iteration 109, loss = 0.41337679\n",
            "Iteration 110, loss = 0.40644005\n",
            "Iteration 111, loss = 0.40963701\n",
            "Iteration 112, loss = 0.40640646\n",
            "Iteration 113, loss = 0.40750907\n",
            "Iteration 114, loss = 0.41296270\n",
            "Iteration 115, loss = 0.41647825\n",
            "Iteration 116, loss = 0.40790431\n",
            "Iteration 117, loss = 0.41921781\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66765794\n",
            "Iteration 2, loss = 0.65773521\n",
            "Iteration 3, loss = 0.65441994\n",
            "Iteration 4, loss = 0.65120914\n",
            "Iteration 5, loss = 0.64799498\n",
            "Iteration 6, loss = 0.64418614\n",
            "Iteration 7, loss = 0.64155872\n",
            "Iteration 8, loss = 0.63785372\n",
            "Iteration 9, loss = 0.63712568\n",
            "Iteration 10, loss = 0.63360411\n",
            "Iteration 11, loss = 0.62987771\n",
            "Iteration 12, loss = 0.62722101\n",
            "Iteration 13, loss = 0.62788453\n",
            "Iteration 14, loss = 0.62334690\n",
            "Iteration 15, loss = 0.62185289\n",
            "Iteration 16, loss = 0.61924139\n",
            "Iteration 17, loss = 0.61634189\n",
            "Iteration 18, loss = 0.61606814\n",
            "Iteration 19, loss = 0.61377957\n",
            "Iteration 20, loss = 0.61508860\n",
            "Iteration 21, loss = 0.61131368\n",
            "Iteration 22, loss = 0.61398928\n",
            "Iteration 23, loss = 0.61180399\n",
            "Iteration 24, loss = 0.61240004\n",
            "Iteration 25, loss = 0.60913274\n",
            "Iteration 26, loss = 0.60845102\n",
            "Iteration 27, loss = 0.60909380\n",
            "Iteration 28, loss = 0.60887738\n",
            "Iteration 29, loss = 0.60851668\n",
            "Iteration 30, loss = 0.60733128\n",
            "Iteration 31, loss = 0.60798279\n",
            "Iteration 32, loss = 0.60753537\n",
            "Iteration 33, loss = 0.60860614\n",
            "Iteration 34, loss = 0.60733703\n",
            "Iteration 35, loss = 0.60811687\n",
            "Iteration 36, loss = 0.60768843\n",
            "Iteration 37, loss = 0.60656871\n",
            "Iteration 38, loss = 0.60629907\n",
            "Iteration 39, loss = 0.60653077\n",
            "Iteration 40, loss = 0.60625020\n",
            "Iteration 41, loss = 0.60687260\n",
            "Iteration 42, loss = 0.60508906\n",
            "Iteration 43, loss = 0.60726998\n",
            "Iteration 44, loss = 0.60652594\n",
            "Iteration 45, loss = 0.60683750\n",
            "Iteration 46, loss = 0.60651766\n",
            "Iteration 47, loss = 0.60370629\n",
            "Iteration 48, loss = 0.60592559\n",
            "Iteration 49, loss = 0.60546398\n",
            "Iteration 50, loss = 0.60559738\n",
            "Iteration 51, loss = 0.60426691\n",
            "Iteration 52, loss = 0.60462376\n",
            "Iteration 53, loss = 0.60423214\n",
            "Iteration 54, loss = 0.60434710\n",
            "Iteration 55, loss = 0.60554441\n",
            "Iteration 56, loss = 0.60275510\n",
            "Iteration 57, loss = 0.60401420\n",
            "Iteration 58, loss = 0.60353457\n",
            "Iteration 59, loss = 0.60363278\n",
            "Iteration 60, loss = 0.60348211\n",
            "Iteration 61, loss = 0.60355187\n",
            "Iteration 62, loss = 0.60326794\n",
            "Iteration 63, loss = 0.60262964\n",
            "Iteration 64, loss = 0.60212610\n",
            "Iteration 65, loss = 0.60401924\n",
            "Iteration 66, loss = 0.60183778\n",
            "Iteration 67, loss = 0.60267327\n",
            "Iteration 68, loss = 0.60364822\n",
            "Iteration 69, loss = 0.60272771\n",
            "Iteration 70, loss = 0.60339807\n",
            "Iteration 71, loss = 0.60315019\n",
            "Iteration 72, loss = 0.60360146\n",
            "Iteration 73, loss = 0.60241936\n",
            "Iteration 74, loss = 0.60213075\n",
            "Iteration 75, loss = 0.60087361\n",
            "Iteration 76, loss = 0.60083707\n",
            "Iteration 77, loss = 0.60242283\n",
            "Iteration 78, loss = 0.60010896\n",
            "Iteration 79, loss = 0.60079956\n",
            "Iteration 80, loss = 0.60076390\n",
            "Iteration 81, loss = 0.60034501\n",
            "Iteration 82, loss = 0.60071604\n",
            "Iteration 83, loss = 0.60049169\n",
            "Iteration 84, loss = 0.60066544\n",
            "Iteration 85, loss = 0.59962966\n",
            "Iteration 86, loss = 0.59958716\n",
            "Iteration 87, loss = 0.60134887\n",
            "Iteration 88, loss = 0.60037789\n",
            "Iteration 89, loss = 0.59816497\n",
            "Iteration 90, loss = 0.59952834\n",
            "Iteration 91, loss = 0.59840280\n",
            "Iteration 92, loss = 0.59913635\n",
            "Iteration 93, loss = 0.59841309\n",
            "Iteration 94, loss = 0.59740311\n",
            "Iteration 95, loss = 0.59778775\n",
            "Iteration 96, loss = 0.59705966\n",
            "Iteration 97, loss = 0.59855659\n",
            "Iteration 98, loss = 0.59954572\n",
            "Iteration 99, loss = 0.59645450\n",
            "Iteration 100, loss = 0.59812470\n",
            "Iteration 101, loss = 0.59698323\n",
            "Iteration 102, loss = 0.59792599\n",
            "Iteration 103, loss = 0.59651291\n",
            "Iteration 104, loss = 0.59782669\n",
            "Iteration 105, loss = 0.59569145\n",
            "Iteration 106, loss = 0.59576838\n",
            "Iteration 107, loss = 0.59600426\n",
            "Iteration 108, loss = 0.59721631\n",
            "Iteration 109, loss = 0.59485268\n",
            "Iteration 110, loss = 0.59392262\n",
            "Iteration 111, loss = 0.59544912\n",
            "Iteration 112, loss = 0.59447595\n",
            "Iteration 113, loss = 0.59437078\n",
            "Iteration 114, loss = 0.59352753\n",
            "Iteration 115, loss = 0.59492007\n",
            "Iteration 116, loss = 0.59320477\n",
            "Iteration 117, loss = 0.59393373\n",
            "Iteration 118, loss = 0.59417496\n",
            "Iteration 119, loss = 0.59364222\n",
            "Iteration 120, loss = 0.59170338\n",
            "Iteration 121, loss = 0.59428509\n",
            "Iteration 122, loss = 0.59680371\n",
            "Iteration 123, loss = 0.59395509\n",
            "Iteration 124, loss = 0.59170894\n",
            "Iteration 125, loss = 0.59275868\n",
            "Iteration 126, loss = 0.59200616\n",
            "Iteration 127, loss = 0.59530570\n",
            "Iteration 128, loss = 0.59360115\n",
            "Iteration 129, loss = 0.58982186\n",
            "Iteration 130, loss = 0.59154197\n",
            "Iteration 131, loss = 0.58979265\n",
            "Iteration 132, loss = 0.59056244\n",
            "Iteration 133, loss = 0.59005873\n",
            "Iteration 134, loss = 0.59106414\n",
            "Iteration 135, loss = 0.58929258\n",
            "Iteration 136, loss = 0.58860676\n",
            "Iteration 137, loss = 0.58763665\n",
            "Iteration 138, loss = 0.58895386\n",
            "Iteration 139, loss = 0.58975614\n",
            "Iteration 140, loss = 0.58476731\n",
            "Iteration 141, loss = 0.58948189\n",
            "Iteration 142, loss = 0.58736627\n",
            "Iteration 143, loss = 0.58710018\n",
            "Iteration 144, loss = 0.58994328\n",
            "Iteration 145, loss = 0.58734878\n",
            "Iteration 146, loss = 0.58537574\n",
            "Iteration 147, loss = 0.58764984\n",
            "Iteration 148, loss = 0.58711051\n",
            "Iteration 149, loss = 0.58548233\n",
            "Iteration 150, loss = 0.58754543\n",
            "Iteration 151, loss = 0.58688365\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.65867576\n",
            "Iteration 2, loss = 0.65647710\n",
            "Iteration 3, loss = 0.65284617\n",
            "Iteration 4, loss = 0.64894234\n",
            "Iteration 5, loss = 0.64653009\n",
            "Iteration 6, loss = 0.64397055\n",
            "Iteration 7, loss = 0.64135730\n",
            "Iteration 8, loss = 0.63700754\n",
            "Iteration 9, loss = 0.63522150\n",
            "Iteration 10, loss = 0.63173013\n",
            "Iteration 11, loss = 0.62928931\n",
            "Iteration 12, loss = 0.62701564\n",
            "Iteration 13, loss = 0.62528503\n",
            "Iteration 14, loss = 0.62314549\n",
            "Iteration 15, loss = 0.62063997\n",
            "Iteration 16, loss = 0.61784169\n",
            "Iteration 17, loss = 0.61518966\n",
            "Iteration 18, loss = 0.61550578\n",
            "Iteration 19, loss = 0.61434560\n",
            "Iteration 20, loss = 0.61235231\n",
            "Iteration 21, loss = 0.61160156\n",
            "Iteration 22, loss = 0.61132811\n",
            "Iteration 23, loss = 0.60912550\n",
            "Iteration 24, loss = 0.61012700\n",
            "Iteration 25, loss = 0.60869241\n",
            "Iteration 26, loss = 0.60827607\n",
            "Iteration 27, loss = 0.60924498\n",
            "Iteration 28, loss = 0.60666222\n",
            "Iteration 29, loss = 0.60861812\n",
            "Iteration 30, loss = 0.60616511\n",
            "Iteration 31, loss = 0.60662608\n",
            "Iteration 32, loss = 0.60544918\n",
            "Iteration 33, loss = 0.60444801\n",
            "Iteration 34, loss = 0.60586297\n",
            "Iteration 35, loss = 0.60459450\n",
            "Iteration 36, loss = 0.60440633\n",
            "Iteration 37, loss = 0.60870716\n",
            "Iteration 38, loss = 0.60520568\n",
            "Iteration 39, loss = 0.60436932\n",
            "Iteration 40, loss = 0.60449377\n",
            "Iteration 41, loss = 0.60390074\n",
            "Iteration 42, loss = 0.60392020\n",
            "Iteration 43, loss = 0.60375610\n",
            "Iteration 44, loss = 0.60636306\n",
            "Iteration 45, loss = 0.60398428\n",
            "Iteration 46, loss = 0.60399663\n",
            "Iteration 47, loss = 0.60371728\n",
            "Iteration 48, loss = 0.60597090\n",
            "Iteration 49, loss = 0.60378541\n",
            "Iteration 50, loss = 0.60495122\n",
            "Iteration 51, loss = 0.60293961\n",
            "Iteration 52, loss = 0.60267081\n",
            "Iteration 53, loss = 0.60350296\n",
            "Iteration 54, loss = 0.60387099\n",
            "Iteration 55, loss = 0.60230279\n",
            "Iteration 56, loss = 0.60369914\n",
            "Iteration 57, loss = 0.60312162\n",
            "Iteration 58, loss = 0.60158330\n",
            "Iteration 59, loss = 0.60515888\n",
            "Iteration 60, loss = 0.60167466\n",
            "Iteration 61, loss = 0.60182496\n",
            "Iteration 62, loss = 0.60158204\n",
            "Iteration 63, loss = 0.60144394\n",
            "Iteration 64, loss = 0.60150568\n",
            "Iteration 65, loss = 0.60026859\n",
            "Iteration 66, loss = 0.60184829\n",
            "Iteration 67, loss = 0.60034473\n",
            "Iteration 68, loss = 0.60239860\n",
            "Iteration 69, loss = 0.60062004\n",
            "Iteration 70, loss = 0.60047448\n",
            "Iteration 71, loss = 0.60298227\n",
            "Iteration 72, loss = 0.59913814\n",
            "Iteration 73, loss = 0.60199694\n",
            "Iteration 74, loss = 0.60050994\n",
            "Iteration 75, loss = 0.60205074\n",
            "Iteration 76, loss = 0.60053474\n",
            "Iteration 77, loss = 0.60001969\n",
            "Iteration 78, loss = 0.59984811\n",
            "Iteration 79, loss = 0.60064580\n",
            "Iteration 80, loss = 0.60020683\n",
            "Iteration 81, loss = 0.59832233\n",
            "Iteration 82, loss = 0.59990614\n",
            "Iteration 83, loss = 0.60038261\n",
            "Iteration 84, loss = 0.60015337\n",
            "Iteration 85, loss = 0.59995151\n",
            "Iteration 86, loss = 0.60069320\n",
            "Iteration 87, loss = 0.59909473\n",
            "Iteration 88, loss = 0.59854041\n",
            "Iteration 89, loss = 0.59940895\n",
            "Iteration 90, loss = 0.59791952\n",
            "Iteration 91, loss = 0.60131804\n",
            "Iteration 92, loss = 0.59544480\n",
            "Iteration 93, loss = 0.60017841\n",
            "Iteration 94, loss = 0.59853213\n",
            "Iteration 95, loss = 0.59984523\n",
            "Iteration 96, loss = 0.59803475\n",
            "Iteration 97, loss = 0.60039570\n",
            "Iteration 98, loss = 0.59752544\n",
            "Iteration 99, loss = 0.59934452\n",
            "Iteration 100, loss = 0.59709264\n",
            "Iteration 101, loss = 0.59927426\n",
            "Iteration 102, loss = 0.59662036\n",
            "Iteration 103, loss = 0.59972103\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67811085\n",
            "Iteration 2, loss = 0.66685334\n",
            "Iteration 3, loss = 0.66115995\n",
            "Iteration 4, loss = 0.66176881\n",
            "Iteration 5, loss = 0.66119840\n",
            "Iteration 6, loss = 0.65606545\n",
            "Iteration 7, loss = 0.65199617\n",
            "Iteration 8, loss = 0.65177832\n",
            "Iteration 9, loss = 0.64987007\n",
            "Iteration 10, loss = 0.64650047\n",
            "Iteration 11, loss = 0.64423581\n",
            "Iteration 12, loss = 0.64368110\n",
            "Iteration 13, loss = 0.64111472\n",
            "Iteration 14, loss = 0.64171570\n",
            "Iteration 15, loss = 0.63656400\n",
            "Iteration 16, loss = 0.63504622\n",
            "Iteration 17, loss = 0.63160751\n",
            "Iteration 18, loss = 0.63161050\n",
            "Iteration 19, loss = 0.63003831\n",
            "Iteration 20, loss = 0.62857149\n",
            "Iteration 21, loss = 0.62733872\n",
            "Iteration 22, loss = 0.62655857\n",
            "Iteration 23, loss = 0.62457336\n",
            "Iteration 24, loss = 0.62695851\n",
            "Iteration 25, loss = 0.62402141\n",
            "Iteration 26, loss = 0.62190141\n",
            "Iteration 27, loss = 0.62145253\n",
            "Iteration 28, loss = 0.62127041\n",
            "Iteration 29, loss = 0.61964434\n",
            "Iteration 30, loss = 0.62053045\n",
            "Iteration 31, loss = 0.61968430\n",
            "Iteration 32, loss = 0.62444506\n",
            "Iteration 33, loss = 0.61933668\n",
            "Iteration 34, loss = 0.61888731\n",
            "Iteration 35, loss = 0.61988308\n",
            "Iteration 36, loss = 0.61779264\n",
            "Iteration 37, loss = 0.61878749\n",
            "Iteration 38, loss = 0.61843240\n",
            "Iteration 39, loss = 0.61849802\n",
            "Iteration 40, loss = 0.61694680\n",
            "Iteration 41, loss = 0.61648353\n",
            "Iteration 42, loss = 0.61958780\n",
            "Iteration 43, loss = 0.61891566\n",
            "Iteration 44, loss = 0.61713359\n",
            "Iteration 45, loss = 0.61705602\n",
            "Iteration 46, loss = 0.61675515\n",
            "Iteration 47, loss = 0.61576690\n",
            "Iteration 48, loss = 0.61615761\n",
            "Iteration 49, loss = 0.61848726\n",
            "Iteration 50, loss = 0.61585381\n",
            "Iteration 51, loss = 0.61653868\n",
            "Iteration 52, loss = 0.61658968\n",
            "Iteration 53, loss = 0.61651304\n",
            "Iteration 54, loss = 0.61628843\n",
            "Iteration 55, loss = 0.61601667\n",
            "Iteration 56, loss = 0.61643977\n",
            "Iteration 57, loss = 0.61536052\n",
            "Iteration 58, loss = 0.61798579\n",
            "Iteration 59, loss = 0.61681471\n",
            "Iteration 60, loss = 0.61419701\n",
            "Iteration 61, loss = 0.61324542\n",
            "Iteration 62, loss = 0.61408273\n",
            "Iteration 63, loss = 0.61490727\n",
            "Iteration 64, loss = 0.61592422\n",
            "Iteration 65, loss = 0.61237146\n",
            "Iteration 66, loss = 0.61587016\n",
            "Iteration 67, loss = 0.61318014\n",
            "Iteration 68, loss = 0.61312800\n",
            "Iteration 69, loss = 0.61500044\n",
            "Iteration 70, loss = 0.61373941\n",
            "Iteration 71, loss = 0.61610573\n",
            "Iteration 72, loss = 0.61363027\n",
            "Iteration 73, loss = 0.61312127\n",
            "Iteration 74, loss = 0.61308790\n",
            "Iteration 75, loss = 0.61262707\n",
            "Iteration 76, loss = 0.61240957\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70133673\n",
            "Iteration 2, loss = 0.66538606\n",
            "Iteration 3, loss = 0.66076464\n",
            "Iteration 4, loss = 0.65701855\n",
            "Iteration 5, loss = 0.65348029\n",
            "Iteration 6, loss = 0.64870275\n",
            "Iteration 7, loss = 0.64623199\n",
            "Iteration 8, loss = 0.64290266\n",
            "Iteration 9, loss = 0.64093328\n",
            "Iteration 10, loss = 0.63825672\n",
            "Iteration 11, loss = 0.63348927\n",
            "Iteration 12, loss = 0.63028130\n",
            "Iteration 13, loss = 0.62787164\n",
            "Iteration 14, loss = 0.62767957\n",
            "Iteration 15, loss = 0.62420348\n",
            "Iteration 16, loss = 0.62126002\n",
            "Iteration 17, loss = 0.61892278\n",
            "Iteration 18, loss = 0.61626651\n",
            "Iteration 19, loss = 0.61464768\n",
            "Iteration 20, loss = 0.61339895\n",
            "Iteration 21, loss = 0.61066157\n",
            "Iteration 22, loss = 0.61021445\n",
            "Iteration 23, loss = 0.61090308\n",
            "Iteration 24, loss = 0.60832665\n",
            "Iteration 25, loss = 0.60745779\n",
            "Iteration 26, loss = 0.60653835\n",
            "Iteration 27, loss = 0.60761361\n",
            "Iteration 28, loss = 0.60585425\n",
            "Iteration 29, loss = 0.60584352\n",
            "Iteration 30, loss = 0.60537365\n",
            "Iteration 31, loss = 0.60621784\n",
            "Iteration 32, loss = 0.60292417\n",
            "Iteration 33, loss = 0.60526460\n",
            "Iteration 34, loss = 0.60356622\n",
            "Iteration 35, loss = 0.60412609\n",
            "Iteration 36, loss = 0.60248768\n",
            "Iteration 37, loss = 0.60287655\n",
            "Iteration 38, loss = 0.60178176\n",
            "Iteration 39, loss = 0.60438132\n",
            "Iteration 40, loss = 0.60136846\n",
            "Iteration 41, loss = 0.60299951\n",
            "Iteration 42, loss = 0.60244585\n",
            "Iteration 43, loss = 0.60188665\n",
            "Iteration 44, loss = 0.60094779\n",
            "Iteration 45, loss = 0.60374416\n",
            "Iteration 46, loss = 0.60050861\n",
            "Iteration 47, loss = 0.60019640\n",
            "Iteration 48, loss = 0.60060640\n",
            "Iteration 49, loss = 0.60048675\n",
            "Iteration 50, loss = 0.60243196\n",
            "Iteration 51, loss = 0.60067641\n",
            "Iteration 52, loss = 0.60070002\n",
            "Iteration 53, loss = 0.59962363\n",
            "Iteration 54, loss = 0.60141463\n",
            "Iteration 55, loss = 0.59934202\n",
            "Iteration 56, loss = 0.59817244\n",
            "Iteration 57, loss = 0.59920684\n",
            "Iteration 58, loss = 0.59946454\n",
            "Iteration 59, loss = 0.59941538\n",
            "Iteration 60, loss = 0.59859958\n",
            "Iteration 61, loss = 0.59819561\n",
            "Iteration 62, loss = 0.59943875\n",
            "Iteration 63, loss = 0.59915014\n",
            "Iteration 64, loss = 0.59802143\n",
            "Iteration 65, loss = 0.59720122\n",
            "Iteration 66, loss = 0.59771706\n",
            "Iteration 67, loss = 0.59770859\n",
            "Iteration 68, loss = 0.59799812\n",
            "Iteration 69, loss = 0.59821127\n",
            "Iteration 70, loss = 0.59638817\n",
            "Iteration 71, loss = 0.59824916\n",
            "Iteration 72, loss = 0.59718851\n",
            "Iteration 73, loss = 0.59585269\n",
            "Iteration 74, loss = 0.59625668\n",
            "Iteration 75, loss = 0.59655156\n",
            "Iteration 76, loss = 0.59708294\n",
            "Iteration 77, loss = 0.59650872\n",
            "Iteration 78, loss = 0.59684010\n",
            "Iteration 79, loss = 0.59814152\n",
            "Iteration 80, loss = 0.59560010\n",
            "Iteration 81, loss = 0.59530859\n",
            "Iteration 82, loss = 0.59565975\n",
            "Iteration 83, loss = 0.59568647\n",
            "Iteration 84, loss = 0.59981811\n",
            "Iteration 85, loss = 0.59598828\n",
            "Iteration 86, loss = 0.59468796\n",
            "Iteration 87, loss = 0.59510333\n",
            "Iteration 88, loss = 0.59417025\n",
            "Iteration 89, loss = 0.59610721\n",
            "Iteration 90, loss = 0.59461841\n",
            "Iteration 91, loss = 0.59563246\n",
            "Iteration 92, loss = 0.59374169\n",
            "Iteration 93, loss = 0.59408672\n",
            "Iteration 94, loss = 0.59475343\n",
            "Iteration 95, loss = 0.59583824\n",
            "Iteration 96, loss = 0.59287513\n",
            "Iteration 97, loss = 0.59521349\n",
            "Iteration 98, loss = 0.59273523\n",
            "Iteration 99, loss = 0.59346343\n",
            "Iteration 100, loss = 0.59343512\n",
            "Iteration 101, loss = 0.59238379\n",
            "Iteration 102, loss = 0.59307786\n",
            "Iteration 103, loss = 0.59339216\n",
            "Iteration 104, loss = 0.59319468\n",
            "Iteration 105, loss = 0.59879888\n",
            "Iteration 106, loss = 0.59298976\n",
            "Iteration 107, loss = 0.59252321\n",
            "Iteration 108, loss = 0.59240526\n",
            "Iteration 109, loss = 0.59233413\n",
            "Iteration 110, loss = 0.59538731\n",
            "Iteration 111, loss = 0.59246659\n",
            "Iteration 112, loss = 0.59146733\n",
            "Iteration 113, loss = 0.59159378\n",
            "Iteration 114, loss = 0.59221793\n",
            "Iteration 115, loss = 0.59111238\n",
            "Iteration 116, loss = 0.59211821\n",
            "Iteration 117, loss = 0.59205156\n",
            "Iteration 118, loss = 0.59245914\n",
            "Iteration 119, loss = 0.59353765\n",
            "Iteration 120, loss = 0.59085756\n",
            "Iteration 121, loss = 0.58998877\n",
            "Iteration 122, loss = 0.59138935\n",
            "Iteration 123, loss = 0.59045342\n",
            "Iteration 124, loss = 0.59238764\n",
            "Iteration 125, loss = 0.59049598\n",
            "Iteration 126, loss = 0.59019615\n",
            "Iteration 127, loss = 0.59094741\n",
            "Iteration 128, loss = 0.59013524\n",
            "Iteration 129, loss = 0.59069009\n",
            "Iteration 130, loss = 0.58981287\n",
            "Iteration 131, loss = 0.58982623\n",
            "Iteration 132, loss = 0.59136497\n",
            "Iteration 133, loss = 0.58853171\n",
            "Iteration 134, loss = 0.59278349\n",
            "Iteration 135, loss = 0.58898426\n",
            "Iteration 136, loss = 0.58956471\n",
            "Iteration 137, loss = 0.58923720\n",
            "Iteration 138, loss = 0.58880171\n",
            "Iteration 139, loss = 0.58918838\n",
            "Iteration 140, loss = 0.58923635\n",
            "Iteration 141, loss = 0.58890619\n",
            "Iteration 142, loss = 0.58889132\n",
            "Iteration 143, loss = 0.58850085\n",
            "Iteration 144, loss = 0.58854601\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66554800\n",
            "Iteration 2, loss = 0.66131226\n",
            "Iteration 3, loss = 0.66087506\n",
            "Iteration 4, loss = 0.65632512\n",
            "Iteration 5, loss = 0.65298560\n",
            "Iteration 6, loss = 0.65127573\n",
            "Iteration 7, loss = 0.64708607\n",
            "Iteration 8, loss = 0.64551147\n",
            "Iteration 9, loss = 0.64240752\n",
            "Iteration 10, loss = 0.64082819\n",
            "Iteration 11, loss = 0.63781988\n",
            "Iteration 12, loss = 0.63448815\n",
            "Iteration 13, loss = 0.63263998\n",
            "Iteration 14, loss = 0.63065833\n",
            "Iteration 15, loss = 0.62904898\n",
            "Iteration 16, loss = 0.62677311\n",
            "Iteration 17, loss = 0.62313654\n",
            "Iteration 18, loss = 0.62255619\n",
            "Iteration 19, loss = 0.62198128\n",
            "Iteration 20, loss = 0.62061990\n",
            "Iteration 21, loss = 0.61924386\n",
            "Iteration 22, loss = 0.61724532\n",
            "Iteration 23, loss = 0.61630919\n",
            "Iteration 24, loss = 0.61534391\n",
            "Iteration 25, loss = 0.61560660\n",
            "Iteration 26, loss = 0.61419012\n",
            "Iteration 27, loss = 0.61364979\n",
            "Iteration 28, loss = 0.61274358\n",
            "Iteration 29, loss = 0.61154677\n",
            "Iteration 30, loss = 0.61171747\n",
            "Iteration 31, loss = 0.60970715\n",
            "Iteration 32, loss = 0.61520490\n",
            "Iteration 33, loss = 0.61023168\n",
            "Iteration 34, loss = 0.61053256\n",
            "Iteration 35, loss = 0.61004801\n",
            "Iteration 36, loss = 0.61068828\n",
            "Iteration 37, loss = 0.61038920\n",
            "Iteration 38, loss = 0.60909433\n",
            "Iteration 39, loss = 0.61024769\n",
            "Iteration 40, loss = 0.60945787\n",
            "Iteration 41, loss = 0.60982976\n",
            "Iteration 42, loss = 0.60890312\n",
            "Iteration 43, loss = 0.61052490\n",
            "Iteration 44, loss = 0.60902224\n",
            "Iteration 45, loss = 0.60878619\n",
            "Iteration 46, loss = 0.60544527\n",
            "Iteration 47, loss = 0.60993392\n",
            "Iteration 48, loss = 0.60795750\n",
            "Iteration 49, loss = 0.60832548\n",
            "Iteration 50, loss = 0.60788611\n",
            "Iteration 51, loss = 0.60866156\n",
            "Iteration 52, loss = 0.60870305\n",
            "Iteration 53, loss = 0.60875298\n",
            "Iteration 54, loss = 0.60843025\n",
            "Iteration 55, loss = 0.60926260\n",
            "Iteration 56, loss = 0.60666125\n",
            "Iteration 57, loss = 0.60860079\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.95610137\n",
            "Iteration 2, loss = 0.61750574\n",
            "Iteration 3, loss = 0.62383063\n",
            "Iteration 4, loss = 0.60327457\n",
            "Iteration 5, loss = 0.58357292\n",
            "Iteration 6, loss = 0.56470552\n",
            "Iteration 7, loss = 0.53304277\n",
            "Iteration 8, loss = 0.52828577\n",
            "Iteration 9, loss = 0.54357972\n",
            "Iteration 10, loss = 0.49497303\n",
            "Iteration 11, loss = 0.52259982\n",
            "Iteration 12, loss = 0.47770810\n",
            "Iteration 13, loss = 0.49867889\n",
            "Iteration 14, loss = 0.50725984\n",
            "Iteration 15, loss = 0.47984926\n",
            "Iteration 16, loss = 0.49157363\n",
            "Iteration 17, loss = 0.49224500\n",
            "Iteration 18, loss = 0.45576191\n",
            "Iteration 19, loss = 0.49801060\n",
            "Iteration 20, loss = 0.50932404\n",
            "Iteration 21, loss = 0.45357715\n",
            "Iteration 22, loss = 0.49836281\n",
            "Iteration 23, loss = 0.44166626\n",
            "Iteration 24, loss = 0.45821408\n",
            "Iteration 25, loss = 0.47238076\n",
            "Iteration 26, loss = 0.44429769\n",
            "Iteration 27, loss = 0.50875230\n",
            "Iteration 28, loss = 0.45735924\n",
            "Iteration 29, loss = 0.44397415\n",
            "Iteration 30, loss = 0.44409206\n",
            "Iteration 31, loss = 0.43625656\n",
            "Iteration 32, loss = 0.44443241\n",
            "Iteration 33, loss = 0.43405318\n",
            "Iteration 34, loss = 0.44486302\n",
            "Iteration 35, loss = 0.44926899\n",
            "Iteration 36, loss = 0.42093068\n",
            "Iteration 37, loss = 0.45926119\n",
            "Iteration 38, loss = 0.43343930\n",
            "Iteration 39, loss = 0.43914391\n",
            "Iteration 40, loss = 0.42948288\n",
            "Iteration 41, loss = 0.43887536\n",
            "Iteration 42, loss = 0.43760647\n",
            "Iteration 43, loss = 0.42603389\n",
            "Iteration 44, loss = 0.43729807\n",
            "Iteration 45, loss = 0.47729514\n",
            "Iteration 46, loss = 0.44916070\n",
            "Iteration 47, loss = 0.43537223\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.89540180\n",
            "Iteration 2, loss = 0.61954923\n",
            "Iteration 3, loss = 0.64991065\n",
            "Iteration 4, loss = 0.59009323\n",
            "Iteration 5, loss = 0.59080399\n",
            "Iteration 6, loss = 0.58516803\n",
            "Iteration 7, loss = 0.55484790\n",
            "Iteration 8, loss = 0.54721575\n",
            "Iteration 9, loss = 0.53102882\n",
            "Iteration 10, loss = 0.50779606\n",
            "Iteration 11, loss = 0.50792452\n",
            "Iteration 12, loss = 0.55573216\n",
            "Iteration 13, loss = 0.51240485\n",
            "Iteration 14, loss = 0.49971368\n",
            "Iteration 15, loss = 0.49371118\n",
            "Iteration 16, loss = 0.50051109\n",
            "Iteration 17, loss = 0.47348455\n",
            "Iteration 18, loss = 0.52472428\n",
            "Iteration 19, loss = 0.49753454\n",
            "Iteration 20, loss = 0.46731370\n",
            "Iteration 21, loss = 0.48641864\n",
            "Iteration 22, loss = 0.45181938\n",
            "Iteration 23, loss = 0.46085087\n",
            "Iteration 24, loss = 0.50336887\n",
            "Iteration 25, loss = 0.46354562\n",
            "Iteration 26, loss = 0.46933208\n",
            "Iteration 27, loss = 0.48183686\n",
            "Iteration 28, loss = 0.46602308\n",
            "Iteration 29, loss = 0.45925379\n",
            "Iteration 30, loss = 0.46098839\n",
            "Iteration 31, loss = 0.47441881\n",
            "Iteration 32, loss = 0.50786551\n",
            "Iteration 33, loss = 0.46604284\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.19969725\n",
            "Iteration 2, loss = 0.59206159\n",
            "Iteration 3, loss = 0.66439611\n",
            "Iteration 4, loss = 0.62219054\n",
            "Iteration 5, loss = 0.57470965\n",
            "Iteration 6, loss = 0.62697696\n",
            "Iteration 7, loss = 0.57592497\n",
            "Iteration 8, loss = 0.56204530\n",
            "Iteration 9, loss = 0.62005519\n",
            "Iteration 10, loss = 0.54533685\n",
            "Iteration 11, loss = 0.54479788\n",
            "Iteration 12, loss = 0.57174732\n",
            "Iteration 13, loss = 0.49996810\n",
            "Iteration 14, loss = 0.56753187\n",
            "Iteration 15, loss = 0.53559454\n",
            "Iteration 16, loss = 0.50041729\n",
            "Iteration 17, loss = 0.53259236\n",
            "Iteration 18, loss = 0.49419601\n",
            "Iteration 19, loss = 0.48973265\n",
            "Iteration 20, loss = 0.57446572\n",
            "Iteration 21, loss = 0.53159060\n",
            "Iteration 22, loss = 0.52849736\n",
            "Iteration 23, loss = 0.46748700\n",
            "Iteration 24, loss = 0.50539769\n",
            "Iteration 25, loss = 0.48936995\n",
            "Iteration 26, loss = 0.54261851\n",
            "Iteration 27, loss = 0.49104788\n",
            "Iteration 28, loss = 0.49712433\n",
            "Iteration 29, loss = 0.48996129\n",
            "Iteration 30, loss = 0.48763040\n",
            "Iteration 31, loss = 0.46181157\n",
            "Iteration 32, loss = 0.46125714\n",
            "Iteration 33, loss = 0.45528786\n",
            "Iteration 34, loss = 0.45544243\n",
            "Iteration 35, loss = 0.49073846\n",
            "Iteration 36, loss = 0.46111888\n",
            "Iteration 37, loss = 0.46545437\n",
            "Iteration 38, loss = 0.50505506\n",
            "Iteration 39, loss = 0.48375224\n",
            "Iteration 40, loss = 0.46317008\n",
            "Iteration 41, loss = 0.49753802\n",
            "Iteration 42, loss = 0.44759936\n",
            "Iteration 43, loss = 0.45917675\n",
            "Iteration 44, loss = 0.46312592\n",
            "Iteration 45, loss = 0.45289199\n",
            "Iteration 46, loss = 0.46225811\n",
            "Iteration 47, loss = 0.46021500\n",
            "Iteration 48, loss = 0.47202105\n",
            "Iteration 49, loss = 0.53920131\n",
            "Iteration 50, loss = 0.45934263\n",
            "Iteration 51, loss = 0.49533590\n",
            "Iteration 52, loss = 0.45148662\n",
            "Iteration 53, loss = 0.51145613\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75046349\n",
            "Iteration 2, loss = 0.64947823\n",
            "Iteration 3, loss = 0.63143012\n",
            "Iteration 4, loss = 0.60534298\n",
            "Iteration 5, loss = 0.61632596\n",
            "Iteration 6, loss = 0.65131157\n",
            "Iteration 7, loss = 0.56493515\n",
            "Iteration 8, loss = 0.54038675\n",
            "Iteration 9, loss = 0.53280356\n",
            "Iteration 10, loss = 0.50681532\n",
            "Iteration 11, loss = 0.49481606\n",
            "Iteration 12, loss = 0.51188684\n",
            "Iteration 13, loss = 0.49326906\n",
            "Iteration 14, loss = 0.48130444\n",
            "Iteration 15, loss = 0.47408911\n",
            "Iteration 16, loss = 0.53408424\n",
            "Iteration 17, loss = 0.52320391\n",
            "Iteration 18, loss = 0.48060322\n",
            "Iteration 19, loss = 0.48283286\n",
            "Iteration 20, loss = 0.50597549\n",
            "Iteration 21, loss = 0.46390271\n",
            "Iteration 22, loss = 0.49013096\n",
            "Iteration 23, loss = 0.46024990\n",
            "Iteration 24, loss = 0.45977850\n",
            "Iteration 25, loss = 0.44287836\n",
            "Iteration 26, loss = 0.47902871\n",
            "Iteration 27, loss = 0.45240214\n",
            "Iteration 28, loss = 0.45896787\n",
            "Iteration 29, loss = 0.46208538\n",
            "Iteration 30, loss = 0.45600275\n",
            "Iteration 31, loss = 0.49933255\n",
            "Iteration 32, loss = 0.47241667\n",
            "Iteration 33, loss = 0.51851297\n",
            "Iteration 34, loss = 0.48079065\n",
            "Iteration 35, loss = 0.43515416\n",
            "Iteration 36, loss = 0.44445408\n",
            "Iteration 37, loss = 0.45719437\n",
            "Iteration 38, loss = 0.48318511\n",
            "Iteration 39, loss = 0.45195605\n",
            "Iteration 40, loss = 0.43920688\n",
            "Iteration 41, loss = 0.43931919\n",
            "Iteration 42, loss = 0.42500141\n",
            "Iteration 43, loss = 0.47201658\n",
            "Iteration 44, loss = 0.43263399\n",
            "Iteration 45, loss = 0.44270052\n",
            "Iteration 46, loss = 0.43816906\n",
            "Iteration 47, loss = 0.43733898\n",
            "Iteration 48, loss = 0.46185817\n",
            "Iteration 49, loss = 0.43527084\n",
            "Iteration 50, loss = 0.42592482\n",
            "Iteration 51, loss = 0.45039390\n",
            "Iteration 52, loss = 0.45722242\n",
            "Iteration 53, loss = 0.43935773\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.91588154\n",
            "Iteration 2, loss = 0.77778676\n",
            "Iteration 3, loss = 0.65901974\n",
            "Iteration 4, loss = 0.64770342\n",
            "Iteration 5, loss = 0.67312942\n",
            "Iteration 6, loss = 0.58105384\n",
            "Iteration 7, loss = 0.57725115\n",
            "Iteration 8, loss = 0.57643391\n",
            "Iteration 9, loss = 0.53959255\n",
            "Iteration 10, loss = 0.51343282\n",
            "Iteration 11, loss = 0.52063921\n",
            "Iteration 12, loss = 0.52750104\n",
            "Iteration 13, loss = 0.53433047\n",
            "Iteration 14, loss = 0.55286917\n",
            "Iteration 15, loss = 0.54101959\n",
            "Iteration 16, loss = 0.60219635\n",
            "Iteration 17, loss = 0.52048112\n",
            "Iteration 18, loss = 0.49167911\n",
            "Iteration 19, loss = 0.51107806\n",
            "Iteration 20, loss = 0.55043474\n",
            "Iteration 21, loss = 0.46988575\n",
            "Iteration 22, loss = 0.48067347\n",
            "Iteration 23, loss = 0.50435990\n",
            "Iteration 24, loss = 0.48534381\n",
            "Iteration 25, loss = 0.45765040\n",
            "Iteration 26, loss = 0.57642713\n",
            "Iteration 27, loss = 0.47142749\n",
            "Iteration 28, loss = 0.48388714\n",
            "Iteration 29, loss = 0.45444250\n",
            "Iteration 30, loss = 0.45993856\n",
            "Iteration 31, loss = 0.49155005\n",
            "Iteration 32, loss = 0.47132813\n",
            "Iteration 33, loss = 0.46737271\n",
            "Iteration 34, loss = 0.48392552\n",
            "Iteration 35, loss = 0.49677047\n",
            "Iteration 36, loss = 0.49079730\n",
            "Iteration 37, loss = 0.45122363\n",
            "Iteration 38, loss = 0.45257705\n",
            "Iteration 39, loss = 0.46208589\n",
            "Iteration 40, loss = 0.46799942\n",
            "Iteration 41, loss = 0.44191010\n",
            "Iteration 42, loss = 0.47600080\n",
            "Iteration 43, loss = 0.46075657\n",
            "Iteration 44, loss = 0.47319565\n",
            "Iteration 45, loss = 0.46862594\n",
            "Iteration 46, loss = 0.44126388\n",
            "Iteration 47, loss = 0.45671923\n",
            "Iteration 48, loss = 0.47224482\n",
            "Iteration 49, loss = 0.46402616\n",
            "Iteration 50, loss = 0.44203356\n",
            "Iteration 51, loss = 0.46387601\n",
            "Iteration 52, loss = 0.44035847\n",
            "Iteration 53, loss = 0.51222436\n",
            "Iteration 54, loss = 0.46036239\n",
            "Iteration 55, loss = 0.47415236\n",
            "Iteration 56, loss = 0.47399588\n",
            "Iteration 57, loss = 0.44025492\n",
            "Iteration 58, loss = 0.44148053\n",
            "Iteration 59, loss = 0.44608692\n",
            "Iteration 60, loss = 0.45615314\n",
            "Iteration 61, loss = 0.45710332\n",
            "Iteration 62, loss = 0.42408134\n",
            "Iteration 63, loss = 0.44537962\n",
            "Iteration 64, loss = 0.42257930\n",
            "Iteration 65, loss = 0.46079823\n",
            "Iteration 66, loss = 0.44794544\n",
            "Iteration 67, loss = 0.45298002\n",
            "Iteration 68, loss = 0.44510655\n",
            "Iteration 69, loss = 0.51955977\n",
            "Iteration 70, loss = 0.47362987\n",
            "Iteration 71, loss = 0.42414694\n",
            "Iteration 72, loss = 0.47090765\n",
            "Iteration 73, loss = 0.42978793\n",
            "Iteration 74, loss = 0.44081473\n",
            "Iteration 75, loss = 0.45219429\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.22658927\n",
            "Iteration 2, loss = 0.66422667\n",
            "Iteration 3, loss = 0.65142736\n",
            "Iteration 4, loss = 0.60272220\n",
            "Iteration 5, loss = 0.60597487\n",
            "Iteration 6, loss = 0.61123047\n",
            "Iteration 7, loss = 0.61073719\n",
            "Iteration 8, loss = 0.60582502\n",
            "Iteration 9, loss = 0.58694448\n",
            "Iteration 10, loss = 0.59100449\n",
            "Iteration 11, loss = 0.59081346\n",
            "Iteration 12, loss = 0.57452412\n",
            "Iteration 13, loss = 0.58328416\n",
            "Iteration 14, loss = 0.58571263\n",
            "Iteration 15, loss = 0.57132575\n",
            "Iteration 16, loss = 0.58017442\n",
            "Iteration 17, loss = 0.56511941\n",
            "Iteration 18, loss = 0.56144779\n",
            "Iteration 19, loss = 0.56790072\n",
            "Iteration 20, loss = 0.55605523\n",
            "Iteration 21, loss = 0.55870632\n",
            "Iteration 22, loss = 0.56297928\n",
            "Iteration 23, loss = 0.55081839\n",
            "Iteration 24, loss = 0.57181915\n",
            "Iteration 25, loss = 0.56103714\n",
            "Iteration 26, loss = 0.55522614\n",
            "Iteration 27, loss = 0.54888966\n",
            "Iteration 28, loss = 0.55756771\n",
            "Iteration 29, loss = 0.54132822\n",
            "Iteration 30, loss = 0.54249892\n",
            "Iteration 31, loss = 0.53602217\n",
            "Iteration 32, loss = 0.53734263\n",
            "Iteration 33, loss = 0.56028268\n",
            "Iteration 34, loss = 0.54452211\n",
            "Iteration 35, loss = 0.54432014\n",
            "Iteration 36, loss = 0.52910862\n",
            "Iteration 37, loss = 0.52327918\n",
            "Iteration 38, loss = 0.54171037\n",
            "Iteration 39, loss = 0.53588700\n",
            "Iteration 40, loss = 0.56625451\n",
            "Iteration 41, loss = 0.53643622\n",
            "Iteration 42, loss = 0.52284638\n",
            "Iteration 43, loss = 0.53278884\n",
            "Iteration 44, loss = 0.53113521\n",
            "Iteration 45, loss = 0.51962712\n",
            "Iteration 46, loss = 0.52289734\n",
            "Iteration 47, loss = 0.52509275\n",
            "Iteration 48, loss = 0.54604018\n",
            "Iteration 49, loss = 0.53069837\n",
            "Iteration 50, loss = 0.52706979\n",
            "Iteration 51, loss = 0.52487074\n",
            "Iteration 52, loss = 0.51750153\n",
            "Iteration 53, loss = 0.51041679\n",
            "Iteration 54, loss = 0.50806683\n",
            "Iteration 55, loss = 0.52670119\n",
            "Iteration 56, loss = 0.51573358\n",
            "Iteration 57, loss = 0.51927389\n",
            "Iteration 58, loss = 0.50475263\n",
            "Iteration 59, loss = 0.50267557\n",
            "Iteration 60, loss = 0.51053516\n",
            "Iteration 61, loss = 0.50561923\n",
            "Iteration 62, loss = 0.50381068\n",
            "Iteration 63, loss = 0.50564045\n",
            "Iteration 64, loss = 0.48939789\n",
            "Iteration 65, loss = 0.50883650\n",
            "Iteration 66, loss = 0.50261948\n",
            "Iteration 67, loss = 0.49697928\n",
            "Iteration 68, loss = 0.50915656\n",
            "Iteration 69, loss = 0.49835067\n",
            "Iteration 70, loss = 0.50136421\n",
            "Iteration 71, loss = 0.50312246\n",
            "Iteration 72, loss = 0.49567956\n",
            "Iteration 73, loss = 0.50154376\n",
            "Iteration 74, loss = 0.48886570\n",
            "Iteration 75, loss = 0.52156765\n",
            "Iteration 76, loss = 0.50721333\n",
            "Iteration 77, loss = 0.50962201\n",
            "Iteration 78, loss = 0.53484565\n",
            "Iteration 79, loss = 0.51877861\n",
            "Iteration 80, loss = 0.49343428\n",
            "Iteration 81, loss = 0.48572493\n",
            "Iteration 82, loss = 0.50021367\n",
            "Iteration 83, loss = 0.49694313\n",
            "Iteration 84, loss = 0.51083820\n",
            "Iteration 85, loss = 0.49335514\n",
            "Iteration 86, loss = 0.49186816\n",
            "Iteration 87, loss = 0.47986436\n",
            "Iteration 88, loss = 0.46873934\n",
            "Iteration 89, loss = 0.48077397\n",
            "Iteration 90, loss = 0.49057433\n",
            "Iteration 91, loss = 0.48486021\n",
            "Iteration 92, loss = 0.47122381\n",
            "Iteration 93, loss = 0.48750344\n",
            "Iteration 94, loss = 0.49737745\n",
            "Iteration 95, loss = 0.49014083\n",
            "Iteration 96, loss = 0.49311640\n",
            "Iteration 97, loss = 0.49476491\n",
            "Iteration 98, loss = 0.48030958\n",
            "Iteration 99, loss = 0.47315804\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.92862950\n",
            "Iteration 2, loss = 0.76260473\n",
            "Iteration 3, loss = 0.66327910\n",
            "Iteration 4, loss = 0.66425730\n",
            "Iteration 5, loss = 0.61282546\n",
            "Iteration 6, loss = 0.60531943\n",
            "Iteration 7, loss = 0.59743425\n",
            "Iteration 8, loss = 0.59052739\n",
            "Iteration 9, loss = 0.59760630\n",
            "Iteration 10, loss = 0.58904233\n",
            "Iteration 11, loss = 0.60859203\n",
            "Iteration 12, loss = 0.58545533\n",
            "Iteration 13, loss = 0.58664478\n",
            "Iteration 14, loss = 0.58143810\n",
            "Iteration 15, loss = 0.57561771\n",
            "Iteration 16, loss = 0.57297382\n",
            "Iteration 17, loss = 0.57303235\n",
            "Iteration 18, loss = 0.58050098\n",
            "Iteration 19, loss = 0.57171000\n",
            "Iteration 20, loss = 0.56693262\n",
            "Iteration 21, loss = 0.56927824\n",
            "Iteration 22, loss = 0.56934049\n",
            "Iteration 23, loss = 0.55752329\n",
            "Iteration 24, loss = 0.55055562\n",
            "Iteration 25, loss = 0.56250115\n",
            "Iteration 26, loss = 0.55338859\n",
            "Iteration 27, loss = 0.56638212\n",
            "Iteration 28, loss = 0.55720194\n",
            "Iteration 29, loss = 0.55117426\n",
            "Iteration 30, loss = 0.54832224\n",
            "Iteration 31, loss = 0.54961732\n",
            "Iteration 32, loss = 0.53981227\n",
            "Iteration 33, loss = 0.54285857\n",
            "Iteration 34, loss = 0.54862666\n",
            "Iteration 35, loss = 0.54105877\n",
            "Iteration 36, loss = 0.54861047\n",
            "Iteration 37, loss = 0.54174598\n",
            "Iteration 38, loss = 0.54014502\n",
            "Iteration 39, loss = 0.52862877\n",
            "Iteration 40, loss = 0.53923289\n",
            "Iteration 41, loss = 0.54221917\n",
            "Iteration 42, loss = 0.53357569\n",
            "Iteration 43, loss = 0.52184581\n",
            "Iteration 44, loss = 0.52802819\n",
            "Iteration 45, loss = 0.53958648\n",
            "Iteration 46, loss = 0.51881416\n",
            "Iteration 47, loss = 0.52266510\n",
            "Iteration 48, loss = 0.53687126\n",
            "Iteration 49, loss = 0.52313560\n",
            "Iteration 50, loss = 0.51810044\n",
            "Iteration 51, loss = 0.52996455\n",
            "Iteration 52, loss = 0.51370970\n",
            "Iteration 53, loss = 0.52429254\n",
            "Iteration 54, loss = 0.51595979\n",
            "Iteration 55, loss = 0.49428385\n",
            "Iteration 56, loss = 0.51573001\n",
            "Iteration 57, loss = 0.50380866\n",
            "Iteration 58, loss = 0.51346524\n",
            "Iteration 59, loss = 0.51468635\n",
            "Iteration 60, loss = 0.50260899\n",
            "Iteration 61, loss = 0.49169793\n",
            "Iteration 62, loss = 0.49726920\n",
            "Iteration 63, loss = 0.49626396\n",
            "Iteration 64, loss = 0.51364553\n",
            "Iteration 65, loss = 0.49902640\n",
            "Iteration 66, loss = 0.50820407\n",
            "Iteration 67, loss = 0.50439688\n",
            "Iteration 68, loss = 0.50804969\n",
            "Iteration 69, loss = 0.51460391\n",
            "Iteration 70, loss = 0.50450406\n",
            "Iteration 71, loss = 0.50855275\n",
            "Iteration 72, loss = 0.51021478\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.93070268\n",
            "Iteration 2, loss = 0.69797441\n",
            "Iteration 3, loss = 0.68002426\n",
            "Iteration 4, loss = 0.63956860\n",
            "Iteration 5, loss = 0.63186386\n",
            "Iteration 6, loss = 0.61529904\n",
            "Iteration 7, loss = 0.59851344\n",
            "Iteration 8, loss = 0.60248006\n",
            "Iteration 9, loss = 0.59446249\n",
            "Iteration 10, loss = 0.60210979\n",
            "Iteration 11, loss = 0.59186749\n",
            "Iteration 12, loss = 0.58531976\n",
            "Iteration 13, loss = 0.58360132\n",
            "Iteration 14, loss = 0.58767713\n",
            "Iteration 15, loss = 0.57887619\n",
            "Iteration 16, loss = 0.58464012\n",
            "Iteration 17, loss = 0.57477695\n",
            "Iteration 18, loss = 0.58907161\n",
            "Iteration 19, loss = 0.56443133\n",
            "Iteration 20, loss = 0.57410829\n",
            "Iteration 21, loss = 0.57992154\n",
            "Iteration 22, loss = 0.56861098\n",
            "Iteration 23, loss = 0.57127240\n",
            "Iteration 24, loss = 0.56106132\n",
            "Iteration 25, loss = 0.55581051\n",
            "Iteration 26, loss = 0.55788699\n",
            "Iteration 27, loss = 0.55855895\n",
            "Iteration 28, loss = 0.54923750\n",
            "Iteration 29, loss = 0.55949309\n",
            "Iteration 30, loss = 0.56028086\n",
            "Iteration 31, loss = 0.55694580\n",
            "Iteration 32, loss = 0.54812091\n",
            "Iteration 33, loss = 0.53445375\n",
            "Iteration 34, loss = 0.53891494\n",
            "Iteration 35, loss = 0.53643706\n",
            "Iteration 36, loss = 0.54464403\n",
            "Iteration 37, loss = 0.52469479\n",
            "Iteration 38, loss = 0.52408512\n",
            "Iteration 39, loss = 0.53542411\n",
            "Iteration 40, loss = 0.55131071\n",
            "Iteration 41, loss = 0.53472867\n",
            "Iteration 42, loss = 0.53502803\n",
            "Iteration 43, loss = 0.52867336\n",
            "Iteration 44, loss = 0.52736480\n",
            "Iteration 45, loss = 0.52201364\n",
            "Iteration 46, loss = 0.52400787\n",
            "Iteration 47, loss = 0.52736360\n",
            "Iteration 48, loss = 0.54376664\n",
            "Iteration 49, loss = 0.53121273\n",
            "Iteration 50, loss = 0.52033753\n",
            "Iteration 51, loss = 0.52057301\n",
            "Iteration 52, loss = 0.51813261\n",
            "Iteration 53, loss = 0.52454225\n",
            "Iteration 54, loss = 0.52090522\n",
            "Iteration 55, loss = 0.51102950\n",
            "Iteration 56, loss = 0.50547021\n",
            "Iteration 57, loss = 0.50002546\n",
            "Iteration 58, loss = 0.50702983\n",
            "Iteration 59, loss = 0.51779281\n",
            "Iteration 60, loss = 0.51110999\n",
            "Iteration 61, loss = 0.51889074\n",
            "Iteration 62, loss = 0.49954876\n",
            "Iteration 63, loss = 0.50662702\n",
            "Iteration 64, loss = 0.50345393\n",
            "Iteration 65, loss = 0.52171097\n",
            "Iteration 66, loss = 0.50763938\n",
            "Iteration 67, loss = 0.50569816\n",
            "Iteration 68, loss = 0.51658322\n",
            "Iteration 69, loss = 0.51467287\n",
            "Iteration 70, loss = 0.49261824\n",
            "Iteration 71, loss = 0.50869995\n",
            "Iteration 72, loss = 0.49479080\n",
            "Iteration 73, loss = 0.49762396\n",
            "Iteration 74, loss = 0.49600986\n",
            "Iteration 75, loss = 0.49897417\n",
            "Iteration 76, loss = 0.50724869\n",
            "Iteration 77, loss = 0.50238992\n",
            "Iteration 78, loss = 0.50226239\n",
            "Iteration 79, loss = 0.49966200\n",
            "Iteration 80, loss = 0.49560254\n",
            "Iteration 81, loss = 0.49447121\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.98793706\n",
            "Iteration 2, loss = 0.83245288\n",
            "Iteration 3, loss = 0.60805807\n",
            "Iteration 4, loss = 0.62279301\n",
            "Iteration 5, loss = 0.63139108\n",
            "Iteration 6, loss = 0.65023759\n",
            "Iteration 7, loss = 0.60701149\n",
            "Iteration 8, loss = 0.60428431\n",
            "Iteration 9, loss = 0.59598805\n",
            "Iteration 10, loss = 0.59162718\n",
            "Iteration 11, loss = 0.58528875\n",
            "Iteration 12, loss = 0.59724235\n",
            "Iteration 13, loss = 0.58333821\n",
            "Iteration 14, loss = 0.58036022\n",
            "Iteration 15, loss = 0.57695676\n",
            "Iteration 16, loss = 0.58268076\n",
            "Iteration 17, loss = 0.59208073\n",
            "Iteration 18, loss = 0.58593446\n",
            "Iteration 19, loss = 0.58039501\n",
            "Iteration 20, loss = 0.57968280\n",
            "Iteration 21, loss = 0.56899532\n",
            "Iteration 22, loss = 0.56703983\n",
            "Iteration 23, loss = 0.57574503\n",
            "Iteration 24, loss = 0.56178133\n",
            "Iteration 25, loss = 0.56050766\n",
            "Iteration 26, loss = 0.56623213\n",
            "Iteration 27, loss = 0.55674823\n",
            "Iteration 28, loss = 0.56932828\n",
            "Iteration 29, loss = 0.55803291\n",
            "Iteration 30, loss = 0.56792507\n",
            "Iteration 31, loss = 0.55099803\n",
            "Iteration 32, loss = 0.56288204\n",
            "Iteration 33, loss = 0.54148605\n",
            "Iteration 34, loss = 0.53949123\n",
            "Iteration 35, loss = 0.56027866\n",
            "Iteration 36, loss = 0.53702927\n",
            "Iteration 37, loss = 0.53918127\n",
            "Iteration 38, loss = 0.54538706\n",
            "Iteration 39, loss = 0.52785367\n",
            "Iteration 40, loss = 0.55417468\n",
            "Iteration 41, loss = 0.55203263\n",
            "Iteration 42, loss = 0.52758546\n",
            "Iteration 43, loss = 0.53985864\n",
            "Iteration 44, loss = 0.53032554\n",
            "Iteration 45, loss = 0.53266629\n",
            "Iteration 46, loss = 0.54145989\n",
            "Iteration 47, loss = 0.52842652\n",
            "Iteration 48, loss = 0.53934726\n",
            "Iteration 49, loss = 0.52775664\n",
            "Iteration 50, loss = 0.52133380\n",
            "Iteration 51, loss = 0.52090163\n",
            "Iteration 52, loss = 0.51866458\n",
            "Iteration 53, loss = 0.50281751\n",
            "Iteration 54, loss = 0.53314575\n",
            "Iteration 55, loss = 0.51968174\n",
            "Iteration 56, loss = 0.50833698\n",
            "Iteration 57, loss = 0.52097901\n",
            "Iteration 58, loss = 0.50928797\n",
            "Iteration 59, loss = 0.51274736\n",
            "Iteration 60, loss = 0.49397785\n",
            "Iteration 61, loss = 0.51183589\n",
            "Iteration 62, loss = 0.51111623\n",
            "Iteration 63, loss = 0.51586462\n",
            "Iteration 64, loss = 0.51800636\n",
            "Iteration 65, loss = 0.48744795\n",
            "Iteration 66, loss = 0.50287654\n",
            "Iteration 67, loss = 0.53950918\n",
            "Iteration 68, loss = 0.52993837\n",
            "Iteration 69, loss = 0.51036673\n",
            "Iteration 70, loss = 0.49614036\n",
            "Iteration 71, loss = 0.49952642\n",
            "Iteration 72, loss = 0.51137644\n",
            "Iteration 73, loss = 0.50798576\n",
            "Iteration 74, loss = 0.50072268\n",
            "Iteration 75, loss = 0.52303311\n",
            "Iteration 76, loss = 0.49948488\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31183035\n",
            "Iteration 2, loss = 0.64471989\n",
            "Iteration 3, loss = 0.64067187\n",
            "Iteration 4, loss = 0.60875803\n",
            "Iteration 5, loss = 0.60486036\n",
            "Iteration 6, loss = 0.61916321\n",
            "Iteration 7, loss = 0.60740441\n",
            "Iteration 8, loss = 0.60590041\n",
            "Iteration 9, loss = 0.60347282\n",
            "Iteration 10, loss = 0.60148524\n",
            "Iteration 11, loss = 0.60691333\n",
            "Iteration 12, loss = 0.59795888\n",
            "Iteration 13, loss = 0.58689056\n",
            "Iteration 14, loss = 0.58212007\n",
            "Iteration 15, loss = 0.60009622\n",
            "Iteration 16, loss = 0.58203227\n",
            "Iteration 17, loss = 0.58804322\n",
            "Iteration 18, loss = 0.57935318\n",
            "Iteration 19, loss = 0.57775517\n",
            "Iteration 20, loss = 0.57436777\n",
            "Iteration 21, loss = 0.57414179\n",
            "Iteration 22, loss = 0.58278730\n",
            "Iteration 23, loss = 0.56588268\n",
            "Iteration 24, loss = 0.58713541\n",
            "Iteration 25, loss = 0.57073720\n",
            "Iteration 26, loss = 0.56974642\n",
            "Iteration 27, loss = 0.56021217\n",
            "Iteration 28, loss = 0.55499691\n",
            "Iteration 29, loss = 0.55443144\n",
            "Iteration 30, loss = 0.55011974\n",
            "Iteration 31, loss = 0.54812087\n",
            "Iteration 32, loss = 0.54356072\n",
            "Iteration 33, loss = 0.56454180\n",
            "Iteration 34, loss = 0.54455186\n",
            "Iteration 35, loss = 0.58826303\n",
            "Iteration 36, loss = 0.54704900\n",
            "Iteration 37, loss = 0.53282872\n",
            "Iteration 38, loss = 0.53479698\n",
            "Iteration 39, loss = 0.54847183\n",
            "Iteration 40, loss = 0.54099772\n",
            "Iteration 41, loss = 0.55186052\n",
            "Iteration 42, loss = 0.53202701\n",
            "Iteration 43, loss = 0.54990291\n",
            "Iteration 44, loss = 0.54234526\n",
            "Iteration 45, loss = 0.53133599\n",
            "Iteration 46, loss = 0.53770742\n",
            "Iteration 47, loss = 0.54392814\n",
            "Iteration 48, loss = 0.53139419\n",
            "Iteration 49, loss = 0.54089403\n",
            "Iteration 50, loss = 0.52516442\n",
            "Iteration 51, loss = 0.54708050\n",
            "Iteration 52, loss = 0.51663040\n",
            "Iteration 53, loss = 0.53008987\n",
            "Iteration 54, loss = 0.53514977\n",
            "Iteration 55, loss = 0.51844777\n",
            "Iteration 56, loss = 0.52657160\n",
            "Iteration 57, loss = 0.53403282\n",
            "Iteration 58, loss = 0.51817011\n",
            "Iteration 59, loss = 0.51246197\n",
            "Iteration 60, loss = 0.51555472\n",
            "Iteration 61, loss = 0.51344241\n",
            "Iteration 62, loss = 0.52990827\n",
            "Iteration 63, loss = 0.52107831\n",
            "Iteration 64, loss = 0.52887843\n",
            "Iteration 65, loss = 0.51599519\n",
            "Iteration 66, loss = 0.52540145\n",
            "Iteration 67, loss = 0.51065871\n",
            "Iteration 68, loss = 0.52347991\n",
            "Iteration 69, loss = 0.51045500\n",
            "Iteration 70, loss = 0.50028689\n",
            "Iteration 71, loss = 0.51550341\n",
            "Iteration 72, loss = 0.51368797\n",
            "Iteration 73, loss = 0.51026400\n",
            "Iteration 74, loss = 0.52035139\n",
            "Iteration 75, loss = 0.52563536\n",
            "Iteration 76, loss = 0.50146084\n",
            "Iteration 77, loss = 0.51039459\n",
            "Iteration 78, loss = 0.50716932\n",
            "Iteration 79, loss = 0.50946463\n",
            "Iteration 80, loss = 0.50075540\n",
            "Iteration 81, loss = 0.50573717\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.40019991\n",
            "Iteration 2, loss = 0.72869934\n",
            "Iteration 3, loss = 0.63541306\n",
            "Iteration 4, loss = 0.62357937\n",
            "Iteration 5, loss = 0.61813386\n",
            "Iteration 6, loss = 0.61776506\n",
            "Iteration 7, loss = 0.62120908\n",
            "Iteration 8, loss = 0.60957547\n",
            "Iteration 9, loss = 0.61265461\n",
            "Iteration 10, loss = 0.61299676\n",
            "Iteration 11, loss = 0.61233988\n",
            "Iteration 12, loss = 0.62379274\n",
            "Iteration 13, loss = 0.61286338\n",
            "Iteration 14, loss = 0.61191308\n",
            "Iteration 15, loss = 0.60710385\n",
            "Iteration 16, loss = 0.60451536\n",
            "Iteration 17, loss = 0.61014512\n",
            "Iteration 18, loss = 0.60013871\n",
            "Iteration 19, loss = 0.60082982\n",
            "Iteration 20, loss = 0.63326282\n",
            "Iteration 21, loss = 0.60233764\n",
            "Iteration 22, loss = 0.59801227\n",
            "Iteration 23, loss = 0.66885559\n",
            "Iteration 24, loss = 0.60512522\n",
            "Iteration 25, loss = 0.61206011\n",
            "Iteration 26, loss = 0.60530559\n",
            "Iteration 27, loss = 0.60315941\n",
            "Iteration 28, loss = 0.59788202\n",
            "Iteration 29, loss = 0.67459130\n",
            "Iteration 30, loss = 0.60337504\n",
            "Iteration 31, loss = 0.59664618\n",
            "Iteration 32, loss = 0.61172967\n",
            "Iteration 33, loss = 0.59980961\n",
            "Iteration 34, loss = 0.59735048\n",
            "Iteration 35, loss = 0.59418856\n",
            "Iteration 36, loss = 0.61606204\n",
            "Iteration 37, loss = 0.59907966\n",
            "Iteration 38, loss = 0.59758572\n",
            "Iteration 39, loss = 0.59888224\n",
            "Iteration 40, loss = 0.59491537\n",
            "Iteration 41, loss = 0.60073891\n",
            "Iteration 42, loss = 0.59869612\n",
            "Iteration 43, loss = 0.59587457\n",
            "Iteration 44, loss = 0.59373495\n",
            "Iteration 45, loss = 0.60482568\n",
            "Iteration 46, loss = 0.59905153\n",
            "Iteration 47, loss = 0.60479037\n",
            "Iteration 48, loss = 0.60142291\n",
            "Iteration 49, loss = 0.59480877\n",
            "Iteration 50, loss = 0.59424713\n",
            "Iteration 51, loss = 0.59623721\n",
            "Iteration 52, loss = 0.59495349\n",
            "Iteration 53, loss = 0.63536922\n",
            "Iteration 54, loss = 0.63365856\n",
            "Iteration 55, loss = 0.59783331\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.00692372\n",
            "Iteration 2, loss = 0.70309398\n",
            "Iteration 3, loss = 0.62763307\n",
            "Iteration 4, loss = 0.62154087\n",
            "Iteration 5, loss = 0.62730114\n",
            "Iteration 6, loss = 0.61455151\n",
            "Iteration 7, loss = 0.68222215\n",
            "Iteration 8, loss = 0.61683271\n",
            "Iteration 9, loss = 0.61924372\n",
            "Iteration 10, loss = 0.62423051\n",
            "Iteration 11, loss = 0.61179801\n",
            "Iteration 12, loss = 0.64855632\n",
            "Iteration 13, loss = 0.60148796\n",
            "Iteration 14, loss = 0.59798394\n",
            "Iteration 15, loss = 0.60397553\n",
            "Iteration 16, loss = 0.60426231\n",
            "Iteration 17, loss = 0.60380332\n",
            "Iteration 18, loss = 0.60424310\n",
            "Iteration 19, loss = 0.61195356\n",
            "Iteration 20, loss = 0.58392594\n",
            "Iteration 21, loss = 0.58408492\n",
            "Iteration 22, loss = 0.58674738\n",
            "Iteration 23, loss = 0.58108908\n",
            "Iteration 24, loss = 0.63744222\n",
            "Iteration 25, loss = 0.57979217\n",
            "Iteration 26, loss = 0.57825325\n",
            "Iteration 27, loss = 0.60294426\n",
            "Iteration 28, loss = 0.58915143\n",
            "Iteration 29, loss = 0.57573945\n",
            "Iteration 30, loss = 0.57744008\n",
            "Iteration 31, loss = 0.58530465\n",
            "Iteration 32, loss = 0.59054150\n",
            "Iteration 33, loss = 0.58549650\n",
            "Iteration 34, loss = 0.57918512\n",
            "Iteration 35, loss = 0.58274853\n",
            "Iteration 36, loss = 0.57299433\n",
            "Iteration 37, loss = 0.57088859\n",
            "Iteration 38, loss = 0.57990616\n",
            "Iteration 39, loss = 0.56760075\n",
            "Iteration 40, loss = 0.56275898\n",
            "Iteration 41, loss = 0.56999166\n",
            "Iteration 42, loss = 0.56511617\n",
            "Iteration 43, loss = 0.56736817\n",
            "Iteration 44, loss = 0.57702358\n",
            "Iteration 45, loss = 0.57694822\n",
            "Iteration 46, loss = 0.55997358\n",
            "Iteration 47, loss = 0.56040294\n",
            "Iteration 48, loss = 0.57419761\n",
            "Iteration 49, loss = 0.56200830\n",
            "Iteration 50, loss = 0.55916327\n",
            "Iteration 51, loss = 0.55218586\n",
            "Iteration 52, loss = 0.55573087\n",
            "Iteration 53, loss = 0.56644003\n",
            "Iteration 54, loss = 0.56332031\n",
            "Iteration 55, loss = 0.55551855\n",
            "Iteration 56, loss = 0.55392964\n",
            "Iteration 57, loss = 0.55083417\n",
            "Iteration 58, loss = 0.55856374\n",
            "Iteration 59, loss = 0.55660828\n",
            "Iteration 60, loss = 0.55423717\n",
            "Iteration 61, loss = 0.60471056\n",
            "Iteration 62, loss = 0.55406174\n",
            "Iteration 63, loss = 0.56536189\n",
            "Iteration 64, loss = 0.69250299\n",
            "Iteration 65, loss = 0.58169055\n",
            "Iteration 66, loss = 0.73911228\n",
            "Iteration 67, loss = 0.58609710\n",
            "Iteration 68, loss = 0.56845503\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.73853347\n",
            "Iteration 2, loss = 0.84172343\n",
            "Iteration 3, loss = 0.73432701\n",
            "Iteration 4, loss = 0.65510414\n",
            "Iteration 5, loss = 0.70912396\n",
            "Iteration 6, loss = 0.64890898\n",
            "Iteration 7, loss = 0.64639262\n",
            "Iteration 8, loss = 0.63833514\n",
            "Iteration 9, loss = 0.61942097\n",
            "Iteration 10, loss = 0.62982338\n",
            "Iteration 11, loss = 0.66601473\n",
            "Iteration 12, loss = 0.61932872\n",
            "Iteration 13, loss = 0.67083260\n",
            "Iteration 14, loss = 0.61613138\n",
            "Iteration 15, loss = 0.63104896\n",
            "Iteration 16, loss = 0.60276356\n",
            "Iteration 17, loss = 0.60203021\n",
            "Iteration 18, loss = 0.61509096\n",
            "Iteration 19, loss = 0.60024708\n",
            "Iteration 20, loss = 0.59683049\n",
            "Iteration 21, loss = 0.59466830\n",
            "Iteration 22, loss = 0.60294296\n",
            "Iteration 23, loss = 0.60701288\n",
            "Iteration 24, loss = 0.66128784\n",
            "Iteration 25, loss = 0.59079662\n",
            "Iteration 26, loss = 0.58736169\n",
            "Iteration 27, loss = 0.59386265\n",
            "Iteration 28, loss = 0.58804224\n",
            "Iteration 29, loss = 0.59198429\n",
            "Iteration 30, loss = 0.59283406\n",
            "Iteration 31, loss = 0.58659034\n",
            "Iteration 32, loss = 0.59078535\n",
            "Iteration 33, loss = 0.59389998\n",
            "Iteration 34, loss = 0.58340942\n",
            "Iteration 35, loss = 0.59068545\n",
            "Iteration 36, loss = 0.58244266\n",
            "Iteration 37, loss = 0.57532508\n",
            "Iteration 38, loss = 0.58656613\n",
            "Iteration 39, loss = 0.58502230\n",
            "Iteration 40, loss = 0.57575872\n",
            "Iteration 41, loss = 0.57579681\n",
            "Iteration 42, loss = 0.58449917\n",
            "Iteration 43, loss = 0.57828333\n",
            "Iteration 44, loss = 0.57288023\n",
            "Iteration 45, loss = 0.58339083\n",
            "Iteration 46, loss = 0.56540804\n",
            "Iteration 47, loss = 0.57216280\n",
            "Iteration 48, loss = 0.57763243\n",
            "Iteration 49, loss = 0.56487634\n",
            "Iteration 50, loss = 0.58726914\n",
            "Iteration 51, loss = 0.57502482\n",
            "Iteration 52, loss = 0.57117415\n",
            "Iteration 53, loss = 0.68092077\n",
            "Iteration 54, loss = 0.59818114\n",
            "Iteration 55, loss = 0.57551054\n",
            "Iteration 56, loss = 0.57065549\n",
            "Iteration 57, loss = 0.60157372\n",
            "Iteration 58, loss = 0.57378587\n",
            "Iteration 59, loss = 0.57104486\n",
            "Iteration 60, loss = 0.57050102\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.90819758\n",
            "Iteration 2, loss = 0.87793153\n",
            "Iteration 3, loss = 0.63593251\n",
            "Iteration 4, loss = 0.65103664\n",
            "Iteration 5, loss = 0.61860271\n",
            "Iteration 6, loss = 0.60856603\n",
            "Iteration 7, loss = 0.61753413\n",
            "Iteration 8, loss = 0.61420194\n",
            "Iteration 9, loss = 0.60389017\n",
            "Iteration 10, loss = 0.60532783\n",
            "Iteration 11, loss = 0.60288693\n",
            "Iteration 12, loss = 0.60833474\n",
            "Iteration 13, loss = 0.61381593\n",
            "Iteration 14, loss = 0.63168214\n",
            "Iteration 15, loss = 0.69007983\n",
            "Iteration 16, loss = 0.70295098\n",
            "Iteration 17, loss = 0.63136861\n",
            "Iteration 18, loss = 0.60544977\n",
            "Iteration 19, loss = 0.59606592\n",
            "Iteration 20, loss = 0.59581213\n",
            "Iteration 21, loss = 0.59489423\n",
            "Iteration 22, loss = 0.59947828\n",
            "Iteration 23, loss = 0.59461470\n",
            "Iteration 24, loss = 0.59195468\n",
            "Iteration 25, loss = 0.59426457\n",
            "Iteration 26, loss = 0.59026126\n",
            "Iteration 27, loss = 0.59703816\n",
            "Iteration 28, loss = 0.59354650\n",
            "Iteration 29, loss = 0.60129814\n",
            "Iteration 30, loss = 0.59300172\n",
            "Iteration 31, loss = 0.58714938\n",
            "Iteration 32, loss = 0.58913919\n",
            "Iteration 33, loss = 0.59186193\n",
            "Iteration 34, loss = 0.58816427\n",
            "Iteration 35, loss = 0.59366963\n",
            "Iteration 36, loss = 0.59623664\n",
            "Iteration 37, loss = 0.58455955\n",
            "Iteration 38, loss = 0.59334984\n",
            "Iteration 39, loss = 0.65890731\n",
            "Iteration 40, loss = 0.60562705\n",
            "Iteration 41, loss = 0.62704275\n",
            "Iteration 42, loss = 0.58743919\n",
            "Iteration 43, loss = 0.58265857\n",
            "Iteration 44, loss = 0.58965766\n",
            "Iteration 45, loss = 0.58378046\n",
            "Iteration 46, loss = 0.58217476\n",
            "Iteration 47, loss = 0.58057018\n",
            "Iteration 48, loss = 0.58526325\n",
            "Iteration 49, loss = 0.59714301\n",
            "Iteration 50, loss = 0.59045440\n",
            "Iteration 51, loss = 0.58193095\n",
            "Iteration 52, loss = 0.58163565\n",
            "Iteration 53, loss = 0.57585520\n",
            "Iteration 54, loss = 0.58348379\n",
            "Iteration 55, loss = 0.58653753\n",
            "Iteration 56, loss = 0.59597287\n",
            "Iteration 57, loss = 0.58317586\n",
            "Iteration 58, loss = 0.57488811\n",
            "Iteration 59, loss = 0.60876914\n",
            "Iteration 60, loss = 0.58066036\n",
            "Iteration 61, loss = 0.57728123\n",
            "Iteration 62, loss = 0.58365909\n",
            "Iteration 63, loss = 0.57402647\n",
            "Iteration 64, loss = 0.57829768\n",
            "Iteration 65, loss = 0.57110758\n",
            "Iteration 66, loss = 0.58317780\n",
            "Iteration 67, loss = 0.57857966\n",
            "Iteration 68, loss = 0.57716937\n",
            "Iteration 69, loss = 0.57070577\n",
            "Iteration 70, loss = 0.57179233\n",
            "Iteration 71, loss = 0.57391650\n",
            "Iteration 72, loss = 0.57225906\n",
            "Iteration 73, loss = 0.61294226\n",
            "Iteration 74, loss = 0.57812019\n",
            "Iteration 75, loss = 0.57498191\n",
            "Iteration 76, loss = 0.57600186\n",
            "Iteration 77, loss = 0.56663842\n",
            "Iteration 78, loss = 0.56584716\n",
            "Iteration 79, loss = 0.56858492\n",
            "Iteration 80, loss = 0.56516465\n",
            "Iteration 81, loss = 0.56695149\n",
            "Iteration 82, loss = 0.56722389\n",
            "Iteration 83, loss = 0.57384969\n",
            "Iteration 84, loss = 0.57408721\n",
            "Iteration 85, loss = 0.56716662\n",
            "Iteration 86, loss = 0.55527579\n",
            "Iteration 87, loss = 0.56498628\n",
            "Iteration 88, loss = 0.57367048\n",
            "Iteration 89, loss = 0.55744541\n",
            "Iteration 90, loss = 0.55760558\n",
            "Iteration 91, loss = 0.55723013\n",
            "Iteration 92, loss = 0.55731353\n",
            "Iteration 93, loss = 0.55274891\n",
            "Iteration 94, loss = 0.57588318\n",
            "Iteration 95, loss = 0.55848460\n",
            "Iteration 96, loss = 0.55699398\n",
            "Iteration 97, loss = 0.55545943\n",
            "Iteration 98, loss = 0.55847166\n",
            "Iteration 99, loss = 0.55862787\n",
            "Iteration 100, loss = 0.56206243\n",
            "Iteration 101, loss = 0.55640524\n",
            "Iteration 102, loss = 0.56546073\n",
            "Iteration 103, loss = 0.54940457\n",
            "Iteration 104, loss = 0.54568210\n",
            "Iteration 105, loss = 0.55019662\n",
            "Iteration 106, loss = 0.56602230\n",
            "Iteration 107, loss = 0.54231105\n",
            "Iteration 108, loss = 0.54801494\n",
            "Iteration 109, loss = 0.55793594\n",
            "Iteration 110, loss = 0.54682928\n",
            "Iteration 111, loss = 0.54233786\n",
            "Iteration 112, loss = 0.56412648\n",
            "Iteration 113, loss = 0.54295128\n",
            "Iteration 114, loss = 0.54498190\n",
            "Iteration 115, loss = 0.54118294\n",
            "Iteration 116, loss = 0.53955913\n",
            "Iteration 117, loss = 0.53731166\n",
            "Iteration 118, loss = 0.53803063\n",
            "Iteration 119, loss = 0.53856602\n",
            "Iteration 120, loss = 0.54145207\n",
            "Iteration 121, loss = 0.53587642\n",
            "Iteration 122, loss = 0.55291894\n",
            "Iteration 123, loss = 0.55167464\n",
            "Iteration 124, loss = 0.53682637\n",
            "Iteration 125, loss = 0.53670366\n",
            "Iteration 126, loss = 0.53379386\n",
            "Iteration 127, loss = 0.54492683\n",
            "Iteration 128, loss = 0.53182431\n",
            "Iteration 129, loss = 0.53439434\n",
            "Iteration 130, loss = 0.53318991\n",
            "Iteration 131, loss = 0.52669849\n",
            "Iteration 132, loss = 0.52577940\n",
            "Iteration 133, loss = 0.53859981\n",
            "Iteration 134, loss = 0.53009610\n",
            "Iteration 135, loss = 0.55007847\n",
            "Iteration 136, loss = 0.53866767\n",
            "Iteration 137, loss = 0.52081425\n",
            "Iteration 138, loss = 0.57639725\n",
            "Iteration 139, loss = 0.53459425\n",
            "Iteration 140, loss = 0.53046239\n",
            "Iteration 141, loss = 0.52930481\n",
            "Iteration 142, loss = 0.52015358\n",
            "Iteration 143, loss = 0.54520534\n",
            "Iteration 144, loss = 0.51556716\n",
            "Iteration 145, loss = 0.51527849\n",
            "Iteration 146, loss = 0.52457357\n",
            "Iteration 147, loss = 0.53523061\n",
            "Iteration 148, loss = 0.53964531\n",
            "Iteration 149, loss = 0.51272882\n",
            "Iteration 150, loss = 0.51424409\n",
            "Iteration 151, loss = 0.51819128\n",
            "Iteration 152, loss = 0.52496960\n",
            "Iteration 153, loss = 0.51602247\n",
            "Iteration 154, loss = 0.51624231\n",
            "Iteration 155, loss = 0.55635134\n",
            "Iteration 156, loss = 0.54766617\n",
            "Iteration 157, loss = 0.52565975\n",
            "Iteration 158, loss = 0.51633734\n",
            "Iteration 159, loss = 0.50537322\n",
            "Iteration 160, loss = 0.51512452\n",
            "Iteration 161, loss = 0.51207210\n",
            "Iteration 162, loss = 0.51711385\n",
            "Iteration 163, loss = 0.49667863\n",
            "Iteration 164, loss = 0.56917048\n",
            "Iteration 165, loss = 0.52586861\n",
            "Iteration 166, loss = 0.51151111\n",
            "Iteration 167, loss = 0.52337511\n",
            "Iteration 168, loss = 0.51311394\n",
            "Iteration 169, loss = 0.52082965\n",
            "Iteration 170, loss = 0.50303078\n",
            "Iteration 171, loss = 0.50541535\n",
            "Iteration 172, loss = 0.53636031\n",
            "Iteration 173, loss = 0.60644538\n",
            "Iteration 174, loss = 0.54715572\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.21170932\n",
            "Iteration 2, loss = 0.79976318\n",
            "Iteration 3, loss = 0.66050195\n",
            "Iteration 4, loss = 0.63161856\n",
            "Iteration 5, loss = 0.63623570\n",
            "Iteration 6, loss = 0.62073469\n",
            "Iteration 7, loss = 0.62738032\n",
            "Iteration 8, loss = 0.71985843\n",
            "Iteration 9, loss = 0.68603025\n",
            "Iteration 10, loss = 0.61085010\n",
            "Iteration 11, loss = 0.61714694\n",
            "Iteration 12, loss = 0.61131119\n",
            "Iteration 13, loss = 0.61341393\n",
            "Iteration 14, loss = 0.61119459\n",
            "Iteration 15, loss = 0.60912738\n",
            "Iteration 16, loss = 0.60866294\n",
            "Iteration 17, loss = 0.60142616\n",
            "Iteration 18, loss = 0.60477106\n",
            "Iteration 19, loss = 0.59858812\n",
            "Iteration 20, loss = 0.60934910\n",
            "Iteration 21, loss = 0.60369446\n",
            "Iteration 22, loss = 0.60573110\n",
            "Iteration 23, loss = 0.60524607\n",
            "Iteration 24, loss = 0.62756845\n",
            "Iteration 25, loss = 0.60337013\n",
            "Iteration 26, loss = 0.60075804\n",
            "Iteration 27, loss = 0.60711788\n",
            "Iteration 28, loss = 0.59589600\n",
            "Iteration 29, loss = 0.59648234\n",
            "Iteration 30, loss = 0.59265799\n",
            "Iteration 31, loss = 0.59200831\n",
            "Iteration 32, loss = 0.66969490\n",
            "Iteration 33, loss = 0.59671745\n",
            "Iteration 34, loss = 0.59376441\n",
            "Iteration 35, loss = 0.70485333\n",
            "Iteration 36, loss = 0.60318826\n",
            "Iteration 37, loss = 0.58999209\n",
            "Iteration 38, loss = 0.60650379\n",
            "Iteration 39, loss = 0.58338945\n",
            "Iteration 40, loss = 0.59604158\n",
            "Iteration 41, loss = 0.58029500\n",
            "Iteration 42, loss = 0.58140453\n",
            "Iteration 43, loss = 0.59278984\n",
            "Iteration 44, loss = 0.58159950\n",
            "Iteration 45, loss = 0.59447119\n",
            "Iteration 46, loss = 0.57696361\n",
            "Iteration 47, loss = 0.57781916\n",
            "Iteration 48, loss = 0.57424945\n",
            "Iteration 49, loss = 0.58433113\n",
            "Iteration 50, loss = 0.57584926\n",
            "Iteration 51, loss = 0.58113357\n",
            "Iteration 52, loss = 0.57225497\n",
            "Iteration 53, loss = 0.57050561\n",
            "Iteration 54, loss = 0.61529492\n",
            "Iteration 55, loss = 0.57683281\n",
            "Iteration 56, loss = 0.57062504\n",
            "Iteration 57, loss = 0.56540525\n",
            "Iteration 58, loss = 0.61036649\n",
            "Iteration 59, loss = 0.56820285\n",
            "Iteration 60, loss = 0.56429749\n",
            "Iteration 61, loss = 0.57705620\n",
            "Iteration 62, loss = 0.57236144\n",
            "Iteration 63, loss = 0.56913657\n",
            "Iteration 64, loss = 0.56503516\n",
            "Iteration 65, loss = 0.59036093\n",
            "Iteration 66, loss = 0.56595350\n",
            "Iteration 67, loss = 0.56501556\n",
            "Iteration 68, loss = 0.59207105\n",
            "Iteration 69, loss = 0.56237726\n",
            "Iteration 70, loss = 0.57122565\n",
            "Iteration 71, loss = 0.65194588\n",
            "Iteration 72, loss = 0.56975537\n",
            "Iteration 73, loss = 0.55576209\n",
            "Iteration 74, loss = 0.56840862\n",
            "Iteration 75, loss = 0.55131096\n",
            "Iteration 76, loss = 0.55698756\n",
            "Iteration 77, loss = 0.54732123\n",
            "Iteration 78, loss = 0.55060010\n",
            "Iteration 79, loss = 0.62548116\n",
            "Iteration 80, loss = 0.54830188\n",
            "Iteration 81, loss = 0.55794541\n",
            "Iteration 82, loss = 0.54557473\n",
            "Iteration 83, loss = 0.56272221\n",
            "Iteration 84, loss = 0.56062571\n",
            "Iteration 85, loss = 0.56000203\n",
            "Iteration 86, loss = 0.54781541\n",
            "Iteration 87, loss = 0.54881800\n",
            "Iteration 88, loss = 0.54313024\n",
            "Iteration 89, loss = 0.55560477\n",
            "Iteration 90, loss = 0.57080224\n",
            "Iteration 91, loss = 0.54528074\n",
            "Iteration 92, loss = 0.54371128\n",
            "Iteration 93, loss = 0.59053769\n",
            "Iteration 94, loss = 0.54750901\n",
            "Iteration 95, loss = 0.59659998\n",
            "Iteration 96, loss = 0.54122448\n",
            "Iteration 97, loss = 0.53458627\n",
            "Iteration 98, loss = 0.53751834\n",
            "Iteration 99, loss = 0.54081388\n",
            "Iteration 100, loss = 0.54938438\n",
            "Iteration 101, loss = 0.53590338\n",
            "Iteration 102, loss = 0.54067876\n",
            "Iteration 103, loss = 0.52975275\n",
            "Iteration 104, loss = 0.53005439\n",
            "Iteration 105, loss = 0.55549029\n",
            "Iteration 106, loss = 0.57309451\n",
            "Iteration 107, loss = 0.53408901\n",
            "Iteration 108, loss = 0.53067050\n",
            "Iteration 109, loss = 0.53371054\n",
            "Iteration 110, loss = 0.52430921\n",
            "Iteration 111, loss = 0.53469078\n",
            "Iteration 112, loss = 0.52947966\n",
            "Iteration 113, loss = 0.52029379\n",
            "Iteration 114, loss = 0.53347011\n",
            "Iteration 115, loss = 0.52232902\n",
            "Iteration 116, loss = 0.51973529\n",
            "Iteration 117, loss = 0.56468298\n",
            "Iteration 118, loss = 0.52339878\n",
            "Iteration 119, loss = 0.54926688\n",
            "Iteration 120, loss = 0.53041129\n",
            "Iteration 121, loss = 0.52985701\n",
            "Iteration 122, loss = 0.55008632\n",
            "Iteration 123, loss = 0.54030823\n",
            "Iteration 124, loss = 0.53997298\n",
            "Iteration 125, loss = 0.52135118\n",
            "Iteration 126, loss = 0.51657569\n",
            "Iteration 127, loss = 0.52522160\n",
            "Iteration 128, loss = 0.51979916\n",
            "Iteration 129, loss = 0.52389060\n",
            "Iteration 130, loss = 0.51713366\n",
            "Iteration 131, loss = 0.52951170\n",
            "Iteration 132, loss = 0.63141868\n",
            "Iteration 133, loss = 0.54193166\n",
            "Iteration 134, loss = 0.53336754\n",
            "Iteration 135, loss = 0.55778339\n",
            "Iteration 136, loss = 0.52989625\n",
            "Iteration 137, loss = 0.53953395\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64995685\n",
            "Iteration 2, loss = 0.60922619\n",
            "Iteration 3, loss = 0.59666373\n",
            "Iteration 4, loss = 0.59930675\n",
            "Iteration 5, loss = 0.59309238\n",
            "Iteration 6, loss = 0.58284162\n",
            "Iteration 7, loss = 0.57148389\n",
            "Iteration 8, loss = 0.55991458\n",
            "Iteration 9, loss = 0.53994122\n",
            "Iteration 10, loss = 0.52370887\n",
            "Iteration 11, loss = 0.50025724\n",
            "Iteration 12, loss = 0.48594931\n",
            "Iteration 13, loss = 0.48623777\n",
            "Iteration 14, loss = 0.47589730\n",
            "Iteration 15, loss = 0.45629019\n",
            "Iteration 16, loss = 0.46288316\n",
            "Iteration 17, loss = 0.46118322\n",
            "Iteration 18, loss = 0.45258524\n",
            "Iteration 19, loss = 0.45760254\n",
            "Iteration 20, loss = 0.44539690\n",
            "Iteration 21, loss = 0.45361560\n",
            "Iteration 22, loss = 0.44107643\n",
            "Iteration 23, loss = 0.44456736\n",
            "Iteration 24, loss = 0.44142753\n",
            "Iteration 25, loss = 0.44119537\n",
            "Iteration 26, loss = 0.45840122\n",
            "Iteration 27, loss = 0.43987175\n",
            "Iteration 28, loss = 0.44203151\n",
            "Iteration 29, loss = 0.43709479\n",
            "Iteration 30, loss = 0.44021819\n",
            "Iteration 31, loss = 0.44821965\n",
            "Iteration 32, loss = 0.44148440\n",
            "Iteration 33, loss = 0.43919225\n",
            "Iteration 34, loss = 0.43699883\n",
            "Iteration 35, loss = 0.44026043\n",
            "Iteration 36, loss = 0.44063139\n",
            "Iteration 37, loss = 0.43348338\n",
            "Iteration 38, loss = 0.44145202\n",
            "Iteration 39, loss = 0.44670147\n",
            "Iteration 40, loss = 0.43634935\n",
            "Iteration 41, loss = 0.43294765\n",
            "Iteration 42, loss = 0.43509095\n",
            "Iteration 43, loss = 0.44818303\n",
            "Iteration 44, loss = 0.43597080\n",
            "Iteration 45, loss = 0.42688496\n",
            "Iteration 46, loss = 0.43255038\n",
            "Iteration 47, loss = 0.43675935\n",
            "Iteration 48, loss = 0.43408905\n",
            "Iteration 49, loss = 0.42880970\n",
            "Iteration 50, loss = 0.42695049\n",
            "Iteration 51, loss = 0.42102597\n",
            "Iteration 52, loss = 0.44178032\n",
            "Iteration 53, loss = 0.43020561\n",
            "Iteration 54, loss = 0.42477872\n",
            "Iteration 55, loss = 0.42558516\n",
            "Iteration 56, loss = 0.43147062\n",
            "Iteration 57, loss = 0.42823141\n",
            "Iteration 58, loss = 0.42342822\n",
            "Iteration 59, loss = 0.42540699\n",
            "Iteration 60, loss = 0.42053194\n",
            "Iteration 61, loss = 0.42282905\n",
            "Iteration 62, loss = 0.42529521\n",
            "Iteration 63, loss = 0.42099518\n",
            "Iteration 64, loss = 0.42348894\n",
            "Iteration 65, loss = 0.42512732\n",
            "Iteration 66, loss = 0.42300435\n",
            "Iteration 67, loss = 0.42170888\n",
            "Iteration 68, loss = 0.42919390\n",
            "Iteration 69, loss = 0.44019005\n",
            "Iteration 70, loss = 0.42716706\n",
            "Iteration 71, loss = 0.41158823\n",
            "Iteration 72, loss = 0.41430408\n",
            "Iteration 73, loss = 0.42484173\n",
            "Iteration 74, loss = 0.42338860\n",
            "Iteration 75, loss = 0.41369527\n",
            "Iteration 76, loss = 0.43284476\n",
            "Iteration 77, loss = 0.42279048\n",
            "Iteration 78, loss = 0.41314167\n",
            "Iteration 79, loss = 0.41889249\n",
            "Iteration 80, loss = 0.41286108\n",
            "Iteration 81, loss = 0.41462793\n",
            "Iteration 82, loss = 0.41856898\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62821730\n",
            "Iteration 2, loss = 0.61049683\n",
            "Iteration 3, loss = 0.60198041\n",
            "Iteration 4, loss = 0.60102008\n",
            "Iteration 5, loss = 0.58816453\n",
            "Iteration 6, loss = 0.58645644\n",
            "Iteration 7, loss = 0.57331981\n",
            "Iteration 8, loss = 0.55426022\n",
            "Iteration 9, loss = 0.54185360\n",
            "Iteration 10, loss = 0.52120785\n",
            "Iteration 11, loss = 0.50571387\n",
            "Iteration 12, loss = 0.48335125\n",
            "Iteration 13, loss = 0.47152747\n",
            "Iteration 14, loss = 0.47665245\n",
            "Iteration 15, loss = 0.46485505\n",
            "Iteration 16, loss = 0.46095712\n",
            "Iteration 17, loss = 0.46234480\n",
            "Iteration 18, loss = 0.45776844\n",
            "Iteration 19, loss = 0.45033028\n",
            "Iteration 20, loss = 0.45293465\n",
            "Iteration 21, loss = 0.46326259\n",
            "Iteration 22, loss = 0.44351311\n",
            "Iteration 23, loss = 0.44139707\n",
            "Iteration 24, loss = 0.44922924\n",
            "Iteration 25, loss = 0.44467719\n",
            "Iteration 26, loss = 0.44325607\n",
            "Iteration 27, loss = 0.44413208\n",
            "Iteration 28, loss = 0.46327649\n",
            "Iteration 29, loss = 0.43661888\n",
            "Iteration 30, loss = 0.43824835\n",
            "Iteration 31, loss = 0.43806771\n",
            "Iteration 32, loss = 0.44026784\n",
            "Iteration 33, loss = 0.44837859\n",
            "Iteration 34, loss = 0.43429834\n",
            "Iteration 35, loss = 0.43770806\n",
            "Iteration 36, loss = 0.43141806\n",
            "Iteration 37, loss = 0.42729254\n",
            "Iteration 38, loss = 0.43660700\n",
            "Iteration 39, loss = 0.42623552\n",
            "Iteration 40, loss = 0.44514440\n",
            "Iteration 41, loss = 0.43172575\n",
            "Iteration 42, loss = 0.43215762\n",
            "Iteration 43, loss = 0.44023537\n",
            "Iteration 44, loss = 0.43109162\n",
            "Iteration 45, loss = 0.42550396\n",
            "Iteration 46, loss = 0.42439440\n",
            "Iteration 47, loss = 0.42967617\n",
            "Iteration 48, loss = 0.42153604\n",
            "Iteration 49, loss = 0.42695847\n",
            "Iteration 50, loss = 0.42409271\n",
            "Iteration 51, loss = 0.42261208\n",
            "Iteration 52, loss = 0.42313768\n",
            "Iteration 53, loss = 0.42489180\n",
            "Iteration 54, loss = 0.42770722\n",
            "Iteration 55, loss = 0.42863113\n",
            "Iteration 56, loss = 0.42235711\n",
            "Iteration 57, loss = 0.41920547\n",
            "Iteration 58, loss = 0.41848437\n",
            "Iteration 59, loss = 0.41612861\n",
            "Iteration 60, loss = 0.42413406\n",
            "Iteration 61, loss = 0.42735593\n",
            "Iteration 62, loss = 0.42628147\n",
            "Iteration 63, loss = 0.41670164\n",
            "Iteration 64, loss = 0.41752470\n",
            "Iteration 65, loss = 0.42545614\n",
            "Iteration 66, loss = 0.41925537\n",
            "Iteration 67, loss = 0.41537845\n",
            "Iteration 68, loss = 0.40953785\n",
            "Iteration 69, loss = 0.41411491\n",
            "Iteration 70, loss = 0.42748224\n",
            "Iteration 71, loss = 0.42125811\n",
            "Iteration 72, loss = 0.41394535\n",
            "Iteration 73, loss = 0.41556532\n",
            "Iteration 74, loss = 0.41574980\n",
            "Iteration 75, loss = 0.42550072\n",
            "Iteration 76, loss = 0.41570652\n",
            "Iteration 77, loss = 0.41112924\n",
            "Iteration 78, loss = 0.41113950\n",
            "Iteration 79, loss = 0.41205936\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64073429\n",
            "Iteration 2, loss = 0.61999285\n",
            "Iteration 3, loss = 0.61783895\n",
            "Iteration 4, loss = 0.60666079\n",
            "Iteration 5, loss = 0.60563506\n",
            "Iteration 6, loss = 0.59283414\n",
            "Iteration 7, loss = 0.57710164\n",
            "Iteration 8, loss = 0.55856511\n",
            "Iteration 9, loss = 0.54050555\n",
            "Iteration 10, loss = 0.51160692\n",
            "Iteration 11, loss = 0.51095126\n",
            "Iteration 12, loss = 0.48205180\n",
            "Iteration 13, loss = 0.47112684\n",
            "Iteration 14, loss = 0.47073589\n",
            "Iteration 15, loss = 0.47387582\n",
            "Iteration 16, loss = 0.48355823\n",
            "Iteration 17, loss = 0.45478781\n",
            "Iteration 18, loss = 0.46918584\n",
            "Iteration 19, loss = 0.45380938\n",
            "Iteration 20, loss = 0.46517187\n",
            "Iteration 21, loss = 0.46478965\n",
            "Iteration 22, loss = 0.45328338\n",
            "Iteration 23, loss = 0.45245901\n",
            "Iteration 24, loss = 0.45511884\n",
            "Iteration 25, loss = 0.45674615\n",
            "Iteration 26, loss = 0.45483790\n",
            "Iteration 27, loss = 0.45975945\n",
            "Iteration 28, loss = 0.45203068\n",
            "Iteration 29, loss = 0.45106594\n",
            "Iteration 30, loss = 0.45445948\n",
            "Iteration 31, loss = 0.44923462\n",
            "Iteration 32, loss = 0.44953653\n",
            "Iteration 33, loss = 0.44723269\n",
            "Iteration 34, loss = 0.44457579\n",
            "Iteration 35, loss = 0.44700649\n",
            "Iteration 36, loss = 0.45581212\n",
            "Iteration 37, loss = 0.45298500\n",
            "Iteration 38, loss = 0.44883921\n",
            "Iteration 39, loss = 0.45991180\n",
            "Iteration 40, loss = 0.44395975\n",
            "Iteration 41, loss = 0.44236801\n",
            "Iteration 42, loss = 0.44245243\n",
            "Iteration 43, loss = 0.45551381\n",
            "Iteration 44, loss = 0.45052715\n",
            "Iteration 45, loss = 0.45646152\n",
            "Iteration 46, loss = 0.44433867\n",
            "Iteration 47, loss = 0.44172577\n",
            "Iteration 48, loss = 0.44088950\n",
            "Iteration 49, loss = 0.43611722\n",
            "Iteration 50, loss = 0.44699667\n",
            "Iteration 51, loss = 0.45053806\n",
            "Iteration 52, loss = 0.43816095\n",
            "Iteration 53, loss = 0.44053320\n",
            "Iteration 54, loss = 0.44543881\n",
            "Iteration 55, loss = 0.45137160\n",
            "Iteration 56, loss = 0.44409789\n",
            "Iteration 57, loss = 0.43768478\n",
            "Iteration 58, loss = 0.44672734\n",
            "Iteration 59, loss = 0.44243154\n",
            "Iteration 60, loss = 0.43426195\n",
            "Iteration 61, loss = 0.43894193\n",
            "Iteration 62, loss = 0.43128422\n",
            "Iteration 63, loss = 0.43140281\n",
            "Iteration 64, loss = 0.43790698\n",
            "Iteration 65, loss = 0.44117258\n",
            "Iteration 66, loss = 0.43985192\n",
            "Iteration 67, loss = 0.43596034\n",
            "Iteration 68, loss = 0.43312595\n",
            "Iteration 69, loss = 0.44099992\n",
            "Iteration 70, loss = 0.43293259\n",
            "Iteration 71, loss = 0.43316286\n",
            "Iteration 72, loss = 0.43156565\n",
            "Iteration 73, loss = 0.43144961\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64333486\n",
            "Iteration 2, loss = 0.60498259\n",
            "Iteration 3, loss = 0.59694743\n",
            "Iteration 4, loss = 0.59318286\n",
            "Iteration 5, loss = 0.58558634\n",
            "Iteration 6, loss = 0.57625216\n",
            "Iteration 7, loss = 0.57757539\n",
            "Iteration 8, loss = 0.56052018\n",
            "Iteration 9, loss = 0.54081021\n",
            "Iteration 10, loss = 0.52856140\n",
            "Iteration 11, loss = 0.52453757\n",
            "Iteration 12, loss = 0.49074686\n",
            "Iteration 13, loss = 0.47865371\n",
            "Iteration 14, loss = 0.47278931\n",
            "Iteration 15, loss = 0.46911377\n",
            "Iteration 16, loss = 0.46659524\n",
            "Iteration 17, loss = 0.45772667\n",
            "Iteration 18, loss = 0.46251345\n",
            "Iteration 19, loss = 0.45591290\n",
            "Iteration 20, loss = 0.45341124\n",
            "Iteration 21, loss = 0.45999488\n",
            "Iteration 22, loss = 0.44924732\n",
            "Iteration 23, loss = 0.45199631\n",
            "Iteration 24, loss = 0.44407477\n",
            "Iteration 25, loss = 0.44999704\n",
            "Iteration 26, loss = 0.44612442\n",
            "Iteration 27, loss = 0.44709734\n",
            "Iteration 28, loss = 0.44967998\n",
            "Iteration 29, loss = 0.44489356\n",
            "Iteration 30, loss = 0.44817396\n",
            "Iteration 31, loss = 0.45126041\n",
            "Iteration 32, loss = 0.45535275\n",
            "Iteration 33, loss = 0.43612041\n",
            "Iteration 34, loss = 0.44309115\n",
            "Iteration 35, loss = 0.43389858\n",
            "Iteration 36, loss = 0.44368147\n",
            "Iteration 37, loss = 0.43623617\n",
            "Iteration 38, loss = 0.43715245\n",
            "Iteration 39, loss = 0.43929044\n",
            "Iteration 40, loss = 0.43795268\n",
            "Iteration 41, loss = 0.44969815\n",
            "Iteration 42, loss = 0.43504770\n",
            "Iteration 43, loss = 0.43153404\n",
            "Iteration 44, loss = 0.43391124\n",
            "Iteration 45, loss = 0.42907761\n",
            "Iteration 46, loss = 0.42550725\n",
            "Iteration 47, loss = 0.43960510\n",
            "Iteration 48, loss = 0.43703164\n",
            "Iteration 49, loss = 0.42450843\n",
            "Iteration 50, loss = 0.42567779\n",
            "Iteration 51, loss = 0.42721198\n",
            "Iteration 52, loss = 0.42740066\n",
            "Iteration 53, loss = 0.42772996\n",
            "Iteration 54, loss = 0.43769714\n",
            "Iteration 55, loss = 0.42790611\n",
            "Iteration 56, loss = 0.42634978\n",
            "Iteration 57, loss = 0.42283168\n",
            "Iteration 58, loss = 0.42803812\n",
            "Iteration 59, loss = 0.42351761\n",
            "Iteration 60, loss = 0.42290083\n",
            "Iteration 61, loss = 0.42982084\n",
            "Iteration 62, loss = 0.42728989\n",
            "Iteration 63, loss = 0.42357401\n",
            "Iteration 64, loss = 0.42039463\n",
            "Iteration 65, loss = 0.42641457\n",
            "Iteration 66, loss = 0.41769282\n",
            "Iteration 67, loss = 0.42962377\n",
            "Iteration 68, loss = 0.42126104\n",
            "Iteration 69, loss = 0.41721602\n",
            "Iteration 70, loss = 0.42281494\n",
            "Iteration 71, loss = 0.41648931\n",
            "Iteration 72, loss = 0.41841317\n",
            "Iteration 73, loss = 0.41881810\n",
            "Iteration 74, loss = 0.41914718\n",
            "Iteration 75, loss = 0.41490428\n",
            "Iteration 76, loss = 0.42467404\n",
            "Iteration 77, loss = 0.42277513\n",
            "Iteration 78, loss = 0.41473096\n",
            "Iteration 79, loss = 0.41412870\n",
            "Iteration 80, loss = 0.43146254\n",
            "Iteration 81, loss = 0.42577717\n",
            "Iteration 82, loss = 0.41493718\n",
            "Iteration 83, loss = 0.41569366\n",
            "Iteration 84, loss = 0.42083160\n",
            "Iteration 85, loss = 0.41072754\n",
            "Iteration 86, loss = 0.41096186\n",
            "Iteration 87, loss = 0.41678731\n",
            "Iteration 88, loss = 0.41010551\n",
            "Iteration 89, loss = 0.41591400\n",
            "Iteration 90, loss = 0.41041300\n",
            "Iteration 91, loss = 0.42069427\n",
            "Iteration 92, loss = 0.40931229\n",
            "Iteration 93, loss = 0.41016056\n",
            "Iteration 94, loss = 0.41047573\n",
            "Iteration 95, loss = 0.41437833\n",
            "Iteration 96, loss = 0.41034262\n",
            "Iteration 97, loss = 0.41758739\n",
            "Iteration 98, loss = 0.41154003\n",
            "Iteration 99, loss = 0.41151070\n",
            "Iteration 100, loss = 0.41828895\n",
            "Iteration 101, loss = 0.41141449\n",
            "Iteration 102, loss = 0.40693326\n",
            "Iteration 103, loss = 0.41815187\n",
            "Iteration 104, loss = 0.40936810\n",
            "Iteration 105, loss = 0.42801263\n",
            "Iteration 106, loss = 0.40888662\n",
            "Iteration 107, loss = 0.40629097\n",
            "Iteration 108, loss = 0.41007204\n",
            "Iteration 109, loss = 0.41637117\n",
            "Iteration 110, loss = 0.41766999\n",
            "Iteration 111, loss = 0.42171153\n",
            "Iteration 112, loss = 0.41461147\n",
            "Iteration 113, loss = 0.40257149\n",
            "Iteration 114, loss = 0.40659274\n",
            "Iteration 115, loss = 0.40195521\n",
            "Iteration 116, loss = 0.40726451\n",
            "Iteration 117, loss = 0.40374312\n",
            "Iteration 118, loss = 0.41379484\n",
            "Iteration 119, loss = 0.40158753\n",
            "Iteration 120, loss = 0.41931652\n",
            "Iteration 121, loss = 0.41042970\n",
            "Iteration 122, loss = 0.40896783\n",
            "Iteration 123, loss = 0.40361811\n",
            "Iteration 124, loss = 0.40363855\n",
            "Iteration 125, loss = 0.40732338\n",
            "Iteration 126, loss = 0.40219238\n",
            "Iteration 127, loss = 0.40034409\n",
            "Iteration 128, loss = 0.41089183\n",
            "Iteration 129, loss = 0.40538698\n",
            "Iteration 130, loss = 0.40212206\n",
            "Iteration 131, loss = 0.40439272\n",
            "Iteration 132, loss = 0.41013443\n",
            "Iteration 133, loss = 0.41318579\n",
            "Iteration 134, loss = 0.41144119\n",
            "Iteration 135, loss = 0.39563262\n",
            "Iteration 136, loss = 0.39790923\n",
            "Iteration 137, loss = 0.40055991\n",
            "Iteration 138, loss = 0.41161980\n",
            "Iteration 139, loss = 0.39943933\n",
            "Iteration 140, loss = 0.39670744\n",
            "Iteration 141, loss = 0.40963083\n",
            "Iteration 142, loss = 0.40152879\n",
            "Iteration 143, loss = 0.39925870\n",
            "Iteration 144, loss = 0.40156681\n",
            "Iteration 145, loss = 0.40252303\n",
            "Iteration 146, loss = 0.41463823\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64005961\n",
            "Iteration 2, loss = 0.62036166\n",
            "Iteration 3, loss = 0.60340600\n",
            "Iteration 4, loss = 0.60407165\n",
            "Iteration 5, loss = 0.59203101\n",
            "Iteration 6, loss = 0.58394490\n",
            "Iteration 7, loss = 0.57451380\n",
            "Iteration 8, loss = 0.56151259\n",
            "Iteration 9, loss = 0.54762214\n",
            "Iteration 10, loss = 0.53142253\n",
            "Iteration 11, loss = 0.51587324\n",
            "Iteration 12, loss = 0.51026638\n",
            "Iteration 13, loss = 0.49031936\n",
            "Iteration 14, loss = 0.48166052\n",
            "Iteration 15, loss = 0.47534274\n",
            "Iteration 16, loss = 0.46879461\n",
            "Iteration 17, loss = 0.47038110\n",
            "Iteration 18, loss = 0.47251604\n",
            "Iteration 19, loss = 0.47543901\n",
            "Iteration 20, loss = 0.46461521\n",
            "Iteration 21, loss = 0.46241158\n",
            "Iteration 22, loss = 0.46954546\n",
            "Iteration 23, loss = 0.46572590\n",
            "Iteration 24, loss = 0.46287020\n",
            "Iteration 25, loss = 0.45240587\n",
            "Iteration 26, loss = 0.46150412\n",
            "Iteration 27, loss = 0.46032672\n",
            "Iteration 28, loss = 0.45615585\n",
            "Iteration 29, loss = 0.45744276\n",
            "Iteration 30, loss = 0.44536841\n",
            "Iteration 31, loss = 0.45640139\n",
            "Iteration 32, loss = 0.45992629\n",
            "Iteration 33, loss = 0.45978992\n",
            "Iteration 34, loss = 0.45363819\n",
            "Iteration 35, loss = 0.44604183\n",
            "Iteration 36, loss = 0.44615483\n",
            "Iteration 37, loss = 0.44170261\n",
            "Iteration 38, loss = 0.47079556\n",
            "Iteration 39, loss = 0.44528345\n",
            "Iteration 40, loss = 0.44625522\n",
            "Iteration 41, loss = 0.44839876\n",
            "Iteration 42, loss = 0.44811975\n",
            "Iteration 43, loss = 0.44085960\n",
            "Iteration 44, loss = 0.44647703\n",
            "Iteration 45, loss = 0.44213844\n",
            "Iteration 46, loss = 0.44585523\n",
            "Iteration 47, loss = 0.43757846\n",
            "Iteration 48, loss = 0.44000860\n",
            "Iteration 49, loss = 0.44174788\n",
            "Iteration 50, loss = 0.44490492\n",
            "Iteration 51, loss = 0.43544561\n",
            "Iteration 52, loss = 0.43273766\n",
            "Iteration 53, loss = 0.43861624\n",
            "Iteration 54, loss = 0.44322762\n",
            "Iteration 55, loss = 0.43866902\n",
            "Iteration 56, loss = 0.44514783\n",
            "Iteration 57, loss = 0.43643663\n",
            "Iteration 58, loss = 0.42965111\n",
            "Iteration 59, loss = 0.42947802\n",
            "Iteration 60, loss = 0.43222116\n",
            "Iteration 61, loss = 0.43660875\n",
            "Iteration 62, loss = 0.42721287\n",
            "Iteration 63, loss = 0.43346998\n",
            "Iteration 64, loss = 0.42696523\n",
            "Iteration 65, loss = 0.43763092\n",
            "Iteration 66, loss = 0.42423295\n",
            "Iteration 67, loss = 0.42623905\n",
            "Iteration 68, loss = 0.42481811\n",
            "Iteration 69, loss = 0.42662925\n",
            "Iteration 70, loss = 0.42836428\n",
            "Iteration 71, loss = 0.42397334\n",
            "Iteration 72, loss = 0.42867299\n",
            "Iteration 73, loss = 0.43202238\n",
            "Iteration 74, loss = 0.44069662\n",
            "Iteration 75, loss = 0.42485262\n",
            "Iteration 76, loss = 0.42031035\n",
            "Iteration 77, loss = 0.42403380\n",
            "Iteration 78, loss = 0.41874205\n",
            "Iteration 79, loss = 0.43131488\n",
            "Iteration 80, loss = 0.42079912\n",
            "Iteration 81, loss = 0.42367377\n",
            "Iteration 82, loss = 0.42391178\n",
            "Iteration 83, loss = 0.41833183\n",
            "Iteration 84, loss = 0.42397836\n",
            "Iteration 85, loss = 0.42844660\n",
            "Iteration 86, loss = 0.41997462\n",
            "Iteration 87, loss = 0.41612568\n",
            "Iteration 88, loss = 0.41796478\n",
            "Iteration 89, loss = 0.42113741\n",
            "Iteration 90, loss = 0.41714296\n",
            "Iteration 91, loss = 0.41732271\n",
            "Iteration 92, loss = 0.41935476\n",
            "Iteration 93, loss = 0.42471049\n",
            "Iteration 94, loss = 0.41576130\n",
            "Iteration 95, loss = 0.41738241\n",
            "Iteration 96, loss = 0.41230331\n",
            "Iteration 97, loss = 0.41346448\n",
            "Iteration 98, loss = 0.42233486\n",
            "Iteration 99, loss = 0.41055550\n",
            "Iteration 100, loss = 0.41329377\n",
            "Iteration 101, loss = 0.41360399\n",
            "Iteration 102, loss = 0.42594959\n",
            "Iteration 103, loss = 0.41676738\n",
            "Iteration 104, loss = 0.41496172\n",
            "Iteration 105, loss = 0.41713683\n",
            "Iteration 106, loss = 0.41999081\n",
            "Iteration 107, loss = 0.41185862\n",
            "Iteration 108, loss = 0.42285864\n",
            "Iteration 109, loss = 0.41181503\n",
            "Iteration 110, loss = 0.41632729\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.65907036\n",
            "Iteration 2, loss = 0.62493746\n",
            "Iteration 3, loss = 0.61115144\n",
            "Iteration 4, loss = 0.60336017\n",
            "Iteration 5, loss = 0.60323565\n",
            "Iteration 6, loss = 0.59418140\n",
            "Iteration 7, loss = 0.58939669\n",
            "Iteration 8, loss = 0.58340294\n",
            "Iteration 9, loss = 0.57830281\n",
            "Iteration 10, loss = 0.57214669\n",
            "Iteration 11, loss = 0.56047447\n",
            "Iteration 12, loss = 0.54775110\n",
            "Iteration 13, loss = 0.53771501\n",
            "Iteration 14, loss = 0.53228848\n",
            "Iteration 15, loss = 0.51456668\n",
            "Iteration 16, loss = 0.49521245\n",
            "Iteration 17, loss = 0.48621558\n",
            "Iteration 18, loss = 0.47629787\n",
            "Iteration 19, loss = 0.47593983\n",
            "Iteration 20, loss = 0.46588280\n",
            "Iteration 21, loss = 0.46111385\n",
            "Iteration 22, loss = 0.45476667\n",
            "Iteration 23, loss = 0.45212577\n",
            "Iteration 24, loss = 0.44742671\n",
            "Iteration 25, loss = 0.44590175\n",
            "Iteration 26, loss = 0.44446792\n",
            "Iteration 27, loss = 0.44925650\n",
            "Iteration 28, loss = 0.44487127\n",
            "Iteration 29, loss = 0.44020242\n",
            "Iteration 30, loss = 0.44680540\n",
            "Iteration 31, loss = 0.43816176\n",
            "Iteration 32, loss = 0.43851571\n",
            "Iteration 33, loss = 0.43135360\n",
            "Iteration 34, loss = 0.43855073\n",
            "Iteration 35, loss = 0.43452677\n",
            "Iteration 36, loss = 0.43127484\n",
            "Iteration 37, loss = 0.43688723\n",
            "Iteration 38, loss = 0.43451882\n",
            "Iteration 39, loss = 0.43290429\n",
            "Iteration 40, loss = 0.43707967\n",
            "Iteration 41, loss = 0.43498280\n",
            "Iteration 42, loss = 0.43576695\n",
            "Iteration 43, loss = 0.43314005\n",
            "Iteration 44, loss = 0.43824296\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64729440\n",
            "Iteration 2, loss = 0.61926907\n",
            "Iteration 3, loss = 0.60400868\n",
            "Iteration 4, loss = 0.60286044\n",
            "Iteration 5, loss = 0.59743087\n",
            "Iteration 6, loss = 0.59022217\n",
            "Iteration 7, loss = 0.58471612\n",
            "Iteration 8, loss = 0.57811372\n",
            "Iteration 9, loss = 0.56460660\n",
            "Iteration 10, loss = 0.55245429\n",
            "Iteration 11, loss = 0.54612649\n",
            "Iteration 12, loss = 0.52577833\n",
            "Iteration 13, loss = 0.51072267\n",
            "Iteration 14, loss = 0.50994029\n",
            "Iteration 15, loss = 0.49635611\n",
            "Iteration 16, loss = 0.48122557\n",
            "Iteration 17, loss = 0.47665217\n",
            "Iteration 18, loss = 0.47449099\n",
            "Iteration 19, loss = 0.46873649\n",
            "Iteration 20, loss = 0.46451821\n",
            "Iteration 21, loss = 0.46939203\n",
            "Iteration 22, loss = 0.46049369\n",
            "Iteration 23, loss = 0.45795466\n",
            "Iteration 24, loss = 0.46261305\n",
            "Iteration 25, loss = 0.46335720\n",
            "Iteration 26, loss = 0.45823480\n",
            "Iteration 27, loss = 0.44920509\n",
            "Iteration 28, loss = 0.45106579\n",
            "Iteration 29, loss = 0.45054589\n",
            "Iteration 30, loss = 0.45002483\n",
            "Iteration 31, loss = 0.44642798\n",
            "Iteration 32, loss = 0.44446124\n",
            "Iteration 33, loss = 0.44431900\n",
            "Iteration 34, loss = 0.44226720\n",
            "Iteration 35, loss = 0.43735230\n",
            "Iteration 36, loss = 0.43774443\n",
            "Iteration 37, loss = 0.45347642\n",
            "Iteration 38, loss = 0.43838444\n",
            "Iteration 39, loss = 0.43772331\n",
            "Iteration 40, loss = 0.43360191\n",
            "Iteration 41, loss = 0.43267842\n",
            "Iteration 42, loss = 0.43215374\n",
            "Iteration 43, loss = 0.43257911\n",
            "Iteration 44, loss = 0.43502153\n",
            "Iteration 45, loss = 0.43769864\n",
            "Iteration 46, loss = 0.43471690\n",
            "Iteration 47, loss = 0.43823492\n",
            "Iteration 48, loss = 0.42824351\n",
            "Iteration 49, loss = 0.43429478\n",
            "Iteration 50, loss = 0.42631963\n",
            "Iteration 51, loss = 0.43967691\n",
            "Iteration 52, loss = 0.43357328\n",
            "Iteration 53, loss = 0.43186749\n",
            "Iteration 54, loss = 0.42732273\n",
            "Iteration 55, loss = 0.42845644\n",
            "Iteration 56, loss = 0.42651700\n",
            "Iteration 57, loss = 0.42938149\n",
            "Iteration 58, loss = 0.42901017\n",
            "Iteration 59, loss = 0.42593893\n",
            "Iteration 60, loss = 0.42171041\n",
            "Iteration 61, loss = 0.42409221\n",
            "Iteration 62, loss = 0.42294854\n",
            "Iteration 63, loss = 0.42151702\n",
            "Iteration 64, loss = 0.42401894\n",
            "Iteration 65, loss = 0.42218058\n",
            "Iteration 66, loss = 0.41892896\n",
            "Iteration 67, loss = 0.42240004\n",
            "Iteration 68, loss = 0.42596009\n",
            "Iteration 69, loss = 0.42600665\n",
            "Iteration 70, loss = 0.42313315\n",
            "Iteration 71, loss = 0.41834246\n",
            "Iteration 72, loss = 0.41396196\n",
            "Iteration 73, loss = 0.42044668\n",
            "Iteration 74, loss = 0.40931552\n",
            "Iteration 75, loss = 0.41854689\n",
            "Iteration 76, loss = 0.41665429\n",
            "Iteration 77, loss = 0.42203025\n",
            "Iteration 78, loss = 0.41436283\n",
            "Iteration 79, loss = 0.41935789\n",
            "Iteration 80, loss = 0.42215413\n",
            "Iteration 81, loss = 0.41884134\n",
            "Iteration 82, loss = 0.41502590\n",
            "Iteration 83, loss = 0.41248791\n",
            "Iteration 84, loss = 0.41347368\n",
            "Iteration 85, loss = 0.41254939\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67269513\n",
            "Iteration 2, loss = 0.62778226\n",
            "Iteration 3, loss = 0.61567903\n",
            "Iteration 4, loss = 0.61084724\n",
            "Iteration 5, loss = 0.60751397\n",
            "Iteration 6, loss = 0.60296643\n",
            "Iteration 7, loss = 0.59257728\n",
            "Iteration 8, loss = 0.59077760\n",
            "Iteration 9, loss = 0.57686531\n",
            "Iteration 10, loss = 0.56332292\n",
            "Iteration 11, loss = 0.55133678\n",
            "Iteration 12, loss = 0.53921462\n",
            "Iteration 13, loss = 0.52244557\n",
            "Iteration 14, loss = 0.51111045\n",
            "Iteration 15, loss = 0.50112437\n",
            "Iteration 16, loss = 0.49341121\n",
            "Iteration 17, loss = 0.48544547\n",
            "Iteration 18, loss = 0.47964417\n",
            "Iteration 19, loss = 0.47494308\n",
            "Iteration 20, loss = 0.47415436\n",
            "Iteration 21, loss = 0.46511521\n",
            "Iteration 22, loss = 0.46459727\n",
            "Iteration 23, loss = 0.46602077\n",
            "Iteration 24, loss = 0.46909695\n",
            "Iteration 25, loss = 0.46397166\n",
            "Iteration 26, loss = 0.46063051\n",
            "Iteration 27, loss = 0.45947729\n",
            "Iteration 28, loss = 0.45338075\n",
            "Iteration 29, loss = 0.46102553\n",
            "Iteration 30, loss = 0.45509729\n",
            "Iteration 31, loss = 0.45087098\n",
            "Iteration 32, loss = 0.45248146\n",
            "Iteration 33, loss = 0.45819570\n",
            "Iteration 34, loss = 0.46782519\n",
            "Iteration 35, loss = 0.45421788\n",
            "Iteration 36, loss = 0.45284842\n",
            "Iteration 37, loss = 0.45166593\n",
            "Iteration 38, loss = 0.44648351\n",
            "Iteration 39, loss = 0.45464627\n",
            "Iteration 40, loss = 0.44208539\n",
            "Iteration 41, loss = 0.45214123\n",
            "Iteration 42, loss = 0.44361658\n",
            "Iteration 43, loss = 0.45535085\n",
            "Iteration 44, loss = 0.45631595\n",
            "Iteration 45, loss = 0.45124611\n",
            "Iteration 46, loss = 0.44299642\n",
            "Iteration 47, loss = 0.44262300\n",
            "Iteration 48, loss = 0.45007153\n",
            "Iteration 49, loss = 0.43885029\n",
            "Iteration 50, loss = 0.44354326\n",
            "Iteration 51, loss = 0.44166938\n",
            "Iteration 52, loss = 0.44586249\n",
            "Iteration 53, loss = 0.43968493\n",
            "Iteration 54, loss = 0.44814313\n",
            "Iteration 55, loss = 0.43769999\n",
            "Iteration 56, loss = 0.43561972\n",
            "Iteration 57, loss = 0.44049819\n",
            "Iteration 58, loss = 0.43874948\n",
            "Iteration 59, loss = 0.44193883\n",
            "Iteration 60, loss = 0.43977908\n",
            "Iteration 61, loss = 0.43696910\n",
            "Iteration 62, loss = 0.43936742\n",
            "Iteration 63, loss = 0.43252905\n",
            "Iteration 64, loss = 0.43278485\n",
            "Iteration 65, loss = 0.43835285\n",
            "Iteration 66, loss = 0.43584449\n",
            "Iteration 67, loss = 0.43753005\n",
            "Iteration 68, loss = 0.43431037\n",
            "Iteration 69, loss = 0.43034729\n",
            "Iteration 70, loss = 0.43502410\n",
            "Iteration 71, loss = 0.43882968\n",
            "Iteration 72, loss = 0.43363014\n",
            "Iteration 73, loss = 0.43081536\n",
            "Iteration 74, loss = 0.42744611\n",
            "Iteration 75, loss = 0.43420693\n",
            "Iteration 76, loss = 0.43142577\n",
            "Iteration 77, loss = 0.43041613\n",
            "Iteration 78, loss = 0.43045157\n",
            "Iteration 79, loss = 0.42987539\n",
            "Iteration 80, loss = 0.42709607\n",
            "Iteration 81, loss = 0.42513086\n",
            "Iteration 82, loss = 0.42495188\n",
            "Iteration 83, loss = 0.42743022\n",
            "Iteration 84, loss = 0.43458342\n",
            "Iteration 85, loss = 0.42693274\n",
            "Iteration 86, loss = 0.42775705\n",
            "Iteration 87, loss = 0.43018904\n",
            "Iteration 88, loss = 0.43111378\n",
            "Iteration 89, loss = 0.43074400\n",
            "Iteration 90, loss = 0.42664283\n",
            "Iteration 91, loss = 0.42493029\n",
            "Iteration 92, loss = 0.42182995\n",
            "Iteration 93, loss = 0.42670023\n",
            "Iteration 94, loss = 0.42762830\n",
            "Iteration 95, loss = 0.42398083\n",
            "Iteration 96, loss = 0.43134778\n",
            "Iteration 97, loss = 0.42635625\n",
            "Iteration 98, loss = 0.42631482\n",
            "Iteration 99, loss = 0.42492647\n",
            "Iteration 100, loss = 0.42213365\n",
            "Iteration 101, loss = 0.42987492\n",
            "Iteration 102, loss = 0.43048798\n",
            "Iteration 103, loss = 0.41948208\n",
            "Iteration 104, loss = 0.42155627\n",
            "Iteration 105, loss = 0.42492017\n",
            "Iteration 106, loss = 0.42948923\n",
            "Iteration 107, loss = 0.42229177\n",
            "Iteration 108, loss = 0.42481552\n",
            "Iteration 109, loss = 0.42605081\n",
            "Iteration 110, loss = 0.41625328\n",
            "Iteration 111, loss = 0.42199731\n",
            "Iteration 112, loss = 0.41778670\n",
            "Iteration 113, loss = 0.42455166\n",
            "Iteration 114, loss = 0.42966349\n",
            "Iteration 115, loss = 0.42295718\n",
            "Iteration 116, loss = 0.42852868\n",
            "Iteration 117, loss = 0.41622074\n",
            "Iteration 118, loss = 0.42032021\n",
            "Iteration 119, loss = 0.41984150\n",
            "Iteration 120, loss = 0.41795472\n",
            "Iteration 121, loss = 0.42152397\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.65105937\n",
            "Iteration 2, loss = 0.61751353\n",
            "Iteration 3, loss = 0.59880128\n",
            "Iteration 4, loss = 0.59593217\n",
            "Iteration 5, loss = 0.59098311\n",
            "Iteration 6, loss = 0.58749865\n",
            "Iteration 7, loss = 0.58454879\n",
            "Iteration 8, loss = 0.57690414\n",
            "Iteration 9, loss = 0.56969189\n",
            "Iteration 10, loss = 0.56355328\n",
            "Iteration 11, loss = 0.55179501\n",
            "Iteration 12, loss = 0.54211628\n",
            "Iteration 13, loss = 0.52964796\n",
            "Iteration 14, loss = 0.52043073\n",
            "Iteration 15, loss = 0.50346929\n",
            "Iteration 16, loss = 0.49590588\n",
            "Iteration 17, loss = 0.48802749\n",
            "Iteration 18, loss = 0.48296922\n",
            "Iteration 19, loss = 0.48282472\n",
            "Iteration 20, loss = 0.47800021\n",
            "Iteration 21, loss = 0.46752860\n",
            "Iteration 22, loss = 0.46673466\n",
            "Iteration 23, loss = 0.47016840\n",
            "Iteration 24, loss = 0.45740656\n",
            "Iteration 25, loss = 0.46247314\n",
            "Iteration 26, loss = 0.45869084\n",
            "Iteration 27, loss = 0.46134940\n",
            "Iteration 28, loss = 0.45893366\n",
            "Iteration 29, loss = 0.45345645\n",
            "Iteration 30, loss = 0.45322626\n",
            "Iteration 31, loss = 0.45741713\n",
            "Iteration 32, loss = 0.44898407\n",
            "Iteration 33, loss = 0.44810587\n",
            "Iteration 34, loss = 0.44995528\n",
            "Iteration 35, loss = 0.44896869\n",
            "Iteration 36, loss = 0.45138673\n",
            "Iteration 37, loss = 0.45325534\n",
            "Iteration 38, loss = 0.44619248\n",
            "Iteration 39, loss = 0.44446490\n",
            "Iteration 40, loss = 0.45246741\n",
            "Iteration 41, loss = 0.44673146\n",
            "Iteration 42, loss = 0.45155283\n",
            "Iteration 43, loss = 0.44012653\n",
            "Iteration 44, loss = 0.44721257\n",
            "Iteration 45, loss = 0.44656426\n",
            "Iteration 46, loss = 0.43908338\n",
            "Iteration 47, loss = 0.44248217\n",
            "Iteration 48, loss = 0.44506735\n",
            "Iteration 49, loss = 0.43960918\n",
            "Iteration 50, loss = 0.44213291\n",
            "Iteration 51, loss = 0.43802048\n",
            "Iteration 52, loss = 0.43555865\n",
            "Iteration 53, loss = 0.43806812\n",
            "Iteration 54, loss = 0.43716644\n",
            "Iteration 55, loss = 0.43597882\n",
            "Iteration 56, loss = 0.43885990\n",
            "Iteration 57, loss = 0.43815075\n",
            "Iteration 58, loss = 0.43784602\n",
            "Iteration 59, loss = 0.43752489\n",
            "Iteration 60, loss = 0.43267727\n",
            "Iteration 61, loss = 0.43318332\n",
            "Iteration 62, loss = 0.43136746\n",
            "Iteration 63, loss = 0.43264909\n",
            "Iteration 64, loss = 0.43060762\n",
            "Iteration 65, loss = 0.43184710\n",
            "Iteration 66, loss = 0.42843720\n",
            "Iteration 67, loss = 0.43144419\n",
            "Iteration 68, loss = 0.43099963\n",
            "Iteration 69, loss = 0.42854380\n",
            "Iteration 70, loss = 0.43447063\n",
            "Iteration 71, loss = 0.42469305\n",
            "Iteration 72, loss = 0.42537145\n",
            "Iteration 73, loss = 0.43245758\n",
            "Iteration 74, loss = 0.42950938\n",
            "Iteration 75, loss = 0.42472298\n",
            "Iteration 76, loss = 0.42499635\n",
            "Iteration 77, loss = 0.42452093\n",
            "Iteration 78, loss = 0.42632975\n",
            "Iteration 79, loss = 0.42592957\n",
            "Iteration 80, loss = 0.42928041\n",
            "Iteration 81, loss = 0.42632499\n",
            "Iteration 82, loss = 0.42470694\n",
            "Iteration 83, loss = 0.42173138\n",
            "Iteration 84, loss = 0.42104063\n",
            "Iteration 85, loss = 0.41525196\n",
            "Iteration 86, loss = 0.42855030\n",
            "Iteration 87, loss = 0.42313591\n",
            "Iteration 88, loss = 0.42278791\n",
            "Iteration 89, loss = 0.41621675\n",
            "Iteration 90, loss = 0.42559191\n",
            "Iteration 91, loss = 0.41968806\n",
            "Iteration 92, loss = 0.42275724\n",
            "Iteration 93, loss = 0.42394686\n",
            "Iteration 94, loss = 0.42004898\n",
            "Iteration 95, loss = 0.42208795\n",
            "Iteration 96, loss = 0.41832433\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68997922\n",
            "Iteration 2, loss = 0.62578249\n",
            "Iteration 3, loss = 0.60975319\n",
            "Iteration 4, loss = 0.60187152\n",
            "Iteration 5, loss = 0.59978694\n",
            "Iteration 6, loss = 0.59539435\n",
            "Iteration 7, loss = 0.58637644\n",
            "Iteration 8, loss = 0.58205895\n",
            "Iteration 9, loss = 0.57472726\n",
            "Iteration 10, loss = 0.57013797\n",
            "Iteration 11, loss = 0.55805981\n",
            "Iteration 12, loss = 0.54571986\n",
            "Iteration 13, loss = 0.54013687\n",
            "Iteration 14, loss = 0.52400920\n",
            "Iteration 15, loss = 0.52166765\n",
            "Iteration 16, loss = 0.50465334\n",
            "Iteration 17, loss = 0.49064289\n",
            "Iteration 18, loss = 0.48839432\n",
            "Iteration 19, loss = 0.48203847\n",
            "Iteration 20, loss = 0.47742834\n",
            "Iteration 21, loss = 0.48061607\n",
            "Iteration 22, loss = 0.47444115\n",
            "Iteration 23, loss = 0.46791352\n",
            "Iteration 24, loss = 0.46814152\n",
            "Iteration 25, loss = 0.46671818\n",
            "Iteration 26, loss = 0.46032147\n",
            "Iteration 27, loss = 0.45831611\n",
            "Iteration 28, loss = 0.46242276\n",
            "Iteration 29, loss = 0.45508515\n",
            "Iteration 30, loss = 0.45947464\n",
            "Iteration 31, loss = 0.45730318\n",
            "Iteration 32, loss = 0.45512047\n",
            "Iteration 33, loss = 0.44676192\n",
            "Iteration 34, loss = 0.44752617\n",
            "Iteration 35, loss = 0.45054222\n",
            "Iteration 36, loss = 0.45656330\n",
            "Iteration 37, loss = 0.44930232\n",
            "Iteration 38, loss = 0.44412739\n",
            "Iteration 39, loss = 0.44909732\n",
            "Iteration 40, loss = 0.44625450\n",
            "Iteration 41, loss = 0.44413562\n",
            "Iteration 42, loss = 0.44596737\n",
            "Iteration 43, loss = 0.44034660\n",
            "Iteration 44, loss = 0.43898774\n",
            "Iteration 45, loss = 0.44734613\n",
            "Iteration 46, loss = 0.44414614\n",
            "Iteration 47, loss = 0.44117713\n",
            "Iteration 48, loss = 0.43639329\n",
            "Iteration 49, loss = 0.44387280\n",
            "Iteration 50, loss = 0.44113592\n",
            "Iteration 51, loss = 0.43799421\n",
            "Iteration 52, loss = 0.43689390\n",
            "Iteration 53, loss = 0.43316588\n",
            "Iteration 54, loss = 0.43750955\n",
            "Iteration 55, loss = 0.43622840\n",
            "Iteration 56, loss = 0.43255193\n",
            "Iteration 57, loss = 0.43276552\n",
            "Iteration 58, loss = 0.43671590\n",
            "Iteration 59, loss = 0.43795806\n",
            "Iteration 60, loss = 0.43357088\n",
            "Iteration 61, loss = 0.43462917\n",
            "Iteration 62, loss = 0.43635915\n",
            "Iteration 63, loss = 0.43210025\n",
            "Iteration 64, loss = 0.42954324\n",
            "Iteration 65, loss = 0.43009345\n",
            "Iteration 66, loss = 0.43438044\n",
            "Iteration 67, loss = 0.43499301\n",
            "Iteration 68, loss = 0.43088101\n",
            "Iteration 69, loss = 0.42651832\n",
            "Iteration 70, loss = 0.43360039\n",
            "Iteration 71, loss = 0.43023069\n",
            "Iteration 72, loss = 0.42877320\n",
            "Iteration 73, loss = 0.42383362\n",
            "Iteration 74, loss = 0.42962608\n",
            "Iteration 75, loss = 0.43151218\n",
            "Iteration 76, loss = 0.42666029\n",
            "Iteration 77, loss = 0.42695078\n",
            "Iteration 78, loss = 0.42907406\n",
            "Iteration 79, loss = 0.42219263\n",
            "Iteration 80, loss = 0.42433950\n",
            "Iteration 81, loss = 0.42274608\n",
            "Iteration 82, loss = 0.42285411\n",
            "Iteration 83, loss = 0.42149635\n",
            "Iteration 84, loss = 0.42484490\n",
            "Iteration 85, loss = 0.42499429\n",
            "Iteration 86, loss = 0.42640672\n",
            "Iteration 87, loss = 0.42523848\n",
            "Iteration 88, loss = 0.42558890\n",
            "Iteration 89, loss = 0.42016035\n",
            "Iteration 90, loss = 0.42259730\n",
            "Iteration 91, loss = 0.42613425\n",
            "Iteration 92, loss = 0.42240392\n",
            "Iteration 93, loss = 0.41764391\n",
            "Iteration 94, loss = 0.42182313\n",
            "Iteration 95, loss = 0.41839820\n",
            "Iteration 96, loss = 0.41784871\n",
            "Iteration 97, loss = 0.42236742\n",
            "Iteration 98, loss = 0.42507814\n",
            "Iteration 99, loss = 0.41642450\n",
            "Iteration 100, loss = 0.41726610\n",
            "Iteration 101, loss = 0.41723095\n",
            "Iteration 102, loss = 0.41413318\n",
            "Iteration 103, loss = 0.41289495\n",
            "Iteration 104, loss = 0.42071448\n",
            "Iteration 105, loss = 0.41420819\n",
            "Iteration 106, loss = 0.42299583\n",
            "Iteration 107, loss = 0.41241193\n",
            "Iteration 108, loss = 0.41882541\n",
            "Iteration 109, loss = 0.41820937\n",
            "Iteration 110, loss = 0.41261025\n",
            "Iteration 111, loss = 0.41636158\n",
            "Iteration 112, loss = 0.41617309\n",
            "Iteration 113, loss = 0.41305277\n",
            "Iteration 114, loss = 0.41198104\n",
            "Iteration 115, loss = 0.41219975\n",
            "Iteration 116, loss = 0.41938828\n",
            "Iteration 117, loss = 0.41621480\n",
            "Iteration 118, loss = 0.41563217\n",
            "Iteration 119, loss = 0.41264236\n",
            "Iteration 120, loss = 0.41297536\n",
            "Iteration 121, loss = 0.40861052\n",
            "Iteration 122, loss = 0.40999693\n",
            "Iteration 123, loss = 0.41688137\n",
            "Iteration 124, loss = 0.40879940\n",
            "Iteration 125, loss = 0.41625498\n",
            "Iteration 126, loss = 0.41478320\n",
            "Iteration 127, loss = 0.40881987\n",
            "Iteration 128, loss = 0.41102613\n",
            "Iteration 129, loss = 0.41051545\n",
            "Iteration 130, loss = 0.41486319\n",
            "Iteration 131, loss = 0.40975247\n",
            "Iteration 132, loss = 0.41381128\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.90874004\n",
            "Iteration 2, loss = 0.63246432\n",
            "Iteration 3, loss = 0.63362338\n",
            "Iteration 4, loss = 0.62241734\n",
            "Iteration 5, loss = 0.60358504\n",
            "Iteration 6, loss = 0.60159358\n",
            "Iteration 7, loss = 0.60159330\n",
            "Iteration 8, loss = 0.60633162\n",
            "Iteration 9, loss = 0.59823471\n",
            "Iteration 10, loss = 0.60442392\n",
            "Iteration 11, loss = 0.60338793\n",
            "Iteration 12, loss = 0.58653247\n",
            "Iteration 13, loss = 0.57464781\n",
            "Iteration 14, loss = 0.57450773\n",
            "Iteration 15, loss = 0.57401794\n",
            "Iteration 16, loss = 0.58164056\n",
            "Iteration 17, loss = 0.57439795\n",
            "Iteration 18, loss = 0.57491011\n",
            "Iteration 19, loss = 0.58351589\n",
            "Iteration 20, loss = 0.56476224\n",
            "Iteration 21, loss = 0.56825055\n",
            "Iteration 22, loss = 0.57358644\n",
            "Iteration 23, loss = 0.56069544\n",
            "Iteration 24, loss = 0.56440413\n",
            "Iteration 25, loss = 0.55490798\n",
            "Iteration 26, loss = 0.55587137\n",
            "Iteration 27, loss = 0.55308052\n",
            "Iteration 28, loss = 0.54661842\n",
            "Iteration 29, loss = 0.55244364\n",
            "Iteration 30, loss = 0.54353622\n",
            "Iteration 31, loss = 0.54665688\n",
            "Iteration 32, loss = 0.55383150\n",
            "Iteration 33, loss = 0.54370175\n",
            "Iteration 34, loss = 0.53844299\n",
            "Iteration 35, loss = 0.53006201\n",
            "Iteration 36, loss = 0.53333794\n",
            "Iteration 37, loss = 0.52312039\n",
            "Iteration 38, loss = 0.52450353\n",
            "Iteration 39, loss = 0.52838979\n",
            "Iteration 40, loss = 0.53027948\n",
            "Iteration 41, loss = 0.52622427\n",
            "Iteration 42, loss = 0.52419774\n",
            "Iteration 43, loss = 0.53088616\n",
            "Iteration 44, loss = 0.51411809\n",
            "Iteration 45, loss = 0.52200466\n",
            "Iteration 46, loss = 0.53085752\n",
            "Iteration 47, loss = 0.51881331\n",
            "Iteration 48, loss = 0.50634145\n",
            "Iteration 49, loss = 0.52590054\n",
            "Iteration 50, loss = 0.53091432\n",
            "Iteration 51, loss = 0.50818272\n",
            "Iteration 52, loss = 0.52142265\n",
            "Iteration 53, loss = 0.52316444\n",
            "Iteration 54, loss = 0.51336621\n",
            "Iteration 55, loss = 0.50211121\n",
            "Iteration 56, loss = 0.50706637\n",
            "Iteration 57, loss = 0.51090475\n",
            "Iteration 58, loss = 0.51742982\n",
            "Iteration 59, loss = 0.49777890\n",
            "Iteration 60, loss = 0.52204653\n",
            "Iteration 61, loss = 0.51159454\n",
            "Iteration 62, loss = 0.50066023\n",
            "Iteration 63, loss = 0.51742974\n",
            "Iteration 64, loss = 0.51406742\n",
            "Iteration 65, loss = 0.50247382\n",
            "Iteration 66, loss = 0.49753770\n",
            "Iteration 67, loss = 0.48978416\n",
            "Iteration 68, loss = 0.50354328\n",
            "Iteration 69, loss = 0.50775611\n",
            "Iteration 70, loss = 0.49129282\n",
            "Iteration 71, loss = 0.50360282\n",
            "Iteration 72, loss = 0.49976037\n",
            "Iteration 73, loss = 0.48423862\n",
            "Iteration 74, loss = 0.48576500\n",
            "Iteration 75, loss = 0.49537171\n",
            "Iteration 76, loss = 0.49221515\n",
            "Iteration 77, loss = 0.49044572\n",
            "Iteration 78, loss = 0.50199585\n",
            "Iteration 79, loss = 0.48256639\n",
            "Iteration 80, loss = 0.48413388\n",
            "Iteration 81, loss = 0.48382951\n",
            "Iteration 82, loss = 0.50724896\n",
            "Iteration 83, loss = 0.47983653\n",
            "Iteration 84, loss = 0.48627881\n",
            "Iteration 85, loss = 0.48422986\n",
            "Iteration 86, loss = 0.48357601\n",
            "Iteration 87, loss = 0.48405937\n",
            "Iteration 88, loss = 0.48821139\n",
            "Iteration 89, loss = 0.48034749\n",
            "Iteration 90, loss = 0.49559899\n",
            "Iteration 91, loss = 0.50520559\n",
            "Iteration 92, loss = 0.47990934\n",
            "Iteration 93, loss = 0.49071688\n",
            "Iteration 94, loss = 0.50274493\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.05088534\n",
            "Iteration 2, loss = 0.65610392\n",
            "Iteration 3, loss = 0.65417726\n",
            "Iteration 4, loss = 0.64028454\n",
            "Iteration 5, loss = 0.61141103\n",
            "Iteration 6, loss = 0.60539572\n",
            "Iteration 7, loss = 0.59366464\n",
            "Iteration 8, loss = 0.60475928\n",
            "Iteration 9, loss = 0.60893084\n",
            "Iteration 10, loss = 0.60323034\n",
            "Iteration 11, loss = 0.58874250\n",
            "Iteration 12, loss = 0.58422958\n",
            "Iteration 13, loss = 0.59347238\n",
            "Iteration 14, loss = 0.58871943\n",
            "Iteration 15, loss = 0.59246341\n",
            "Iteration 16, loss = 0.58196984\n",
            "Iteration 17, loss = 0.56902921\n",
            "Iteration 18, loss = 0.58830583\n",
            "Iteration 19, loss = 0.59030453\n",
            "Iteration 20, loss = 0.57745456\n",
            "Iteration 21, loss = 0.57366825\n",
            "Iteration 22, loss = 0.58216679\n",
            "Iteration 23, loss = 0.56806943\n",
            "Iteration 24, loss = 0.56551645\n",
            "Iteration 25, loss = 0.56528187\n",
            "Iteration 26, loss = 0.56144602\n",
            "Iteration 27, loss = 0.56465978\n",
            "Iteration 28, loss = 0.55361684\n",
            "Iteration 29, loss = 0.55710469\n",
            "Iteration 30, loss = 0.57021711\n",
            "Iteration 31, loss = 0.57135493\n",
            "Iteration 32, loss = 0.54859471\n",
            "Iteration 33, loss = 0.54657174\n",
            "Iteration 34, loss = 0.54402796\n",
            "Iteration 35, loss = 0.55029874\n",
            "Iteration 36, loss = 0.56316604\n",
            "Iteration 37, loss = 0.55081932\n",
            "Iteration 38, loss = 0.53815280\n",
            "Iteration 39, loss = 0.53589524\n",
            "Iteration 40, loss = 0.51767706\n",
            "Iteration 41, loss = 0.52355349\n",
            "Iteration 42, loss = 0.52697428\n",
            "Iteration 43, loss = 0.52159573\n",
            "Iteration 44, loss = 0.51237727\n",
            "Iteration 45, loss = 0.53390367\n",
            "Iteration 46, loss = 0.52292397\n",
            "Iteration 47, loss = 0.52552525\n",
            "Iteration 48, loss = 0.54477889\n",
            "Iteration 49, loss = 0.53551817\n",
            "Iteration 50, loss = 0.53514985\n",
            "Iteration 51, loss = 0.51007747\n",
            "Iteration 52, loss = 0.51278064\n",
            "Iteration 53, loss = 0.50619597\n",
            "Iteration 54, loss = 0.51105541\n",
            "Iteration 55, loss = 0.51475992\n",
            "Iteration 56, loss = 0.50413083\n",
            "Iteration 57, loss = 0.52586373\n",
            "Iteration 58, loss = 0.51933417\n",
            "Iteration 59, loss = 0.51078513\n",
            "Iteration 60, loss = 0.51117029\n",
            "Iteration 61, loss = 0.51023540\n",
            "Iteration 62, loss = 0.53572533\n",
            "Iteration 63, loss = 0.51833490\n",
            "Iteration 64, loss = 0.49525934\n",
            "Iteration 65, loss = 0.53341787\n",
            "Iteration 66, loss = 0.51244632\n",
            "Iteration 67, loss = 0.49755852\n",
            "Iteration 68, loss = 0.50169706\n",
            "Iteration 69, loss = 0.49980202\n",
            "Iteration 70, loss = 0.50466365\n",
            "Iteration 71, loss = 0.51554691\n",
            "Iteration 72, loss = 0.49041939\n",
            "Iteration 73, loss = 0.48006762\n",
            "Iteration 74, loss = 0.49777073\n",
            "Iteration 75, loss = 0.48863324\n",
            "Iteration 76, loss = 0.49473491\n",
            "Iteration 77, loss = 0.50832364\n",
            "Iteration 78, loss = 0.50254916\n",
            "Iteration 79, loss = 0.49647815\n",
            "Iteration 80, loss = 0.50241122\n",
            "Iteration 81, loss = 0.48953887\n",
            "Iteration 82, loss = 0.51904110\n",
            "Iteration 83, loss = 0.49936033\n",
            "Iteration 84, loss = 0.50350243\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.34773910\n",
            "Iteration 2, loss = 0.65631507\n",
            "Iteration 3, loss = 0.67704191\n",
            "Iteration 4, loss = 0.64188526\n",
            "Iteration 5, loss = 0.63045719\n",
            "Iteration 6, loss = 0.62825371\n",
            "Iteration 7, loss = 0.60659493\n",
            "Iteration 8, loss = 0.60763667\n",
            "Iteration 9, loss = 0.59429296\n",
            "Iteration 10, loss = 0.61210001\n",
            "Iteration 11, loss = 0.59074173\n",
            "Iteration 12, loss = 0.60626736\n",
            "Iteration 13, loss = 0.58437346\n",
            "Iteration 14, loss = 0.59952983\n",
            "Iteration 15, loss = 0.59316149\n",
            "Iteration 16, loss = 0.58262659\n",
            "Iteration 17, loss = 0.57493224\n",
            "Iteration 18, loss = 0.60243681\n",
            "Iteration 19, loss = 0.57751246\n",
            "Iteration 20, loss = 0.58355328\n",
            "Iteration 21, loss = 0.56753243\n",
            "Iteration 22, loss = 0.58003767\n",
            "Iteration 23, loss = 0.55504964\n",
            "Iteration 24, loss = 0.56161098\n",
            "Iteration 25, loss = 0.56478002\n",
            "Iteration 26, loss = 0.56812788\n",
            "Iteration 27, loss = 0.55860802\n",
            "Iteration 28, loss = 0.54766930\n",
            "Iteration 29, loss = 0.56693549\n",
            "Iteration 30, loss = 0.55007738\n",
            "Iteration 31, loss = 0.55194071\n",
            "Iteration 32, loss = 0.57100842\n",
            "Iteration 33, loss = 0.54971743\n",
            "Iteration 34, loss = 0.54377715\n",
            "Iteration 35, loss = 0.55365425\n",
            "Iteration 36, loss = 0.54133629\n",
            "Iteration 37, loss = 0.53165309\n",
            "Iteration 38, loss = 0.53416518\n",
            "Iteration 39, loss = 0.53334328\n",
            "Iteration 40, loss = 0.53155529\n",
            "Iteration 41, loss = 0.55316738\n",
            "Iteration 42, loss = 0.53582350\n",
            "Iteration 43, loss = 0.53665326\n",
            "Iteration 44, loss = 0.53015028\n",
            "Iteration 45, loss = 0.52391679\n",
            "Iteration 46, loss = 0.52386987\n",
            "Iteration 47, loss = 0.52492404\n",
            "Iteration 48, loss = 0.52263399\n",
            "Iteration 49, loss = 0.52214429\n",
            "Iteration 50, loss = 0.51384093\n",
            "Iteration 51, loss = 0.50472699\n",
            "Iteration 52, loss = 0.51887288\n",
            "Iteration 53, loss = 0.52778094\n",
            "Iteration 54, loss = 0.51766728\n",
            "Iteration 55, loss = 0.53288536\n",
            "Iteration 56, loss = 0.52225973\n",
            "Iteration 57, loss = 0.51592015\n",
            "Iteration 58, loss = 0.51254934\n",
            "Iteration 59, loss = 0.51589139\n",
            "Iteration 60, loss = 0.50846813\n",
            "Iteration 61, loss = 0.49604470\n",
            "Iteration 62, loss = 0.51468068\n",
            "Iteration 63, loss = 0.51644164\n",
            "Iteration 64, loss = 0.50430477\n",
            "Iteration 65, loss = 0.51331315\n",
            "Iteration 66, loss = 0.49718538\n",
            "Iteration 67, loss = 0.49872342\n",
            "Iteration 68, loss = 0.50243025\n",
            "Iteration 69, loss = 0.51790259\n",
            "Iteration 70, loss = 0.51061684\n",
            "Iteration 71, loss = 0.50180180\n",
            "Iteration 72, loss = 0.51082685\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.82952467\n",
            "Iteration 2, loss = 0.73307239\n",
            "Iteration 3, loss = 0.63608688\n",
            "Iteration 4, loss = 0.62137180\n",
            "Iteration 5, loss = 0.64349599\n",
            "Iteration 6, loss = 0.62168375\n",
            "Iteration 7, loss = 0.63459498\n",
            "Iteration 8, loss = 0.60061222\n",
            "Iteration 9, loss = 0.59999155\n",
            "Iteration 10, loss = 0.60480086\n",
            "Iteration 11, loss = 0.60786487\n",
            "Iteration 12, loss = 0.59301043\n",
            "Iteration 13, loss = 0.59918907\n",
            "Iteration 14, loss = 0.58433692\n",
            "Iteration 15, loss = 0.58177504\n",
            "Iteration 16, loss = 0.57917002\n",
            "Iteration 17, loss = 0.57572307\n",
            "Iteration 18, loss = 0.60293229\n",
            "Iteration 19, loss = 0.57825342\n",
            "Iteration 20, loss = 0.57868365\n",
            "Iteration 21, loss = 0.57412300\n",
            "Iteration 22, loss = 0.57746109\n",
            "Iteration 23, loss = 0.57797074\n",
            "Iteration 24, loss = 0.57347253\n",
            "Iteration 25, loss = 0.56499913\n",
            "Iteration 26, loss = 0.58000347\n",
            "Iteration 27, loss = 0.56459340\n",
            "Iteration 28, loss = 0.56317171\n",
            "Iteration 29, loss = 0.56718942\n",
            "Iteration 30, loss = 0.55940819\n",
            "Iteration 31, loss = 0.56071867\n",
            "Iteration 32, loss = 0.56270243\n",
            "Iteration 33, loss = 0.54534828\n",
            "Iteration 34, loss = 0.55235090\n",
            "Iteration 35, loss = 0.55800267\n",
            "Iteration 36, loss = 0.54101068\n",
            "Iteration 37, loss = 0.56094011\n",
            "Iteration 38, loss = 0.54947655\n",
            "Iteration 39, loss = 0.54941992\n",
            "Iteration 40, loss = 0.54452481\n",
            "Iteration 41, loss = 0.52917344\n",
            "Iteration 42, loss = 0.54324290\n",
            "Iteration 43, loss = 0.53035735\n",
            "Iteration 44, loss = 0.53633501\n",
            "Iteration 45, loss = 0.52205837\n",
            "Iteration 46, loss = 0.51715551\n",
            "Iteration 47, loss = 0.52551413\n",
            "Iteration 48, loss = 0.53982867\n",
            "Iteration 49, loss = 0.53712764\n",
            "Iteration 50, loss = 0.54101229\n",
            "Iteration 51, loss = 0.52053371\n",
            "Iteration 52, loss = 0.54188641\n",
            "Iteration 53, loss = 0.51269794\n",
            "Iteration 54, loss = 0.53619381\n",
            "Iteration 55, loss = 0.52804992\n",
            "Iteration 56, loss = 0.51696673\n",
            "Iteration 57, loss = 0.52462887\n",
            "Iteration 58, loss = 0.53067983\n",
            "Iteration 59, loss = 0.52883812\n",
            "Iteration 60, loss = 0.51581324\n",
            "Iteration 61, loss = 0.51934934\n",
            "Iteration 62, loss = 0.53806566\n",
            "Iteration 63, loss = 0.51206599\n",
            "Iteration 64, loss = 0.52257471\n",
            "Iteration 65, loss = 0.51688468\n",
            "Iteration 66, loss = 0.51729685\n",
            "Iteration 67, loss = 0.49747311\n",
            "Iteration 68, loss = 0.52132966\n",
            "Iteration 69, loss = 0.52022973\n",
            "Iteration 70, loss = 0.49155186\n",
            "Iteration 71, loss = 0.49553157\n",
            "Iteration 72, loss = 0.51099191\n",
            "Iteration 73, loss = 0.51596237\n",
            "Iteration 74, loss = 0.50613912\n",
            "Iteration 75, loss = 0.50688860\n",
            "Iteration 76, loss = 0.49778363\n",
            "Iteration 77, loss = 0.51143976\n",
            "Iteration 78, loss = 0.52316077\n",
            "Iteration 79, loss = 0.50922677\n",
            "Iteration 80, loss = 0.48992802\n",
            "Iteration 81, loss = 0.49586151\n",
            "Iteration 82, loss = 0.47538550\n",
            "Iteration 83, loss = 0.49132793\n",
            "Iteration 84, loss = 0.51684816\n",
            "Iteration 85, loss = 0.49911495\n",
            "Iteration 86, loss = 0.51861870\n",
            "Iteration 87, loss = 0.50068663\n",
            "Iteration 88, loss = 0.50328552\n",
            "Iteration 89, loss = 0.49918206\n",
            "Iteration 90, loss = 0.50842375\n",
            "Iteration 91, loss = 0.49211792\n",
            "Iteration 92, loss = 0.50809172\n",
            "Iteration 93, loss = 0.49711391\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.94839572\n",
            "Iteration 2, loss = 0.75444528\n",
            "Iteration 3, loss = 0.64080862\n",
            "Iteration 4, loss = 0.65298734\n",
            "Iteration 5, loss = 0.61452136\n",
            "Iteration 6, loss = 0.61651181\n",
            "Iteration 7, loss = 0.60172919\n",
            "Iteration 8, loss = 0.59533535\n",
            "Iteration 9, loss = 0.58669899\n",
            "Iteration 10, loss = 0.61503712\n",
            "Iteration 11, loss = 0.59579489\n",
            "Iteration 12, loss = 0.60215441\n",
            "Iteration 13, loss = 0.59095777\n",
            "Iteration 14, loss = 0.58027085\n",
            "Iteration 15, loss = 0.58303323\n",
            "Iteration 16, loss = 0.57955785\n",
            "Iteration 17, loss = 0.58329214\n",
            "Iteration 18, loss = 0.57932051\n",
            "Iteration 19, loss = 0.57412078\n",
            "Iteration 20, loss = 0.57504696\n",
            "Iteration 21, loss = 0.57446004\n",
            "Iteration 22, loss = 0.57507077\n",
            "Iteration 23, loss = 0.55564944\n",
            "Iteration 24, loss = 0.56910272\n",
            "Iteration 25, loss = 0.56566017\n",
            "Iteration 26, loss = 0.56363476\n",
            "Iteration 27, loss = 0.55278110\n",
            "Iteration 28, loss = 0.56994830\n",
            "Iteration 29, loss = 0.59916019\n",
            "Iteration 30, loss = 0.56565699\n",
            "Iteration 31, loss = 0.55230707\n",
            "Iteration 32, loss = 0.55223489\n",
            "Iteration 33, loss = 0.53695812\n",
            "Iteration 34, loss = 0.54848280\n",
            "Iteration 35, loss = 0.54781933\n",
            "Iteration 36, loss = 0.54059576\n",
            "Iteration 37, loss = 0.55949578\n",
            "Iteration 38, loss = 0.55354732\n",
            "Iteration 39, loss = 0.55614809\n",
            "Iteration 40, loss = 0.54681373\n",
            "Iteration 41, loss = 0.54083545\n",
            "Iteration 42, loss = 0.54086539\n",
            "Iteration 43, loss = 0.53210253\n",
            "Iteration 44, loss = 0.53467912\n",
            "Iteration 45, loss = 0.52905511\n",
            "Iteration 46, loss = 0.53021935\n",
            "Iteration 47, loss = 0.53766498\n",
            "Iteration 48, loss = 0.53655223\n",
            "Iteration 49, loss = 0.53410390\n",
            "Iteration 50, loss = 0.51609847\n",
            "Iteration 51, loss = 0.53246544\n",
            "Iteration 52, loss = 0.51379215\n",
            "Iteration 53, loss = 0.51502427\n",
            "Iteration 54, loss = 0.53729600\n",
            "Iteration 55, loss = 0.52582402\n",
            "Iteration 56, loss = 0.54184238\n",
            "Iteration 57, loss = 0.51370890\n",
            "Iteration 58, loss = 0.51431241\n",
            "Iteration 59, loss = 0.51209161\n",
            "Iteration 60, loss = 0.51220955\n",
            "Iteration 61, loss = 0.52152866\n",
            "Iteration 62, loss = 0.50931938\n",
            "Iteration 63, loss = 0.50310469\n",
            "Iteration 64, loss = 0.52634180\n",
            "Iteration 65, loss = 0.51842039\n",
            "Iteration 66, loss = 0.51018400\n",
            "Iteration 67, loss = 0.52613713\n",
            "Iteration 68, loss = 0.49936386\n",
            "Iteration 69, loss = 0.50474533\n",
            "Iteration 70, loss = 0.50285388\n",
            "Iteration 71, loss = 0.50747423\n",
            "Iteration 72, loss = 0.51485057\n",
            "Iteration 73, loss = 0.50799953\n",
            "Iteration 74, loss = 0.49644592\n",
            "Iteration 75, loss = 0.51513460\n",
            "Iteration 76, loss = 0.49886969\n",
            "Iteration 77, loss = 0.48943037\n",
            "Iteration 78, loss = 0.49954207\n",
            "Iteration 79, loss = 0.51427299\n",
            "Iteration 80, loss = 0.51005281\n",
            "Iteration 81, loss = 0.50236161\n",
            "Iteration 82, loss = 0.49006643\n",
            "Iteration 83, loss = 0.51006100\n",
            "Iteration 84, loss = 0.51520322\n",
            "Iteration 85, loss = 0.49848943\n",
            "Iteration 86, loss = 0.49615611\n",
            "Iteration 87, loss = 0.50014266\n",
            "Iteration 88, loss = 0.49451581\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62065915\n",
            "Iteration 2, loss = 0.60805255\n",
            "Iteration 3, loss = 0.60103499\n",
            "Iteration 4, loss = 0.59611426\n",
            "Iteration 5, loss = 0.58659187\n",
            "Iteration 6, loss = 0.58509658\n",
            "Iteration 7, loss = 0.56750771\n",
            "Iteration 8, loss = 0.55527525\n",
            "Iteration 9, loss = 0.54614092\n",
            "Iteration 10, loss = 0.52317066\n",
            "Iteration 11, loss = 0.51615873\n",
            "Iteration 12, loss = 0.51355525\n",
            "Iteration 13, loss = 0.48283829\n",
            "Iteration 14, loss = 0.47363435\n",
            "Iteration 15, loss = 0.46306044\n",
            "Iteration 16, loss = 0.46268777\n",
            "Iteration 17, loss = 0.45304254\n",
            "Iteration 18, loss = 0.45781161\n",
            "Iteration 19, loss = 0.46000829\n",
            "Iteration 20, loss = 0.44097057\n",
            "Iteration 21, loss = 0.45220810\n",
            "Iteration 22, loss = 0.44100624\n",
            "Iteration 23, loss = 0.43908183\n",
            "Iteration 24, loss = 0.44926875\n",
            "Iteration 25, loss = 0.43419075\n",
            "Iteration 26, loss = 0.43028565\n",
            "Iteration 27, loss = 0.43153328\n",
            "Iteration 28, loss = 0.44037546\n",
            "Iteration 29, loss = 0.43605663\n",
            "Iteration 30, loss = 0.42933320\n",
            "Iteration 31, loss = 0.43278335\n",
            "Iteration 32, loss = 0.42853973\n",
            "Iteration 33, loss = 0.43691247\n",
            "Iteration 34, loss = 0.43445048\n",
            "Iteration 35, loss = 0.42050373\n",
            "Iteration 36, loss = 0.41980790\n",
            "Iteration 37, loss = 0.42149418\n",
            "Iteration 38, loss = 0.42018199\n",
            "Iteration 39, loss = 0.42456854\n",
            "Iteration 40, loss = 0.41884423\n",
            "Iteration 41, loss = 0.41595256\n",
            "Iteration 42, loss = 0.42310176\n",
            "Iteration 43, loss = 0.41819191\n",
            "Iteration 44, loss = 0.41357860\n",
            "Iteration 45, loss = 0.42009864\n",
            "Iteration 46, loss = 0.41493988\n",
            "Iteration 47, loss = 0.41889133\n",
            "Iteration 48, loss = 0.40969042\n",
            "Iteration 49, loss = 0.41195140\n",
            "Iteration 50, loss = 0.41199369\n",
            "Iteration 51, loss = 0.41231907\n",
            "Iteration 52, loss = 0.41042731\n",
            "Iteration 53, loss = 0.42719509\n",
            "Iteration 54, loss = 0.40932617\n",
            "Iteration 55, loss = 0.41125233\n",
            "Iteration 56, loss = 0.41355605\n",
            "Iteration 57, loss = 0.40803100\n",
            "Iteration 58, loss = 0.40436836\n",
            "Iteration 59, loss = 0.42303332\n",
            "Iteration 60, loss = 0.40009613\n",
            "Iteration 61, loss = 0.40887549\n",
            "Iteration 62, loss = 0.40831592\n",
            "Iteration 63, loss = 0.40362828\n",
            "Iteration 64, loss = 0.40610435\n",
            "Iteration 65, loss = 0.41619452\n",
            "Iteration 66, loss = 0.40886555\n",
            "Iteration 67, loss = 0.40068252\n",
            "Iteration 68, loss = 0.39896417\n",
            "Iteration 69, loss = 0.40330315\n",
            "Iteration 70, loss = 0.40173829\n",
            "Iteration 71, loss = 0.39973174\n",
            "Iteration 72, loss = 0.41085368\n",
            "Iteration 73, loss = 0.40512264\n",
            "Iteration 74, loss = 0.40212710\n",
            "Iteration 75, loss = 0.41291184\n",
            "Iteration 76, loss = 0.40108007\n",
            "Iteration 77, loss = 0.40745015\n",
            "Iteration 78, loss = 0.39847260\n",
            "Iteration 79, loss = 0.40012968\n",
            "Iteration 80, loss = 0.39609162\n",
            "Iteration 81, loss = 0.39896817\n",
            "Iteration 82, loss = 0.39113964\n",
            "Iteration 83, loss = 0.39534759\n",
            "Iteration 84, loss = 0.39853514\n",
            "Iteration 85, loss = 0.40220347\n",
            "Iteration 86, loss = 0.40186509\n",
            "Iteration 87, loss = 0.40001478\n",
            "Iteration 88, loss = 0.39592597\n",
            "Iteration 89, loss = 0.39388168\n",
            "Iteration 90, loss = 0.39384564\n",
            "Iteration 91, loss = 0.39455049\n",
            "Iteration 92, loss = 0.39355793\n",
            "Iteration 93, loss = 0.39131961\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "RandomizedSearchCV took 846.82 seconds for 1000 candidates parameter settings \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "start = time()\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
        "      \" parameter settings \\n\\n\" % ((time() - start), 1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 329,
      "metadata": {
        "id": "ScIsAryGjFEw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "cb279067-b2b8-485b-ec4d-d98f1e2f75a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='tanh', batch_size=10, hidden_layer_sizes=(25, 25),\n",
              "              max_iter=400, verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, batch_size=10, hidden_layer_sizes=(25, 25),\n",
              "              max_iter=400, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, batch_size=10, hidden_layer_sizes=(25, 25),\n",
              "              max_iter=400, verbose=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 329
        }
      ],
      "source": [
        "random_search.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = random_search.best_estimator_"
      ],
      "metadata": {
        "id": "KDQrJ-3AeUNG"
      },
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnwrifU-egWe",
        "outputId": "eb2060d3-1cce-4dba-c601-ae4984cd1b65"
      },
      "execution_count": 331,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8507265521796565"
            ]
          },
          "metadata": {},
          "execution_count": 331
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_best_model = random_search.best_estimator_.predict(X_val)"
      ],
      "metadata": {
        "id": "ScJkNdvSeku9"
      },
      "execution_count": 332,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_val, y_pred_best_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueaybKyrfmZm",
        "outputId": "3d6789ee-c29d-4791-adbc-f94fd1c72a1f"
      },
      "execution_count": 333,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7910447761194029"
            ]
          },
          "metadata": {},
          "execution_count": 333
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnf_matrix_best_score = confusion_matrix(y_val, y_pred_best_model)"
      ],
      "metadata": {
        "id": "tEILBleAfuT5"
      },
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnf_matrix_best_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaZehLsCf9vW",
        "outputId": "767b2e0e-8753-4331-cb2d-4b4deade958e"
      },
      "execution_count": 335,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[70,  9],\n",
              "       [19, 36]])"
            ]
          },
          "metadata": {},
          "execution_count": 335
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnf_table = pd.DataFrame(data=cnf_matrix_best_score, index=['Survived', 'Died'], columns=['Survived', 'Died'])"
      ],
      "metadata": {
        "id": "VInRx4jBgAkR"
      },
      "execution_count": 336,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cnf_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYm1Kptxgj6-",
        "outputId": "4ba80cf1-f1e9-4900-ad71-3ff9bf96dd26"
      },
      "execution_count": 337,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Survived  Died\n",
            "Survived        70     9\n",
            "Died            19    36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_val, y_pred_best_model, target_names=['Survived', 'Died']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTIlLPFPgvaH",
        "outputId": "45660679-e3c3-4e36-de5d-957d8690092b"
      },
      "execution_count": 338,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Survived       0.79      0.89      0.83        79\n",
            "        Died       0.80      0.65      0.72        55\n",
            "\n",
            "    accuracy                           0.79       134\n",
            "   macro avg       0.79      0.77      0.78       134\n",
            "weighted avg       0.79      0.79      0.79       134\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ECDzOOR6V5C",
        "outputId": "bc61cbe0-143e-42a4-fa1a-580607ef6fac"
      },
      "execution_count": 339,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "423    0\n",
              "177    0\n",
              "305    1\n",
              "292    0\n",
              "889    1\n",
              "      ..\n",
              "812    0\n",
              "297    0\n",
              "97     1\n",
              "212    0\n",
              "390    1\n",
              "Name: Survived, Length: 72, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 339
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test.head(5)\n",
        "submissions_pred = model.predict(test)"
      ],
      "metadata": {
        "id": "BfeRuDOY74ka"
      },
      "execution_count": 349,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(test_ids), len(submissions_pred))\n",
        "sub_df = pd.DataFrame({\"PassengerId\": test_ids.values, \"Survived\": submissions_pred})\n"
      ],
      "metadata": {
        "id": "fqRa6RVbIlT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f40b453-eebc-4457-caf4-99fbeb7f4ead"
      },
      "execution_count": 366,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "418 418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub_df.to_csv('titanic_submission.csv', index=False)"
      ],
      "metadata": {
        "id": "UZoWp2VNJRZS"
      },
      "execution_count": 367,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wQClWcSgNpJC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}