{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s15ktC_bHLX2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from time import time\n",
        "from scipy.stats import randint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zTHZvcBpHN8x"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "0VDK0N1KHV4n",
        "outputId": "987b8483-50c6-4ca2-b717-e33eb8f19b2c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   PassengerId  Survived  Pclass  \\\n",
              "0            1         0       3   \n",
              "1            2         1       1   \n",
              "2            3         1       3   \n",
              "3            4         1       1   \n",
              "4            5         0       3   \n",
              "5            6         0       3   \n",
              "6            7         0       1   \n",
              "7            8         0       3   \n",
              "8            9         1       3   \n",
              "9           10         1       2   \n",
              "\n",
              "                                                Name     Sex   Age  SibSp  \\\n",
              "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
              "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
              "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
              "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
              "4                           Allen, Mr. William Henry    male  35.0      0   \n",
              "5                                   Moran, Mr. James    male   NaN      0   \n",
              "6                            McCarthy, Mr. Timothy J    male  54.0      0   \n",
              "7                     Palsson, Master. Gosta Leonard    male   2.0      3   \n",
              "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
              "9                Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
              "\n",
              "   Parch            Ticket     Fare Cabin Embarked  \n",
              "0      0         A/5 21171   7.2500   NaN        S  \n",
              "1      0          PC 17599  71.2833   C85        C  \n",
              "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
              "3      0            113803  53.1000  C123        S  \n",
              "4      0            373450   8.0500   NaN        S  \n",
              "5      0            330877   8.4583   NaN        Q  \n",
              "6      0             17463  51.8625   E46        S  \n",
              "7      1            349909  21.0750   NaN        S  \n",
              "8      2            347742  11.1333   NaN        S  \n",
              "9      0            237736  30.0708   NaN        C  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e5fa5ae1-da15-4b9e-a7a6-f013a24bb6eb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113803</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>C123</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>373450</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Moran, Mr. James</td>\n",
              "      <td>male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>330877</td>\n",
              "      <td>8.4583</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>McCarthy, Mr. Timothy J</td>\n",
              "      <td>male</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>17463</td>\n",
              "      <td>51.8625</td>\n",
              "      <td>E46</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Palsson, Master. Gosta Leonard</td>\n",
              "      <td>male</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>349909</td>\n",
              "      <td>21.0750</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
              "      <td>female</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>347742</td>\n",
              "      <td>11.1333</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
              "      <td>female</td>\n",
              "      <td>14.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>237736</td>\n",
              "      <td>30.0708</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e5fa5ae1-da15-4b9e-a7a6-f013a24bb6eb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e5fa5ae1-da15-4b9e-a7a6-f013a24bb6eb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e5fa5ae1-da15-4b9e-a7a6-f013a24bb6eb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-be3fd64f-1925-4c20-b81a-ddab34ef3b1a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-be3fd64f-1925-4c20-b81a-ddab34ef3b1a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-be3fd64f-1925-4c20-b81a-ddab34ef3b1a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eLv_0cgqHXbK"
      },
      "outputs": [],
      "source": [
        "df = df.drop(['Ticket', 'Name', 'Cabin', 'Embarked'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmgAXMLTII2G",
        "outputId": "2e2aa96b-355e-4fc7-a5bc-a8840fb53476"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(891, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hcrghqbrIv1Q"
      },
      "outputs": [],
      "source": [
        "df = df.dropna(subset='Age')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQNu-sj9JXuB",
        "outputId": "06924d70-e70e-472a-991f-c6500181eb6f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(714, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HElMCoy1JYqE",
        "outputId": "6d160428-c81f-4fce-fd88-c6d2c5141e52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are no NaN values in the 'Age' column.\n"
          ]
        }
      ],
      "source": [
        "if df['Age'].isnull().any():\n",
        "    print(\"There are NaN values in the 'Age' column.\")\n",
        "else:\n",
        "    print(\"There are no NaN values in the 'Age' column.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uflzsAd5J3ZC"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "95k1WDqbKXu1"
      },
      "outputs": [],
      "source": [
        "df['sex_encoded'] = label_encoder.fit_transform(df['Sex'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-fNrPOoWKimb",
        "outputId": "76be63a6-7f05-4703-be16-9dcedf7fb424"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   PassengerId  Survived  Pclass     Sex   Age  SibSp  Parch     Fare  \\\n",
              "0            1         0       3    male  22.0      1      0   7.2500   \n",
              "1            2         1       1  female  38.0      1      0  71.2833   \n",
              "2            3         1       3  female  26.0      0      0   7.9250   \n",
              "3            4         1       1  female  35.0      1      0  53.1000   \n",
              "4            5         0       3    male  35.0      0      0   8.0500   \n",
              "\n",
              "   sex_encoded  \n",
              "0            1  \n",
              "1            0  \n",
              "2            0  \n",
              "3            0  \n",
              "4            1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-652652b9-a6cb-4d19-8c95-67fc8ac781fc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>sex_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-652652b9-a6cb-4d19-8c95-67fc8ac781fc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-652652b9-a6cb-4d19-8c95-67fc8ac781fc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-652652b9-a6cb-4d19-8c95-67fc8ac781fc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8269f613-a791-4a40-acf8-ae53bf87133a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8269f613-a791-4a40-acf8-ae53bf87133a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8269f613-a791-4a40-acf8-ae53bf87133a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "evnTwPgoKmj7"
      },
      "outputs": [],
      "source": [
        "df = df.drop('Sex', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "PDdsR5HtK8aT",
        "outputId": "ab30dcd0-bf3d-497c-e673-97efc98465d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   PassengerId  Survived  Pclass   Age  SibSp  Parch     Fare  sex_encoded\n",
              "0            1         0       3  22.0      1      0   7.2500            1\n",
              "1            2         1       1  38.0      1      0  71.2833            0\n",
              "2            3         1       3  26.0      0      0   7.9250            0\n",
              "3            4         1       1  35.0      1      0  53.1000            0\n",
              "4            5         0       3  35.0      0      0   8.0500            1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e71805db-bcdd-4423-bece-4f3b6974661e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>sex_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e71805db-bcdd-4423-bece-4f3b6974661e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e71805db-bcdd-4423-bece-4f3b6974661e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e71805db-bcdd-4423-bece-4f3b6974661e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ba1c21df-58e5-4633-b2c6-074dc0b42430\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ba1c21df-58e5-4633-b2c6-074dc0b42430')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ba1c21df-58e5-4633-b2c6-074dc0b42430 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VI0RSX2sLLV8"
      },
      "outputs": [],
      "source": [
        "X = df.drop(['Survived', 'PassengerId'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "L7bMXkHLLyTW"
      },
      "outputs": [],
      "source": [
        "y = df['Survived']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "l3RFYtKnOTeN"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WLS72D_POspp"
      },
      "outputs": [],
      "source": [
        "column_types = X_train.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAgq1qSiTAyu",
        "outputId": "a356b998-02df-4ed7-9322-435a53f09d0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pclass           int64\n",
            "Age            float64\n",
            "SibSp            int64\n",
            "Parch            int64\n",
            "Fare           float64\n",
            "sex_encoded      int64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(column_types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "b9w16pw_TM04",
        "outputId": "9a3b0feb-3116-4570-a49c-0f6a3bf5cb13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Pclass   Age  SibSp  Parch      Fare  sex_encoded\n",
              "417       2  18.0      0      2   13.0000            0\n",
              "558       1  39.0      1      1   79.6500            0\n",
              "195       1  58.0      0      0  146.5208            0\n",
              "681       1  27.0      0      0   76.7292            1\n",
              "677       3  18.0      0      0    9.8417            0\n",
              "..      ...   ...    ...    ...       ...          ...\n",
              "883       2  28.0      0      0   10.5000            1\n",
              "238       2  19.0      0      0   10.5000            1\n",
              "789       1  46.0      0      0   79.2000            1\n",
              "704       3  26.0      1      0    7.8542            1\n",
              "856       1  45.0      1      1  164.8667            0\n",
              "\n",
              "[642 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-caea601a-fc55-4b62-9892-a9245668497c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>sex_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>417</th>\n",
              "      <td>2</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>13.0000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>558</th>\n",
              "      <td>1</td>\n",
              "      <td>39.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>79.6500</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>1</td>\n",
              "      <td>58.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>146.5208</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>681</th>\n",
              "      <td>1</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>76.7292</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>677</th>\n",
              "      <td>3</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9.8417</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>883</th>\n",
              "      <td>2</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10.5000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>238</th>\n",
              "      <td>2</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10.5000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>789</th>\n",
              "      <td>1</td>\n",
              "      <td>46.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>79.2000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>704</th>\n",
              "      <td>3</td>\n",
              "      <td>26.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.8542</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>856</th>\n",
              "      <td>1</td>\n",
              "      <td>45.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>164.8667</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>642 rows  6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-caea601a-fc55-4b62-9892-a9245668497c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-caea601a-fc55-4b62-9892-a9245668497c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-caea601a-fc55-4b62-9892-a9245668497c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d8cc40c5-f312-4c84-926c-9d2b807e4e91\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d8cc40c5-f312-4c84-926c-9d2b807e4e91')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d8cc40c5-f312-4c84-926c-9d2b807e4e91 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "X_train\n",
        "#X_train.shape\n",
        "#y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "aWK9TqZXXvg9"
      },
      "outputs": [],
      "source": [
        "nn = MLPClassifier(verbose=True, max_iter=1000, hidden_layer_sizes=(100, 100), solver='sgd')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "045SNlKPZdnT",
        "outputId": "67b73702-5eaa-44e0-d727-ed99f698c173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.47498851\n",
            "Iteration 2, loss = 0.81229594\n",
            "Iteration 3, loss = 0.66510227\n",
            "Iteration 4, loss = 0.66982554\n",
            "Iteration 5, loss = 0.80679065\n",
            "Iteration 6, loss = 0.66546109\n",
            "Iteration 7, loss = 0.61776872\n",
            "Iteration 8, loss = 0.62656717\n",
            "Iteration 9, loss = 0.62659781\n",
            "Iteration 10, loss = 0.61546149\n",
            "Iteration 11, loss = 0.60479994\n",
            "Iteration 12, loss = 0.60915940\n",
            "Iteration 13, loss = 0.59465768\n",
            "Iteration 14, loss = 0.59355631\n",
            "Iteration 15, loss = 0.60547757\n",
            "Iteration 16, loss = 0.59492479\n",
            "Iteration 17, loss = 0.60572296\n",
            "Iteration 18, loss = 0.59749689\n",
            "Iteration 19, loss = 0.69143338\n",
            "Iteration 20, loss = 0.59393668\n",
            "Iteration 21, loss = 0.61935201\n",
            "Iteration 22, loss = 0.59019795\n",
            "Iteration 23, loss = 0.70271871\n",
            "Iteration 24, loss = 0.58917462\n",
            "Iteration 25, loss = 0.61847674\n",
            "Iteration 26, loss = 0.58160753\n",
            "Iteration 27, loss = 0.59868297\n",
            "Iteration 28, loss = 0.58546344\n",
            "Iteration 29, loss = 0.63098978\n",
            "Iteration 30, loss = 0.57795452\n",
            "Iteration 31, loss = 0.59019732\n",
            "Iteration 32, loss = 0.62813456\n",
            "Iteration 33, loss = 0.65899709\n",
            "Iteration 34, loss = 0.59050574\n",
            "Iteration 35, loss = 0.72508453\n",
            "Iteration 36, loss = 0.60116430\n",
            "Iteration 37, loss = 0.58382748\n",
            "Iteration 38, loss = 0.60345451\n",
            "Iteration 39, loss = 0.58165069\n",
            "Iteration 40, loss = 0.68497357\n",
            "Iteration 41, loss = 0.57550285\n",
            "Iteration 42, loss = 0.56981245\n",
            "Iteration 43, loss = 0.57708183\n",
            "Iteration 44, loss = 0.65392173\n",
            "Iteration 45, loss = 0.57457925\n",
            "Iteration 46, loss = 0.59326204\n",
            "Iteration 47, loss = 0.57265670\n",
            "Iteration 48, loss = 0.57740428\n",
            "Iteration 49, loss = 0.59410325\n",
            "Iteration 50, loss = 0.56315492\n",
            "Iteration 51, loss = 0.57031475\n",
            "Iteration 52, loss = 0.57685094\n",
            "Iteration 53, loss = 0.57595066\n",
            "Iteration 54, loss = 0.56280859\n",
            "Iteration 55, loss = 0.56202555\n",
            "Iteration 56, loss = 0.60695213\n",
            "Iteration 57, loss = 0.57349463\n",
            "Iteration 58, loss = 0.55817802\n",
            "Iteration 59, loss = 0.62836495\n",
            "Iteration 60, loss = 0.56476062\n",
            "Iteration 61, loss = 0.58230631\n",
            "Iteration 62, loss = 0.55770509\n",
            "Iteration 63, loss = 0.55930687\n",
            "Iteration 64, loss = 0.65070546\n",
            "Iteration 65, loss = 0.58064012\n",
            "Iteration 66, loss = 0.60009936\n",
            "Iteration 67, loss = 0.56940080\n",
            "Iteration 68, loss = 0.56427696\n",
            "Iteration 69, loss = 0.55650031\n",
            "Iteration 70, loss = 0.55401568\n",
            "Iteration 71, loss = 0.56160982\n",
            "Iteration 72, loss = 0.56988772\n",
            "Iteration 73, loss = 0.55975720\n",
            "Iteration 74, loss = 0.60010976\n",
            "Iteration 75, loss = 0.61332845\n",
            "Iteration 76, loss = 0.57613935\n",
            "Iteration 77, loss = 0.56626360\n",
            "Iteration 78, loss = 0.56951370\n",
            "Iteration 79, loss = 0.59720861\n",
            "Iteration 80, loss = 0.55731639\n",
            "Iteration 81, loss = 0.56843194\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, solver='sgd',\n",
              "              verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, solver=&#x27;sgd&#x27;,\n",
              "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, solver=&#x27;sgd&#x27;,\n",
              "              verbose=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "nn.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "oBivNUtbZmFM"
      },
      "outputs": [],
      "source": [
        "forecast = nn.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShxjPEWNd8e1",
        "outputId": "27fddfd0-b3b9-4e36-8cfb-727c6447b15a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
              "       0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
              "       1, 0, 1, 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "forecast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJTrREBpeA6Z",
        "outputId": "16ae1f10-1e38-4463-d72e-272f25c6c35b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6944444444444444"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "accuracy_score(y_test, forecast)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "5N-ULfqNeZK2"
      },
      "outputs": [],
      "source": [
        "cnf_matrix = confusion_matrix(y_test, forecast)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZF741TVe8mz",
        "outputId": "85c28690-d16b-4845-9c02-0e1034b68424"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[30, 14],\n",
              "       [ 8, 20]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "cnf_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "xo2elIG2fhcc"
      },
      "outputs": [],
      "source": [
        "nn_params = {\n",
        "    'activation': ['relu', 'logistic', 'tanh'],\n",
        "    'solver': ['adam', 'sgd'],\n",
        "    'batch_size': [10, 60],\n",
        "    'hidden_layer_sizes': [(25, 25), (50, 50), (100, 100), (200, 200)],\n",
        "    'max_iter': [200, 400]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "hpj9nihJiz6p"
      },
      "outputs": [],
      "source": [
        "random_search = RandomizedSearchCV(\n",
        "    nn,\n",
        "    param_distributions = nn_params,\n",
        "    cv = 5,\n",
        "    random_state = 0,\n",
        "    n_iter=50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uHUqXSVjAg9",
        "outputId": "e78e6ff7-b9c3-4bfc-aa28-c27a8de86155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.80233716\n",
            "Iteration 2, loss = 0.66988578\n",
            "Iteration 3, loss = 0.65243040\n",
            "Iteration 4, loss = 0.60123486\n",
            "Iteration 5, loss = 0.58329074\n",
            "Iteration 6, loss = 0.56948134\n",
            "Iteration 7, loss = 0.60425770\n",
            "Iteration 8, loss = 0.57527111\n",
            "Iteration 9, loss = 0.61105256\n",
            "Iteration 10, loss = 0.57643794\n",
            "Iteration 11, loss = 0.58757812\n",
            "Iteration 12, loss = 0.54251599\n",
            "Iteration 13, loss = 0.54865724\n",
            "Iteration 14, loss = 0.53316532\n",
            "Iteration 15, loss = 0.53122171\n",
            "Iteration 16, loss = 0.53782139\n",
            "Iteration 17, loss = 0.51844615\n",
            "Iteration 18, loss = 0.50692988\n",
            "Iteration 19, loss = 0.51413895\n",
            "Iteration 20, loss = 0.50859186\n",
            "Iteration 21, loss = 0.48631185\n",
            "Iteration 22, loss = 0.48604033\n",
            "Iteration 23, loss = 0.48372535\n",
            "Iteration 24, loss = 0.49075292\n",
            "Iteration 25, loss = 0.46638966\n",
            "Iteration 26, loss = 0.47126593\n",
            "Iteration 27, loss = 0.54296083\n",
            "Iteration 28, loss = 0.49423544\n",
            "Iteration 29, loss = 0.45685261\n",
            "Iteration 30, loss = 0.47030994\n",
            "Iteration 31, loss = 0.45977735\n",
            "Iteration 32, loss = 0.47032181\n",
            "Iteration 33, loss = 0.46106263\n",
            "Iteration 34, loss = 0.47544257\n",
            "Iteration 35, loss = 0.44803845\n",
            "Iteration 36, loss = 0.47830045\n",
            "Iteration 37, loss = 0.47524967\n",
            "Iteration 38, loss = 0.44404329\n",
            "Iteration 39, loss = 0.45132112\n",
            "Iteration 40, loss = 0.46727807\n",
            "Iteration 41, loss = 0.44004647\n",
            "Iteration 42, loss = 0.45300554\n",
            "Iteration 43, loss = 0.48095479\n",
            "Iteration 44, loss = 0.49226739\n",
            "Iteration 45, loss = 0.46455061\n",
            "Iteration 46, loss = 0.42950134\n",
            "Iteration 47, loss = 0.43339536\n",
            "Iteration 48, loss = 0.43659065\n",
            "Iteration 49, loss = 0.42504503\n",
            "Iteration 50, loss = 0.47863831\n",
            "Iteration 51, loss = 0.50565909\n",
            "Iteration 52, loss = 0.51937959\n",
            "Iteration 53, loss = 0.49533725\n",
            "Iteration 54, loss = 0.46564233\n",
            "Iteration 55, loss = 0.47638764\n",
            "Iteration 56, loss = 0.45035162\n",
            "Iteration 57, loss = 0.43714492\n",
            "Iteration 58, loss = 0.42505303\n",
            "Iteration 59, loss = 0.43269840\n",
            "Iteration 60, loss = 0.42296160\n",
            "Iteration 61, loss = 0.43681488\n",
            "Iteration 62, loss = 0.51027719\n",
            "Iteration 63, loss = 0.46295785\n",
            "Iteration 64, loss = 0.46738748\n",
            "Iteration 65, loss = 0.42499213\n",
            "Iteration 66, loss = 0.42064613\n",
            "Iteration 67, loss = 0.42119247\n",
            "Iteration 68, loss = 0.40850449\n",
            "Iteration 69, loss = 0.41266045\n",
            "Iteration 70, loss = 0.42431829\n",
            "Iteration 71, loss = 0.42089217\n",
            "Iteration 72, loss = 0.41457682\n",
            "Iteration 73, loss = 0.42506258\n",
            "Iteration 74, loss = 0.41798274\n",
            "Iteration 75, loss = 0.41911918\n",
            "Iteration 76, loss = 0.40665019\n",
            "Iteration 77, loss = 0.41034924\n",
            "Iteration 78, loss = 0.40998883\n",
            "Iteration 79, loss = 0.41215574\n",
            "Iteration 80, loss = 0.43338008\n",
            "Iteration 81, loss = 0.42757022\n",
            "Iteration 82, loss = 0.41160709\n",
            "Iteration 83, loss = 0.41420228\n",
            "Iteration 84, loss = 0.44464576\n",
            "Iteration 85, loss = 0.48617462\n",
            "Iteration 86, loss = 0.46312696\n",
            "Iteration 87, loss = 0.42443637\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.94780262\n",
            "Iteration 2, loss = 0.78413610\n",
            "Iteration 3, loss = 0.62257407\n",
            "Iteration 4, loss = 0.60127552\n",
            "Iteration 5, loss = 0.61423667\n",
            "Iteration 6, loss = 0.60479928\n",
            "Iteration 7, loss = 0.58087595\n",
            "Iteration 8, loss = 0.58532990\n",
            "Iteration 9, loss = 0.55945121\n",
            "Iteration 10, loss = 0.55461801\n",
            "Iteration 11, loss = 0.57060016\n",
            "Iteration 12, loss = 0.60922199\n",
            "Iteration 13, loss = 0.53656453\n",
            "Iteration 14, loss = 0.53022837\n",
            "Iteration 15, loss = 0.55920310\n",
            "Iteration 16, loss = 0.57509549\n",
            "Iteration 17, loss = 0.57038316\n",
            "Iteration 18, loss = 0.51184600\n",
            "Iteration 19, loss = 0.50193470\n",
            "Iteration 20, loss = 0.49761313\n",
            "Iteration 21, loss = 0.50580733\n",
            "Iteration 22, loss = 0.52217101\n",
            "Iteration 23, loss = 0.49322210\n",
            "Iteration 24, loss = 0.49641502\n",
            "Iteration 25, loss = 0.51003314\n",
            "Iteration 26, loss = 0.49363044\n",
            "Iteration 27, loss = 0.46413734\n",
            "Iteration 28, loss = 0.47623225\n",
            "Iteration 29, loss = 0.46360292\n",
            "Iteration 30, loss = 0.48566727\n",
            "Iteration 31, loss = 0.45687344\n",
            "Iteration 32, loss = 0.49835043\n",
            "Iteration 33, loss = 0.50557799\n",
            "Iteration 34, loss = 0.46008694\n",
            "Iteration 35, loss = 0.46182098\n",
            "Iteration 36, loss = 0.46638316\n",
            "Iteration 37, loss = 0.46302647\n",
            "Iteration 38, loss = 0.49459051\n",
            "Iteration 39, loss = 0.48007829\n",
            "Iteration 40, loss = 0.47418090\n",
            "Iteration 41, loss = 0.46051525\n",
            "Iteration 42, loss = 0.46251527\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.31988294\n",
            "Iteration 2, loss = 0.77009836\n",
            "Iteration 3, loss = 0.74312966\n",
            "Iteration 4, loss = 0.62763931\n",
            "Iteration 5, loss = 0.59033029\n",
            "Iteration 6, loss = 0.60613096\n",
            "Iteration 7, loss = 0.58917801\n",
            "Iteration 8, loss = 0.57290687\n",
            "Iteration 9, loss = 0.56307200\n",
            "Iteration 10, loss = 0.56524418\n",
            "Iteration 11, loss = 0.54998247\n",
            "Iteration 12, loss = 0.54574829\n",
            "Iteration 13, loss = 0.54837475\n",
            "Iteration 14, loss = 0.54079659\n",
            "Iteration 15, loss = 0.53448653\n",
            "Iteration 16, loss = 0.53569077\n",
            "Iteration 17, loss = 0.52274799\n",
            "Iteration 18, loss = 0.53906501\n",
            "Iteration 19, loss = 0.52739519\n",
            "Iteration 20, loss = 0.54534166\n",
            "Iteration 21, loss = 0.56672107\n",
            "Iteration 22, loss = 0.52053308\n",
            "Iteration 23, loss = 0.51475927\n",
            "Iteration 24, loss = 0.49561158\n",
            "Iteration 25, loss = 0.54480035\n",
            "Iteration 26, loss = 0.50693686\n",
            "Iteration 27, loss = 0.49311874\n",
            "Iteration 28, loss = 0.49308595\n",
            "Iteration 29, loss = 0.48648554\n",
            "Iteration 30, loss = 0.48270682\n",
            "Iteration 31, loss = 0.48384173\n",
            "Iteration 32, loss = 0.47875596\n",
            "Iteration 33, loss = 0.47266384\n",
            "Iteration 34, loss = 0.46687857\n",
            "Iteration 35, loss = 0.46554392\n",
            "Iteration 36, loss = 0.49912220\n",
            "Iteration 37, loss = 0.52644713\n",
            "Iteration 38, loss = 0.53211849\n",
            "Iteration 39, loss = 0.51168500\n",
            "Iteration 40, loss = 0.47753277\n",
            "Iteration 41, loss = 0.46985729\n",
            "Iteration 42, loss = 0.46233518\n",
            "Iteration 43, loss = 0.44281361\n",
            "Iteration 44, loss = 0.43656226\n",
            "Iteration 45, loss = 0.46421072\n",
            "Iteration 46, loss = 0.43742335\n",
            "Iteration 47, loss = 0.43556388\n",
            "Iteration 48, loss = 0.43471929\n",
            "Iteration 49, loss = 0.43339058\n",
            "Iteration 50, loss = 0.43268720\n",
            "Iteration 51, loss = 0.43244784\n",
            "Iteration 52, loss = 0.47372211\n",
            "Iteration 53, loss = 0.50187328\n",
            "Iteration 54, loss = 0.46866039\n",
            "Iteration 55, loss = 0.46814738\n",
            "Iteration 56, loss = 0.46701238\n",
            "Iteration 57, loss = 0.43500618\n",
            "Iteration 58, loss = 0.42253886\n",
            "Iteration 59, loss = 0.41229922\n",
            "Iteration 60, loss = 0.41952619\n",
            "Iteration 61, loss = 0.42166390\n",
            "Iteration 62, loss = 0.41494612\n",
            "Iteration 63, loss = 0.40713948\n",
            "Iteration 64, loss = 0.40597684\n",
            "Iteration 65, loss = 0.40816844\n",
            "Iteration 66, loss = 0.41856591\n",
            "Iteration 67, loss = 0.41551746\n",
            "Iteration 68, loss = 0.43622918\n",
            "Iteration 69, loss = 0.44350225\n",
            "Iteration 70, loss = 0.41427164\n",
            "Iteration 71, loss = 0.42206439\n",
            "Iteration 72, loss = 0.40755308\n",
            "Iteration 73, loss = 0.41093608\n",
            "Iteration 74, loss = 0.40856907\n",
            "Iteration 75, loss = 0.40624635\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.26440123\n",
            "Iteration 2, loss = 0.93461340\n",
            "Iteration 3, loss = 0.72526784\n",
            "Iteration 4, loss = 0.71093766\n",
            "Iteration 5, loss = 0.66449115\n",
            "Iteration 6, loss = 0.66272445\n",
            "Iteration 7, loss = 0.63832205\n",
            "Iteration 8, loss = 0.63273151\n",
            "Iteration 9, loss = 0.65221975\n",
            "Iteration 10, loss = 0.62211359\n",
            "Iteration 11, loss = 0.59317522\n",
            "Iteration 12, loss = 0.58936244\n",
            "Iteration 13, loss = 0.59096578\n",
            "Iteration 14, loss = 0.59261796\n",
            "Iteration 15, loss = 0.58725952\n",
            "Iteration 16, loss = 0.56623930\n",
            "Iteration 17, loss = 0.57650729\n",
            "Iteration 18, loss = 0.57670591\n",
            "Iteration 19, loss = 0.57501470\n",
            "Iteration 20, loss = 0.55162346\n",
            "Iteration 21, loss = 0.53847540\n",
            "Iteration 22, loss = 0.54042759\n",
            "Iteration 23, loss = 0.53876791\n",
            "Iteration 24, loss = 0.56105959\n",
            "Iteration 25, loss = 0.60821300\n",
            "Iteration 26, loss = 0.56232371\n",
            "Iteration 27, loss = 0.54717989\n",
            "Iteration 28, loss = 0.53468475\n",
            "Iteration 29, loss = 0.52827762\n",
            "Iteration 30, loss = 0.51607469\n",
            "Iteration 31, loss = 0.53827787\n",
            "Iteration 32, loss = 0.48563828\n",
            "Iteration 33, loss = 0.49230041\n",
            "Iteration 34, loss = 0.49657314\n",
            "Iteration 35, loss = 0.49833974\n",
            "Iteration 36, loss = 0.49808095\n",
            "Iteration 37, loss = 0.50773869\n",
            "Iteration 38, loss = 0.49597677\n",
            "Iteration 39, loss = 0.48606988\n",
            "Iteration 40, loss = 0.47636575\n",
            "Iteration 41, loss = 0.47513867\n",
            "Iteration 42, loss = 0.48068277\n",
            "Iteration 43, loss = 0.47819733\n",
            "Iteration 44, loss = 0.48219045\n",
            "Iteration 45, loss = 0.46845928\n",
            "Iteration 46, loss = 0.50018886\n",
            "Iteration 47, loss = 0.47902622\n",
            "Iteration 48, loss = 0.47359551\n",
            "Iteration 49, loss = 0.51968027\n",
            "Iteration 50, loss = 0.49043845\n",
            "Iteration 51, loss = 0.44637944\n",
            "Iteration 52, loss = 0.45179883\n",
            "Iteration 53, loss = 0.45710413\n",
            "Iteration 54, loss = 0.45686958\n",
            "Iteration 55, loss = 0.43980461\n",
            "Iteration 56, loss = 0.43786267\n",
            "Iteration 57, loss = 0.44034411\n",
            "Iteration 58, loss = 0.44057948\n",
            "Iteration 59, loss = 0.44521488\n",
            "Iteration 60, loss = 0.46121910\n",
            "Iteration 61, loss = 0.44275131\n",
            "Iteration 62, loss = 0.43441714\n",
            "Iteration 63, loss = 0.43278409\n",
            "Iteration 64, loss = 0.44066561\n",
            "Iteration 65, loss = 0.43033937\n",
            "Iteration 66, loss = 0.43622288\n",
            "Iteration 67, loss = 0.45641072\n",
            "Iteration 68, loss = 0.43411994\n",
            "Iteration 69, loss = 0.43363850\n",
            "Iteration 70, loss = 0.42774519\n",
            "Iteration 71, loss = 0.43586431\n",
            "Iteration 72, loss = 0.43096556\n",
            "Iteration 73, loss = 0.43658891\n",
            "Iteration 74, loss = 0.42199758\n",
            "Iteration 75, loss = 0.43418218\n",
            "Iteration 76, loss = 0.41970533\n",
            "Iteration 77, loss = 0.42008455\n",
            "Iteration 78, loss = 0.42918457\n",
            "Iteration 79, loss = 0.42333177\n",
            "Iteration 80, loss = 0.41503248\n",
            "Iteration 81, loss = 0.42200768\n",
            "Iteration 82, loss = 0.41848513\n",
            "Iteration 83, loss = 0.41591276\n",
            "Iteration 84, loss = 0.42794053\n",
            "Iteration 85, loss = 0.41527056\n",
            "Iteration 86, loss = 0.41735250\n",
            "Iteration 87, loss = 0.42272477\n",
            "Iteration 88, loss = 0.42464909\n",
            "Iteration 89, loss = 0.44544938\n",
            "Iteration 90, loss = 0.40873439\n",
            "Iteration 91, loss = 0.43761260\n",
            "Iteration 92, loss = 0.42989697\n",
            "Iteration 93, loss = 0.41188379\n",
            "Iteration 94, loss = 0.40522647\n",
            "Iteration 95, loss = 0.42804927\n",
            "Iteration 96, loss = 0.43275519\n",
            "Iteration 97, loss = 0.43563807\n",
            "Iteration 98, loss = 0.41695743\n",
            "Iteration 99, loss = 0.40695723\n",
            "Iteration 100, loss = 0.40916052\n",
            "Iteration 101, loss = 0.40415631\n",
            "Iteration 102, loss = 0.41096388\n",
            "Iteration 103, loss = 0.40916360\n",
            "Iteration 104, loss = 0.42066251\n",
            "Iteration 105, loss = 0.42009943\n",
            "Iteration 106, loss = 0.42521803\n",
            "Iteration 107, loss = 0.43970061\n",
            "Iteration 108, loss = 0.40611625\n",
            "Iteration 109, loss = 0.41710400\n",
            "Iteration 110, loss = 0.40394658\n",
            "Iteration 111, loss = 0.39719071\n",
            "Iteration 112, loss = 0.39997524\n",
            "Iteration 113, loss = 0.40342512\n",
            "Iteration 114, loss = 0.40304475\n",
            "Iteration 115, loss = 0.42172135\n",
            "Iteration 116, loss = 0.41908144\n",
            "Iteration 117, loss = 0.40383287\n",
            "Iteration 118, loss = 0.41989853\n",
            "Iteration 119, loss = 0.39712481\n",
            "Iteration 120, loss = 0.40160075\n",
            "Iteration 121, loss = 0.40692166\n",
            "Iteration 122, loss = 0.42524288\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.89815447\n",
            "Iteration 2, loss = 0.68813424\n",
            "Iteration 3, loss = 0.65800065\n",
            "Iteration 4, loss = 0.60106306\n",
            "Iteration 5, loss = 0.59225639\n",
            "Iteration 6, loss = 0.59988831\n",
            "Iteration 7, loss = 0.61870836\n",
            "Iteration 8, loss = 0.61787996\n",
            "Iteration 9, loss = 0.58362062\n",
            "Iteration 10, loss = 0.58629360\n",
            "Iteration 11, loss = 0.55770036\n",
            "Iteration 12, loss = 0.54967279\n",
            "Iteration 13, loss = 0.60424902\n",
            "Iteration 14, loss = 0.57947389\n",
            "Iteration 15, loss = 0.56452555\n",
            "Iteration 16, loss = 0.54419656\n",
            "Iteration 17, loss = 0.53456022\n",
            "Iteration 18, loss = 0.52565015\n",
            "Iteration 19, loss = 0.53563793\n",
            "Iteration 20, loss = 0.50737988\n",
            "Iteration 21, loss = 0.51266894\n",
            "Iteration 22, loss = 0.50680955\n",
            "Iteration 23, loss = 0.49837555\n",
            "Iteration 24, loss = 0.52947410\n",
            "Iteration 25, loss = 0.56440282\n",
            "Iteration 26, loss = 0.49283729\n",
            "Iteration 27, loss = 0.49133749\n",
            "Iteration 28, loss = 0.48999318\n",
            "Iteration 29, loss = 0.51838130\n",
            "Iteration 30, loss = 0.50170414\n",
            "Iteration 31, loss = 0.49131226\n",
            "Iteration 32, loss = 0.50467702\n",
            "Iteration 33, loss = 0.46984756\n",
            "Iteration 34, loss = 0.48698589\n",
            "Iteration 35, loss = 0.49391874\n",
            "Iteration 36, loss = 0.46219259\n",
            "Iteration 37, loss = 0.45691808\n",
            "Iteration 38, loss = 0.46873663\n",
            "Iteration 39, loss = 0.48953949\n",
            "Iteration 40, loss = 0.48154238\n",
            "Iteration 41, loss = 0.46267833\n",
            "Iteration 42, loss = 0.45645090\n",
            "Iteration 43, loss = 0.46869894\n",
            "Iteration 44, loss = 0.45447208\n",
            "Iteration 45, loss = 0.44846190\n",
            "Iteration 46, loss = 0.44767628\n",
            "Iteration 47, loss = 0.46005083\n",
            "Iteration 48, loss = 0.48560261\n",
            "Iteration 49, loss = 0.44924620\n",
            "Iteration 50, loss = 0.44190348\n",
            "Iteration 51, loss = 0.44765675\n",
            "Iteration 52, loss = 0.44033646\n",
            "Iteration 53, loss = 0.44678170\n",
            "Iteration 54, loss = 0.50775957\n",
            "Iteration 55, loss = 0.47180682\n",
            "Iteration 56, loss = 0.46662545\n",
            "Iteration 57, loss = 0.47618446\n",
            "Iteration 58, loss = 0.46387610\n",
            "Iteration 59, loss = 0.45032349\n",
            "Iteration 60, loss = 0.47236198\n",
            "Iteration 61, loss = 0.49487110\n",
            "Iteration 62, loss = 0.45010838\n",
            "Iteration 63, loss = 0.46191246\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.01390328\n",
            "Iteration 2, loss = 0.67486303\n",
            "Iteration 3, loss = 0.62958115\n",
            "Iteration 4, loss = 0.60670975\n",
            "Iteration 5, loss = 0.60418439\n",
            "Iteration 6, loss = 0.59619417\n",
            "Iteration 7, loss = 0.59170417\n",
            "Iteration 8, loss = 0.58732435\n",
            "Iteration 9, loss = 0.58317249\n",
            "Iteration 10, loss = 0.58092613\n",
            "Iteration 11, loss = 0.57550543\n",
            "Iteration 12, loss = 0.57160753\n",
            "Iteration 13, loss = 0.56661567\n",
            "Iteration 14, loss = 0.56249773\n",
            "Iteration 15, loss = 0.56015569\n",
            "Iteration 16, loss = 0.55558033\n",
            "Iteration 17, loss = 0.55269127\n",
            "Iteration 18, loss = 0.54862941\n",
            "Iteration 19, loss = 0.54408583\n",
            "Iteration 20, loss = 0.54218981\n",
            "Iteration 21, loss = 0.53797302\n",
            "Iteration 22, loss = 0.53581182\n",
            "Iteration 23, loss = 0.52909266\n",
            "Iteration 24, loss = 0.52910273\n",
            "Iteration 25, loss = 0.52278935\n",
            "Iteration 26, loss = 0.51620666\n",
            "Iteration 27, loss = 0.51336955\n",
            "Iteration 28, loss = 0.51213187\n",
            "Iteration 29, loss = 0.51332905\n",
            "Iteration 30, loss = 0.51122131\n",
            "Iteration 31, loss = 0.50183963\n",
            "Iteration 32, loss = 0.49244155\n",
            "Iteration 33, loss = 0.48957408\n",
            "Iteration 34, loss = 0.48507482\n",
            "Iteration 35, loss = 0.48151969\n",
            "Iteration 36, loss = 0.47600119\n",
            "Iteration 37, loss = 0.47256362\n",
            "Iteration 38, loss = 0.48524263\n",
            "Iteration 39, loss = 0.46632568\n",
            "Iteration 40, loss = 0.46199404\n",
            "Iteration 41, loss = 0.45926119\n",
            "Iteration 42, loss = 0.45482105\n",
            "Iteration 43, loss = 0.45420134\n",
            "Iteration 44, loss = 0.45361797\n",
            "Iteration 45, loss = 0.44824578\n",
            "Iteration 46, loss = 0.44502520\n",
            "Iteration 47, loss = 0.44219310\n",
            "Iteration 48, loss = 0.44417897\n",
            "Iteration 49, loss = 0.43992005\n",
            "Iteration 50, loss = 0.43908555\n",
            "Iteration 51, loss = 0.43049837\n",
            "Iteration 52, loss = 0.43059161\n",
            "Iteration 53, loss = 0.43321255\n",
            "Iteration 54, loss = 0.42681535\n",
            "Iteration 55, loss = 0.43416379\n",
            "Iteration 56, loss = 0.42969630\n",
            "Iteration 57, loss = 0.42580289\n",
            "Iteration 58, loss = 0.42030094\n",
            "Iteration 59, loss = 0.42383495\n",
            "Iteration 60, loss = 0.42094827\n",
            "Iteration 61, loss = 0.41966470\n",
            "Iteration 62, loss = 0.42533107\n",
            "Iteration 63, loss = 0.41484437\n",
            "Iteration 64, loss = 0.41196135\n",
            "Iteration 65, loss = 0.41212414\n",
            "Iteration 66, loss = 0.41049961\n",
            "Iteration 67, loss = 0.41232669\n",
            "Iteration 68, loss = 0.40825046\n",
            "Iteration 69, loss = 0.40982918\n",
            "Iteration 70, loss = 0.40858087\n",
            "Iteration 71, loss = 0.40488082\n",
            "Iteration 72, loss = 0.40344739\n",
            "Iteration 73, loss = 0.41245085\n",
            "Iteration 74, loss = 0.40864458\n",
            "Iteration 75, loss = 0.40570613\n",
            "Iteration 76, loss = 0.40588100\n",
            "Iteration 77, loss = 0.40698543\n",
            "Iteration 78, loss = 0.40742451\n",
            "Iteration 79, loss = 0.40041135\n",
            "Iteration 80, loss = 0.40080979\n",
            "Iteration 81, loss = 0.39532273\n",
            "Iteration 82, loss = 0.39576330\n",
            "Iteration 83, loss = 0.39168235\n",
            "Iteration 84, loss = 0.39786822\n",
            "Iteration 85, loss = 0.39375058\n",
            "Iteration 86, loss = 0.39762651\n",
            "Iteration 87, loss = 0.38891640\n",
            "Iteration 88, loss = 0.39177768\n",
            "Iteration 89, loss = 0.40121058\n",
            "Iteration 90, loss = 0.39345461\n",
            "Iteration 91, loss = 0.38935684\n",
            "Iteration 92, loss = 0.39140742\n",
            "Iteration 93, loss = 0.39033423\n",
            "Iteration 94, loss = 0.39094272\n",
            "Iteration 95, loss = 0.38921933\n",
            "Iteration 96, loss = 0.38563873\n",
            "Iteration 97, loss = 0.38403822\n",
            "Iteration 98, loss = 0.38354164\n",
            "Iteration 99, loss = 0.38465352\n",
            "Iteration 100, loss = 0.38486890\n",
            "Iteration 101, loss = 0.38898468\n",
            "Iteration 102, loss = 0.39789274\n",
            "Iteration 103, loss = 0.38232125\n",
            "Iteration 104, loss = 0.38192126\n",
            "Iteration 105, loss = 0.37700157\n",
            "Iteration 106, loss = 0.37540861\n",
            "Iteration 107, loss = 0.37433984\n",
            "Iteration 108, loss = 0.37619099\n",
            "Iteration 109, loss = 0.38190414\n",
            "Iteration 110, loss = 0.38404550\n",
            "Iteration 111, loss = 0.37805624\n",
            "Iteration 112, loss = 0.37194156\n",
            "Iteration 113, loss = 0.37451136\n",
            "Iteration 114, loss = 0.37167623\n",
            "Iteration 115, loss = 0.37151269\n",
            "Iteration 116, loss = 0.37735107\n",
            "Iteration 117, loss = 0.37677932\n",
            "Iteration 118, loss = 0.36981139\n",
            "Iteration 119, loss = 0.36681720\n",
            "Iteration 120, loss = 0.37346563\n",
            "Iteration 121, loss = 0.37266225\n",
            "Iteration 122, loss = 0.37172817\n",
            "Iteration 123, loss = 0.36957774\n",
            "Iteration 124, loss = 0.37443540\n",
            "Iteration 125, loss = 0.39839727\n",
            "Iteration 126, loss = 0.38850478\n",
            "Iteration 127, loss = 0.36652876\n",
            "Iteration 128, loss = 0.36757007\n",
            "Iteration 129, loss = 0.36594968\n",
            "Iteration 130, loss = 0.36486651\n",
            "Iteration 131, loss = 0.36275277\n",
            "Iteration 132, loss = 0.36724989\n",
            "Iteration 133, loss = 0.36831450\n",
            "Iteration 134, loss = 0.36432495\n",
            "Iteration 135, loss = 0.36201230\n",
            "Iteration 136, loss = 0.36980772\n",
            "Iteration 137, loss = 0.36523529\n",
            "Iteration 138, loss = 0.35748776\n",
            "Iteration 139, loss = 0.36238530\n",
            "Iteration 140, loss = 0.36147093\n",
            "Iteration 141, loss = 0.35522580\n",
            "Iteration 142, loss = 0.36428717\n",
            "Iteration 143, loss = 0.36323739\n",
            "Iteration 144, loss = 0.37150933\n",
            "Iteration 145, loss = 0.35951130\n",
            "Iteration 146, loss = 0.36095982\n",
            "Iteration 147, loss = 0.36910780\n",
            "Iteration 148, loss = 0.36581298\n",
            "Iteration 149, loss = 0.35242022\n",
            "Iteration 150, loss = 0.35752257\n",
            "Iteration 151, loss = 0.36128341\n",
            "Iteration 152, loss = 0.35216426\n",
            "Iteration 153, loss = 0.35751706\n",
            "Iteration 154, loss = 0.35645774\n",
            "Iteration 155, loss = 0.35927661\n",
            "Iteration 156, loss = 0.35755059\n",
            "Iteration 157, loss = 0.35152611\n",
            "Iteration 158, loss = 0.35235668\n",
            "Iteration 159, loss = 0.34927018\n",
            "Iteration 160, loss = 0.35048508\n",
            "Iteration 161, loss = 0.35195358\n",
            "Iteration 162, loss = 0.35306435\n",
            "Iteration 163, loss = 0.35476110\n",
            "Iteration 164, loss = 0.36019821\n",
            "Iteration 165, loss = 0.35302254\n",
            "Iteration 166, loss = 0.34800503\n",
            "Iteration 167, loss = 0.34654146\n",
            "Iteration 168, loss = 0.34783613\n",
            "Iteration 169, loss = 0.34713143\n",
            "Iteration 170, loss = 0.35107321\n",
            "Iteration 171, loss = 0.35706177\n",
            "Iteration 172, loss = 0.35665987\n",
            "Iteration 173, loss = 0.35000553\n",
            "Iteration 174, loss = 0.34704967\n",
            "Iteration 175, loss = 0.35072067\n",
            "Iteration 176, loss = 0.34782893\n",
            "Iteration 177, loss = 0.34394090\n",
            "Iteration 178, loss = 0.34600951\n",
            "Iteration 179, loss = 0.34797011\n",
            "Iteration 180, loss = 0.34682657\n",
            "Iteration 181, loss = 0.35178768\n",
            "Iteration 182, loss = 0.34264740\n",
            "Iteration 183, loss = 0.34475682\n",
            "Iteration 184, loss = 0.34464281\n",
            "Iteration 185, loss = 0.34175534\n",
            "Iteration 186, loss = 0.34735452\n",
            "Iteration 187, loss = 0.34227425\n",
            "Iteration 188, loss = 0.34515742\n",
            "Iteration 189, loss = 0.34784538\n",
            "Iteration 190, loss = 0.35805652\n",
            "Iteration 191, loss = 0.33791761\n",
            "Iteration 192, loss = 0.34136786\n",
            "Iteration 193, loss = 0.34026991\n",
            "Iteration 194, loss = 0.34026365\n",
            "Iteration 195, loss = 0.33784578\n",
            "Iteration 196, loss = 0.33921115\n",
            "Iteration 197, loss = 0.34394682\n",
            "Iteration 198, loss = 0.35261759\n",
            "Iteration 199, loss = 0.35545885\n",
            "Iteration 200, loss = 0.34263229\n",
            "Iteration 1, loss = 0.65291969\n",
            "Iteration 2, loss = 0.61509496\n",
            "Iteration 3, loss = 0.60235732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4, loss = 0.59187824\n",
            "Iteration 5, loss = 0.58243759\n",
            "Iteration 6, loss = 0.57586369\n",
            "Iteration 7, loss = 0.57257905\n",
            "Iteration 8, loss = 0.56566084\n",
            "Iteration 9, loss = 0.56106909\n",
            "Iteration 10, loss = 0.55643235\n",
            "Iteration 11, loss = 0.54916171\n",
            "Iteration 12, loss = 0.54600560\n",
            "Iteration 13, loss = 0.54142737\n",
            "Iteration 14, loss = 0.53402855\n",
            "Iteration 15, loss = 0.53133029\n",
            "Iteration 16, loss = 0.52505823\n",
            "Iteration 17, loss = 0.51719536\n",
            "Iteration 18, loss = 0.51444685\n",
            "Iteration 19, loss = 0.50920014\n",
            "Iteration 20, loss = 0.50328653\n",
            "Iteration 21, loss = 0.49498866\n",
            "Iteration 22, loss = 0.48951427\n",
            "Iteration 23, loss = 0.48999836\n",
            "Iteration 24, loss = 0.47862088\n",
            "Iteration 25, loss = 0.47488906\n",
            "Iteration 26, loss = 0.46706472\n",
            "Iteration 27, loss = 0.46275707\n",
            "Iteration 28, loss = 0.45596060\n",
            "Iteration 29, loss = 0.45014431\n",
            "Iteration 30, loss = 0.44420547\n",
            "Iteration 31, loss = 0.43936312\n",
            "Iteration 32, loss = 0.43481802\n",
            "Iteration 33, loss = 0.43576286\n",
            "Iteration 34, loss = 0.43462665\n",
            "Iteration 35, loss = 0.43271866\n",
            "Iteration 36, loss = 0.42457314\n",
            "Iteration 37, loss = 0.42318051\n",
            "Iteration 38, loss = 0.41882692\n",
            "Iteration 39, loss = 0.42776544\n",
            "Iteration 40, loss = 0.41149420\n",
            "Iteration 41, loss = 0.40851356\n",
            "Iteration 42, loss = 0.42061231\n",
            "Iteration 43, loss = 0.41756589\n",
            "Iteration 44, loss = 0.41555124\n",
            "Iteration 45, loss = 0.40575672\n",
            "Iteration 46, loss = 0.39956584\n",
            "Iteration 47, loss = 0.41060140\n",
            "Iteration 48, loss = 0.39821184\n",
            "Iteration 49, loss = 0.39747964\n",
            "Iteration 50, loss = 0.39281031\n",
            "Iteration 51, loss = 0.39617788\n",
            "Iteration 52, loss = 0.39566472\n",
            "Iteration 53, loss = 0.39246576\n",
            "Iteration 54, loss = 0.39102941\n",
            "Iteration 55, loss = 0.39596627\n",
            "Iteration 56, loss = 0.39667533\n",
            "Iteration 57, loss = 0.39892856\n",
            "Iteration 58, loss = 0.40430944\n",
            "Iteration 59, loss = 0.38671381\n",
            "Iteration 60, loss = 0.38361916\n",
            "Iteration 61, loss = 0.38729018\n",
            "Iteration 62, loss = 0.38356853\n",
            "Iteration 63, loss = 0.38190808\n",
            "Iteration 64, loss = 0.38065459\n",
            "Iteration 65, loss = 0.37790094\n",
            "Iteration 66, loss = 0.37613129\n",
            "Iteration 67, loss = 0.37347965\n",
            "Iteration 68, loss = 0.37514836\n",
            "Iteration 69, loss = 0.37540966\n",
            "Iteration 70, loss = 0.37012195\n",
            "Iteration 71, loss = 0.37716299\n",
            "Iteration 72, loss = 0.37597172\n",
            "Iteration 73, loss = 0.39247549\n",
            "Iteration 74, loss = 0.38031961\n",
            "Iteration 75, loss = 0.38066496\n",
            "Iteration 76, loss = 0.36813584\n",
            "Iteration 77, loss = 0.36823557\n",
            "Iteration 78, loss = 0.37054592\n",
            "Iteration 79, loss = 0.36498606\n",
            "Iteration 80, loss = 0.36129161\n",
            "Iteration 81, loss = 0.36623072\n",
            "Iteration 82, loss = 0.36789122\n",
            "Iteration 83, loss = 0.36666372\n",
            "Iteration 84, loss = 0.35982711\n",
            "Iteration 85, loss = 0.36862599\n",
            "Iteration 86, loss = 0.35915590\n",
            "Iteration 87, loss = 0.36373298\n",
            "Iteration 88, loss = 0.35534713\n",
            "Iteration 89, loss = 0.35956469\n",
            "Iteration 90, loss = 0.35505966\n",
            "Iteration 91, loss = 0.35706431\n",
            "Iteration 92, loss = 0.36096009\n",
            "Iteration 93, loss = 0.36896500\n",
            "Iteration 94, loss = 0.35543503\n",
            "Iteration 95, loss = 0.35362705\n",
            "Iteration 96, loss = 0.35923845\n",
            "Iteration 97, loss = 0.35921888\n",
            "Iteration 98, loss = 0.35596627\n",
            "Iteration 99, loss = 0.36229330\n",
            "Iteration 100, loss = 0.35258246\n",
            "Iteration 101, loss = 0.34518747\n",
            "Iteration 102, loss = 0.35011305\n",
            "Iteration 103, loss = 0.34679648\n",
            "Iteration 104, loss = 0.34717933\n",
            "Iteration 105, loss = 0.34420884\n",
            "Iteration 106, loss = 0.34457423\n",
            "Iteration 107, loss = 0.34173679\n",
            "Iteration 108, loss = 0.34080947\n",
            "Iteration 109, loss = 0.34236062\n",
            "Iteration 110, loss = 0.34055927\n",
            "Iteration 111, loss = 0.33970640\n",
            "Iteration 112, loss = 0.34858359\n",
            "Iteration 113, loss = 0.34645019\n",
            "Iteration 114, loss = 0.35105130\n",
            "Iteration 115, loss = 0.34886681\n",
            "Iteration 116, loss = 0.34016457\n",
            "Iteration 117, loss = 0.33511609\n",
            "Iteration 118, loss = 0.33534538\n",
            "Iteration 119, loss = 0.34428673\n",
            "Iteration 120, loss = 0.34373287\n",
            "Iteration 121, loss = 0.33982525\n",
            "Iteration 122, loss = 0.33797466\n",
            "Iteration 123, loss = 0.37751055\n",
            "Iteration 124, loss = 0.37788528\n",
            "Iteration 125, loss = 0.33348357\n",
            "Iteration 126, loss = 0.33084603\n",
            "Iteration 127, loss = 0.33506100\n",
            "Iteration 128, loss = 0.34721038\n",
            "Iteration 129, loss = 0.33807292\n",
            "Iteration 130, loss = 0.33898417\n",
            "Iteration 131, loss = 0.33344558\n",
            "Iteration 132, loss = 0.32910335\n",
            "Iteration 133, loss = 0.33063321\n",
            "Iteration 134, loss = 0.32990116\n",
            "Iteration 135, loss = 0.32521410\n",
            "Iteration 136, loss = 0.32447800\n",
            "Iteration 137, loss = 0.33198839\n",
            "Iteration 138, loss = 0.33235317\n",
            "Iteration 139, loss = 0.32730285\n",
            "Iteration 140, loss = 0.33007501\n",
            "Iteration 141, loss = 0.32644593\n",
            "Iteration 142, loss = 0.34531099\n",
            "Iteration 143, loss = 0.33490542\n",
            "Iteration 144, loss = 0.32353752\n",
            "Iteration 145, loss = 0.33400555\n",
            "Iteration 146, loss = 0.31918465\n",
            "Iteration 147, loss = 0.32016117\n",
            "Iteration 148, loss = 0.32055644\n",
            "Iteration 149, loss = 0.32294361\n",
            "Iteration 150, loss = 0.32072816\n",
            "Iteration 151, loss = 0.32592066\n",
            "Iteration 152, loss = 0.32158373\n",
            "Iteration 153, loss = 0.31601022\n",
            "Iteration 154, loss = 0.31864073\n",
            "Iteration 155, loss = 0.31799555\n",
            "Iteration 156, loss = 0.31990984\n",
            "Iteration 157, loss = 0.31849910\n",
            "Iteration 158, loss = 0.32683383\n",
            "Iteration 159, loss = 0.32493171\n",
            "Iteration 160, loss = 0.32003019\n",
            "Iteration 161, loss = 0.31778914\n",
            "Iteration 162, loss = 0.31192838\n",
            "Iteration 163, loss = 0.31877475\n",
            "Iteration 164, loss = 0.32082881\n",
            "Iteration 165, loss = 0.32308253\n",
            "Iteration 166, loss = 0.32397101\n",
            "Iteration 167, loss = 0.31943760\n",
            "Iteration 168, loss = 0.31594971\n",
            "Iteration 169, loss = 0.30848287\n",
            "Iteration 170, loss = 0.31930526\n",
            "Iteration 171, loss = 0.31559575\n",
            "Iteration 172, loss = 0.31832888\n",
            "Iteration 173, loss = 0.32005053\n",
            "Iteration 174, loss = 0.31438515\n",
            "Iteration 175, loss = 0.31109203\n",
            "Iteration 176, loss = 0.30944800\n",
            "Iteration 177, loss = 0.31101143\n",
            "Iteration 178, loss = 0.32031696\n",
            "Iteration 179, loss = 0.32083853\n",
            "Iteration 180, loss = 0.31039424\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67591568\n",
            "Iteration 2, loss = 0.61735877\n",
            "Iteration 3, loss = 0.60091342\n",
            "Iteration 4, loss = 0.58800474\n",
            "Iteration 5, loss = 0.58535115\n",
            "Iteration 6, loss = 0.57374817\n",
            "Iteration 7, loss = 0.57186040\n",
            "Iteration 8, loss = 0.56467093\n",
            "Iteration 9, loss = 0.55977798\n",
            "Iteration 10, loss = 0.55983935\n",
            "Iteration 11, loss = 0.55052210\n",
            "Iteration 12, loss = 0.54906928\n",
            "Iteration 13, loss = 0.54426051\n",
            "Iteration 14, loss = 0.53765202\n",
            "Iteration 15, loss = 0.53540095\n",
            "Iteration 16, loss = 0.53118168\n",
            "Iteration 17, loss = 0.52535577\n",
            "Iteration 18, loss = 0.51751143\n",
            "Iteration 19, loss = 0.51190204\n",
            "Iteration 20, loss = 0.50538204\n",
            "Iteration 21, loss = 0.50540701\n",
            "Iteration 22, loss = 0.49466192\n",
            "Iteration 23, loss = 0.48953235\n",
            "Iteration 24, loss = 0.48824681\n",
            "Iteration 25, loss = 0.47552313\n",
            "Iteration 26, loss = 0.47093499\n",
            "Iteration 27, loss = 0.46763671\n",
            "Iteration 28, loss = 0.46023978\n",
            "Iteration 29, loss = 0.45265947\n",
            "Iteration 30, loss = 0.45377294\n",
            "Iteration 31, loss = 0.45230810\n",
            "Iteration 32, loss = 0.44850588\n",
            "Iteration 33, loss = 0.44121300\n",
            "Iteration 34, loss = 0.43012001\n",
            "Iteration 35, loss = 0.42913519\n",
            "Iteration 36, loss = 0.42749829\n",
            "Iteration 37, loss = 0.43025006\n",
            "Iteration 38, loss = 0.41816602\n",
            "Iteration 39, loss = 0.42027268\n",
            "Iteration 40, loss = 0.43222061\n",
            "Iteration 41, loss = 0.41416475\n",
            "Iteration 42, loss = 0.42597635\n",
            "Iteration 43, loss = 0.42324692\n",
            "Iteration 44, loss = 0.41205863\n",
            "Iteration 45, loss = 0.40299835\n",
            "Iteration 46, loss = 0.40517440\n",
            "Iteration 47, loss = 0.41164199\n",
            "Iteration 48, loss = 0.40308410\n",
            "Iteration 49, loss = 0.40143525\n",
            "Iteration 50, loss = 0.40495299\n",
            "Iteration 51, loss = 0.40650558\n",
            "Iteration 52, loss = 0.39983169\n",
            "Iteration 53, loss = 0.40252790\n",
            "Iteration 54, loss = 0.39806357\n",
            "Iteration 55, loss = 0.39538248\n",
            "Iteration 56, loss = 0.39446292\n",
            "Iteration 57, loss = 0.39362578\n",
            "Iteration 58, loss = 0.39712251\n",
            "Iteration 59, loss = 0.39048325\n",
            "Iteration 60, loss = 0.39295420\n",
            "Iteration 61, loss = 0.38922926\n",
            "Iteration 62, loss = 0.39101526\n",
            "Iteration 63, loss = 0.39239898\n",
            "Iteration 64, loss = 0.38466204\n",
            "Iteration 65, loss = 0.38434038\n",
            "Iteration 66, loss = 0.38817669\n",
            "Iteration 67, loss = 0.39588004\n",
            "Iteration 68, loss = 0.38248144\n",
            "Iteration 69, loss = 0.38373204\n",
            "Iteration 70, loss = 0.38150678\n",
            "Iteration 71, loss = 0.38181886\n",
            "Iteration 72, loss = 0.38103230\n",
            "Iteration 73, loss = 0.38090244\n",
            "Iteration 74, loss = 0.38719340\n",
            "Iteration 75, loss = 0.38675699\n",
            "Iteration 76, loss = 0.40001785\n",
            "Iteration 77, loss = 0.38474201\n",
            "Iteration 78, loss = 0.38477839\n",
            "Iteration 79, loss = 0.37568896\n",
            "Iteration 80, loss = 0.38827461\n",
            "Iteration 81, loss = 0.38415404\n",
            "Iteration 82, loss = 0.37076165\n",
            "Iteration 83, loss = 0.38110150\n",
            "Iteration 84, loss = 0.37605174\n",
            "Iteration 85, loss = 0.37150333\n",
            "Iteration 86, loss = 0.37568334\n",
            "Iteration 87, loss = 0.37650128\n",
            "Iteration 88, loss = 0.36977044\n",
            "Iteration 89, loss = 0.37156133\n",
            "Iteration 90, loss = 0.36753200\n",
            "Iteration 91, loss = 0.36731945\n",
            "Iteration 92, loss = 0.36744640\n",
            "Iteration 93, loss = 0.36776133\n",
            "Iteration 94, loss = 0.36853780\n",
            "Iteration 95, loss = 0.36855372\n",
            "Iteration 96, loss = 0.36618124\n",
            "Iteration 97, loss = 0.36717957\n",
            "Iteration 98, loss = 0.36309608\n",
            "Iteration 99, loss = 0.36370852\n",
            "Iteration 100, loss = 0.36658406\n",
            "Iteration 101, loss = 0.37799785\n",
            "Iteration 102, loss = 0.36259525\n",
            "Iteration 103, loss = 0.36836320\n",
            "Iteration 104, loss = 0.36777961\n",
            "Iteration 105, loss = 0.36916678\n",
            "Iteration 106, loss = 0.36094388\n",
            "Iteration 107, loss = 0.36814168\n",
            "Iteration 108, loss = 0.36751862\n",
            "Iteration 109, loss = 0.35990677\n",
            "Iteration 110, loss = 0.36391697\n",
            "Iteration 111, loss = 0.36858369\n",
            "Iteration 112, loss = 0.36437426\n",
            "Iteration 113, loss = 0.36073783\n",
            "Iteration 114, loss = 0.35487387\n",
            "Iteration 115, loss = 0.36677272\n",
            "Iteration 116, loss = 0.36318721\n",
            "Iteration 117, loss = 0.37473157\n",
            "Iteration 118, loss = 0.35914310\n",
            "Iteration 119, loss = 0.35490886\n",
            "Iteration 120, loss = 0.35652801\n",
            "Iteration 121, loss = 0.36059568\n",
            "Iteration 122, loss = 0.35618996\n",
            "Iteration 123, loss = 0.35392552\n",
            "Iteration 124, loss = 0.35266913\n",
            "Iteration 125, loss = 0.35661432\n",
            "Iteration 126, loss = 0.34987287\n",
            "Iteration 127, loss = 0.35962813\n",
            "Iteration 128, loss = 0.35364733\n",
            "Iteration 129, loss = 0.35482371\n",
            "Iteration 130, loss = 0.35748094\n",
            "Iteration 131, loss = 0.35674224\n",
            "Iteration 132, loss = 0.34920370\n",
            "Iteration 133, loss = 0.35397289\n",
            "Iteration 134, loss = 0.35544300\n",
            "Iteration 135, loss = 0.35454556\n",
            "Iteration 136, loss = 0.36194890\n",
            "Iteration 137, loss = 0.35102598\n",
            "Iteration 138, loss = 0.34745720\n",
            "Iteration 139, loss = 0.35506065\n",
            "Iteration 140, loss = 0.34612640\n",
            "Iteration 141, loss = 0.34642142\n",
            "Iteration 142, loss = 0.34983829\n",
            "Iteration 143, loss = 0.34940083\n",
            "Iteration 144, loss = 0.35549410\n",
            "Iteration 145, loss = 0.35626841\n",
            "Iteration 146, loss = 0.36128987\n",
            "Iteration 147, loss = 0.35558601\n",
            "Iteration 148, loss = 0.35145199\n",
            "Iteration 149, loss = 0.35184868\n",
            "Iteration 150, loss = 0.35119182\n",
            "Iteration 151, loss = 0.34572922\n",
            "Iteration 152, loss = 0.36267743\n",
            "Iteration 153, loss = 0.35487244\n",
            "Iteration 154, loss = 0.35062105\n",
            "Iteration 155, loss = 0.34954282\n",
            "Iteration 156, loss = 0.35523919\n",
            "Iteration 157, loss = 0.35897634\n",
            "Iteration 158, loss = 0.35140175\n",
            "Iteration 159, loss = 0.37325564\n",
            "Iteration 160, loss = 0.36918615\n",
            "Iteration 161, loss = 0.34350949\n",
            "Iteration 162, loss = 0.34188162\n",
            "Iteration 163, loss = 0.34367088\n",
            "Iteration 164, loss = 0.34760034\n",
            "Iteration 165, loss = 0.34317376\n",
            "Iteration 166, loss = 0.34930352\n",
            "Iteration 167, loss = 0.34746722\n",
            "Iteration 168, loss = 0.36260229\n",
            "Iteration 169, loss = 0.35489135\n",
            "Iteration 170, loss = 0.33983691\n",
            "Iteration 171, loss = 0.34208325\n",
            "Iteration 172, loss = 0.33496013\n",
            "Iteration 173, loss = 0.33623357\n",
            "Iteration 174, loss = 0.33506030\n",
            "Iteration 175, loss = 0.33581995\n",
            "Iteration 176, loss = 0.34787271\n",
            "Iteration 177, loss = 0.35663218\n",
            "Iteration 178, loss = 0.35198868\n",
            "Iteration 179, loss = 0.34182883\n",
            "Iteration 180, loss = 0.34082179\n",
            "Iteration 181, loss = 0.34463505\n",
            "Iteration 182, loss = 0.33438199\n",
            "Iteration 183, loss = 0.33876605\n",
            "Iteration 184, loss = 0.33328845\n",
            "Iteration 185, loss = 0.33258731\n",
            "Iteration 186, loss = 0.33896604\n",
            "Iteration 187, loss = 0.34206880\n",
            "Iteration 188, loss = 0.33158645\n",
            "Iteration 189, loss = 0.34374667\n",
            "Iteration 190, loss = 0.33482258\n",
            "Iteration 191, loss = 0.33388370\n",
            "Iteration 192, loss = 0.33823425\n",
            "Iteration 193, loss = 0.33254959\n",
            "Iteration 194, loss = 0.33530624\n",
            "Iteration 195, loss = 0.33079972\n",
            "Iteration 196, loss = 0.33140923\n",
            "Iteration 197, loss = 0.33109146\n",
            "Iteration 198, loss = 0.33889064\n",
            "Iteration 199, loss = 0.33192196\n",
            "Iteration 200, loss = 0.32732839\n",
            "Iteration 1, loss = 0.65849248\n",
            "Iteration 2, loss = 0.62493433\n",
            "Iteration 3, loss = 0.61974999\n",
            "Iteration 4, loss = 0.61449748\n",
            "Iteration 5, loss = 0.60766052\n",
            "Iteration 6, loss = 0.60300773\n",
            "Iteration 7, loss = 0.60344288\n",
            "Iteration 8, loss = 0.59930337\n",
            "Iteration 9, loss = 0.59518100\n",
            "Iteration 10, loss = 0.59001853\n",
            "Iteration 11, loss = 0.58963693\n",
            "Iteration 12, loss = 0.58331616\n",
            "Iteration 13, loss = 0.57915101\n",
            "Iteration 14, loss = 0.57316626\n",
            "Iteration 15, loss = 0.57167306\n",
            "Iteration 16, loss = 0.56787096\n",
            "Iteration 17, loss = 0.55825517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 18, loss = 0.55723643\n",
            "Iteration 19, loss = 0.55063761\n",
            "Iteration 20, loss = 0.54453234\n",
            "Iteration 21, loss = 0.53701225\n",
            "Iteration 22, loss = 0.52888752\n",
            "Iteration 23, loss = 0.51668859\n",
            "Iteration 24, loss = 0.51184113\n",
            "Iteration 25, loss = 0.50942267\n",
            "Iteration 26, loss = 0.49242606\n",
            "Iteration 27, loss = 0.48978695\n",
            "Iteration 28, loss = 0.48046211\n",
            "Iteration 29, loss = 0.47970524\n",
            "Iteration 30, loss = 0.48356304\n",
            "Iteration 31, loss = 0.46661151\n",
            "Iteration 32, loss = 0.45724813\n",
            "Iteration 33, loss = 0.45089812\n",
            "Iteration 34, loss = 0.46293165\n",
            "Iteration 35, loss = 0.44446482\n",
            "Iteration 36, loss = 0.44558523\n",
            "Iteration 37, loss = 0.43983476\n",
            "Iteration 38, loss = 0.43145014\n",
            "Iteration 39, loss = 0.43649473\n",
            "Iteration 40, loss = 0.43797997\n",
            "Iteration 41, loss = 0.42744861\n",
            "Iteration 42, loss = 0.42726805\n",
            "Iteration 43, loss = 0.42204063\n",
            "Iteration 44, loss = 0.43350240\n",
            "Iteration 45, loss = 0.42195431\n",
            "Iteration 46, loss = 0.42260862\n",
            "Iteration 47, loss = 0.42247475\n",
            "Iteration 48, loss = 0.41740233\n",
            "Iteration 49, loss = 0.42072906\n",
            "Iteration 50, loss = 0.41252583\n",
            "Iteration 51, loss = 0.40640485\n",
            "Iteration 52, loss = 0.40464313\n",
            "Iteration 53, loss = 0.40562898\n",
            "Iteration 54, loss = 0.40189638\n",
            "Iteration 55, loss = 0.40428217\n",
            "Iteration 56, loss = 0.39991010\n",
            "Iteration 57, loss = 0.40545254\n",
            "Iteration 58, loss = 0.41178191\n",
            "Iteration 59, loss = 0.40392895\n",
            "Iteration 60, loss = 0.41326550\n",
            "Iteration 61, loss = 0.40408717\n",
            "Iteration 62, loss = 0.39048448\n",
            "Iteration 63, loss = 0.39445039\n",
            "Iteration 64, loss = 0.39509469\n",
            "Iteration 65, loss = 0.39778674\n",
            "Iteration 66, loss = 0.39080172\n",
            "Iteration 67, loss = 0.39126210\n",
            "Iteration 68, loss = 0.38532880\n",
            "Iteration 69, loss = 0.38982049\n",
            "Iteration 70, loss = 0.39399881\n",
            "Iteration 71, loss = 0.38542922\n",
            "Iteration 72, loss = 0.38600069\n",
            "Iteration 73, loss = 0.38913103\n",
            "Iteration 74, loss = 0.38116113\n",
            "Iteration 75, loss = 0.37993352\n",
            "Iteration 76, loss = 0.38880794\n",
            "Iteration 77, loss = 0.38069119\n",
            "Iteration 78, loss = 0.38640249\n",
            "Iteration 79, loss = 0.39719983\n",
            "Iteration 80, loss = 0.37537867\n",
            "Iteration 81, loss = 0.38238777\n",
            "Iteration 82, loss = 0.37493996\n",
            "Iteration 83, loss = 0.38031549\n",
            "Iteration 84, loss = 0.37334926\n",
            "Iteration 85, loss = 0.37019359\n",
            "Iteration 86, loss = 0.36914288\n",
            "Iteration 87, loss = 0.37582755\n",
            "Iteration 88, loss = 0.37660072\n",
            "Iteration 89, loss = 0.37938524\n",
            "Iteration 90, loss = 0.37174650\n",
            "Iteration 91, loss = 0.36970709\n",
            "Iteration 92, loss = 0.37104182\n",
            "Iteration 93, loss = 0.37786937\n",
            "Iteration 94, loss = 0.36679082\n",
            "Iteration 95, loss = 0.36118864\n",
            "Iteration 96, loss = 0.37434751\n",
            "Iteration 97, loss = 0.37073613\n",
            "Iteration 98, loss = 0.36709484\n",
            "Iteration 99, loss = 0.36119348\n",
            "Iteration 100, loss = 0.36383673\n",
            "Iteration 101, loss = 0.36484459\n",
            "Iteration 102, loss = 0.37194864\n",
            "Iteration 103, loss = 0.37506210\n",
            "Iteration 104, loss = 0.40036635\n",
            "Iteration 105, loss = 0.37373354\n",
            "Iteration 106, loss = 0.36471300\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62798113\n",
            "Iteration 2, loss = 0.61623041\n",
            "Iteration 3, loss = 0.60818453\n",
            "Iteration 4, loss = 0.59743752\n",
            "Iteration 5, loss = 0.59107927\n",
            "Iteration 6, loss = 0.58599666\n",
            "Iteration 7, loss = 0.58117286\n",
            "Iteration 8, loss = 0.57388626\n",
            "Iteration 9, loss = 0.57039369\n",
            "Iteration 10, loss = 0.56644482\n",
            "Iteration 11, loss = 0.56084934\n",
            "Iteration 12, loss = 0.55790475\n",
            "Iteration 13, loss = 0.55234823\n",
            "Iteration 14, loss = 0.55018758\n",
            "Iteration 15, loss = 0.54197157\n",
            "Iteration 16, loss = 0.53657615\n",
            "Iteration 17, loss = 0.53167454\n",
            "Iteration 18, loss = 0.53176600\n",
            "Iteration 19, loss = 0.52268667\n",
            "Iteration 20, loss = 0.51933135\n",
            "Iteration 21, loss = 0.51554921\n",
            "Iteration 22, loss = 0.51139442\n",
            "Iteration 23, loss = 0.50558154\n",
            "Iteration 24, loss = 0.50193357\n",
            "Iteration 25, loss = 0.49652562\n",
            "Iteration 26, loss = 0.49154360\n",
            "Iteration 27, loss = 0.48690280\n",
            "Iteration 28, loss = 0.48798020\n",
            "Iteration 29, loss = 0.48617230\n",
            "Iteration 30, loss = 0.48138382\n",
            "Iteration 31, loss = 0.47870783\n",
            "Iteration 32, loss = 0.47322410\n",
            "Iteration 33, loss = 0.47287970\n",
            "Iteration 34, loss = 0.46449326\n",
            "Iteration 35, loss = 0.46509928\n",
            "Iteration 36, loss = 0.46283532\n",
            "Iteration 37, loss = 0.46145361\n",
            "Iteration 38, loss = 0.46251516\n",
            "Iteration 39, loss = 0.46637437\n",
            "Iteration 40, loss = 0.45361399\n",
            "Iteration 41, loss = 0.45022237\n",
            "Iteration 42, loss = 0.44928175\n",
            "Iteration 43, loss = 0.44314011\n",
            "Iteration 44, loss = 0.44592398\n",
            "Iteration 45, loss = 0.44240125\n",
            "Iteration 46, loss = 0.43739012\n",
            "Iteration 47, loss = 0.43820096\n",
            "Iteration 48, loss = 0.44093484\n",
            "Iteration 49, loss = 0.44611751\n",
            "Iteration 50, loss = 0.43371311\n",
            "Iteration 51, loss = 0.43717041\n",
            "Iteration 52, loss = 0.43814429\n",
            "Iteration 53, loss = 0.42592878\n",
            "Iteration 54, loss = 0.42404665\n",
            "Iteration 55, loss = 0.43044323\n",
            "Iteration 56, loss = 0.43456008\n",
            "Iteration 57, loss = 0.42245266\n",
            "Iteration 58, loss = 0.43168785\n",
            "Iteration 59, loss = 0.42405481\n",
            "Iteration 60, loss = 0.42066511\n",
            "Iteration 61, loss = 0.41826720\n",
            "Iteration 62, loss = 0.41653442\n",
            "Iteration 63, loss = 0.41705754\n",
            "Iteration 64, loss = 0.41414657\n",
            "Iteration 65, loss = 0.42170599\n",
            "Iteration 66, loss = 0.41294334\n",
            "Iteration 67, loss = 0.41075957\n",
            "Iteration 68, loss = 0.41637754\n",
            "Iteration 69, loss = 0.42078336\n",
            "Iteration 70, loss = 0.42269845\n",
            "Iteration 71, loss = 0.40776052\n",
            "Iteration 72, loss = 0.40368242\n",
            "Iteration 73, loss = 0.41738250\n",
            "Iteration 74, loss = 0.40955599\n",
            "Iteration 75, loss = 0.40892137\n",
            "Iteration 76, loss = 0.40967030\n",
            "Iteration 77, loss = 0.40508057\n",
            "Iteration 78, loss = 0.40087073\n",
            "Iteration 79, loss = 0.40010222\n",
            "Iteration 80, loss = 0.40251516\n",
            "Iteration 81, loss = 0.40538050\n",
            "Iteration 82, loss = 0.40069060\n",
            "Iteration 83, loss = 0.40179613\n",
            "Iteration 84, loss = 0.39481210\n",
            "Iteration 85, loss = 0.39916553\n",
            "Iteration 86, loss = 0.39596573\n",
            "Iteration 87, loss = 0.40069558\n",
            "Iteration 88, loss = 0.40286579\n",
            "Iteration 89, loss = 0.39865489\n",
            "Iteration 90, loss = 0.40426809\n",
            "Iteration 91, loss = 0.40627987\n",
            "Iteration 92, loss = 0.40056624\n",
            "Iteration 93, loss = 0.40616871\n",
            "Iteration 94, loss = 0.40270418\n",
            "Iteration 95, loss = 0.40048708\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.24467022\n",
            "Iteration 2, loss = 0.65296528\n",
            "Iteration 3, loss = 0.62775745\n",
            "Iteration 4, loss = 0.58993262\n",
            "Iteration 5, loss = 0.57317871\n",
            "Iteration 6, loss = 0.56868974\n",
            "Iteration 7, loss = 0.55539107\n",
            "Iteration 8, loss = 0.61562463\n",
            "Iteration 9, loss = 0.57140834\n",
            "Iteration 10, loss = 0.54701146\n",
            "Iteration 11, loss = 0.55747994\n",
            "Iteration 12, loss = 0.54047832\n",
            "Iteration 13, loss = 0.52786165\n",
            "Iteration 14, loss = 0.50685646\n",
            "Iteration 15, loss = 0.51798587\n",
            "Iteration 16, loss = 0.51662253\n",
            "Iteration 17, loss = 0.50168641\n",
            "Iteration 18, loss = 0.49423656\n",
            "Iteration 19, loss = 0.49577836\n",
            "Iteration 20, loss = 0.49948527\n",
            "Iteration 21, loss = 0.49149361\n",
            "Iteration 22, loss = 0.48855487\n",
            "Iteration 23, loss = 0.48681020\n",
            "Iteration 24, loss = 0.47980291\n",
            "Iteration 25, loss = 0.54034321\n",
            "Iteration 26, loss = 0.48367159\n",
            "Iteration 27, loss = 0.51463004\n",
            "Iteration 28, loss = 0.47291814\n",
            "Iteration 29, loss = 0.47711349\n",
            "Iteration 30, loss = 0.46696958\n",
            "Iteration 31, loss = 0.46209920\n",
            "Iteration 32, loss = 0.47310385\n",
            "Iteration 33, loss = 0.47777665\n",
            "Iteration 34, loss = 0.47333318\n",
            "Iteration 35, loss = 0.46995932\n",
            "Iteration 36, loss = 0.45934900\n",
            "Iteration 37, loss = 0.48118040\n",
            "Iteration 38, loss = 0.44302891\n",
            "Iteration 39, loss = 0.49670940\n",
            "Iteration 40, loss = 0.46107111\n",
            "Iteration 41, loss = 0.46677421\n",
            "Iteration 42, loss = 0.46358003\n",
            "Iteration 43, loss = 0.45388403\n",
            "Iteration 44, loss = 0.45489919\n",
            "Iteration 45, loss = 0.50474180\n",
            "Iteration 46, loss = 0.52072343\n",
            "Iteration 47, loss = 0.45160823\n",
            "Iteration 48, loss = 0.47482285\n",
            "Iteration 49, loss = 0.48141538\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.87343691\n",
            "Iteration 2, loss = 0.67093531\n",
            "Iteration 3, loss = 0.66405113\n",
            "Iteration 4, loss = 0.62741615\n",
            "Iteration 5, loss = 0.60086445\n",
            "Iteration 6, loss = 0.62443452\n",
            "Iteration 7, loss = 0.57108820\n",
            "Iteration 8, loss = 0.58173002\n",
            "Iteration 9, loss = 0.54326343\n",
            "Iteration 10, loss = 0.57031079\n",
            "Iteration 11, loss = 0.56946824\n",
            "Iteration 12, loss = 0.53404022\n",
            "Iteration 13, loss = 0.51225706\n",
            "Iteration 14, loss = 0.56884134\n",
            "Iteration 15, loss = 0.53676644\n",
            "Iteration 16, loss = 0.57787838\n",
            "Iteration 17, loss = 0.50160913\n",
            "Iteration 18, loss = 0.47643835\n",
            "Iteration 19, loss = 0.54250309\n",
            "Iteration 20, loss = 0.47351563\n",
            "Iteration 21, loss = 0.50962164\n",
            "Iteration 22, loss = 0.46449216\n",
            "Iteration 23, loss = 0.46205592\n",
            "Iteration 24, loss = 0.45084247\n",
            "Iteration 25, loss = 0.48498787\n",
            "Iteration 26, loss = 0.45680833\n",
            "Iteration 27, loss = 0.44913951\n",
            "Iteration 28, loss = 0.45572141\n",
            "Iteration 29, loss = 0.45717336\n",
            "Iteration 30, loss = 0.59978710\n",
            "Iteration 31, loss = 0.49395961\n",
            "Iteration 32, loss = 0.43868300\n",
            "Iteration 33, loss = 0.42425969\n",
            "Iteration 34, loss = 0.45181966\n",
            "Iteration 35, loss = 0.43462889\n",
            "Iteration 36, loss = 0.43140873\n",
            "Iteration 37, loss = 0.49323639\n",
            "Iteration 38, loss = 0.46977149\n",
            "Iteration 39, loss = 0.42516005\n",
            "Iteration 40, loss = 0.42664745\n",
            "Iteration 41, loss = 0.42131185\n",
            "Iteration 42, loss = 0.41954966\n",
            "Iteration 43, loss = 0.41296106\n",
            "Iteration 44, loss = 0.58130725\n",
            "Iteration 45, loss = 0.45441912\n",
            "Iteration 46, loss = 0.42900898\n",
            "Iteration 47, loss = 0.48871476\n",
            "Iteration 48, loss = 0.41467218\n",
            "Iteration 49, loss = 0.46352508\n",
            "Iteration 50, loss = 0.42301001\n",
            "Iteration 51, loss = 0.43256677\n",
            "Iteration 52, loss = 0.41736335\n",
            "Iteration 53, loss = 0.40242523\n",
            "Iteration 54, loss = 0.41571913\n",
            "Iteration 55, loss = 0.40782232\n",
            "Iteration 56, loss = 0.42344696\n",
            "Iteration 57, loss = 0.43202673\n",
            "Iteration 58, loss = 0.45960035\n",
            "Iteration 59, loss = 0.42078079\n",
            "Iteration 60, loss = 0.44894872\n",
            "Iteration 61, loss = 0.46482875\n",
            "Iteration 62, loss = 0.40332239\n",
            "Iteration 63, loss = 0.41785648\n",
            "Iteration 64, loss = 0.40428587\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.04092103\n",
            "Iteration 2, loss = 0.59309150\n",
            "Iteration 3, loss = 0.59964951\n",
            "Iteration 4, loss = 0.60745415\n",
            "Iteration 5, loss = 0.57546149\n",
            "Iteration 6, loss = 0.57425776\n",
            "Iteration 7, loss = 0.57021300\n",
            "Iteration 8, loss = 0.56210121\n",
            "Iteration 9, loss = 0.55210265\n",
            "Iteration 10, loss = 0.53798786\n",
            "Iteration 11, loss = 0.55874438\n",
            "Iteration 12, loss = 0.58405226\n",
            "Iteration 13, loss = 0.55685118\n",
            "Iteration 14, loss = 0.53602143\n",
            "Iteration 15, loss = 0.52376256\n",
            "Iteration 16, loss = 0.52665836\n",
            "Iteration 17, loss = 0.57159266\n",
            "Iteration 18, loss = 0.51980432\n",
            "Iteration 19, loss = 0.52320988\n",
            "Iteration 20, loss = 0.53035689\n",
            "Iteration 21, loss = 0.49384496\n",
            "Iteration 22, loss = 0.63733037\n",
            "Iteration 23, loss = 0.49354485\n",
            "Iteration 24, loss = 0.49808767\n",
            "Iteration 25, loss = 0.49921118\n",
            "Iteration 26, loss = 0.47832899\n",
            "Iteration 27, loss = 0.50069381\n",
            "Iteration 28, loss = 0.49761028\n",
            "Iteration 29, loss = 0.47697395\n",
            "Iteration 30, loss = 0.52073187\n",
            "Iteration 31, loss = 0.49374362\n",
            "Iteration 32, loss = 0.47679169\n",
            "Iteration 33, loss = 0.48532640\n",
            "Iteration 34, loss = 0.46449879\n",
            "Iteration 35, loss = 0.45721442\n",
            "Iteration 36, loss = 0.45433138\n",
            "Iteration 37, loss = 0.44903354\n",
            "Iteration 38, loss = 0.45796105\n",
            "Iteration 39, loss = 0.45565350\n",
            "Iteration 40, loss = 0.46207900\n",
            "Iteration 41, loss = 0.45350079\n",
            "Iteration 42, loss = 0.44151415\n",
            "Iteration 43, loss = 0.44171655\n",
            "Iteration 44, loss = 0.45025936\n",
            "Iteration 45, loss = 0.46701913\n",
            "Iteration 46, loss = 0.43384138\n",
            "Iteration 47, loss = 0.43232275\n",
            "Iteration 48, loss = 0.44581950\n",
            "Iteration 49, loss = 0.44364105\n",
            "Iteration 50, loss = 0.44559348\n",
            "Iteration 51, loss = 0.44174665\n",
            "Iteration 52, loss = 0.44691873\n",
            "Iteration 53, loss = 0.45135549\n",
            "Iteration 54, loss = 0.41432307\n",
            "Iteration 55, loss = 0.43956530\n",
            "Iteration 56, loss = 0.43834676\n",
            "Iteration 57, loss = 0.45878554\n",
            "Iteration 58, loss = 0.44919811\n",
            "Iteration 59, loss = 0.43354456\n",
            "Iteration 60, loss = 0.41901480\n",
            "Iteration 61, loss = 0.43332184\n",
            "Iteration 62, loss = 0.46175068\n",
            "Iteration 63, loss = 0.43078370\n",
            "Iteration 64, loss = 0.43066593\n",
            "Iteration 65, loss = 0.43391216\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.61451324\n",
            "Iteration 2, loss = 0.78303932\n",
            "Iteration 3, loss = 0.66995968\n",
            "Iteration 4, loss = 0.65700009\n",
            "Iteration 5, loss = 0.62255994\n",
            "Iteration 6, loss = 0.58802775\n",
            "Iteration 7, loss = 0.58746020\n",
            "Iteration 8, loss = 0.57676318\n",
            "Iteration 9, loss = 0.56829103\n",
            "Iteration 10, loss = 0.58537576\n",
            "Iteration 11, loss = 0.58418449\n",
            "Iteration 12, loss = 0.55143801\n",
            "Iteration 13, loss = 0.56453509\n",
            "Iteration 14, loss = 0.58604901\n",
            "Iteration 15, loss = 0.57701138\n",
            "Iteration 16, loss = 0.55814824\n",
            "Iteration 17, loss = 0.62316960\n",
            "Iteration 18, loss = 0.55716237\n",
            "Iteration 19, loss = 0.54443489\n",
            "Iteration 20, loss = 0.49701200\n",
            "Iteration 21, loss = 0.52845620\n",
            "Iteration 22, loss = 0.53171362\n",
            "Iteration 23, loss = 0.49002700\n",
            "Iteration 24, loss = 0.50456161\n",
            "Iteration 25, loss = 0.49764142\n",
            "Iteration 26, loss = 0.48676163\n",
            "Iteration 27, loss = 0.47907336\n",
            "Iteration 28, loss = 0.52881471\n",
            "Iteration 29, loss = 0.49500083\n",
            "Iteration 30, loss = 0.46573139\n",
            "Iteration 31, loss = 0.50605014\n",
            "Iteration 32, loss = 0.48751117\n",
            "Iteration 33, loss = 0.48316067\n",
            "Iteration 34, loss = 0.52697589\n",
            "Iteration 35, loss = 0.46800703\n",
            "Iteration 36, loss = 0.49693838\n",
            "Iteration 37, loss = 0.47652628\n",
            "Iteration 38, loss = 0.47970425\n",
            "Iteration 39, loss = 0.46241565\n",
            "Iteration 40, loss = 0.44599791\n",
            "Iteration 41, loss = 0.45795059\n",
            "Iteration 42, loss = 0.47787291\n",
            "Iteration 43, loss = 0.48059984\n",
            "Iteration 44, loss = 0.46382212\n",
            "Iteration 45, loss = 0.45714528\n",
            "Iteration 46, loss = 0.49040772\n",
            "Iteration 47, loss = 0.45195631\n",
            "Iteration 48, loss = 0.44663180\n",
            "Iteration 49, loss = 0.45255401\n",
            "Iteration 50, loss = 0.44118374\n",
            "Iteration 51, loss = 0.49037847\n",
            "Iteration 52, loss = 0.43579124\n",
            "Iteration 53, loss = 0.46763749\n",
            "Iteration 54, loss = 0.49382453\n",
            "Iteration 55, loss = 0.47083348\n",
            "Iteration 56, loss = 0.44695529\n",
            "Iteration 57, loss = 0.44595330\n",
            "Iteration 58, loss = 0.43435569\n",
            "Iteration 59, loss = 0.43484417\n",
            "Iteration 60, loss = 0.43088637\n",
            "Iteration 61, loss = 0.46067539\n",
            "Iteration 62, loss = 0.44793449\n",
            "Iteration 63, loss = 0.46843678\n",
            "Iteration 64, loss = 0.50189393\n",
            "Iteration 65, loss = 0.46898299\n",
            "Iteration 66, loss = 0.44997858\n",
            "Iteration 67, loss = 0.44890314\n",
            "Iteration 68, loss = 0.42908722\n",
            "Iteration 69, loss = 0.43523096\n",
            "Iteration 70, loss = 0.43015066\n",
            "Iteration 71, loss = 0.43508227\n",
            "Iteration 72, loss = 0.43750655\n",
            "Iteration 73, loss = 0.43146542\n",
            "Iteration 74, loss = 0.47546885\n",
            "Iteration 75, loss = 0.45266751\n",
            "Iteration 76, loss = 0.43554284\n",
            "Iteration 77, loss = 0.44906444\n",
            "Iteration 78, loss = 0.42413704\n",
            "Iteration 79, loss = 0.41740613\n",
            "Iteration 80, loss = 0.43730302\n",
            "Iteration 81, loss = 0.43263297\n",
            "Iteration 82, loss = 0.44174191\n",
            "Iteration 83, loss = 0.43634021\n",
            "Iteration 84, loss = 0.43647553\n",
            "Iteration 85, loss = 0.43359297\n",
            "Iteration 86, loss = 0.47546827\n",
            "Iteration 87, loss = 0.42625436\n",
            "Iteration 88, loss = 0.44839974\n",
            "Iteration 89, loss = 0.43913531\n",
            "Iteration 90, loss = 0.43004806\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.74427367\n",
            "Iteration 2, loss = 0.66781974\n",
            "Iteration 3, loss = 0.67034891\n",
            "Iteration 4, loss = 0.63272876\n",
            "Iteration 5, loss = 0.62244729\n",
            "Iteration 6, loss = 0.61526407\n",
            "Iteration 7, loss = 0.59662581\n",
            "Iteration 8, loss = 0.60681071\n",
            "Iteration 9, loss = 0.60162033\n",
            "Iteration 10, loss = 0.58949715\n",
            "Iteration 11, loss = 0.59917704\n",
            "Iteration 12, loss = 0.58003310\n",
            "Iteration 13, loss = 0.56524568\n",
            "Iteration 14, loss = 0.54774702\n",
            "Iteration 15, loss = 0.55127658\n",
            "Iteration 16, loss = 0.54372305\n",
            "Iteration 17, loss = 0.54352637\n",
            "Iteration 18, loss = 0.55094992\n",
            "Iteration 19, loss = 0.54296939\n",
            "Iteration 20, loss = 0.52779229\n",
            "Iteration 21, loss = 0.50635132\n",
            "Iteration 22, loss = 0.49494988\n",
            "Iteration 23, loss = 0.51332529\n",
            "Iteration 24, loss = 0.51591849\n",
            "Iteration 25, loss = 0.53272757\n",
            "Iteration 26, loss = 0.48744121\n",
            "Iteration 27, loss = 0.52121132\n",
            "Iteration 28, loss = 0.51852372\n",
            "Iteration 29, loss = 0.48129288\n",
            "Iteration 30, loss = 0.49093544\n",
            "Iteration 31, loss = 0.52079883\n",
            "Iteration 32, loss = 0.47500685\n",
            "Iteration 33, loss = 0.47849927\n",
            "Iteration 34, loss = 0.49445399\n",
            "Iteration 35, loss = 0.47824038\n",
            "Iteration 36, loss = 0.48933674\n",
            "Iteration 37, loss = 0.53630307\n",
            "Iteration 38, loss = 0.47603671\n",
            "Iteration 39, loss = 0.48526491\n",
            "Iteration 40, loss = 0.47433275\n",
            "Iteration 41, loss = 0.46251070\n",
            "Iteration 42, loss = 0.46483296\n",
            "Iteration 43, loss = 0.46907142\n",
            "Iteration 44, loss = 0.45506224\n",
            "Iteration 45, loss = 0.45083772\n",
            "Iteration 46, loss = 0.46407009\n",
            "Iteration 47, loss = 0.44972802\n",
            "Iteration 48, loss = 0.47032508\n",
            "Iteration 49, loss = 0.45291092\n",
            "Iteration 50, loss = 0.45833664\n",
            "Iteration 51, loss = 0.45830398\n",
            "Iteration 52, loss = 0.46098472\n",
            "Iteration 53, loss = 0.44687139\n",
            "Iteration 54, loss = 0.44759215\n",
            "Iteration 55, loss = 0.45209041\n",
            "Iteration 56, loss = 0.46453838\n",
            "Iteration 57, loss = 0.45003071\n",
            "Iteration 58, loss = 0.45381420\n",
            "Iteration 59, loss = 0.45172352\n",
            "Iteration 60, loss = 0.49025057\n",
            "Iteration 61, loss = 0.44574046\n",
            "Iteration 62, loss = 0.45122812\n",
            "Iteration 63, loss = 0.44287501\n",
            "Iteration 64, loss = 0.44131179\n",
            "Iteration 65, loss = 0.44059772\n",
            "Iteration 66, loss = 0.49492764\n",
            "Iteration 67, loss = 0.47668558\n",
            "Iteration 68, loss = 0.44477874\n",
            "Iteration 69, loss = 0.47243583\n",
            "Iteration 70, loss = 0.44026033\n",
            "Iteration 71, loss = 0.42432038\n",
            "Iteration 72, loss = 0.44585358\n",
            "Iteration 73, loss = 0.45188485\n",
            "Iteration 74, loss = 0.48116184\n",
            "Iteration 75, loss = 0.43279343\n",
            "Iteration 76, loss = 0.43469588\n",
            "Iteration 77, loss = 0.43578520\n",
            "Iteration 78, loss = 0.42649127\n",
            "Iteration 79, loss = 0.45467785\n",
            "Iteration 80, loss = 0.42157107\n",
            "Iteration 81, loss = 0.44174462\n",
            "Iteration 82, loss = 0.44052471\n",
            "Iteration 83, loss = 0.42969181\n",
            "Iteration 84, loss = 0.43617150\n",
            "Iteration 85, loss = 0.44040058\n",
            "Iteration 86, loss = 0.44416947\n",
            "Iteration 87, loss = 0.43630399\n",
            "Iteration 88, loss = 0.44775473\n",
            "Iteration 89, loss = 0.42745439\n",
            "Iteration 90, loss = 0.42534525\n",
            "Iteration 91, loss = 0.43560203\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70933509\n",
            "Iteration 2, loss = 0.69652637\n",
            "Iteration 3, loss = 0.68621536\n",
            "Iteration 4, loss = 0.67943866\n",
            "Iteration 5, loss = 0.67610444\n",
            "Iteration 6, loss = 0.67396064\n",
            "Iteration 7, loss = 0.67338037\n",
            "Iteration 8, loss = 0.67267219\n",
            "Iteration 9, loss = 0.67247742\n",
            "Iteration 10, loss = 0.67193059\n",
            "Iteration 11, loss = 0.67162806\n",
            "Iteration 12, loss = 0.67125897\n",
            "Iteration 13, loss = 0.67087034\n",
            "Iteration 14, loss = 0.67056527\n",
            "Iteration 15, loss = 0.67011415\n",
            "Iteration 16, loss = 0.66981673\n",
            "Iteration 17, loss = 0.66951311\n",
            "Iteration 18, loss = 0.66914562\n",
            "Iteration 19, loss = 0.66883728\n",
            "Iteration 20, loss = 0.66848088\n",
            "Iteration 21, loss = 0.66812794\n",
            "Iteration 22, loss = 0.66778212\n",
            "Iteration 23, loss = 0.66752705\n",
            "Iteration 24, loss = 0.66707661\n",
            "Iteration 25, loss = 0.66681167\n",
            "Iteration 26, loss = 0.66650259\n",
            "Iteration 27, loss = 0.66612402\n",
            "Iteration 28, loss = 0.66579667\n",
            "Iteration 29, loss = 0.66528960\n",
            "Iteration 30, loss = 0.66499397\n",
            "Iteration 31, loss = 0.66455689\n",
            "Iteration 32, loss = 0.66421165\n",
            "Iteration 33, loss = 0.66383867\n",
            "Iteration 34, loss = 0.66341216\n",
            "Iteration 35, loss = 0.66300493\n",
            "Iteration 36, loss = 0.66262907\n",
            "Iteration 37, loss = 0.66220414\n",
            "Iteration 38, loss = 0.66180527\n",
            "Iteration 39, loss = 0.66141154\n",
            "Iteration 40, loss = 0.66104450\n",
            "Iteration 41, loss = 0.66062643\n",
            "Iteration 42, loss = 0.66027251\n",
            "Iteration 43, loss = 0.66000551\n",
            "Iteration 44, loss = 0.65956929\n",
            "Iteration 45, loss = 0.65912589\n",
            "Iteration 46, loss = 0.65867866\n",
            "Iteration 47, loss = 0.65838729\n",
            "Iteration 48, loss = 0.65801044\n",
            "Iteration 49, loss = 0.65767389\n",
            "Iteration 50, loss = 0.65720002\n",
            "Iteration 51, loss = 0.65682027\n",
            "Iteration 52, loss = 0.65644278\n",
            "Iteration 53, loss = 0.65609225\n",
            "Iteration 54, loss = 0.65572712\n",
            "Iteration 55, loss = 0.65533585\n",
            "Iteration 56, loss = 0.65496304\n",
            "Iteration 57, loss = 0.65455790\n",
            "Iteration 58, loss = 0.65418827\n",
            "Iteration 59, loss = 0.65384880\n",
            "Iteration 60, loss = 0.65346902\n",
            "Iteration 61, loss = 0.65314846\n",
            "Iteration 62, loss = 0.65273553\n",
            "Iteration 63, loss = 0.65237761\n",
            "Iteration 64, loss = 0.65193536\n",
            "Iteration 65, loss = 0.65163870\n",
            "Iteration 66, loss = 0.65127618\n",
            "Iteration 67, loss = 0.65082964\n",
            "Iteration 68, loss = 0.65041597\n",
            "Iteration 69, loss = 0.65003480\n",
            "Iteration 70, loss = 0.64969917\n",
            "Iteration 71, loss = 0.64936855\n",
            "Iteration 72, loss = 0.64906703\n",
            "Iteration 73, loss = 0.64855253\n",
            "Iteration 74, loss = 0.64816553\n",
            "Iteration 75, loss = 0.64777125\n",
            "Iteration 76, loss = 0.64738264\n",
            "Iteration 77, loss = 0.64699590\n",
            "Iteration 78, loss = 0.64666708\n",
            "Iteration 79, loss = 0.64623175\n",
            "Iteration 80, loss = 0.64588443\n",
            "Iteration 81, loss = 0.64546429\n",
            "Iteration 82, loss = 0.64508282\n",
            "Iteration 83, loss = 0.64481549\n",
            "Iteration 84, loss = 0.64431322\n",
            "Iteration 85, loss = 0.64391818\n",
            "Iteration 86, loss = 0.64358965\n",
            "Iteration 87, loss = 0.64311354\n",
            "Iteration 88, loss = 0.64281942\n",
            "Iteration 89, loss = 0.64240044\n",
            "Iteration 90, loss = 0.64200615\n",
            "Iteration 91, loss = 0.64175088\n",
            "Iteration 92, loss = 0.64117798\n",
            "Iteration 93, loss = 0.64089228\n",
            "Iteration 94, loss = 0.64044500\n",
            "Iteration 95, loss = 0.64002859\n",
            "Iteration 96, loss = 0.63966980\n",
            "Iteration 97, loss = 0.63932306\n",
            "Iteration 98, loss = 0.63897098\n",
            "Iteration 99, loss = 0.63853671\n",
            "Iteration 100, loss = 0.63822563\n",
            "Iteration 101, loss = 0.63790352\n",
            "Iteration 102, loss = 0.63742572\n",
            "Iteration 103, loss = 0.63702392\n",
            "Iteration 104, loss = 0.63676906\n",
            "Iteration 105, loss = 0.63650425\n",
            "Iteration 106, loss = 0.63591679\n",
            "Iteration 107, loss = 0.63558899\n",
            "Iteration 108, loss = 0.63522016\n",
            "Iteration 109, loss = 0.63488096\n",
            "Iteration 110, loss = 0.63459018\n",
            "Iteration 111, loss = 0.63417932\n",
            "Iteration 112, loss = 0.63380554\n",
            "Iteration 113, loss = 0.63344618\n",
            "Iteration 114, loss = 0.63303660\n",
            "Iteration 115, loss = 0.63276108\n",
            "Iteration 116, loss = 0.63229752\n",
            "Iteration 117, loss = 0.63211132\n",
            "Iteration 118, loss = 0.63167058\n",
            "Iteration 119, loss = 0.63155783\n",
            "Iteration 120, loss = 0.63084560\n",
            "Iteration 121, loss = 0.63055970\n",
            "Iteration 122, loss = 0.63023501\n",
            "Iteration 123, loss = 0.62982017\n",
            "Iteration 124, loss = 0.62949788\n",
            "Iteration 125, loss = 0.62924751\n",
            "Iteration 126, loss = 0.62885284\n",
            "Iteration 127, loss = 0.62847840\n",
            "Iteration 128, loss = 0.62814281\n",
            "Iteration 129, loss = 0.62789387\n",
            "Iteration 130, loss = 0.62752044\n",
            "Iteration 131, loss = 0.62723631\n",
            "Iteration 132, loss = 0.62698406\n",
            "Iteration 133, loss = 0.62659723\n",
            "Iteration 134, loss = 0.62622801\n",
            "Iteration 135, loss = 0.62586071\n",
            "Iteration 136, loss = 0.62552395\n",
            "Iteration 137, loss = 0.62520112\n",
            "Iteration 138, loss = 0.62516326\n",
            "Iteration 139, loss = 0.62478033\n",
            "Iteration 140, loss = 0.62429732\n",
            "Iteration 141, loss = 0.62406921\n",
            "Iteration 142, loss = 0.62374449\n",
            "Iteration 143, loss = 0.62340682\n",
            "Iteration 144, loss = 0.62305500\n",
            "Iteration 145, loss = 0.62277481\n",
            "Iteration 146, loss = 0.62260209\n",
            "Iteration 147, loss = 0.62215489\n",
            "Iteration 148, loss = 0.62190599\n",
            "Iteration 149, loss = 0.62169548\n",
            "Iteration 150, loss = 0.62135632\n",
            "Iteration 151, loss = 0.62111845\n",
            "Iteration 152, loss = 0.62080064\n",
            "Iteration 153, loss = 0.62055625\n",
            "Iteration 154, loss = 0.62030502\n",
            "Iteration 155, loss = 0.62007928\n",
            "Iteration 156, loss = 0.61979712\n",
            "Iteration 157, loss = 0.61947631\n",
            "Iteration 158, loss = 0.61938015\n",
            "Iteration 159, loss = 0.61911163\n",
            "Iteration 160, loss = 0.61877522\n",
            "Iteration 161, loss = 0.61844111\n",
            "Iteration 162, loss = 0.61831303\n",
            "Iteration 163, loss = 0.61805376\n",
            "Iteration 164, loss = 0.61778910\n",
            "Iteration 165, loss = 0.61753205\n",
            "Iteration 166, loss = 0.61722024\n",
            "Iteration 167, loss = 0.61701968\n",
            "Iteration 168, loss = 0.61677782\n",
            "Iteration 169, loss = 0.61662655\n",
            "Iteration 170, loss = 0.61641606\n",
            "Iteration 171, loss = 0.61613698\n",
            "Iteration 172, loss = 0.61586746\n",
            "Iteration 173, loss = 0.61577213\n",
            "Iteration 174, loss = 0.61549774\n",
            "Iteration 175, loss = 0.61521321\n",
            "Iteration 176, loss = 0.61500928\n",
            "Iteration 177, loss = 0.61482020\n",
            "Iteration 178, loss = 0.61455914\n",
            "Iteration 179, loss = 0.61447222\n",
            "Iteration 180, loss = 0.61425195\n",
            "Iteration 181, loss = 0.61420982\n",
            "Iteration 182, loss = 0.61377260\n",
            "Iteration 183, loss = 0.61363925\n",
            "Iteration 184, loss = 0.61341384\n",
            "Iteration 185, loss = 0.61323689\n",
            "Iteration 186, loss = 0.61305574\n",
            "Iteration 187, loss = 0.61286504\n",
            "Iteration 188, loss = 0.61264294\n",
            "Iteration 189, loss = 0.61246200\n",
            "Iteration 190, loss = 0.61234778\n",
            "Iteration 191, loss = 0.61214142\n",
            "Iteration 192, loss = 0.61201937\n",
            "Iteration 193, loss = 0.61177961\n",
            "Iteration 194, loss = 0.61168631\n",
            "Iteration 195, loss = 0.61151276\n",
            "Iteration 196, loss = 0.61130986\n",
            "Iteration 197, loss = 0.61117234\n",
            "Iteration 198, loss = 0.61097105\n",
            "Iteration 199, loss = 0.61089361\n",
            "Iteration 200, loss = 0.61081203\n",
            "Iteration 201, loss = 0.61057740\n",
            "Iteration 202, loss = 0.61047431\n",
            "Iteration 203, loss = 0.61028473\n",
            "Iteration 204, loss = 0.61012059\n",
            "Iteration 205, loss = 0.61015673\n",
            "Iteration 206, loss = 0.60992922\n",
            "Iteration 207, loss = 0.60975146\n",
            "Iteration 208, loss = 0.60983977\n",
            "Iteration 209, loss = 0.60943849\n",
            "Iteration 210, loss = 0.60933552\n",
            "Iteration 211, loss = 0.60919825\n",
            "Iteration 212, loss = 0.60907737\n",
            "Iteration 213, loss = 0.60894395\n",
            "Iteration 214, loss = 0.60897852\n",
            "Iteration 215, loss = 0.60868594\n",
            "Iteration 216, loss = 0.60861893\n",
            "Iteration 217, loss = 0.60844106\n",
            "Iteration 218, loss = 0.60838630\n",
            "Iteration 219, loss = 0.60822276\n",
            "Iteration 220, loss = 0.60811754\n",
            "Iteration 221, loss = 0.60797889\n",
            "Iteration 222, loss = 0.60792139\n",
            "Iteration 223, loss = 0.60784745\n",
            "Iteration 224, loss = 0.60768862\n",
            "Iteration 225, loss = 0.60762182\n",
            "Iteration 226, loss = 0.60744455\n",
            "Iteration 227, loss = 0.60735755\n",
            "Iteration 228, loss = 0.60738987\n",
            "Iteration 229, loss = 0.60719327\n",
            "Iteration 230, loss = 0.60700624\n",
            "Iteration 231, loss = 0.60694592\n",
            "Iteration 232, loss = 0.60685514\n",
            "Iteration 233, loss = 0.60689580\n",
            "Iteration 234, loss = 0.60670735\n",
            "Iteration 235, loss = 0.60653727\n",
            "Iteration 236, loss = 0.60672336\n",
            "Iteration 237, loss = 0.60632984\n",
            "Iteration 238, loss = 0.60628300\n",
            "Iteration 239, loss = 0.60614923\n",
            "Iteration 240, loss = 0.60617746\n",
            "Iteration 241, loss = 0.60619699\n",
            "Iteration 242, loss = 0.60598162\n",
            "Iteration 243, loss = 0.60598099\n",
            "Iteration 244, loss = 0.60576097\n",
            "Iteration 245, loss = 0.60565644\n",
            "Iteration 246, loss = 0.60557421\n",
            "Iteration 247, loss = 0.60562721\n",
            "Iteration 248, loss = 0.60532132\n",
            "Iteration 249, loss = 0.60524962\n",
            "Iteration 250, loss = 0.60534912\n",
            "Iteration 251, loss = 0.60527498\n",
            "Iteration 252, loss = 0.60503012\n",
            "Iteration 253, loss = 0.60494667\n",
            "Iteration 254, loss = 0.60502459\n",
            "Iteration 255, loss = 0.60503170\n",
            "Iteration 256, loss = 0.60488030\n",
            "Iteration 257, loss = 0.60480198\n",
            "Iteration 258, loss = 0.60469934\n",
            "Iteration 259, loss = 0.60461902\n",
            "Iteration 260, loss = 0.60442915\n",
            "Iteration 261, loss = 0.60435644\n",
            "Iteration 262, loss = 0.60436557\n",
            "Iteration 263, loss = 0.60421841\n",
            "Iteration 264, loss = 0.60421760\n",
            "Iteration 265, loss = 0.60411934\n",
            "Iteration 266, loss = 0.60397158\n",
            "Iteration 267, loss = 0.60393088\n",
            "Iteration 268, loss = 0.60399160\n",
            "Iteration 269, loss = 0.60380643\n",
            "Iteration 270, loss = 0.60373914\n",
            "Iteration 271, loss = 0.60370091\n",
            "Iteration 272, loss = 0.60360388\n",
            "Iteration 273, loss = 0.60353705\n",
            "Iteration 274, loss = 0.60347204\n",
            "Iteration 275, loss = 0.60348689\n",
            "Iteration 276, loss = 0.60331852\n",
            "Iteration 277, loss = 0.60322068\n",
            "Iteration 278, loss = 0.60314794\n",
            "Iteration 279, loss = 0.60314173\n",
            "Iteration 280, loss = 0.60319366\n",
            "Iteration 281, loss = 0.60308238\n",
            "Iteration 282, loss = 0.60293627\n",
            "Iteration 283, loss = 0.60289679\n",
            "Iteration 284, loss = 0.60282196\n",
            "Iteration 285, loss = 0.60274882\n",
            "Iteration 286, loss = 0.60268896\n",
            "Iteration 287, loss = 0.60271043\n",
            "Iteration 288, loss = 0.60254162\n",
            "Iteration 289, loss = 0.60251420\n",
            "Iteration 290, loss = 0.60248174\n",
            "Iteration 291, loss = 0.60247915\n",
            "Iteration 292, loss = 0.60235330\n",
            "Iteration 293, loss = 0.60224187\n",
            "Iteration 294, loss = 0.60220550\n",
            "Iteration 295, loss = 0.60210239\n",
            "Iteration 296, loss = 0.60219019\n",
            "Iteration 297, loss = 0.60200074\n",
            "Iteration 298, loss = 0.60195401\n",
            "Iteration 299, loss = 0.60185572\n",
            "Iteration 300, loss = 0.60180726\n",
            "Iteration 301, loss = 0.60178361\n",
            "Iteration 302, loss = 0.60174759\n",
            "Iteration 303, loss = 0.60175800\n",
            "Iteration 304, loss = 0.60162744\n",
            "Iteration 305, loss = 0.60155979\n",
            "Iteration 306, loss = 0.60159997\n",
            "Iteration 307, loss = 0.60142231\n",
            "Iteration 308, loss = 0.60136977\n",
            "Iteration 309, loss = 0.60140663\n",
            "Iteration 310, loss = 0.60130691\n",
            "Iteration 311, loss = 0.60123384\n",
            "Iteration 312, loss = 0.60126662\n",
            "Iteration 313, loss = 0.60112809\n",
            "Iteration 314, loss = 0.60113699\n",
            "Iteration 315, loss = 0.60095269\n",
            "Iteration 316, loss = 0.60105458\n",
            "Iteration 317, loss = 0.60098938\n",
            "Iteration 318, loss = 0.60090221\n",
            "Iteration 319, loss = 0.60081476\n",
            "Iteration 320, loss = 0.60084828\n",
            "Iteration 321, loss = 0.60065595\n",
            "Iteration 322, loss = 0.60060053\n",
            "Iteration 323, loss = 0.60070851\n",
            "Iteration 324, loss = 0.60052037\n",
            "Iteration 325, loss = 0.60058319\n",
            "Iteration 326, loss = 0.60045467\n",
            "Iteration 327, loss = 0.60044994\n",
            "Iteration 328, loss = 0.60038612\n",
            "Iteration 329, loss = 0.60029005\n",
            "Iteration 330, loss = 0.60024655\n",
            "Iteration 331, loss = 0.60027295\n",
            "Iteration 332, loss = 0.60013105\n",
            "Iteration 333, loss = 0.60009075\n",
            "Iteration 334, loss = 0.60009829\n",
            "Iteration 335, loss = 0.60001112\n",
            "Iteration 336, loss = 0.60009138\n",
            "Iteration 337, loss = 0.59991494\n",
            "Iteration 338, loss = 0.59993972\n",
            "Iteration 339, loss = 0.59982106\n",
            "Iteration 340, loss = 0.59975403\n",
            "Iteration 341, loss = 0.59977193\n",
            "Iteration 342, loss = 0.59968773\n",
            "Iteration 343, loss = 0.59972089\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67941499\n",
            "Iteration 2, loss = 0.67797066\n",
            "Iteration 3, loss = 0.67670196\n",
            "Iteration 4, loss = 0.67582671\n",
            "Iteration 5, loss = 0.67469943\n",
            "Iteration 6, loss = 0.67396043\n",
            "Iteration 7, loss = 0.67355926\n",
            "Iteration 8, loss = 0.67298536\n",
            "Iteration 9, loss = 0.67250061\n",
            "Iteration 10, loss = 0.67197799\n",
            "Iteration 11, loss = 0.67149302\n",
            "Iteration 12, loss = 0.67101703\n",
            "Iteration 13, loss = 0.67055698\n",
            "Iteration 14, loss = 0.66993655\n",
            "Iteration 15, loss = 0.66952806\n",
            "Iteration 16, loss = 0.66904994\n",
            "Iteration 17, loss = 0.66852392\n",
            "Iteration 18, loss = 0.66802444\n",
            "Iteration 19, loss = 0.66758318\n",
            "Iteration 20, loss = 0.66699619\n",
            "Iteration 21, loss = 0.66650987\n",
            "Iteration 22, loss = 0.66606050\n",
            "Iteration 23, loss = 0.66557902\n",
            "Iteration 24, loss = 0.66517889\n",
            "Iteration 25, loss = 0.66484320\n",
            "Iteration 26, loss = 0.66442020\n",
            "Iteration 27, loss = 0.66393555\n",
            "Iteration 28, loss = 0.66369995\n",
            "Iteration 29, loss = 0.66310409\n",
            "Iteration 30, loss = 0.66287865\n",
            "Iteration 31, loss = 0.66228350\n",
            "Iteration 32, loss = 0.66195067\n",
            "Iteration 33, loss = 0.66140105\n",
            "Iteration 34, loss = 0.66093388\n",
            "Iteration 35, loss = 0.66056145\n",
            "Iteration 36, loss = 0.66017099\n",
            "Iteration 37, loss = 0.65978802\n",
            "Iteration 38, loss = 0.65937311\n",
            "Iteration 39, loss = 0.65885286\n",
            "Iteration 40, loss = 0.65840939\n",
            "Iteration 41, loss = 0.65800285\n",
            "Iteration 42, loss = 0.65761741\n",
            "Iteration 43, loss = 0.65712452\n",
            "Iteration 44, loss = 0.65674033\n",
            "Iteration 45, loss = 0.65623239\n",
            "Iteration 46, loss = 0.65570194\n",
            "Iteration 47, loss = 0.65519824\n",
            "Iteration 48, loss = 0.65461089\n",
            "Iteration 49, loss = 0.65401280\n",
            "Iteration 50, loss = 0.65343558\n",
            "Iteration 51, loss = 0.65288669\n",
            "Iteration 52, loss = 0.65227387\n",
            "Iteration 53, loss = 0.65178861\n",
            "Iteration 54, loss = 0.65118019\n",
            "Iteration 55, loss = 0.65069829\n",
            "Iteration 56, loss = 0.65016202\n",
            "Iteration 57, loss = 0.64994940\n",
            "Iteration 58, loss = 0.64922025\n",
            "Iteration 59, loss = 0.64884123\n",
            "Iteration 60, loss = 0.64830875\n",
            "Iteration 61, loss = 0.64791159\n",
            "Iteration 62, loss = 0.64733976\n",
            "Iteration 63, loss = 0.64688271\n",
            "Iteration 64, loss = 0.64652558\n",
            "Iteration 65, loss = 0.64594718\n",
            "Iteration 66, loss = 0.64554429\n",
            "Iteration 67, loss = 0.64515440\n",
            "Iteration 68, loss = 0.64476210\n",
            "Iteration 69, loss = 0.64417516\n",
            "Iteration 70, loss = 0.64371075\n",
            "Iteration 71, loss = 0.64324133\n",
            "Iteration 72, loss = 0.64285076\n",
            "Iteration 73, loss = 0.64248458\n",
            "Iteration 74, loss = 0.64194960\n",
            "Iteration 75, loss = 0.64156208\n",
            "Iteration 76, loss = 0.64113058\n",
            "Iteration 77, loss = 0.64067401\n",
            "Iteration 78, loss = 0.64029352\n",
            "Iteration 79, loss = 0.63998217\n",
            "Iteration 80, loss = 0.63930273\n",
            "Iteration 81, loss = 0.63893607\n",
            "Iteration 82, loss = 0.63852139\n",
            "Iteration 83, loss = 0.63808792\n",
            "Iteration 84, loss = 0.63772781\n",
            "Iteration 85, loss = 0.63721032\n",
            "Iteration 86, loss = 0.63683029\n",
            "Iteration 87, loss = 0.63641951\n",
            "Iteration 88, loss = 0.63603732\n",
            "Iteration 89, loss = 0.63555962\n",
            "Iteration 90, loss = 0.63523102\n",
            "Iteration 91, loss = 0.63474648\n",
            "Iteration 92, loss = 0.63423676\n",
            "Iteration 93, loss = 0.63385048\n",
            "Iteration 94, loss = 0.63345518\n",
            "Iteration 95, loss = 0.63301136\n",
            "Iteration 96, loss = 0.63266150\n",
            "Iteration 97, loss = 0.63213806\n",
            "Iteration 98, loss = 0.63181552\n",
            "Iteration 99, loss = 0.63137808\n",
            "Iteration 100, loss = 0.63103647\n",
            "Iteration 101, loss = 0.63061698\n",
            "Iteration 102, loss = 0.63014546\n",
            "Iteration 103, loss = 0.62982302\n",
            "Iteration 104, loss = 0.62936842\n",
            "Iteration 105, loss = 0.62900040\n",
            "Iteration 106, loss = 0.62866114\n",
            "Iteration 107, loss = 0.62820703\n",
            "Iteration 108, loss = 0.62780202\n",
            "Iteration 109, loss = 0.62755257\n",
            "Iteration 110, loss = 0.62714684\n",
            "Iteration 111, loss = 0.62667329\n",
            "Iteration 112, loss = 0.62632236\n",
            "Iteration 113, loss = 0.62599082\n",
            "Iteration 114, loss = 0.62562530\n",
            "Iteration 115, loss = 0.62523199\n",
            "Iteration 116, loss = 0.62489252\n",
            "Iteration 117, loss = 0.62465863\n",
            "Iteration 118, loss = 0.62418345\n",
            "Iteration 119, loss = 0.62387819\n",
            "Iteration 120, loss = 0.62356769\n",
            "Iteration 121, loss = 0.62320635\n",
            "Iteration 122, loss = 0.62281189\n",
            "Iteration 123, loss = 0.62250593\n",
            "Iteration 124, loss = 0.62214965\n",
            "Iteration 125, loss = 0.62196580\n",
            "Iteration 126, loss = 0.62147372\n",
            "Iteration 127, loss = 0.62125898\n",
            "Iteration 128, loss = 0.62100685\n",
            "Iteration 129, loss = 0.62068119\n",
            "Iteration 130, loss = 0.62030805\n",
            "Iteration 131, loss = 0.62012190\n",
            "Iteration 132, loss = 0.61978284\n",
            "Iteration 133, loss = 0.61935829\n",
            "Iteration 134, loss = 0.61916327\n",
            "Iteration 135, loss = 0.61887029\n",
            "Iteration 136, loss = 0.61858757\n",
            "Iteration 137, loss = 0.61830413\n",
            "Iteration 138, loss = 0.61812856\n",
            "Iteration 139, loss = 0.61771598\n",
            "Iteration 140, loss = 0.61764007\n",
            "Iteration 141, loss = 0.61720768\n",
            "Iteration 142, loss = 0.61694299\n",
            "Iteration 143, loss = 0.61673194\n",
            "Iteration 144, loss = 0.61643128\n",
            "Iteration 145, loss = 0.61628966\n",
            "Iteration 146, loss = 0.61588221\n",
            "Iteration 147, loss = 0.61565227\n",
            "Iteration 148, loss = 0.61547644\n",
            "Iteration 149, loss = 0.61517193\n",
            "Iteration 150, loss = 0.61494458\n",
            "Iteration 151, loss = 0.61477266\n",
            "Iteration 152, loss = 0.61452385\n",
            "Iteration 153, loss = 0.61433546\n",
            "Iteration 154, loss = 0.61406447\n",
            "Iteration 155, loss = 0.61389338\n",
            "Iteration 156, loss = 0.61375622\n",
            "Iteration 157, loss = 0.61336026\n",
            "Iteration 158, loss = 0.61321572\n",
            "Iteration 159, loss = 0.61298340\n",
            "Iteration 160, loss = 0.61290015\n",
            "Iteration 161, loss = 0.61259667\n",
            "Iteration 162, loss = 0.61242058\n",
            "Iteration 163, loss = 0.61227557\n",
            "Iteration 164, loss = 0.61200933\n",
            "Iteration 165, loss = 0.61188384\n",
            "Iteration 166, loss = 0.61166738\n",
            "Iteration 167, loss = 0.61153220\n",
            "Iteration 168, loss = 0.61144291\n",
            "Iteration 169, loss = 0.61118772\n",
            "Iteration 170, loss = 0.61094714\n",
            "Iteration 171, loss = 0.61081515\n",
            "Iteration 172, loss = 0.61080325\n",
            "Iteration 173, loss = 0.61049767\n",
            "Iteration 174, loss = 0.61030536\n",
            "Iteration 175, loss = 0.61024263\n",
            "Iteration 176, loss = 0.61002396\n",
            "Iteration 177, loss = 0.61005944\n",
            "Iteration 178, loss = 0.60964952\n",
            "Iteration 179, loss = 0.60965404\n",
            "Iteration 180, loss = 0.60938730\n",
            "Iteration 181, loss = 0.60929562\n",
            "Iteration 182, loss = 0.60905442\n",
            "Iteration 183, loss = 0.60891009\n",
            "Iteration 184, loss = 0.60879483\n",
            "Iteration 185, loss = 0.60867664\n",
            "Iteration 186, loss = 0.60872887\n",
            "Iteration 187, loss = 0.60842285\n",
            "Iteration 188, loss = 0.60828568\n",
            "Iteration 189, loss = 0.60813480\n",
            "Iteration 190, loss = 0.60801264\n",
            "Iteration 191, loss = 0.60795242\n",
            "Iteration 192, loss = 0.60779097\n",
            "Iteration 193, loss = 0.60777938\n",
            "Iteration 194, loss = 0.60760888\n",
            "Iteration 195, loss = 0.60750553\n",
            "Iteration 196, loss = 0.60745241\n",
            "Iteration 197, loss = 0.60719278\n",
            "Iteration 198, loss = 0.60707840\n",
            "Iteration 199, loss = 0.60708394\n",
            "Iteration 200, loss = 0.60684404\n",
            "Iteration 201, loss = 0.60679753\n",
            "Iteration 202, loss = 0.60665255\n",
            "Iteration 203, loss = 0.60669398\n",
            "Iteration 204, loss = 0.60649534\n",
            "Iteration 205, loss = 0.60639654\n",
            "Iteration 206, loss = 0.60623233\n",
            "Iteration 207, loss = 0.60610088\n",
            "Iteration 208, loss = 0.60601407\n",
            "Iteration 209, loss = 0.60595039\n",
            "Iteration 210, loss = 0.60585379\n",
            "Iteration 211, loss = 0.60580419\n",
            "Iteration 212, loss = 0.60565051\n",
            "Iteration 213, loss = 0.60554336\n",
            "Iteration 214, loss = 0.60542160\n",
            "Iteration 215, loss = 0.60535766\n",
            "Iteration 216, loss = 0.60547014\n",
            "Iteration 217, loss = 0.60514993\n",
            "Iteration 218, loss = 0.60522864\n",
            "Iteration 219, loss = 0.60496137\n",
            "Iteration 220, loss = 0.60510088\n",
            "Iteration 221, loss = 0.60483611\n",
            "Iteration 222, loss = 0.60480531\n",
            "Iteration 223, loss = 0.60464724\n",
            "Iteration 224, loss = 0.60455287\n",
            "Iteration 225, loss = 0.60451452\n",
            "Iteration 226, loss = 0.60445634\n",
            "Iteration 227, loss = 0.60433633\n",
            "Iteration 228, loss = 0.60429831\n",
            "Iteration 229, loss = 0.60420696\n",
            "Iteration 230, loss = 0.60416501\n",
            "Iteration 231, loss = 0.60397624\n",
            "Iteration 232, loss = 0.60399415\n",
            "Iteration 233, loss = 0.60382008\n",
            "Iteration 234, loss = 0.60381639\n",
            "Iteration 235, loss = 0.60370567\n",
            "Iteration 236, loss = 0.60360709\n",
            "Iteration 237, loss = 0.60361302\n",
            "Iteration 238, loss = 0.60370259\n",
            "Iteration 239, loss = 0.60346448\n",
            "Iteration 240, loss = 0.60340416\n",
            "Iteration 241, loss = 0.60329180\n",
            "Iteration 242, loss = 0.60326620\n",
            "Iteration 243, loss = 0.60324192\n",
            "Iteration 244, loss = 0.60308855\n",
            "Iteration 245, loss = 0.60309796\n",
            "Iteration 246, loss = 0.60299233\n",
            "Iteration 247, loss = 0.60306759\n",
            "Iteration 248, loss = 0.60287645\n",
            "Iteration 249, loss = 0.60280406\n",
            "Iteration 250, loss = 0.60268585\n",
            "Iteration 251, loss = 0.60267317\n",
            "Iteration 252, loss = 0.60258160\n",
            "Iteration 253, loss = 0.60254236\n",
            "Iteration 254, loss = 0.60250014\n",
            "Iteration 255, loss = 0.60238237\n",
            "Iteration 256, loss = 0.60227867\n",
            "Iteration 257, loss = 0.60229645\n",
            "Iteration 258, loss = 0.60218490\n",
            "Iteration 259, loss = 0.60217521\n",
            "Iteration 260, loss = 0.60209259\n",
            "Iteration 261, loss = 0.60198705\n",
            "Iteration 262, loss = 0.60196298\n",
            "Iteration 263, loss = 0.60185480\n",
            "Iteration 264, loss = 0.60180916\n",
            "Iteration 265, loss = 0.60177920\n",
            "Iteration 266, loss = 0.60210526\n",
            "Iteration 267, loss = 0.60169820\n",
            "Iteration 268, loss = 0.60160509\n",
            "Iteration 269, loss = 0.60152530\n",
            "Iteration 270, loss = 0.60141540\n",
            "Iteration 271, loss = 0.60146262\n",
            "Iteration 272, loss = 0.60133198\n",
            "Iteration 273, loss = 0.60127068\n",
            "Iteration 274, loss = 0.60132510\n",
            "Iteration 275, loss = 0.60119352\n",
            "Iteration 276, loss = 0.60111595\n",
            "Iteration 277, loss = 0.60111737\n",
            "Iteration 278, loss = 0.60106114\n",
            "Iteration 279, loss = 0.60085786\n",
            "Iteration 280, loss = 0.60105206\n",
            "Iteration 281, loss = 0.60076147\n",
            "Iteration 282, loss = 0.60081650\n",
            "Iteration 283, loss = 0.60067716\n",
            "Iteration 284, loss = 0.60061206\n",
            "Iteration 285, loss = 0.60059951\n",
            "Iteration 286, loss = 0.60054634\n",
            "Iteration 287, loss = 0.60052648\n",
            "Iteration 288, loss = 0.60038116\n",
            "Iteration 289, loss = 0.60039955\n",
            "Iteration 290, loss = 0.60026398\n",
            "Iteration 291, loss = 0.60024952\n",
            "Iteration 292, loss = 0.60017412\n",
            "Iteration 293, loss = 0.60011134\n",
            "Iteration 294, loss = 0.60009842\n",
            "Iteration 295, loss = 0.60018153\n",
            "Iteration 296, loss = 0.60017059\n",
            "Iteration 297, loss = 0.59995296\n",
            "Iteration 298, loss = 0.59987168\n",
            "Iteration 299, loss = 0.59982671\n",
            "Iteration 300, loss = 0.59972014\n",
            "Iteration 301, loss = 0.59978931\n",
            "Iteration 302, loss = 0.59964866\n",
            "Iteration 303, loss = 0.59957342\n",
            "Iteration 304, loss = 0.59969061\n",
            "Iteration 305, loss = 0.59946986\n",
            "Iteration 306, loss = 0.59942365\n",
            "Iteration 307, loss = 0.59958300\n",
            "Iteration 308, loss = 0.59940583\n",
            "Iteration 309, loss = 0.59946398\n",
            "Iteration 310, loss = 0.59930930\n",
            "Iteration 311, loss = 0.59925171\n",
            "Iteration 312, loss = 0.59927522\n",
            "Iteration 313, loss = 0.59902689\n",
            "Iteration 314, loss = 0.59925053\n",
            "Iteration 315, loss = 0.59897376\n",
            "Iteration 316, loss = 0.59886893\n",
            "Iteration 317, loss = 0.59883084\n",
            "Iteration 318, loss = 0.59880233\n",
            "Iteration 319, loss = 0.59876166\n",
            "Iteration 320, loss = 0.59867023\n",
            "Iteration 321, loss = 0.59861380\n",
            "Iteration 322, loss = 0.59862924\n",
            "Iteration 323, loss = 0.59846924\n",
            "Iteration 324, loss = 0.59860087\n",
            "Iteration 325, loss = 0.59834969\n",
            "Iteration 326, loss = 0.59834660\n",
            "Iteration 327, loss = 0.59835032\n",
            "Iteration 328, loss = 0.59836112\n",
            "Iteration 329, loss = 0.59821446\n",
            "Iteration 330, loss = 0.59811997\n",
            "Iteration 331, loss = 0.59814789\n",
            "Iteration 332, loss = 0.59813686\n",
            "Iteration 333, loss = 0.59793132\n",
            "Iteration 334, loss = 0.59807825\n",
            "Iteration 335, loss = 0.59793785\n",
            "Iteration 336, loss = 0.59796778\n",
            "Iteration 337, loss = 0.59796800\n",
            "Iteration 338, loss = 0.59783359\n",
            "Iteration 339, loss = 0.59788450\n",
            "Iteration 340, loss = 0.59763717\n",
            "Iteration 341, loss = 0.59761395\n",
            "Iteration 342, loss = 0.59770217\n",
            "Iteration 343, loss = 0.59758505\n",
            "Iteration 344, loss = 0.59763565\n",
            "Iteration 345, loss = 0.59736308\n",
            "Iteration 346, loss = 0.59743529\n",
            "Iteration 347, loss = 0.59743062\n",
            "Iteration 348, loss = 0.59735751\n",
            "Iteration 349, loss = 0.59722703\n",
            "Iteration 350, loss = 0.59720044\n",
            "Iteration 351, loss = 0.59713399\n",
            "Iteration 352, loss = 0.59710222\n",
            "Iteration 353, loss = 0.59717289\n",
            "Iteration 354, loss = 0.59703411\n",
            "Iteration 355, loss = 0.59698821\n",
            "Iteration 356, loss = 0.59699020\n",
            "Iteration 357, loss = 0.59699205\n",
            "Iteration 358, loss = 0.59691966\n",
            "Iteration 359, loss = 0.59682633\n",
            "Iteration 360, loss = 0.59674238\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69182488\n",
            "Iteration 2, loss = 0.68610800\n",
            "Iteration 3, loss = 0.67995784\n",
            "Iteration 4, loss = 0.67720353\n",
            "Iteration 5, loss = 0.67513557\n",
            "Iteration 6, loss = 0.67395311\n",
            "Iteration 7, loss = 0.67336544\n",
            "Iteration 8, loss = 0.67283003\n",
            "Iteration 9, loss = 0.67238155\n",
            "Iteration 10, loss = 0.67190852\n",
            "Iteration 11, loss = 0.67153838\n",
            "Iteration 12, loss = 0.67101175\n",
            "Iteration 13, loss = 0.67061087\n",
            "Iteration 14, loss = 0.67020087\n",
            "Iteration 15, loss = 0.66970174\n",
            "Iteration 16, loss = 0.66940163\n",
            "Iteration 17, loss = 0.66895467\n",
            "Iteration 18, loss = 0.66867649\n",
            "Iteration 19, loss = 0.66811265\n",
            "Iteration 20, loss = 0.66768699\n",
            "Iteration 21, loss = 0.66730071\n",
            "Iteration 22, loss = 0.66686581\n",
            "Iteration 23, loss = 0.66647915\n",
            "Iteration 24, loss = 0.66607215\n",
            "Iteration 25, loss = 0.66576015\n",
            "Iteration 26, loss = 0.66539265\n",
            "Iteration 27, loss = 0.66493159\n",
            "Iteration 28, loss = 0.66458147\n",
            "Iteration 29, loss = 0.66408822\n",
            "Iteration 30, loss = 0.66374135\n",
            "Iteration 31, loss = 0.66338326\n",
            "Iteration 32, loss = 0.66297210\n",
            "Iteration 33, loss = 0.66255247\n",
            "Iteration 34, loss = 0.66223132\n",
            "Iteration 35, loss = 0.66181368\n",
            "Iteration 36, loss = 0.66145420\n",
            "Iteration 37, loss = 0.66102220\n",
            "Iteration 38, loss = 0.66066731\n",
            "Iteration 39, loss = 0.66026824\n",
            "Iteration 40, loss = 0.65985750\n",
            "Iteration 41, loss = 0.65947487\n",
            "Iteration 42, loss = 0.65906019\n",
            "Iteration 43, loss = 0.65873430\n",
            "Iteration 44, loss = 0.65829215\n",
            "Iteration 45, loss = 0.65793600\n",
            "Iteration 46, loss = 0.65757166\n",
            "Iteration 47, loss = 0.65717730\n",
            "Iteration 48, loss = 0.65678718\n",
            "Iteration 49, loss = 0.65635077\n",
            "Iteration 50, loss = 0.65606821\n",
            "Iteration 51, loss = 0.65561672\n",
            "Iteration 52, loss = 0.65542639\n",
            "Iteration 53, loss = 0.65499206\n",
            "Iteration 54, loss = 0.65441980\n",
            "Iteration 55, loss = 0.65399354\n",
            "Iteration 56, loss = 0.65365103\n",
            "Iteration 57, loss = 0.65334224\n",
            "Iteration 58, loss = 0.65305773\n",
            "Iteration 59, loss = 0.65253915\n",
            "Iteration 60, loss = 0.65221452\n",
            "Iteration 61, loss = 0.65182103\n",
            "Iteration 62, loss = 0.65140871\n",
            "Iteration 63, loss = 0.65104713\n",
            "Iteration 64, loss = 0.65062115\n",
            "Iteration 65, loss = 0.65030145\n",
            "Iteration 66, loss = 0.64993256\n",
            "Iteration 67, loss = 0.64966495\n",
            "Iteration 68, loss = 0.64922176\n",
            "Iteration 69, loss = 0.64890298\n",
            "Iteration 70, loss = 0.64847475\n",
            "Iteration 71, loss = 0.64809806\n",
            "Iteration 72, loss = 0.64774055\n",
            "Iteration 73, loss = 0.64741038\n",
            "Iteration 74, loss = 0.64705542\n",
            "Iteration 75, loss = 0.64662740\n",
            "Iteration 76, loss = 0.64621399\n",
            "Iteration 77, loss = 0.64584489\n",
            "Iteration 78, loss = 0.64556920\n",
            "Iteration 79, loss = 0.64513421\n",
            "Iteration 80, loss = 0.64477597\n",
            "Iteration 81, loss = 0.64437365\n",
            "Iteration 82, loss = 0.64413037\n",
            "Iteration 83, loss = 0.64370243\n",
            "Iteration 84, loss = 0.64325647\n",
            "Iteration 85, loss = 0.64312601\n",
            "Iteration 86, loss = 0.64264628\n",
            "Iteration 87, loss = 0.64224506\n",
            "Iteration 88, loss = 0.64186444\n",
            "Iteration 89, loss = 0.64151821\n",
            "Iteration 90, loss = 0.64106264\n",
            "Iteration 91, loss = 0.64075785\n",
            "Iteration 92, loss = 0.64036362\n",
            "Iteration 93, loss = 0.64001190\n",
            "Iteration 94, loss = 0.63962443\n",
            "Iteration 95, loss = 0.63930268\n",
            "Iteration 96, loss = 0.63894855\n",
            "Iteration 97, loss = 0.63859324\n",
            "Iteration 98, loss = 0.63820222\n",
            "Iteration 99, loss = 0.63797037\n",
            "Iteration 100, loss = 0.63753905\n",
            "Iteration 101, loss = 0.63707047\n",
            "Iteration 102, loss = 0.63668420\n",
            "Iteration 103, loss = 0.63647847\n",
            "Iteration 104, loss = 0.63612681\n",
            "Iteration 105, loss = 0.63572044\n",
            "Iteration 106, loss = 0.63561171\n",
            "Iteration 107, loss = 0.63496121\n",
            "Iteration 108, loss = 0.63464286\n",
            "Iteration 109, loss = 0.63430168\n",
            "Iteration 110, loss = 0.63399567\n",
            "Iteration 111, loss = 0.63358421\n",
            "Iteration 112, loss = 0.63324542\n",
            "Iteration 113, loss = 0.63296236\n",
            "Iteration 114, loss = 0.63273778\n",
            "Iteration 115, loss = 0.63226437\n",
            "Iteration 116, loss = 0.63205727\n",
            "Iteration 117, loss = 0.63167041\n",
            "Iteration 118, loss = 0.63135243\n",
            "Iteration 119, loss = 0.63100106\n",
            "Iteration 120, loss = 0.63070321\n",
            "Iteration 121, loss = 0.63032299\n",
            "Iteration 122, loss = 0.63002754\n",
            "Iteration 123, loss = 0.62977589\n",
            "Iteration 124, loss = 0.62938992\n",
            "Iteration 125, loss = 0.62919726\n",
            "Iteration 126, loss = 0.62878799\n",
            "Iteration 127, loss = 0.62856852\n",
            "Iteration 128, loss = 0.62814013\n",
            "Iteration 129, loss = 0.62782747\n",
            "Iteration 130, loss = 0.62749902\n",
            "Iteration 131, loss = 0.62723696\n",
            "Iteration 132, loss = 0.62684455\n",
            "Iteration 133, loss = 0.62657316\n",
            "Iteration 134, loss = 0.62640921\n",
            "Iteration 135, loss = 0.62598727\n",
            "Iteration 136, loss = 0.62571123\n",
            "Iteration 137, loss = 0.62542944\n",
            "Iteration 138, loss = 0.62518201\n",
            "Iteration 139, loss = 0.62505801\n",
            "Iteration 140, loss = 0.62454978\n",
            "Iteration 141, loss = 0.62436249\n",
            "Iteration 142, loss = 0.62409985\n",
            "Iteration 143, loss = 0.62370272\n",
            "Iteration 144, loss = 0.62348100\n",
            "Iteration 145, loss = 0.62332966\n",
            "Iteration 146, loss = 0.62303441\n",
            "Iteration 147, loss = 0.62269870\n",
            "Iteration 148, loss = 0.62254301\n",
            "Iteration 149, loss = 0.62220882\n",
            "Iteration 150, loss = 0.62200046\n",
            "Iteration 151, loss = 0.62174192\n",
            "Iteration 152, loss = 0.62156482\n",
            "Iteration 153, loss = 0.62129944\n",
            "Iteration 154, loss = 0.62118218\n",
            "Iteration 155, loss = 0.62097582\n",
            "Iteration 156, loss = 0.62056744\n",
            "Iteration 157, loss = 0.62034698\n",
            "Iteration 158, loss = 0.62015791\n",
            "Iteration 159, loss = 0.61992596\n",
            "Iteration 160, loss = 0.61957725\n",
            "Iteration 161, loss = 0.61936281\n",
            "Iteration 162, loss = 0.61918793\n",
            "Iteration 163, loss = 0.61894820\n",
            "Iteration 164, loss = 0.61877224\n",
            "Iteration 165, loss = 0.61868463\n",
            "Iteration 166, loss = 0.61837228\n",
            "Iteration 167, loss = 0.61823069\n",
            "Iteration 168, loss = 0.61801354\n",
            "Iteration 169, loss = 0.61780150\n",
            "Iteration 170, loss = 0.61767184\n",
            "Iteration 171, loss = 0.61735653\n",
            "Iteration 172, loss = 0.61719670\n",
            "Iteration 173, loss = 0.61699263\n",
            "Iteration 174, loss = 0.61681029\n",
            "Iteration 175, loss = 0.61667414\n",
            "Iteration 176, loss = 0.61648523\n",
            "Iteration 177, loss = 0.61631131\n",
            "Iteration 178, loss = 0.61624873\n",
            "Iteration 179, loss = 0.61606027\n",
            "Iteration 180, loss = 0.61576626\n",
            "Iteration 181, loss = 0.61570908\n",
            "Iteration 182, loss = 0.61545155\n",
            "Iteration 183, loss = 0.61528055\n",
            "Iteration 184, loss = 0.61508287\n",
            "Iteration 185, loss = 0.61495665\n",
            "Iteration 186, loss = 0.61477246\n",
            "Iteration 187, loss = 0.61480447\n",
            "Iteration 188, loss = 0.61444025\n",
            "Iteration 189, loss = 0.61432773\n",
            "Iteration 190, loss = 0.61417777\n",
            "Iteration 191, loss = 0.61404741\n",
            "Iteration 192, loss = 0.61389295\n",
            "Iteration 193, loss = 0.61382906\n",
            "Iteration 194, loss = 0.61364510\n",
            "Iteration 195, loss = 0.61365158\n",
            "Iteration 196, loss = 0.61337439\n",
            "Iteration 197, loss = 0.61326656\n",
            "Iteration 198, loss = 0.61310215\n",
            "Iteration 199, loss = 0.61296552\n",
            "Iteration 200, loss = 0.61293188\n",
            "Iteration 201, loss = 0.61276364\n",
            "Iteration 202, loss = 0.61261227\n",
            "Iteration 203, loss = 0.61251621\n",
            "Iteration 204, loss = 0.61261877\n",
            "Iteration 205, loss = 0.61243863\n",
            "Iteration 206, loss = 0.61221242\n",
            "Iteration 207, loss = 0.61228546\n",
            "Iteration 208, loss = 0.61211286\n",
            "Iteration 209, loss = 0.61176648\n",
            "Iteration 210, loss = 0.61168826\n",
            "Iteration 211, loss = 0.61158588\n",
            "Iteration 212, loss = 0.61144296\n",
            "Iteration 213, loss = 0.61140253\n",
            "Iteration 214, loss = 0.61128079\n",
            "Iteration 215, loss = 0.61127442\n",
            "Iteration 216, loss = 0.61107997\n",
            "Iteration 217, loss = 0.61095932\n",
            "Iteration 218, loss = 0.61090873\n",
            "Iteration 219, loss = 0.61082037\n",
            "Iteration 220, loss = 0.61070705\n",
            "Iteration 221, loss = 0.61061417\n",
            "Iteration 222, loss = 0.61055024\n",
            "Iteration 223, loss = 0.61044055\n",
            "Iteration 224, loss = 0.61037856\n",
            "Iteration 225, loss = 0.61031212\n",
            "Iteration 226, loss = 0.61020570\n",
            "Iteration 227, loss = 0.61021402\n",
            "Iteration 228, loss = 0.60995904\n",
            "Iteration 229, loss = 0.61007336\n",
            "Iteration 230, loss = 0.60977183\n",
            "Iteration 231, loss = 0.60964077\n",
            "Iteration 232, loss = 0.60959283\n",
            "Iteration 233, loss = 0.60952666\n",
            "Iteration 234, loss = 0.60946522\n",
            "Iteration 235, loss = 0.60946600\n",
            "Iteration 236, loss = 0.60937677\n",
            "Iteration 237, loss = 0.60924518\n",
            "Iteration 238, loss = 0.60920745\n",
            "Iteration 239, loss = 0.60907893\n",
            "Iteration 240, loss = 0.60915981\n",
            "Iteration 241, loss = 0.60903547\n",
            "Iteration 242, loss = 0.60891477\n",
            "Iteration 243, loss = 0.60880223\n",
            "Iteration 244, loss = 0.60881829\n",
            "Iteration 245, loss = 0.60871585\n",
            "Iteration 246, loss = 0.60856769\n",
            "Iteration 247, loss = 0.60880607\n",
            "Iteration 248, loss = 0.60864408\n",
            "Iteration 249, loss = 0.60835707\n",
            "Iteration 250, loss = 0.60826840\n",
            "Iteration 251, loss = 0.60840287\n",
            "Iteration 252, loss = 0.60815725\n",
            "Iteration 253, loss = 0.60807687\n",
            "Iteration 254, loss = 0.60800286\n",
            "Iteration 255, loss = 0.60795446\n",
            "Iteration 256, loss = 0.60784619\n",
            "Iteration 257, loss = 0.60777589\n",
            "Iteration 258, loss = 0.60778517\n",
            "Iteration 259, loss = 0.60766146\n",
            "Iteration 260, loss = 0.60760679\n",
            "Iteration 261, loss = 0.60757305\n",
            "Iteration 262, loss = 0.60757408\n",
            "Iteration 263, loss = 0.60737857\n",
            "Iteration 264, loss = 0.60753191\n",
            "Iteration 265, loss = 0.60730763\n",
            "Iteration 266, loss = 0.60732124\n",
            "Iteration 267, loss = 0.60709809\n",
            "Iteration 268, loss = 0.60708873\n",
            "Iteration 269, loss = 0.60696688\n",
            "Iteration 270, loss = 0.60691707\n",
            "Iteration 271, loss = 0.60691167\n",
            "Iteration 272, loss = 0.60688648\n",
            "Iteration 273, loss = 0.60675446\n",
            "Iteration 274, loss = 0.60667854\n",
            "Iteration 275, loss = 0.60672843\n",
            "Iteration 276, loss = 0.60658843\n",
            "Iteration 277, loss = 0.60656036\n",
            "Iteration 278, loss = 0.60649179\n",
            "Iteration 279, loss = 0.60646657\n",
            "Iteration 280, loss = 0.60638086\n",
            "Iteration 281, loss = 0.60636720\n",
            "Iteration 282, loss = 0.60621458\n",
            "Iteration 283, loss = 0.60620381\n",
            "Iteration 284, loss = 0.60606771\n",
            "Iteration 285, loss = 0.60614547\n",
            "Iteration 286, loss = 0.60604769\n",
            "Iteration 287, loss = 0.60601490\n",
            "Iteration 288, loss = 0.60590867\n",
            "Iteration 289, loss = 0.60585155\n",
            "Iteration 290, loss = 0.60588968\n",
            "Iteration 291, loss = 0.60584220\n",
            "Iteration 292, loss = 0.60569417\n",
            "Iteration 293, loss = 0.60569804\n",
            "Iteration 294, loss = 0.60555513\n",
            "Iteration 295, loss = 0.60548713\n",
            "Iteration 296, loss = 0.60539271\n",
            "Iteration 297, loss = 0.60533759\n",
            "Iteration 298, loss = 0.60534647\n",
            "Iteration 299, loss = 0.60528300\n",
            "Iteration 300, loss = 0.60520118\n",
            "Iteration 301, loss = 0.60520237\n",
            "Iteration 302, loss = 0.60510179\n",
            "Iteration 303, loss = 0.60500719\n",
            "Iteration 304, loss = 0.60509312\n",
            "Iteration 305, loss = 0.60489197\n",
            "Iteration 306, loss = 0.60484872\n",
            "Iteration 307, loss = 0.60484673\n",
            "Iteration 308, loss = 0.60473137\n",
            "Iteration 309, loss = 0.60484849\n",
            "Iteration 310, loss = 0.60459800\n",
            "Iteration 311, loss = 0.60483252\n",
            "Iteration 312, loss = 0.60455842\n",
            "Iteration 313, loss = 0.60452392\n",
            "Iteration 314, loss = 0.60443471\n",
            "Iteration 315, loss = 0.60446374\n",
            "Iteration 316, loss = 0.60432507\n",
            "Iteration 317, loss = 0.60422474\n",
            "Iteration 318, loss = 0.60421166\n",
            "Iteration 319, loss = 0.60421664\n",
            "Iteration 320, loss = 0.60414044\n",
            "Iteration 321, loss = 0.60401336\n",
            "Iteration 322, loss = 0.60403351\n",
            "Iteration 323, loss = 0.60393625\n",
            "Iteration 324, loss = 0.60393770\n",
            "Iteration 325, loss = 0.60389471\n",
            "Iteration 326, loss = 0.60382867\n",
            "Iteration 327, loss = 0.60382857\n",
            "Iteration 328, loss = 0.60368051\n",
            "Iteration 329, loss = 0.60393432\n",
            "Iteration 330, loss = 0.60358081\n",
            "Iteration 331, loss = 0.60359452\n",
            "Iteration 332, loss = 0.60352591\n",
            "Iteration 333, loss = 0.60341714\n",
            "Iteration 334, loss = 0.60336914\n",
            "Iteration 335, loss = 0.60336556\n",
            "Iteration 336, loss = 0.60329105\n",
            "Iteration 337, loss = 0.60316066\n",
            "Iteration 338, loss = 0.60318201\n",
            "Iteration 339, loss = 0.60314651\n",
            "Iteration 340, loss = 0.60323813\n",
            "Iteration 341, loss = 0.60305953\n",
            "Iteration 342, loss = 0.60302122\n",
            "Iteration 343, loss = 0.60298135\n",
            "Iteration 344, loss = 0.60323159\n",
            "Iteration 345, loss = 0.60290380\n",
            "Iteration 346, loss = 0.60280007\n",
            "Iteration 347, loss = 0.60293594\n",
            "Iteration 348, loss = 0.60271296\n",
            "Iteration 349, loss = 0.60262919\n",
            "Iteration 350, loss = 0.60266641\n",
            "Iteration 351, loss = 0.60256351\n",
            "Iteration 352, loss = 0.60250868\n",
            "Iteration 353, loss = 0.60242831\n",
            "Iteration 354, loss = 0.60242714\n",
            "Iteration 355, loss = 0.60246262\n",
            "Iteration 356, loss = 0.60229486\n",
            "Iteration 357, loss = 0.60232468\n",
            "Iteration 358, loss = 0.60243934\n",
            "Iteration 359, loss = 0.60219197\n",
            "Iteration 360, loss = 0.60229213\n",
            "Iteration 361, loss = 0.60226172\n",
            "Iteration 362, loss = 0.60208479\n",
            "Iteration 363, loss = 0.60208232\n",
            "Iteration 364, loss = 0.60206839\n",
            "Iteration 365, loss = 0.60188954\n",
            "Iteration 366, loss = 0.60195707\n",
            "Iteration 367, loss = 0.60186134\n",
            "Iteration 368, loss = 0.60188799\n",
            "Iteration 369, loss = 0.60174071\n",
            "Iteration 370, loss = 0.60176667\n",
            "Iteration 371, loss = 0.60171824\n",
            "Iteration 372, loss = 0.60170454\n",
            "Iteration 373, loss = 0.60193335\n",
            "Iteration 374, loss = 0.60163512\n",
            "Iteration 375, loss = 0.60154328\n",
            "Iteration 376, loss = 0.60162924\n",
            "Iteration 377, loss = 0.60140360\n",
            "Iteration 378, loss = 0.60136448\n",
            "Iteration 379, loss = 0.60136807\n",
            "Iteration 380, loss = 0.60146409\n",
            "Iteration 381, loss = 0.60149330\n",
            "Iteration 382, loss = 0.60124568\n",
            "Iteration 383, loss = 0.60122445\n",
            "Iteration 384, loss = 0.60122476\n",
            "Iteration 385, loss = 0.60105993\n",
            "Iteration 386, loss = 0.60106056\n",
            "Iteration 387, loss = 0.60103139\n",
            "Iteration 388, loss = 0.60094335\n",
            "Iteration 389, loss = 0.60115174\n",
            "Iteration 390, loss = 0.60086876\n",
            "Iteration 391, loss = 0.60089363\n",
            "Iteration 392, loss = 0.60088201\n",
            "Iteration 393, loss = 0.60074511\n",
            "Iteration 394, loss = 0.60071990\n",
            "Iteration 395, loss = 0.60083129\n",
            "Iteration 396, loss = 0.60062783\n",
            "Iteration 397, loss = 0.60066197\n",
            "Iteration 398, loss = 0.60067206\n",
            "Iteration 399, loss = 0.60057364\n",
            "Iteration 400, loss = 0.60045435\n",
            "Iteration 1, loss = 0.71340011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss = 0.70187470\n",
            "Iteration 3, loss = 0.69008808\n",
            "Iteration 4, loss = 0.68436369\n",
            "Iteration 5, loss = 0.67972534\n",
            "Iteration 6, loss = 0.67824054\n",
            "Iteration 7, loss = 0.67682499\n",
            "Iteration 8, loss = 0.67634702\n",
            "Iteration 9, loss = 0.67561244\n",
            "Iteration 10, loss = 0.67532375\n",
            "Iteration 11, loss = 0.67481601\n",
            "Iteration 12, loss = 0.67435981\n",
            "Iteration 13, loss = 0.67406962\n",
            "Iteration 14, loss = 0.67371090\n",
            "Iteration 15, loss = 0.67312407\n",
            "Iteration 16, loss = 0.67280135\n",
            "Iteration 17, loss = 0.67232294\n",
            "Iteration 18, loss = 0.67191877\n",
            "Iteration 19, loss = 0.67155031\n",
            "Iteration 20, loss = 0.67130182\n",
            "Iteration 21, loss = 0.67074814\n",
            "Iteration 22, loss = 0.67043191\n",
            "Iteration 23, loss = 0.67001759\n",
            "Iteration 24, loss = 0.66956644\n",
            "Iteration 25, loss = 0.66929345\n",
            "Iteration 26, loss = 0.66882309\n",
            "Iteration 27, loss = 0.66844522\n",
            "Iteration 28, loss = 0.66797553\n",
            "Iteration 29, loss = 0.66750311\n",
            "Iteration 30, loss = 0.66720006\n",
            "Iteration 31, loss = 0.66668924\n",
            "Iteration 32, loss = 0.66647871\n",
            "Iteration 33, loss = 0.66597615\n",
            "Iteration 34, loss = 0.66571342\n",
            "Iteration 35, loss = 0.66540961\n",
            "Iteration 36, loss = 0.66496185\n",
            "Iteration 37, loss = 0.66469427\n",
            "Iteration 38, loss = 0.66426875\n",
            "Iteration 39, loss = 0.66396212\n",
            "Iteration 40, loss = 0.66359601\n",
            "Iteration 41, loss = 0.66320869\n",
            "Iteration 42, loss = 0.66306739\n",
            "Iteration 43, loss = 0.66276032\n",
            "Iteration 44, loss = 0.66226823\n",
            "Iteration 45, loss = 0.66185713\n",
            "Iteration 46, loss = 0.66161783\n",
            "Iteration 47, loss = 0.66126475\n",
            "Iteration 48, loss = 0.66084927\n",
            "Iteration 49, loss = 0.66085000\n",
            "Iteration 50, loss = 0.66023020\n",
            "Iteration 51, loss = 0.65987264\n",
            "Iteration 52, loss = 0.65955713\n",
            "Iteration 53, loss = 0.65918260\n",
            "Iteration 54, loss = 0.65884106\n",
            "Iteration 55, loss = 0.65847227\n",
            "Iteration 56, loss = 0.65822689\n",
            "Iteration 57, loss = 0.65790098\n",
            "Iteration 58, loss = 0.65758140\n",
            "Iteration 59, loss = 0.65722961\n",
            "Iteration 60, loss = 0.65688186\n",
            "Iteration 61, loss = 0.65653183\n",
            "Iteration 62, loss = 0.65618715\n",
            "Iteration 63, loss = 0.65598129\n",
            "Iteration 64, loss = 0.65554578\n",
            "Iteration 65, loss = 0.65522738\n",
            "Iteration 66, loss = 0.65490830\n",
            "Iteration 67, loss = 0.65456271\n",
            "Iteration 68, loss = 0.65435380\n",
            "Iteration 69, loss = 0.65389772\n",
            "Iteration 70, loss = 0.65357260\n",
            "Iteration 71, loss = 0.65320185\n",
            "Iteration 72, loss = 0.65305309\n",
            "Iteration 73, loss = 0.65262358\n",
            "Iteration 74, loss = 0.65229740\n",
            "Iteration 75, loss = 0.65192563\n",
            "Iteration 76, loss = 0.65166777\n",
            "Iteration 77, loss = 0.65128790\n",
            "Iteration 78, loss = 0.65097702\n",
            "Iteration 79, loss = 0.65077416\n",
            "Iteration 80, loss = 0.65043431\n",
            "Iteration 81, loss = 0.65005569\n",
            "Iteration 82, loss = 0.64976036\n",
            "Iteration 83, loss = 0.64939517\n",
            "Iteration 84, loss = 0.64911723\n",
            "Iteration 85, loss = 0.64885160\n",
            "Iteration 86, loss = 0.64857672\n",
            "Iteration 87, loss = 0.64842072\n",
            "Iteration 88, loss = 0.64795007\n",
            "Iteration 89, loss = 0.64764643\n",
            "Iteration 90, loss = 0.64736455\n",
            "Iteration 91, loss = 0.64714124\n",
            "Iteration 92, loss = 0.64689951\n",
            "Iteration 93, loss = 0.64657341\n",
            "Iteration 94, loss = 0.64621521\n",
            "Iteration 95, loss = 0.64598875\n",
            "Iteration 96, loss = 0.64571755\n",
            "Iteration 97, loss = 0.64537714\n",
            "Iteration 98, loss = 0.64506605\n",
            "Iteration 99, loss = 0.64493864\n",
            "Iteration 100, loss = 0.64459728\n",
            "Iteration 101, loss = 0.64426948\n",
            "Iteration 102, loss = 0.64401017\n",
            "Iteration 103, loss = 0.64379553\n",
            "Iteration 104, loss = 0.64341658\n",
            "Iteration 105, loss = 0.64313985\n",
            "Iteration 106, loss = 0.64290036\n",
            "Iteration 107, loss = 0.64265001\n",
            "Iteration 108, loss = 0.64246872\n",
            "Iteration 109, loss = 0.64211834\n",
            "Iteration 110, loss = 0.64180618\n",
            "Iteration 111, loss = 0.64158881\n",
            "Iteration 112, loss = 0.64135573\n",
            "Iteration 113, loss = 0.64104097\n",
            "Iteration 114, loss = 0.64080228\n",
            "Iteration 115, loss = 0.64055994\n",
            "Iteration 116, loss = 0.64029287\n",
            "Iteration 117, loss = 0.64011904\n",
            "Iteration 118, loss = 0.63977723\n",
            "Iteration 119, loss = 0.63952675\n",
            "Iteration 120, loss = 0.63942244\n",
            "Iteration 121, loss = 0.63902399\n",
            "Iteration 122, loss = 0.63877632\n",
            "Iteration 123, loss = 0.63870215\n",
            "Iteration 124, loss = 0.63839987\n",
            "Iteration 125, loss = 0.63817348\n",
            "Iteration 126, loss = 0.63783225\n",
            "Iteration 127, loss = 0.63759493\n",
            "Iteration 128, loss = 0.63741945\n",
            "Iteration 129, loss = 0.63722059\n",
            "Iteration 130, loss = 0.63692962\n",
            "Iteration 131, loss = 0.63666366\n",
            "Iteration 132, loss = 0.63646084\n",
            "Iteration 133, loss = 0.63631483\n",
            "Iteration 134, loss = 0.63602327\n",
            "Iteration 135, loss = 0.63582831\n",
            "Iteration 136, loss = 0.63563138\n",
            "Iteration 137, loss = 0.63538935\n",
            "Iteration 138, loss = 0.63522284\n",
            "Iteration 139, loss = 0.63498394\n",
            "Iteration 140, loss = 0.63479276\n",
            "Iteration 141, loss = 0.63460940\n",
            "Iteration 142, loss = 0.63435043\n",
            "Iteration 143, loss = 0.63415567\n",
            "Iteration 144, loss = 0.63400833\n",
            "Iteration 145, loss = 0.63369886\n",
            "Iteration 146, loss = 0.63348461\n",
            "Iteration 147, loss = 0.63333027\n",
            "Iteration 148, loss = 0.63310687\n",
            "Iteration 149, loss = 0.63298574\n",
            "Iteration 150, loss = 0.63272201\n",
            "Iteration 151, loss = 0.63279702\n",
            "Iteration 152, loss = 0.63231494\n",
            "Iteration 153, loss = 0.63212810\n",
            "Iteration 154, loss = 0.63205077\n",
            "Iteration 155, loss = 0.63182719\n",
            "Iteration 156, loss = 0.63167457\n",
            "Iteration 157, loss = 0.63148505\n",
            "Iteration 158, loss = 0.63132221\n",
            "Iteration 159, loss = 0.63108127\n",
            "Iteration 160, loss = 0.63097968\n",
            "Iteration 161, loss = 0.63080289\n",
            "Iteration 162, loss = 0.63062818\n",
            "Iteration 163, loss = 0.63040711\n",
            "Iteration 164, loss = 0.63030520\n",
            "Iteration 165, loss = 0.63007770\n",
            "Iteration 166, loss = 0.62997244\n",
            "Iteration 167, loss = 0.62980183\n",
            "Iteration 168, loss = 0.62966464\n",
            "Iteration 169, loss = 0.62947719\n",
            "Iteration 170, loss = 0.62938611\n",
            "Iteration 171, loss = 0.62917044\n",
            "Iteration 172, loss = 0.62893211\n",
            "Iteration 173, loss = 0.62878647\n",
            "Iteration 174, loss = 0.62863053\n",
            "Iteration 175, loss = 0.62854057\n",
            "Iteration 176, loss = 0.62836692\n",
            "Iteration 177, loss = 0.62825294\n",
            "Iteration 178, loss = 0.62805993\n",
            "Iteration 179, loss = 0.62797238\n",
            "Iteration 180, loss = 0.62779456\n",
            "Iteration 181, loss = 0.62771916\n",
            "Iteration 182, loss = 0.62766557\n",
            "Iteration 183, loss = 0.62737200\n",
            "Iteration 184, loss = 0.62726064\n",
            "Iteration 185, loss = 0.62707869\n",
            "Iteration 186, loss = 0.62709290\n",
            "Iteration 187, loss = 0.62687374\n",
            "Iteration 188, loss = 0.62670277\n",
            "Iteration 189, loss = 0.62661428\n",
            "Iteration 190, loss = 0.62642913\n",
            "Iteration 191, loss = 0.62636014\n",
            "Iteration 192, loss = 0.62624539\n",
            "Iteration 193, loss = 0.62607144\n",
            "Iteration 194, loss = 0.62597489\n",
            "Iteration 195, loss = 0.62581651\n",
            "Iteration 196, loss = 0.62573120\n",
            "Iteration 197, loss = 0.62556860\n",
            "Iteration 198, loss = 0.62547258\n",
            "Iteration 199, loss = 0.62569832\n",
            "Iteration 200, loss = 0.62524167\n",
            "Iteration 201, loss = 0.62519986\n",
            "Iteration 202, loss = 0.62505118\n",
            "Iteration 203, loss = 0.62491404\n",
            "Iteration 204, loss = 0.62480402\n",
            "Iteration 205, loss = 0.62468075\n",
            "Iteration 206, loss = 0.62461799\n",
            "Iteration 207, loss = 0.62450640\n",
            "Iteration 208, loss = 0.62436320\n",
            "Iteration 209, loss = 0.62425873\n",
            "Iteration 210, loss = 0.62429771\n",
            "Iteration 211, loss = 0.62412605\n",
            "Iteration 212, loss = 0.62404697\n",
            "Iteration 213, loss = 0.62387784\n",
            "Iteration 214, loss = 0.62380232\n",
            "Iteration 215, loss = 0.62366213\n",
            "Iteration 216, loss = 0.62365084\n",
            "Iteration 217, loss = 0.62350358\n",
            "Iteration 218, loss = 0.62343285\n",
            "Iteration 219, loss = 0.62334908\n",
            "Iteration 220, loss = 0.62325175\n",
            "Iteration 221, loss = 0.62318167\n",
            "Iteration 222, loss = 0.62310239\n",
            "Iteration 223, loss = 0.62294961\n",
            "Iteration 224, loss = 0.62295543\n",
            "Iteration 225, loss = 0.62276692\n",
            "Iteration 226, loss = 0.62288535\n",
            "Iteration 227, loss = 0.62268833\n",
            "Iteration 228, loss = 0.62255690\n",
            "Iteration 229, loss = 0.62256301\n",
            "Iteration 230, loss = 0.62242011\n",
            "Iteration 231, loss = 0.62234472\n",
            "Iteration 232, loss = 0.62227990\n",
            "Iteration 233, loss = 0.62216871\n",
            "Iteration 234, loss = 0.62210812\n",
            "Iteration 235, loss = 0.62206342\n",
            "Iteration 236, loss = 0.62201214\n",
            "Iteration 237, loss = 0.62186140\n",
            "Iteration 238, loss = 0.62187948\n",
            "Iteration 239, loss = 0.62172813\n",
            "Iteration 240, loss = 0.62162949\n",
            "Iteration 241, loss = 0.62161021\n",
            "Iteration 242, loss = 0.62152566\n",
            "Iteration 243, loss = 0.62151944\n",
            "Iteration 244, loss = 0.62140065\n",
            "Iteration 245, loss = 0.62136424\n",
            "Iteration 246, loss = 0.62125152\n",
            "Iteration 247, loss = 0.62114359\n",
            "Iteration 248, loss = 0.62112090\n",
            "Iteration 249, loss = 0.62104662\n",
            "Iteration 250, loss = 0.62106497\n",
            "Iteration 251, loss = 0.62094631\n",
            "Iteration 252, loss = 0.62098813\n",
            "Iteration 253, loss = 0.62093197\n",
            "Iteration 254, loss = 0.62074220\n",
            "Iteration 255, loss = 0.62066408\n",
            "Iteration 256, loss = 0.62070262\n",
            "Iteration 257, loss = 0.62058979\n",
            "Iteration 258, loss = 0.62051947\n",
            "Iteration 259, loss = 0.62040812\n",
            "Iteration 260, loss = 0.62041155\n",
            "Iteration 261, loss = 0.62046291\n",
            "Iteration 262, loss = 0.62028899\n",
            "Iteration 263, loss = 0.62026617\n",
            "Iteration 264, loss = 0.62018431\n",
            "Iteration 265, loss = 0.62010030\n",
            "Iteration 266, loss = 0.62011769\n",
            "Iteration 267, loss = 0.62002913\n",
            "Iteration 268, loss = 0.61991731\n",
            "Iteration 269, loss = 0.62004410\n",
            "Iteration 270, loss = 0.61985396\n",
            "Iteration 271, loss = 0.61980468\n",
            "Iteration 272, loss = 0.61965725\n",
            "Iteration 273, loss = 0.61958862\n",
            "Iteration 274, loss = 0.61958914\n",
            "Iteration 275, loss = 0.61954108\n",
            "Iteration 276, loss = 0.61942300\n",
            "Iteration 277, loss = 0.61945173\n",
            "Iteration 278, loss = 0.61940056\n",
            "Iteration 279, loss = 0.61941127\n",
            "Iteration 280, loss = 0.61923121\n",
            "Iteration 281, loss = 0.61918841\n",
            "Iteration 282, loss = 0.61921818\n",
            "Iteration 283, loss = 0.61924189\n",
            "Iteration 284, loss = 0.61917123\n",
            "Iteration 285, loss = 0.61903655\n",
            "Iteration 286, loss = 0.61893877\n",
            "Iteration 287, loss = 0.61894771\n",
            "Iteration 288, loss = 0.61882852\n",
            "Iteration 289, loss = 0.61876629\n",
            "Iteration 290, loss = 0.61877149\n",
            "Iteration 291, loss = 0.61871437\n",
            "Iteration 292, loss = 0.61858957\n",
            "Iteration 293, loss = 0.61865242\n",
            "Iteration 294, loss = 0.61844026\n",
            "Iteration 295, loss = 0.61848979\n",
            "Iteration 296, loss = 0.61852348\n",
            "Iteration 297, loss = 0.61854832\n",
            "Iteration 298, loss = 0.61831873\n",
            "Iteration 299, loss = 0.61854871\n",
            "Iteration 300, loss = 0.61823412\n",
            "Iteration 301, loss = 0.61818242\n",
            "Iteration 302, loss = 0.61820520\n",
            "Iteration 303, loss = 0.61809486\n",
            "Iteration 304, loss = 0.61802942\n",
            "Iteration 305, loss = 0.61806243\n",
            "Iteration 306, loss = 0.61808404\n",
            "Iteration 307, loss = 0.61791884\n",
            "Iteration 308, loss = 0.61788274\n",
            "Iteration 309, loss = 0.61775504\n",
            "Iteration 310, loss = 0.61774700\n",
            "Iteration 311, loss = 0.61783685\n",
            "Iteration 312, loss = 0.61786000\n",
            "Iteration 313, loss = 0.61765181\n",
            "Iteration 314, loss = 0.61757289\n",
            "Iteration 315, loss = 0.61751252\n",
            "Iteration 316, loss = 0.61746126\n",
            "Iteration 317, loss = 0.61761879\n",
            "Iteration 318, loss = 0.61737652\n",
            "Iteration 319, loss = 0.61741027\n",
            "Iteration 320, loss = 0.61727619\n",
            "Iteration 321, loss = 0.61732127\n",
            "Iteration 322, loss = 0.61721470\n",
            "Iteration 323, loss = 0.61719244\n",
            "Iteration 324, loss = 0.61713739\n",
            "Iteration 325, loss = 0.61712319\n",
            "Iteration 326, loss = 0.61722729\n",
            "Iteration 327, loss = 0.61701220\n",
            "Iteration 328, loss = 0.61703476\n",
            "Iteration 329, loss = 0.61691381\n",
            "Iteration 330, loss = 0.61703818\n",
            "Iteration 331, loss = 0.61696370\n",
            "Iteration 332, loss = 0.61690506\n",
            "Iteration 333, loss = 0.61678399\n",
            "Iteration 334, loss = 0.61675049\n",
            "Iteration 335, loss = 0.61671601\n",
            "Iteration 336, loss = 0.61661425\n",
            "Iteration 337, loss = 0.61665901\n",
            "Iteration 338, loss = 0.61651259\n",
            "Iteration 339, loss = 0.61654360\n",
            "Iteration 340, loss = 0.61661086\n",
            "Iteration 341, loss = 0.61641914\n",
            "Iteration 342, loss = 0.61635859\n",
            "Iteration 343, loss = 0.61633033\n",
            "Iteration 344, loss = 0.61628343\n",
            "Iteration 345, loss = 0.61646273\n",
            "Iteration 346, loss = 0.61636910\n",
            "Iteration 347, loss = 0.61622463\n",
            "Iteration 348, loss = 0.61622462\n",
            "Iteration 349, loss = 0.61617065\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68173857\n",
            "Iteration 2, loss = 0.68144142\n",
            "Iteration 3, loss = 0.68092307\n",
            "Iteration 4, loss = 0.68046228\n",
            "Iteration 5, loss = 0.67986627\n",
            "Iteration 6, loss = 0.67926353\n",
            "Iteration 7, loss = 0.67864276\n",
            "Iteration 8, loss = 0.67798866\n",
            "Iteration 9, loss = 0.67735707\n",
            "Iteration 10, loss = 0.67668500\n",
            "Iteration 11, loss = 0.67614260\n",
            "Iteration 12, loss = 0.67585384\n",
            "Iteration 13, loss = 0.67536942\n",
            "Iteration 14, loss = 0.67486953\n",
            "Iteration 15, loss = 0.67450040\n",
            "Iteration 16, loss = 0.67420834\n",
            "Iteration 17, loss = 0.67359923\n",
            "Iteration 18, loss = 0.67316790\n",
            "Iteration 19, loss = 0.67278963\n",
            "Iteration 20, loss = 0.67225955\n",
            "Iteration 21, loss = 0.67188110\n",
            "Iteration 22, loss = 0.67133531\n",
            "Iteration 23, loss = 0.67095093\n",
            "Iteration 24, loss = 0.67049169\n",
            "Iteration 25, loss = 0.67000468\n",
            "Iteration 26, loss = 0.66963866\n",
            "Iteration 27, loss = 0.66907716\n",
            "Iteration 28, loss = 0.66867472\n",
            "Iteration 29, loss = 0.66821427\n",
            "Iteration 30, loss = 0.66769913\n",
            "Iteration 31, loss = 0.66734323\n",
            "Iteration 32, loss = 0.66685539\n",
            "Iteration 33, loss = 0.66646639\n",
            "Iteration 34, loss = 0.66612643\n",
            "Iteration 35, loss = 0.66583342\n",
            "Iteration 36, loss = 0.66530228\n",
            "Iteration 37, loss = 0.66499163\n",
            "Iteration 38, loss = 0.66456868\n",
            "Iteration 39, loss = 0.66417255\n",
            "Iteration 40, loss = 0.66405460\n",
            "Iteration 41, loss = 0.66347696\n",
            "Iteration 42, loss = 0.66326480\n",
            "Iteration 43, loss = 0.66277193\n",
            "Iteration 44, loss = 0.66241131\n",
            "Iteration 45, loss = 0.66215119\n",
            "Iteration 46, loss = 0.66177683\n",
            "Iteration 47, loss = 0.66128419\n",
            "Iteration 48, loss = 0.66169617\n",
            "Iteration 49, loss = 0.66076346\n",
            "Iteration 50, loss = 0.66053937\n",
            "Iteration 51, loss = 0.66012320\n",
            "Iteration 52, loss = 0.65980336\n",
            "Iteration 53, loss = 0.65951530\n",
            "Iteration 54, loss = 0.65931511\n",
            "Iteration 55, loss = 0.65902148\n",
            "Iteration 56, loss = 0.65872039\n",
            "Iteration 57, loss = 0.65829280\n",
            "Iteration 58, loss = 0.65805321\n",
            "Iteration 59, loss = 0.65768161\n",
            "Iteration 60, loss = 0.65746030\n",
            "Iteration 61, loss = 0.65713415\n",
            "Iteration 62, loss = 0.65682807\n",
            "Iteration 63, loss = 0.65653728\n",
            "Iteration 64, loss = 0.65626165\n",
            "Iteration 65, loss = 0.65592339\n",
            "Iteration 66, loss = 0.65564155\n",
            "Iteration 67, loss = 0.65552770\n",
            "Iteration 68, loss = 0.65497690\n",
            "Iteration 69, loss = 0.65468252\n",
            "Iteration 70, loss = 0.65437551\n",
            "Iteration 71, loss = 0.65404188\n",
            "Iteration 72, loss = 0.65385480\n",
            "Iteration 73, loss = 0.65365642\n",
            "Iteration 74, loss = 0.65318951\n",
            "Iteration 75, loss = 0.65283514\n",
            "Iteration 76, loss = 0.65247136\n",
            "Iteration 77, loss = 0.65215952\n",
            "Iteration 78, loss = 0.65183214\n",
            "Iteration 79, loss = 0.65149807\n",
            "Iteration 80, loss = 0.65127461\n",
            "Iteration 81, loss = 0.65086899\n",
            "Iteration 82, loss = 0.65059974\n",
            "Iteration 83, loss = 0.65026485\n",
            "Iteration 84, loss = 0.64996501\n",
            "Iteration 85, loss = 0.64966919\n",
            "Iteration 86, loss = 0.64943689\n",
            "Iteration 87, loss = 0.64906804\n",
            "Iteration 88, loss = 0.64875156\n",
            "Iteration 89, loss = 0.64854712\n",
            "Iteration 90, loss = 0.64817881\n",
            "Iteration 91, loss = 0.64777395\n",
            "Iteration 92, loss = 0.64745165\n",
            "Iteration 93, loss = 0.64730995\n",
            "Iteration 94, loss = 0.64697089\n",
            "Iteration 95, loss = 0.64662990\n",
            "Iteration 96, loss = 0.64629022\n",
            "Iteration 97, loss = 0.64603396\n",
            "Iteration 98, loss = 0.64576202\n",
            "Iteration 99, loss = 0.64545623\n",
            "Iteration 100, loss = 0.64505591\n",
            "Iteration 101, loss = 0.64473809\n",
            "Iteration 102, loss = 0.64462880\n",
            "Iteration 103, loss = 0.64434330\n",
            "Iteration 104, loss = 0.64387343\n",
            "Iteration 105, loss = 0.64364905\n",
            "Iteration 106, loss = 0.64326780\n",
            "Iteration 107, loss = 0.64290610\n",
            "Iteration 108, loss = 0.64261665\n",
            "Iteration 109, loss = 0.64226396\n",
            "Iteration 110, loss = 0.64200845\n",
            "Iteration 111, loss = 0.64162019\n",
            "Iteration 112, loss = 0.64133193\n",
            "Iteration 113, loss = 0.64111969\n",
            "Iteration 114, loss = 0.64075710\n",
            "Iteration 115, loss = 0.64046365\n",
            "Iteration 116, loss = 0.64037086\n",
            "Iteration 117, loss = 0.63982176\n",
            "Iteration 118, loss = 0.63954563\n",
            "Iteration 119, loss = 0.63928629\n",
            "Iteration 120, loss = 0.63899293\n",
            "Iteration 121, loss = 0.63895372\n",
            "Iteration 122, loss = 0.63859083\n",
            "Iteration 123, loss = 0.63828904\n",
            "Iteration 124, loss = 0.63793846\n",
            "Iteration 125, loss = 0.63766844\n",
            "Iteration 126, loss = 0.63740044\n",
            "Iteration 127, loss = 0.63714552\n",
            "Iteration 128, loss = 0.63689212\n",
            "Iteration 129, loss = 0.63658895\n",
            "Iteration 130, loss = 0.63630414\n",
            "Iteration 131, loss = 0.63615343\n",
            "Iteration 132, loss = 0.63573874\n",
            "Iteration 133, loss = 0.63547019\n",
            "Iteration 134, loss = 0.63521106\n",
            "Iteration 135, loss = 0.63503792\n",
            "Iteration 136, loss = 0.63466950\n",
            "Iteration 137, loss = 0.63451900\n",
            "Iteration 138, loss = 0.63423918\n",
            "Iteration 139, loss = 0.63397283\n",
            "Iteration 140, loss = 0.63376825\n",
            "Iteration 141, loss = 0.63352501\n",
            "Iteration 142, loss = 0.63326160\n",
            "Iteration 143, loss = 0.63310124\n",
            "Iteration 144, loss = 0.63282781\n",
            "Iteration 145, loss = 0.63257282\n",
            "Iteration 146, loss = 0.63241097\n",
            "Iteration 147, loss = 0.63211876\n",
            "Iteration 148, loss = 0.63197839\n",
            "Iteration 149, loss = 0.63171355\n",
            "Iteration 150, loss = 0.63149610\n",
            "Iteration 151, loss = 0.63130629\n",
            "Iteration 152, loss = 0.63105784\n",
            "Iteration 153, loss = 0.63090295\n",
            "Iteration 154, loss = 0.63066443\n",
            "Iteration 155, loss = 0.63043303\n",
            "Iteration 156, loss = 0.63028580\n",
            "Iteration 157, loss = 0.63000747\n",
            "Iteration 158, loss = 0.62995179\n",
            "Iteration 159, loss = 0.62981705\n",
            "Iteration 160, loss = 0.62954052\n",
            "Iteration 161, loss = 0.62924352\n",
            "Iteration 162, loss = 0.62912442\n",
            "Iteration 163, loss = 0.62895147\n",
            "Iteration 164, loss = 0.62868342\n",
            "Iteration 165, loss = 0.62849157\n",
            "Iteration 166, loss = 0.62860757\n",
            "Iteration 167, loss = 0.62811141\n",
            "Iteration 168, loss = 0.62795926\n",
            "Iteration 169, loss = 0.62777296\n",
            "Iteration 170, loss = 0.62762999\n",
            "Iteration 171, loss = 0.62741700\n",
            "Iteration 172, loss = 0.62727175\n",
            "Iteration 173, loss = 0.62711935\n",
            "Iteration 174, loss = 0.62699005\n",
            "Iteration 175, loss = 0.62686522\n",
            "Iteration 176, loss = 0.62661657\n",
            "Iteration 177, loss = 0.62655673\n",
            "Iteration 178, loss = 0.62635989\n",
            "Iteration 179, loss = 0.62622672\n",
            "Iteration 180, loss = 0.62597352\n",
            "Iteration 181, loss = 0.62576874\n",
            "Iteration 182, loss = 0.62567770\n",
            "Iteration 183, loss = 0.62552107\n",
            "Iteration 184, loss = 0.62540386\n",
            "Iteration 185, loss = 0.62526091\n",
            "Iteration 186, loss = 0.62516099\n",
            "Iteration 187, loss = 0.62494297\n",
            "Iteration 188, loss = 0.62481475\n",
            "Iteration 189, loss = 0.62493175\n",
            "Iteration 190, loss = 0.62458945\n",
            "Iteration 191, loss = 0.62442828\n",
            "Iteration 192, loss = 0.62425796\n",
            "Iteration 193, loss = 0.62411564\n",
            "Iteration 194, loss = 0.62396125\n",
            "Iteration 195, loss = 0.62384619\n",
            "Iteration 196, loss = 0.62379198\n",
            "Iteration 197, loss = 0.62359488\n",
            "Iteration 198, loss = 0.62351299\n",
            "Iteration 199, loss = 0.62339421\n",
            "Iteration 200, loss = 0.62331791\n",
            "Iteration 201, loss = 0.62314664\n",
            "Iteration 202, loss = 0.62307184\n",
            "Iteration 203, loss = 0.62295442\n",
            "Iteration 204, loss = 0.62288055\n",
            "Iteration 205, loss = 0.62280787\n",
            "Iteration 206, loss = 0.62250834\n",
            "Iteration 207, loss = 0.62250956\n",
            "Iteration 208, loss = 0.62244436\n",
            "Iteration 209, loss = 0.62232789\n",
            "Iteration 210, loss = 0.62226532\n",
            "Iteration 211, loss = 0.62211659\n",
            "Iteration 212, loss = 0.62208743\n",
            "Iteration 213, loss = 0.62188384\n",
            "Iteration 214, loss = 0.62185220\n",
            "Iteration 215, loss = 0.62172664\n",
            "Iteration 216, loss = 0.62162763\n",
            "Iteration 217, loss = 0.62156020\n",
            "Iteration 218, loss = 0.62153141\n",
            "Iteration 219, loss = 0.62146817\n",
            "Iteration 220, loss = 0.62132327\n",
            "Iteration 221, loss = 0.62116982\n",
            "Iteration 222, loss = 0.62106042\n",
            "Iteration 223, loss = 0.62110222\n",
            "Iteration 224, loss = 0.62103164\n",
            "Iteration 225, loss = 0.62084842\n",
            "Iteration 226, loss = 0.62083234\n",
            "Iteration 227, loss = 0.62072537\n",
            "Iteration 228, loss = 0.62054725\n",
            "Iteration 229, loss = 0.62065968\n",
            "Iteration 230, loss = 0.62055861\n",
            "Iteration 231, loss = 0.62045452\n",
            "Iteration 232, loss = 0.62038547\n",
            "Iteration 233, loss = 0.62039530\n",
            "Iteration 234, loss = 0.62025729\n",
            "Iteration 235, loss = 0.62013133\n",
            "Iteration 236, loss = 0.62017451\n",
            "Iteration 237, loss = 0.61998490\n",
            "Iteration 238, loss = 0.61993882\n",
            "Iteration 239, loss = 0.61988508\n",
            "Iteration 240, loss = 0.61984826\n",
            "Iteration 241, loss = 0.61974902\n",
            "Iteration 242, loss = 0.61980451\n",
            "Iteration 243, loss = 0.61965691\n",
            "Iteration 244, loss = 0.61961508\n",
            "Iteration 245, loss = 0.61962571\n",
            "Iteration 246, loss = 0.61950723\n",
            "Iteration 247, loss = 0.61947756\n",
            "Iteration 248, loss = 0.61935225\n",
            "Iteration 249, loss = 0.61926620\n",
            "Iteration 250, loss = 0.61927305\n",
            "Iteration 251, loss = 0.61914835\n",
            "Iteration 252, loss = 0.61911141\n",
            "Iteration 253, loss = 0.61910989\n",
            "Iteration 254, loss = 0.61903816\n",
            "Iteration 255, loss = 0.61899676\n",
            "Iteration 256, loss = 0.61896975\n",
            "Iteration 257, loss = 0.61894786\n",
            "Iteration 258, loss = 0.61886461\n",
            "Iteration 259, loss = 0.61883214\n",
            "Iteration 260, loss = 0.61874587\n",
            "Iteration 261, loss = 0.61867687\n",
            "Iteration 262, loss = 0.61863725\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61711317\n",
            "Iteration 2, loss = 0.58588297\n",
            "Iteration 3, loss = 0.56397514\n",
            "Iteration 4, loss = 0.56739631\n",
            "Iteration 5, loss = 0.52834911\n",
            "Iteration 6, loss = 0.52560225\n",
            "Iteration 7, loss = 0.50580347\n",
            "Iteration 8, loss = 0.48990233\n",
            "Iteration 9, loss = 0.48602133\n",
            "Iteration 10, loss = 0.46871075\n",
            "Iteration 11, loss = 0.46404233\n",
            "Iteration 12, loss = 0.46578914\n",
            "Iteration 13, loss = 0.46188702\n",
            "Iteration 14, loss = 0.44630478\n",
            "Iteration 15, loss = 0.45874584\n",
            "Iteration 16, loss = 0.44002475\n",
            "Iteration 17, loss = 0.45153089\n",
            "Iteration 18, loss = 0.45246042\n",
            "Iteration 19, loss = 0.44399495\n",
            "Iteration 20, loss = 0.42143253\n",
            "Iteration 21, loss = 0.42686138\n",
            "Iteration 22, loss = 0.41850114\n",
            "Iteration 23, loss = 0.42764283\n",
            "Iteration 24, loss = 0.41498874\n",
            "Iteration 25, loss = 0.42745589\n",
            "Iteration 26, loss = 0.44615709\n",
            "Iteration 27, loss = 0.41959998\n",
            "Iteration 28, loss = 0.41697881\n",
            "Iteration 29, loss = 0.41455437\n",
            "Iteration 30, loss = 0.41872752\n",
            "Iteration 31, loss = 0.42938986\n",
            "Iteration 32, loss = 0.41094457\n",
            "Iteration 33, loss = 0.41023070\n",
            "Iteration 34, loss = 0.41660907\n",
            "Iteration 35, loss = 0.40022671\n",
            "Iteration 36, loss = 0.41498646\n",
            "Iteration 37, loss = 0.40988036\n",
            "Iteration 38, loss = 0.40548649\n",
            "Iteration 39, loss = 0.41150051\n",
            "Iteration 40, loss = 0.39597944\n",
            "Iteration 41, loss = 0.39224526\n",
            "Iteration 42, loss = 0.40413601\n",
            "Iteration 43, loss = 0.39216193\n",
            "Iteration 44, loss = 0.40485356\n",
            "Iteration 45, loss = 0.39361303\n",
            "Iteration 46, loss = 0.38428486\n",
            "Iteration 47, loss = 0.39337015\n",
            "Iteration 48, loss = 0.38582640\n",
            "Iteration 49, loss = 0.39061277\n",
            "Iteration 50, loss = 0.38938946\n",
            "Iteration 51, loss = 0.38850945\n",
            "Iteration 52, loss = 0.39246423\n",
            "Iteration 53, loss = 0.38783452\n",
            "Iteration 54, loss = 0.40227668\n",
            "Iteration 55, loss = 0.38857541\n",
            "Iteration 56, loss = 0.39394124\n",
            "Iteration 57, loss = 0.39845216\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62671105\n",
            "Iteration 2, loss = 0.59339007\n",
            "Iteration 3, loss = 0.59230336\n",
            "Iteration 4, loss = 0.58323995\n",
            "Iteration 5, loss = 0.56902776\n",
            "Iteration 6, loss = 0.55287532\n",
            "Iteration 7, loss = 0.54286343\n",
            "Iteration 8, loss = 0.54834198\n",
            "Iteration 9, loss = 0.52265940\n",
            "Iteration 10, loss = 0.49720811\n",
            "Iteration 11, loss = 0.49295301\n",
            "Iteration 12, loss = 0.47655295\n",
            "Iteration 13, loss = 0.46591361\n",
            "Iteration 14, loss = 0.46706480\n",
            "Iteration 15, loss = 0.45557815\n",
            "Iteration 16, loss = 0.46048223\n",
            "Iteration 17, loss = 0.46506028\n",
            "Iteration 18, loss = 0.42962104\n",
            "Iteration 19, loss = 0.42403260\n",
            "Iteration 20, loss = 0.42268793\n",
            "Iteration 21, loss = 0.43107730\n",
            "Iteration 22, loss = 0.42477715\n",
            "Iteration 23, loss = 0.41007781\n",
            "Iteration 24, loss = 0.40027922\n",
            "Iteration 25, loss = 0.42095328\n",
            "Iteration 26, loss = 0.41461561\n",
            "Iteration 27, loss = 0.39162355\n",
            "Iteration 28, loss = 0.39486761\n",
            "Iteration 29, loss = 0.40731902\n",
            "Iteration 30, loss = 0.40249640\n",
            "Iteration 31, loss = 0.38715598\n",
            "Iteration 32, loss = 0.39573640\n",
            "Iteration 33, loss = 0.38153019\n",
            "Iteration 34, loss = 0.38728137\n",
            "Iteration 35, loss = 0.38296617\n",
            "Iteration 36, loss = 0.37815238\n",
            "Iteration 37, loss = 0.37454904\n",
            "Iteration 38, loss = 0.40443768\n",
            "Iteration 39, loss = 0.38749549\n",
            "Iteration 40, loss = 0.38620440\n",
            "Iteration 41, loss = 0.38038873\n",
            "Iteration 42, loss = 0.38165642\n",
            "Iteration 43, loss = 0.37134285\n",
            "Iteration 44, loss = 0.38121529\n",
            "Iteration 45, loss = 0.37170459\n",
            "Iteration 46, loss = 0.37330884\n",
            "Iteration 47, loss = 0.37875571\n",
            "Iteration 48, loss = 0.37128295\n",
            "Iteration 49, loss = 0.35995209\n",
            "Iteration 50, loss = 0.36387926\n",
            "Iteration 51, loss = 0.36835882\n",
            "Iteration 52, loss = 0.36599113\n",
            "Iteration 53, loss = 0.38275437\n",
            "Iteration 54, loss = 0.36416039\n",
            "Iteration 55, loss = 0.36271951\n",
            "Iteration 56, loss = 0.35412801\n",
            "Iteration 57, loss = 0.36005283\n",
            "Iteration 58, loss = 0.37678003\n",
            "Iteration 59, loss = 0.35877341\n",
            "Iteration 60, loss = 0.36577449\n",
            "Iteration 61, loss = 0.34657182\n",
            "Iteration 62, loss = 0.34888738\n",
            "Iteration 63, loss = 0.36251310\n",
            "Iteration 64, loss = 0.36486351\n",
            "Iteration 65, loss = 0.36503490\n",
            "Iteration 66, loss = 0.34125180\n",
            "Iteration 67, loss = 0.35619959\n",
            "Iteration 68, loss = 0.34985099\n",
            "Iteration 69, loss = 0.34463130\n",
            "Iteration 70, loss = 0.34391437\n",
            "Iteration 71, loss = 0.34613242\n",
            "Iteration 72, loss = 0.35635658\n",
            "Iteration 73, loss = 0.34701813\n",
            "Iteration 74, loss = 0.36216969\n",
            "Iteration 75, loss = 0.35178287\n",
            "Iteration 76, loss = 0.35628081\n",
            "Iteration 77, loss = 0.34523731\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.65748571\n",
            "Iteration 2, loss = 0.59317381\n",
            "Iteration 3, loss = 0.58267427\n",
            "Iteration 4, loss = 0.58989102\n",
            "Iteration 5, loss = 0.55412028\n",
            "Iteration 6, loss = 0.53621455\n",
            "Iteration 7, loss = 0.52482132\n",
            "Iteration 8, loss = 0.51427479\n",
            "Iteration 9, loss = 0.49188346\n",
            "Iteration 10, loss = 0.48913300\n",
            "Iteration 11, loss = 0.48790341\n",
            "Iteration 12, loss = 0.48106166\n",
            "Iteration 13, loss = 0.47788704\n",
            "Iteration 14, loss = 0.45606693\n",
            "Iteration 15, loss = 0.44991647\n",
            "Iteration 16, loss = 0.45184257\n",
            "Iteration 17, loss = 0.43953231\n",
            "Iteration 18, loss = 0.44595767\n",
            "Iteration 19, loss = 0.44930384\n",
            "Iteration 20, loss = 0.43253886\n",
            "Iteration 21, loss = 0.44896437\n",
            "Iteration 22, loss = 0.43261186\n",
            "Iteration 23, loss = 0.43885558\n",
            "Iteration 24, loss = 0.42668533\n",
            "Iteration 25, loss = 0.41963454\n",
            "Iteration 26, loss = 0.43368251\n",
            "Iteration 27, loss = 0.42402902\n",
            "Iteration 28, loss = 0.40938189\n",
            "Iteration 29, loss = 0.40673849\n",
            "Iteration 30, loss = 0.41276641\n",
            "Iteration 31, loss = 0.45184338\n",
            "Iteration 32, loss = 0.43863955\n",
            "Iteration 33, loss = 0.40327103\n",
            "Iteration 34, loss = 0.40422484\n",
            "Iteration 35, loss = 0.40824817\n",
            "Iteration 36, loss = 0.40376217\n",
            "Iteration 37, loss = 0.39915655\n",
            "Iteration 38, loss = 0.39731521\n",
            "Iteration 39, loss = 0.41003219\n",
            "Iteration 40, loss = 0.41661523\n",
            "Iteration 41, loss = 0.40078440\n",
            "Iteration 42, loss = 0.38729789\n",
            "Iteration 43, loss = 0.38974454\n",
            "Iteration 44, loss = 0.39090117\n",
            "Iteration 45, loss = 0.40482501\n",
            "Iteration 46, loss = 0.38194910\n",
            "Iteration 47, loss = 0.38562295\n",
            "Iteration 48, loss = 0.38834142\n",
            "Iteration 49, loss = 0.37707854\n",
            "Iteration 50, loss = 0.40235316\n",
            "Iteration 51, loss = 0.37779902\n",
            "Iteration 52, loss = 0.38557151\n",
            "Iteration 53, loss = 0.38294223\n",
            "Iteration 54, loss = 0.37395905\n",
            "Iteration 55, loss = 0.39015522\n",
            "Iteration 56, loss = 0.38546745\n",
            "Iteration 57, loss = 0.37948295\n",
            "Iteration 58, loss = 0.37515127\n",
            "Iteration 59, loss = 0.37668642\n",
            "Iteration 60, loss = 0.37192043\n",
            "Iteration 61, loss = 0.37708555\n",
            "Iteration 62, loss = 0.38083877\n",
            "Iteration 63, loss = 0.38930043\n",
            "Iteration 64, loss = 0.37156719\n",
            "Iteration 65, loss = 0.38106516\n",
            "Iteration 66, loss = 0.37368238\n",
            "Iteration 67, loss = 0.36600781\n",
            "Iteration 68, loss = 0.36114810\n",
            "Iteration 69, loss = 0.36130185\n",
            "Iteration 70, loss = 0.37057530\n",
            "Iteration 71, loss = 0.36084175\n",
            "Iteration 72, loss = 0.36872913\n",
            "Iteration 73, loss = 0.36806882\n",
            "Iteration 74, loss = 0.35679156\n",
            "Iteration 75, loss = 0.39090992\n",
            "Iteration 76, loss = 0.36493217\n",
            "Iteration 77, loss = 0.36205140\n",
            "Iteration 78, loss = 0.36362738\n",
            "Iteration 79, loss = 0.35481557\n",
            "Iteration 80, loss = 0.36142606\n",
            "Iteration 81, loss = 0.35940653\n",
            "Iteration 82, loss = 0.35580438\n",
            "Iteration 83, loss = 0.36771273\n",
            "Iteration 84, loss = 0.35750301\n",
            "Iteration 85, loss = 0.37552709\n",
            "Iteration 86, loss = 0.35114353\n",
            "Iteration 87, loss = 0.35239638\n",
            "Iteration 88, loss = 0.35841663\n",
            "Iteration 89, loss = 0.34503897\n",
            "Iteration 90, loss = 0.35104546\n",
            "Iteration 91, loss = 0.34920434\n",
            "Iteration 92, loss = 0.36038051\n",
            "Iteration 93, loss = 0.35524395\n",
            "Iteration 94, loss = 0.35641830\n",
            "Iteration 95, loss = 0.34662566\n",
            "Iteration 96, loss = 0.34863265\n",
            "Iteration 97, loss = 0.36142589\n",
            "Iteration 98, loss = 0.34594429\n",
            "Iteration 99, loss = 0.34547916\n",
            "Iteration 100, loss = 0.35379678\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.63680743\n",
            "Iteration 2, loss = 0.61695773\n",
            "Iteration 3, loss = 0.59733933\n",
            "Iteration 4, loss = 0.58369125\n",
            "Iteration 5, loss = 0.55773883\n",
            "Iteration 6, loss = 0.53248525\n",
            "Iteration 7, loss = 0.53636970\n",
            "Iteration 8, loss = 0.50278957\n",
            "Iteration 9, loss = 0.48615134\n",
            "Iteration 10, loss = 0.48091452\n",
            "Iteration 11, loss = 0.48263308\n",
            "Iteration 12, loss = 0.46061881\n",
            "Iteration 13, loss = 0.46179419\n",
            "Iteration 14, loss = 0.46149672\n",
            "Iteration 15, loss = 0.45101947\n",
            "Iteration 16, loss = 0.47836267\n",
            "Iteration 17, loss = 0.44122466\n",
            "Iteration 18, loss = 0.44235724\n",
            "Iteration 19, loss = 0.45079313\n",
            "Iteration 20, loss = 0.45398760\n",
            "Iteration 21, loss = 0.44964968\n",
            "Iteration 22, loss = 0.43498622\n",
            "Iteration 23, loss = 0.41865514\n",
            "Iteration 24, loss = 0.42324671\n",
            "Iteration 25, loss = 0.44352870\n",
            "Iteration 26, loss = 0.42879573\n",
            "Iteration 27, loss = 0.42614768\n",
            "Iteration 28, loss = 0.41757296\n",
            "Iteration 29, loss = 0.42952166\n",
            "Iteration 30, loss = 0.43648299\n",
            "Iteration 31, loss = 0.43526196\n",
            "Iteration 32, loss = 0.45440501\n",
            "Iteration 33, loss = 0.41543310\n",
            "Iteration 34, loss = 0.42504310\n",
            "Iteration 35, loss = 0.41750825\n",
            "Iteration 36, loss = 0.42077820\n",
            "Iteration 37, loss = 0.42716758\n",
            "Iteration 38, loss = 0.41325012\n",
            "Iteration 39, loss = 0.41921715\n",
            "Iteration 40, loss = 0.41050272\n",
            "Iteration 41, loss = 0.42693071\n",
            "Iteration 42, loss = 0.40598191\n",
            "Iteration 43, loss = 0.40480453\n",
            "Iteration 44, loss = 0.40214106\n",
            "Iteration 45, loss = 0.42092387\n",
            "Iteration 46, loss = 0.39629938\n",
            "Iteration 47, loss = 0.39857198\n",
            "Iteration 48, loss = 0.39670752\n",
            "Iteration 49, loss = 0.41440441\n",
            "Iteration 50, loss = 0.42194471\n",
            "Iteration 51, loss = 0.39504040\n",
            "Iteration 52, loss = 0.42019160\n",
            "Iteration 53, loss = 0.41297821\n",
            "Iteration 54, loss = 0.40656219\n",
            "Iteration 55, loss = 0.40758513\n",
            "Iteration 56, loss = 0.39514579\n",
            "Iteration 57, loss = 0.39202394\n",
            "Iteration 58, loss = 0.40041680\n",
            "Iteration 59, loss = 0.38687388\n",
            "Iteration 60, loss = 0.39020015\n",
            "Iteration 61, loss = 0.39274624\n",
            "Iteration 62, loss = 0.38857247\n",
            "Iteration 63, loss = 0.39105767\n",
            "Iteration 64, loss = 0.38421371\n",
            "Iteration 65, loss = 0.38461783\n",
            "Iteration 66, loss = 0.40105030\n",
            "Iteration 67, loss = 0.38643128\n",
            "Iteration 68, loss = 0.39176649\n",
            "Iteration 69, loss = 0.38725896\n",
            "Iteration 70, loss = 0.38224932\n",
            "Iteration 71, loss = 0.38799285\n",
            "Iteration 72, loss = 0.40385031\n",
            "Iteration 73, loss = 0.37659796\n",
            "Iteration 74, loss = 0.37855137\n",
            "Iteration 75, loss = 0.38886705\n",
            "Iteration 76, loss = 0.38684582\n",
            "Iteration 77, loss = 0.37573135\n",
            "Iteration 78, loss = 0.37857401\n",
            "Iteration 79, loss = 0.38312280\n",
            "Iteration 80, loss = 0.37108269\n",
            "Iteration 81, loss = 0.38331342\n",
            "Iteration 82, loss = 0.39493035\n",
            "Iteration 83, loss = 0.41680401\n",
            "Iteration 84, loss = 0.38118648\n",
            "Iteration 85, loss = 0.37500447\n",
            "Iteration 86, loss = 0.36776815\n",
            "Iteration 87, loss = 0.37987752\n",
            "Iteration 88, loss = 0.37257294\n",
            "Iteration 89, loss = 0.36097044\n",
            "Iteration 90, loss = 0.36381773\n",
            "Iteration 91, loss = 0.37711485\n",
            "Iteration 92, loss = 0.37309653\n",
            "Iteration 93, loss = 0.37263719\n",
            "Iteration 94, loss = 0.36701556\n",
            "Iteration 95, loss = 0.37318403\n",
            "Iteration 96, loss = 0.37529207\n",
            "Iteration 97, loss = 0.37005269\n",
            "Iteration 98, loss = 0.40481791\n",
            "Iteration 99, loss = 0.36391863\n",
            "Iteration 100, loss = 0.36261214\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64839391\n",
            "Iteration 2, loss = 0.61179238\n",
            "Iteration 3, loss = 0.60597546\n",
            "Iteration 4, loss = 0.58784028\n",
            "Iteration 5, loss = 0.56050849\n",
            "Iteration 6, loss = 0.54157457\n",
            "Iteration 7, loss = 0.52296071\n",
            "Iteration 8, loss = 0.50348896\n",
            "Iteration 9, loss = 0.49127502\n",
            "Iteration 10, loss = 0.49525517\n",
            "Iteration 11, loss = 0.47402514\n",
            "Iteration 12, loss = 0.46424438\n",
            "Iteration 13, loss = 0.48443750\n",
            "Iteration 14, loss = 0.46213446\n",
            "Iteration 15, loss = 0.46185241\n",
            "Iteration 16, loss = 0.43997318\n",
            "Iteration 17, loss = 0.43612561\n",
            "Iteration 18, loss = 0.43540054\n",
            "Iteration 19, loss = 0.44095115\n",
            "Iteration 20, loss = 0.43057059\n",
            "Iteration 21, loss = 0.43234121\n",
            "Iteration 22, loss = 0.43427242\n",
            "Iteration 23, loss = 0.42655875\n",
            "Iteration 24, loss = 0.41948030\n",
            "Iteration 25, loss = 0.42269591\n",
            "Iteration 26, loss = 0.42234685\n",
            "Iteration 27, loss = 0.41586023\n",
            "Iteration 28, loss = 0.42120910\n",
            "Iteration 29, loss = 0.42538454\n",
            "Iteration 30, loss = 0.40711185\n",
            "Iteration 31, loss = 0.43328582\n",
            "Iteration 32, loss = 0.41535658\n",
            "Iteration 33, loss = 0.40908893\n",
            "Iteration 34, loss = 0.42076361\n",
            "Iteration 35, loss = 0.41093956\n",
            "Iteration 36, loss = 0.40716212\n",
            "Iteration 37, loss = 0.40767591\n",
            "Iteration 38, loss = 0.39980514\n",
            "Iteration 39, loss = 0.40319899\n",
            "Iteration 40, loss = 0.44287221\n",
            "Iteration 41, loss = 0.40703639\n",
            "Iteration 42, loss = 0.39609263\n",
            "Iteration 43, loss = 0.39956411\n",
            "Iteration 44, loss = 0.39451082\n",
            "Iteration 45, loss = 0.38956361\n",
            "Iteration 46, loss = 0.41218149\n",
            "Iteration 47, loss = 0.38886620\n",
            "Iteration 48, loss = 0.39375207\n",
            "Iteration 49, loss = 0.39117439\n",
            "Iteration 50, loss = 0.39554460\n",
            "Iteration 51, loss = 0.39295446\n",
            "Iteration 52, loss = 0.38748549\n",
            "Iteration 53, loss = 0.40045370\n",
            "Iteration 54, loss = 0.39957702\n",
            "Iteration 55, loss = 0.40133728\n",
            "Iteration 56, loss = 0.38475338\n",
            "Iteration 57, loss = 0.40600512\n",
            "Iteration 58, loss = 0.39374614\n",
            "Iteration 59, loss = 0.39151386\n",
            "Iteration 60, loss = 0.39221409\n",
            "Iteration 61, loss = 0.38068512\n",
            "Iteration 62, loss = 0.39865320\n",
            "Iteration 63, loss = 0.38237982\n",
            "Iteration 64, loss = 0.38095419\n",
            "Iteration 65, loss = 0.38998381\n",
            "Iteration 66, loss = 0.38282925\n",
            "Iteration 67, loss = 0.38160158\n",
            "Iteration 68, loss = 0.38654377\n",
            "Iteration 69, loss = 0.38827768\n",
            "Iteration 70, loss = 0.38992035\n",
            "Iteration 71, loss = 0.38798798\n",
            "Iteration 72, loss = 0.38300016\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.80190705\n",
            "Iteration 2, loss = 0.69226586\n",
            "Iteration 3, loss = 0.64221016\n",
            "Iteration 4, loss = 0.62634972\n",
            "Iteration 5, loss = 0.61881388\n",
            "Iteration 6, loss = 0.61318813\n",
            "Iteration 7, loss = 0.60995210\n",
            "Iteration 8, loss = 0.60805246\n",
            "Iteration 9, loss = 0.60647844\n",
            "Iteration 10, loss = 0.60504760\n",
            "Iteration 11, loss = 0.60387904\n",
            "Iteration 12, loss = 0.60263989\n",
            "Iteration 13, loss = 0.60143478\n",
            "Iteration 14, loss = 0.60056057\n",
            "Iteration 15, loss = 0.59958728\n",
            "Iteration 16, loss = 0.59902113\n",
            "Iteration 17, loss = 0.59850448\n",
            "Iteration 18, loss = 0.59792033\n",
            "Iteration 19, loss = 0.59690262\n",
            "Iteration 20, loss = 0.59698516\n",
            "Iteration 21, loss = 0.59607531\n",
            "Iteration 22, loss = 0.59576065\n",
            "Iteration 23, loss = 0.59549047\n",
            "Iteration 24, loss = 0.59482912\n",
            "Iteration 25, loss = 0.59395239\n",
            "Iteration 26, loss = 0.59351886\n",
            "Iteration 27, loss = 0.59374974\n",
            "Iteration 28, loss = 0.59328606\n",
            "Iteration 29, loss = 0.59251939\n",
            "Iteration 30, loss = 0.59176201\n",
            "Iteration 31, loss = 0.59184025\n",
            "Iteration 32, loss = 0.59145273\n",
            "Iteration 33, loss = 0.59137100\n",
            "Iteration 34, loss = 0.59068368\n",
            "Iteration 35, loss = 0.59065053\n",
            "Iteration 36, loss = 0.59011719\n",
            "Iteration 37, loss = 0.58987345\n",
            "Iteration 38, loss = 0.58973325\n",
            "Iteration 39, loss = 0.58945747\n",
            "Iteration 40, loss = 0.58928983\n",
            "Iteration 41, loss = 0.58874280\n",
            "Iteration 42, loss = 0.58885594\n",
            "Iteration 43, loss = 0.58817459\n",
            "Iteration 44, loss = 0.58800424\n",
            "Iteration 45, loss = 0.58799656\n",
            "Iteration 46, loss = 0.58760215\n",
            "Iteration 47, loss = 0.58779009\n",
            "Iteration 48, loss = 0.58702039\n",
            "Iteration 49, loss = 0.58678605\n",
            "Iteration 50, loss = 0.58662325\n",
            "Iteration 51, loss = 0.58611668\n",
            "Iteration 52, loss = 0.58639863\n",
            "Iteration 53, loss = 0.58596064\n",
            "Iteration 54, loss = 0.58587252\n",
            "Iteration 55, loss = 0.58523865\n",
            "Iteration 56, loss = 0.58519625\n",
            "Iteration 57, loss = 0.58506862\n",
            "Iteration 58, loss = 0.58481296\n",
            "Iteration 59, loss = 0.58457168\n",
            "Iteration 60, loss = 0.58433041\n",
            "Iteration 61, loss = 0.58459108\n",
            "Iteration 62, loss = 0.58387347\n",
            "Iteration 63, loss = 0.58364365\n",
            "Iteration 64, loss = 0.58316449\n",
            "Iteration 65, loss = 0.58339332\n",
            "Iteration 66, loss = 0.58284609\n",
            "Iteration 67, loss = 0.58246140\n",
            "Iteration 68, loss = 0.58272487\n",
            "Iteration 69, loss = 0.58253999\n",
            "Iteration 70, loss = 0.58199537\n",
            "Iteration 71, loss = 0.58249355\n",
            "Iteration 72, loss = 0.58173526\n",
            "Iteration 73, loss = 0.58173980\n",
            "Iteration 74, loss = 0.58132771\n",
            "Iteration 75, loss = 0.58108338\n",
            "Iteration 76, loss = 0.58168851\n",
            "Iteration 77, loss = 0.58058319\n",
            "Iteration 78, loss = 0.58074711\n",
            "Iteration 79, loss = 0.58032930\n",
            "Iteration 80, loss = 0.58050071\n",
            "Iteration 81, loss = 0.57994390\n",
            "Iteration 82, loss = 0.58045887\n",
            "Iteration 83, loss = 0.57943221\n",
            "Iteration 84, loss = 0.57913602\n",
            "Iteration 85, loss = 0.58046592\n",
            "Iteration 86, loss = 0.57958558\n",
            "Iteration 87, loss = 0.57905549\n",
            "Iteration 88, loss = 0.57884402\n",
            "Iteration 89, loss = 0.57878431\n",
            "Iteration 90, loss = 0.57841644\n",
            "Iteration 91, loss = 0.57792308\n",
            "Iteration 92, loss = 0.57809396\n",
            "Iteration 93, loss = 0.57741969\n",
            "Iteration 94, loss = 0.57734396\n",
            "Iteration 95, loss = 0.57693701\n",
            "Iteration 96, loss = 0.57726844\n",
            "Iteration 97, loss = 0.57665780\n",
            "Iteration 98, loss = 0.57647906\n",
            "Iteration 99, loss = 0.57649380\n",
            "Iteration 100, loss = 0.57726225\n",
            "Iteration 101, loss = 0.57720238\n",
            "Iteration 102, loss = 0.57571376\n",
            "Iteration 103, loss = 0.57607925\n",
            "Iteration 104, loss = 0.57588645\n",
            "Iteration 105, loss = 0.57551073\n",
            "Iteration 106, loss = 0.57573089\n",
            "Iteration 107, loss = 0.57494738\n",
            "Iteration 108, loss = 0.57511898\n",
            "Iteration 109, loss = 0.57460602\n",
            "Iteration 110, loss = 0.57439484\n",
            "Iteration 111, loss = 0.57463469\n",
            "Iteration 112, loss = 0.57460155\n",
            "Iteration 113, loss = 0.57369372\n",
            "Iteration 114, loss = 0.57396369\n",
            "Iteration 115, loss = 0.57371942\n",
            "Iteration 116, loss = 0.57380102\n",
            "Iteration 117, loss = 0.57338518\n",
            "Iteration 118, loss = 0.57364665\n",
            "Iteration 119, loss = 0.57332799\n",
            "Iteration 120, loss = 0.57333954\n",
            "Iteration 121, loss = 0.57222626\n",
            "Iteration 122, loss = 0.57240012\n",
            "Iteration 123, loss = 0.57344898\n",
            "Iteration 124, loss = 0.57227217\n",
            "Iteration 125, loss = 0.57208801\n",
            "Iteration 126, loss = 0.57212620\n",
            "Iteration 127, loss = 0.57175157\n",
            "Iteration 128, loss = 0.57175979\n",
            "Iteration 129, loss = 0.57099244\n",
            "Iteration 130, loss = 0.57215600\n",
            "Iteration 131, loss = 0.57065739\n",
            "Iteration 132, loss = 0.57151088\n",
            "Iteration 133, loss = 0.57067094\n",
            "Iteration 134, loss = 0.57102245\n",
            "Iteration 135, loss = 0.57012561\n",
            "Iteration 136, loss = 0.57160551\n",
            "Iteration 137, loss = 0.57113688\n",
            "Iteration 138, loss = 0.57124780\n",
            "Iteration 139, loss = 0.57023492\n",
            "Iteration 140, loss = 0.56928341\n",
            "Iteration 141, loss = 0.56913685\n",
            "Iteration 142, loss = 0.56941745\n",
            "Iteration 143, loss = 0.56962267\n",
            "Iteration 144, loss = 0.56916505\n",
            "Iteration 145, loss = 0.56901728\n",
            "Iteration 146, loss = 0.56949658\n",
            "Iteration 147, loss = 0.56827862\n",
            "Iteration 148, loss = 0.56808762\n",
            "Iteration 149, loss = 0.56862674\n",
            "Iteration 150, loss = 0.56853569\n",
            "Iteration 151, loss = 0.56703388\n",
            "Iteration 152, loss = 0.56756340\n",
            "Iteration 153, loss = 0.56758454\n",
            "Iteration 154, loss = 0.56779729\n",
            "Iteration 155, loss = 0.56738777\n",
            "Iteration 156, loss = 0.56717678\n",
            "Iteration 157, loss = 0.56734583\n",
            "Iteration 158, loss = 0.56787759\n",
            "Iteration 159, loss = 0.56640236\n",
            "Iteration 160, loss = 0.56705682\n",
            "Iteration 161, loss = 0.56594615\n",
            "Iteration 162, loss = 0.56584187\n",
            "Iteration 163, loss = 0.56726510\n",
            "Iteration 164, loss = 0.56663657\n",
            "Iteration 165, loss = 0.56563584\n",
            "Iteration 166, loss = 0.56619451\n",
            "Iteration 167, loss = 0.56547694\n",
            "Iteration 168, loss = 0.56585915\n",
            "Iteration 169, loss = 0.56515412\n",
            "Iteration 170, loss = 0.56442858\n",
            "Iteration 171, loss = 0.56551102\n",
            "Iteration 172, loss = 0.56465398\n",
            "Iteration 173, loss = 0.56444029\n",
            "Iteration 174, loss = 0.56504134\n",
            "Iteration 175, loss = 0.56445825\n",
            "Iteration 176, loss = 0.56507056\n",
            "Iteration 177, loss = 0.56461554\n",
            "Iteration 178, loss = 0.56351602\n",
            "Iteration 179, loss = 0.56363555\n",
            "Iteration 180, loss = 0.56308311\n",
            "Iteration 181, loss = 0.56400103\n",
            "Iteration 182, loss = 0.56352099\n",
            "Iteration 183, loss = 0.56252636\n",
            "Iteration 184, loss = 0.56327321\n",
            "Iteration 185, loss = 0.56183244\n",
            "Iteration 186, loss = 0.56136714\n",
            "Iteration 187, loss = 0.56173428\n",
            "Iteration 188, loss = 0.56084475\n",
            "Iteration 189, loss = 0.56105647\n",
            "Iteration 190, loss = 0.56035482\n",
            "Iteration 191, loss = 0.56154501\n",
            "Iteration 192, loss = 0.56026779\n",
            "Iteration 193, loss = 0.56108869\n",
            "Iteration 194, loss = 0.55969338\n",
            "Iteration 195, loss = 0.55943653\n",
            "Iteration 196, loss = 0.55934153\n",
            "Iteration 197, loss = 0.55952610\n",
            "Iteration 198, loss = 0.55895564\n",
            "Iteration 199, loss = 0.55905793\n",
            "Iteration 200, loss = 0.55899882\n",
            "Iteration 1, loss = 0.63004176\n",
            "Iteration 2, loss = 0.62606107\n",
            "Iteration 3, loss = 0.62184196\n",
            "Iteration 4, loss = 0.61844606\n",
            "Iteration 5, loss = 0.61548438\n",
            "Iteration 6, loss = 0.61356104\n",
            "Iteration 7, loss = 0.61172384\n",
            "Iteration 8, loss = 0.61054058\n",
            "Iteration 9, loss = 0.60906623\n",
            "Iteration 10, loss = 0.60786953\n",
            "Iteration 11, loss = 0.60679852\n",
            "Iteration 12, loss = 0.60610711\n",
            "Iteration 13, loss = 0.60505618\n",
            "Iteration 14, loss = 0.60437590\n",
            "Iteration 15, loss = 0.60394425\n",
            "Iteration 16, loss = 0.60324572\n",
            "Iteration 17, loss = 0.60277071\n",
            "Iteration 18, loss = 0.60235933\n",
            "Iteration 19, loss = 0.60200510\n",
            "Iteration 20, loss = 0.60121850\n",
            "Iteration 21, loss = 0.60082179\n",
            "Iteration 22, loss = 0.60038403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 23, loss = 0.60008446\n",
            "Iteration 24, loss = 0.59981759\n",
            "Iteration 25, loss = 0.59944064\n",
            "Iteration 26, loss = 0.59915115\n",
            "Iteration 27, loss = 0.59895464\n",
            "Iteration 28, loss = 0.59855969\n",
            "Iteration 29, loss = 0.59832097\n",
            "Iteration 30, loss = 0.59795600\n",
            "Iteration 31, loss = 0.59789951\n",
            "Iteration 32, loss = 0.59781430\n",
            "Iteration 33, loss = 0.59741165\n",
            "Iteration 34, loss = 0.59708326\n",
            "Iteration 35, loss = 0.59698147\n",
            "Iteration 36, loss = 0.59667963\n",
            "Iteration 37, loss = 0.59635218\n",
            "Iteration 38, loss = 0.59620589\n",
            "Iteration 39, loss = 0.59629469\n",
            "Iteration 40, loss = 0.59606770\n",
            "Iteration 41, loss = 0.59573556\n",
            "Iteration 42, loss = 0.59560397\n",
            "Iteration 43, loss = 0.59558126\n",
            "Iteration 44, loss = 0.59524365\n",
            "Iteration 45, loss = 0.59522175\n",
            "Iteration 46, loss = 0.59500881\n",
            "Iteration 47, loss = 0.59474425\n",
            "Iteration 48, loss = 0.59463213\n",
            "Iteration 49, loss = 0.59446679\n",
            "Iteration 50, loss = 0.59446452\n",
            "Iteration 51, loss = 0.59417786\n",
            "Iteration 52, loss = 0.59386483\n",
            "Iteration 53, loss = 0.59422597\n",
            "Iteration 54, loss = 0.59358475\n",
            "Iteration 55, loss = 0.59344031\n",
            "Iteration 56, loss = 0.59331989\n",
            "Iteration 57, loss = 0.59327058\n",
            "Iteration 58, loss = 0.59302625\n",
            "Iteration 59, loss = 0.59315631\n",
            "Iteration 60, loss = 0.59270061\n",
            "Iteration 61, loss = 0.59297511\n",
            "Iteration 62, loss = 0.59239948\n",
            "Iteration 63, loss = 0.59249598\n",
            "Iteration 64, loss = 0.59226632\n",
            "Iteration 65, loss = 0.59220456\n",
            "Iteration 66, loss = 0.59218685\n",
            "Iteration 67, loss = 0.59172599\n",
            "Iteration 68, loss = 0.59170795\n",
            "Iteration 69, loss = 0.59174629\n",
            "Iteration 70, loss = 0.59194158\n",
            "Iteration 71, loss = 0.59132340\n",
            "Iteration 72, loss = 0.59111763\n",
            "Iteration 73, loss = 0.59108442\n",
            "Iteration 74, loss = 0.59105924\n",
            "Iteration 75, loss = 0.59088164\n",
            "Iteration 76, loss = 0.59054287\n",
            "Iteration 77, loss = 0.59060347\n",
            "Iteration 78, loss = 0.59024606\n",
            "Iteration 79, loss = 0.59022568\n",
            "Iteration 80, loss = 0.59014882\n",
            "Iteration 81, loss = 0.59019531\n",
            "Iteration 82, loss = 0.58997044\n",
            "Iteration 83, loss = 0.59004770\n",
            "Iteration 84, loss = 0.58977269\n",
            "Iteration 85, loss = 0.58954140\n",
            "Iteration 86, loss = 0.58941592\n",
            "Iteration 87, loss = 0.58932528\n",
            "Iteration 88, loss = 0.58918603\n",
            "Iteration 89, loss = 0.58905598\n",
            "Iteration 90, loss = 0.58904698\n",
            "Iteration 91, loss = 0.58880816\n",
            "Iteration 92, loss = 0.58901085\n",
            "Iteration 93, loss = 0.58881525\n",
            "Iteration 94, loss = 0.58865515\n",
            "Iteration 95, loss = 0.58889376\n",
            "Iteration 96, loss = 0.58843300\n",
            "Iteration 97, loss = 0.58834440\n",
            "Iteration 98, loss = 0.58804853\n",
            "Iteration 99, loss = 0.58814561\n",
            "Iteration 100, loss = 0.58795619\n",
            "Iteration 101, loss = 0.58771144\n",
            "Iteration 102, loss = 0.58780064\n",
            "Iteration 103, loss = 0.58761038\n",
            "Iteration 104, loss = 0.58748987\n",
            "Iteration 105, loss = 0.58746109\n",
            "Iteration 106, loss = 0.58738269\n",
            "Iteration 107, loss = 0.58764393\n",
            "Iteration 108, loss = 0.58728037\n",
            "Iteration 109, loss = 0.58726738\n",
            "Iteration 110, loss = 0.58732199\n",
            "Iteration 111, loss = 0.58698574\n",
            "Iteration 112, loss = 0.58667492\n",
            "Iteration 113, loss = 0.58642214\n",
            "Iteration 114, loss = 0.58727929\n",
            "Iteration 115, loss = 0.58659134\n",
            "Iteration 116, loss = 0.58610843\n",
            "Iteration 117, loss = 0.58623288\n",
            "Iteration 118, loss = 0.58606960\n",
            "Iteration 119, loss = 0.58593785\n",
            "Iteration 120, loss = 0.58602410\n",
            "Iteration 121, loss = 0.58611994\n",
            "Iteration 122, loss = 0.58560052\n",
            "Iteration 123, loss = 0.58561001\n",
            "Iteration 124, loss = 0.58544594\n",
            "Iteration 125, loss = 0.58513354\n",
            "Iteration 126, loss = 0.58499827\n",
            "Iteration 127, loss = 0.58487716\n",
            "Iteration 128, loss = 0.58479329\n",
            "Iteration 129, loss = 0.58459960\n",
            "Iteration 130, loss = 0.58485701\n",
            "Iteration 131, loss = 0.58446172\n",
            "Iteration 132, loss = 0.58481000\n",
            "Iteration 133, loss = 0.58501527\n",
            "Iteration 134, loss = 0.58486465\n",
            "Iteration 135, loss = 0.58436370\n",
            "Iteration 136, loss = 0.58389491\n",
            "Iteration 137, loss = 0.58394248\n",
            "Iteration 138, loss = 0.58408547\n",
            "Iteration 139, loss = 0.58355956\n",
            "Iteration 140, loss = 0.58370369\n",
            "Iteration 141, loss = 0.58343591\n",
            "Iteration 142, loss = 0.58349758\n",
            "Iteration 143, loss = 0.58311766\n",
            "Iteration 144, loss = 0.58336256\n",
            "Iteration 145, loss = 0.58397319\n",
            "Iteration 146, loss = 0.58286495\n",
            "Iteration 147, loss = 0.58255069\n",
            "Iteration 148, loss = 0.58262399\n",
            "Iteration 149, loss = 0.58248273\n",
            "Iteration 150, loss = 0.58222158\n",
            "Iteration 151, loss = 0.58216030\n",
            "Iteration 152, loss = 0.58229351\n",
            "Iteration 153, loss = 0.58227697\n",
            "Iteration 154, loss = 0.58218007\n",
            "Iteration 155, loss = 0.58162461\n",
            "Iteration 156, loss = 0.58170155\n",
            "Iteration 157, loss = 0.58273005\n",
            "Iteration 158, loss = 0.58146853\n",
            "Iteration 159, loss = 0.58117275\n",
            "Iteration 160, loss = 0.58119500\n",
            "Iteration 161, loss = 0.58166362\n",
            "Iteration 162, loss = 0.58108658\n",
            "Iteration 163, loss = 0.58083250\n",
            "Iteration 164, loss = 0.58057337\n",
            "Iteration 165, loss = 0.58012628\n",
            "Iteration 166, loss = 0.58046142\n",
            "Iteration 167, loss = 0.58008193\n",
            "Iteration 168, loss = 0.57992782\n",
            "Iteration 169, loss = 0.57983026\n",
            "Iteration 170, loss = 0.57955445\n",
            "Iteration 171, loss = 0.57987516\n",
            "Iteration 172, loss = 0.57949443\n",
            "Iteration 173, loss = 0.57958552\n",
            "Iteration 174, loss = 0.57885171\n",
            "Iteration 175, loss = 0.57924480\n",
            "Iteration 176, loss = 0.57906660\n",
            "Iteration 177, loss = 0.57872652\n",
            "Iteration 178, loss = 0.57832279\n",
            "Iteration 179, loss = 0.57878759\n",
            "Iteration 180, loss = 0.57797754\n",
            "Iteration 181, loss = 0.57767639\n",
            "Iteration 182, loss = 0.57782387\n",
            "Iteration 183, loss = 0.57751313\n",
            "Iteration 184, loss = 0.57769597\n",
            "Iteration 185, loss = 0.57711339\n",
            "Iteration 186, loss = 0.57707378\n",
            "Iteration 187, loss = 0.57678524\n",
            "Iteration 188, loss = 0.57652150\n",
            "Iteration 189, loss = 0.57667987\n",
            "Iteration 190, loss = 0.57629963\n",
            "Iteration 191, loss = 0.57597635\n",
            "Iteration 192, loss = 0.57706477\n",
            "Iteration 193, loss = 0.57615954\n",
            "Iteration 194, loss = 0.57545673\n",
            "Iteration 195, loss = 0.57539051\n",
            "Iteration 196, loss = 0.57506853\n",
            "Iteration 197, loss = 0.57520469\n",
            "Iteration 198, loss = 0.57460728\n",
            "Iteration 199, loss = 0.57452150\n",
            "Iteration 200, loss = 0.57487263\n",
            "Iteration 1, loss = 0.71360589\n",
            "Iteration 2, loss = 0.68031225\n",
            "Iteration 3, loss = 0.65950196\n",
            "Iteration 4, loss = 0.64594313\n",
            "Iteration 5, loss = 0.63654000\n",
            "Iteration 6, loss = 0.62841463\n",
            "Iteration 7, loss = 0.62366696\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8, loss = 0.61987621\n",
            "Iteration 9, loss = 0.61668573\n",
            "Iteration 10, loss = 0.61399785\n",
            "Iteration 11, loss = 0.61257099\n",
            "Iteration 12, loss = 0.61034684\n",
            "Iteration 13, loss = 0.60913764\n",
            "Iteration 14, loss = 0.60784557\n",
            "Iteration 15, loss = 0.60674409\n",
            "Iteration 16, loss = 0.60568093\n",
            "Iteration 17, loss = 0.60524120\n",
            "Iteration 18, loss = 0.60444872\n",
            "Iteration 19, loss = 0.60341468\n",
            "Iteration 20, loss = 0.60333557\n",
            "Iteration 21, loss = 0.60189287\n",
            "Iteration 22, loss = 0.60102214\n",
            "Iteration 23, loss = 0.60018923\n",
            "Iteration 24, loss = 0.59968233\n",
            "Iteration 25, loss = 0.59843431\n",
            "Iteration 26, loss = 0.59765194\n",
            "Iteration 27, loss = 0.59603727\n",
            "Iteration 28, loss = 0.59533773\n",
            "Iteration 29, loss = 0.59377874\n",
            "Iteration 30, loss = 0.59378185\n",
            "Iteration 31, loss = 0.59267716\n",
            "Iteration 32, loss = 0.59209064\n",
            "Iteration 33, loss = 0.59186886\n",
            "Iteration 34, loss = 0.59168327\n",
            "Iteration 35, loss = 0.59094255\n",
            "Iteration 36, loss = 0.59043983\n",
            "Iteration 37, loss = 0.59006691\n",
            "Iteration 38, loss = 0.58968878\n",
            "Iteration 39, loss = 0.58901638\n",
            "Iteration 40, loss = 0.58889057\n",
            "Iteration 41, loss = 0.58846022\n",
            "Iteration 42, loss = 0.58817151\n",
            "Iteration 43, loss = 0.58774126\n",
            "Iteration 44, loss = 0.58791059\n",
            "Iteration 45, loss = 0.58685986\n",
            "Iteration 46, loss = 0.58682742\n",
            "Iteration 47, loss = 0.58636734\n",
            "Iteration 48, loss = 0.58589511\n",
            "Iteration 49, loss = 0.58568485\n",
            "Iteration 50, loss = 0.58582885\n",
            "Iteration 51, loss = 0.58579289\n",
            "Iteration 52, loss = 0.58461378\n",
            "Iteration 53, loss = 0.58410193\n",
            "Iteration 54, loss = 0.58426944\n",
            "Iteration 55, loss = 0.58359435\n",
            "Iteration 56, loss = 0.58331158\n",
            "Iteration 57, loss = 0.58273040\n",
            "Iteration 58, loss = 0.58322918\n",
            "Iteration 59, loss = 0.58257121\n",
            "Iteration 60, loss = 0.58295435\n",
            "Iteration 61, loss = 0.58205421\n",
            "Iteration 62, loss = 0.58192005\n",
            "Iteration 63, loss = 0.58119169\n",
            "Iteration 64, loss = 0.58092746\n",
            "Iteration 65, loss = 0.58056232\n",
            "Iteration 66, loss = 0.58034400\n",
            "Iteration 67, loss = 0.58041103\n",
            "Iteration 68, loss = 0.57986538\n",
            "Iteration 69, loss = 0.58002506\n",
            "Iteration 70, loss = 0.57916883\n",
            "Iteration 71, loss = 0.58053561\n",
            "Iteration 72, loss = 0.57897755\n",
            "Iteration 73, loss = 0.57909485\n",
            "Iteration 74, loss = 0.57909790\n",
            "Iteration 75, loss = 0.57776416\n",
            "Iteration 76, loss = 0.57852452\n",
            "Iteration 77, loss = 0.57740213\n",
            "Iteration 78, loss = 0.57800515\n",
            "Iteration 79, loss = 0.57809569\n",
            "Iteration 80, loss = 0.57753537\n",
            "Iteration 81, loss = 0.57607707\n",
            "Iteration 82, loss = 0.57607150\n",
            "Iteration 83, loss = 0.57547649\n",
            "Iteration 84, loss = 0.57614487\n",
            "Iteration 85, loss = 0.57603545\n",
            "Iteration 86, loss = 0.57479354\n",
            "Iteration 87, loss = 0.57500433\n",
            "Iteration 88, loss = 0.57500272\n",
            "Iteration 89, loss = 0.57514301\n",
            "Iteration 90, loss = 0.57349100\n",
            "Iteration 91, loss = 0.57399362\n",
            "Iteration 92, loss = 0.57353788\n",
            "Iteration 93, loss = 0.57442098\n",
            "Iteration 94, loss = 0.57365090\n",
            "Iteration 95, loss = 0.57390918\n",
            "Iteration 96, loss = 0.57322582\n",
            "Iteration 97, loss = 0.57201468\n",
            "Iteration 98, loss = 0.57223075\n",
            "Iteration 99, loss = 0.57207319\n",
            "Iteration 100, loss = 0.57132629\n",
            "Iteration 101, loss = 0.57136285\n",
            "Iteration 102, loss = 0.57186097\n",
            "Iteration 103, loss = 0.57144360\n",
            "Iteration 104, loss = 0.57175359\n",
            "Iteration 105, loss = 0.57043036\n",
            "Iteration 106, loss = 0.57108479\n",
            "Iteration 107, loss = 0.57162437\n",
            "Iteration 108, loss = 0.57061284\n",
            "Iteration 109, loss = 0.57014417\n",
            "Iteration 110, loss = 0.57085209\n",
            "Iteration 111, loss = 0.56975772\n",
            "Iteration 112, loss = 0.57039487\n",
            "Iteration 113, loss = 0.56870452\n",
            "Iteration 114, loss = 0.57117207\n",
            "Iteration 115, loss = 0.56959969\n",
            "Iteration 116, loss = 0.56816304\n",
            "Iteration 117, loss = 0.56809371\n",
            "Iteration 118, loss = 0.56907137\n",
            "Iteration 119, loss = 0.56746822\n",
            "Iteration 120, loss = 0.56699093\n",
            "Iteration 121, loss = 0.56801043\n",
            "Iteration 122, loss = 0.56614099\n",
            "Iteration 123, loss = 0.56836391\n",
            "Iteration 124, loss = 0.56632727\n",
            "Iteration 125, loss = 0.56668029\n",
            "Iteration 126, loss = 0.56623804\n",
            "Iteration 127, loss = 0.56756707\n",
            "Iteration 128, loss = 0.56565807\n",
            "Iteration 129, loss = 0.56629045\n",
            "Iteration 130, loss = 0.56598938\n",
            "Iteration 131, loss = 0.56541207\n",
            "Iteration 132, loss = 0.56614424\n",
            "Iteration 133, loss = 0.56451126\n",
            "Iteration 134, loss = 0.56831667\n",
            "Iteration 135, loss = 0.56489212\n",
            "Iteration 136, loss = 0.56511547\n",
            "Iteration 137, loss = 0.56357817\n",
            "Iteration 138, loss = 0.56383377\n",
            "Iteration 139, loss = 0.56348100\n",
            "Iteration 140, loss = 0.56338551\n",
            "Iteration 141, loss = 0.56357722\n",
            "Iteration 142, loss = 0.56333924\n",
            "Iteration 143, loss = 0.56245121\n",
            "Iteration 144, loss = 0.56607252\n",
            "Iteration 145, loss = 0.56185426\n",
            "Iteration 146, loss = 0.56199110\n",
            "Iteration 147, loss = 0.56154344\n",
            "Iteration 148, loss = 0.56167759\n",
            "Iteration 149, loss = 0.56304908\n",
            "Iteration 150, loss = 0.56031830\n",
            "Iteration 151, loss = 0.56025810\n",
            "Iteration 152, loss = 0.56057913\n",
            "Iteration 153, loss = 0.56021273\n",
            "Iteration 154, loss = 0.56260298\n",
            "Iteration 155, loss = 0.56274044\n",
            "Iteration 156, loss = 0.55960980\n",
            "Iteration 157, loss = 0.56270477\n",
            "Iteration 158, loss = 0.55968769\n",
            "Iteration 159, loss = 0.55795882\n",
            "Iteration 160, loss = 0.55791041\n",
            "Iteration 161, loss = 0.55820271\n",
            "Iteration 162, loss = 0.56078045\n",
            "Iteration 163, loss = 0.55810487\n",
            "Iteration 164, loss = 0.55699321\n",
            "Iteration 165, loss = 0.56123044\n",
            "Iteration 166, loss = 0.55660812\n",
            "Iteration 167, loss = 0.55717191\n",
            "Iteration 168, loss = 0.55697410\n",
            "Iteration 169, loss = 0.55655285\n",
            "Iteration 170, loss = 0.55873247\n",
            "Iteration 171, loss = 0.55939021\n",
            "Iteration 172, loss = 0.55611461\n",
            "Iteration 173, loss = 0.55874109\n",
            "Iteration 174, loss = 0.55633339\n",
            "Iteration 175, loss = 0.55505983\n",
            "Iteration 176, loss = 0.55549503\n",
            "Iteration 177, loss = 0.55581089\n",
            "Iteration 178, loss = 0.55517146\n",
            "Iteration 179, loss = 0.56135190\n",
            "Iteration 180, loss = 0.55615271\n",
            "Iteration 181, loss = 0.55557267\n",
            "Iteration 182, loss = 0.55438387\n",
            "Iteration 183, loss = 0.55383631\n",
            "Iteration 184, loss = 0.55255095\n",
            "Iteration 185, loss = 0.55261674\n",
            "Iteration 186, loss = 0.55319537\n",
            "Iteration 187, loss = 0.55308966\n",
            "Iteration 188, loss = 0.55365308\n",
            "Iteration 189, loss = 0.55454928\n",
            "Iteration 190, loss = 0.55239712\n",
            "Iteration 191, loss = 0.55305359\n",
            "Iteration 192, loss = 0.55249616\n",
            "Iteration 193, loss = 0.55196079\n",
            "Iteration 194, loss = 0.55336823\n",
            "Iteration 195, loss = 0.55681583\n",
            "Iteration 196, loss = 0.55273363\n",
            "Iteration 197, loss = 0.55157052\n",
            "Iteration 198, loss = 0.55206166\n",
            "Iteration 199, loss = 0.55011000\n",
            "Iteration 200, loss = 0.55137088\n",
            "Iteration 1, loss = 0.73744990\n",
            "Iteration 2, loss = 0.70323309\n",
            "Iteration 3, loss = 0.68157197\n",
            "Iteration 4, loss = 0.66527460\n",
            "Iteration 5, loss = 0.65417418\n",
            "Iteration 6, loss = 0.64632400\n",
            "Iteration 7, loss = 0.63824280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8, loss = 0.63370269\n",
            "Iteration 9, loss = 0.63040990\n",
            "Iteration 10, loss = 0.62907031\n",
            "Iteration 11, loss = 0.62863754\n",
            "Iteration 12, loss = 0.62759345\n",
            "Iteration 13, loss = 0.62717941\n",
            "Iteration 14, loss = 0.62730765\n",
            "Iteration 15, loss = 0.62582571\n",
            "Iteration 16, loss = 0.62511766\n",
            "Iteration 17, loss = 0.62370373\n",
            "Iteration 18, loss = 0.62252204\n",
            "Iteration 19, loss = 0.62221607\n",
            "Iteration 20, loss = 0.62192814\n",
            "Iteration 21, loss = 0.62128594\n",
            "Iteration 22, loss = 0.62083667\n",
            "Iteration 23, loss = 0.62115634\n",
            "Iteration 24, loss = 0.62179455\n",
            "Iteration 25, loss = 0.61929396\n",
            "Iteration 26, loss = 0.61921451\n",
            "Iteration 27, loss = 0.61838995\n",
            "Iteration 28, loss = 0.61835977\n",
            "Iteration 29, loss = 0.61758889\n",
            "Iteration 30, loss = 0.61733891\n",
            "Iteration 31, loss = 0.61693396\n",
            "Iteration 32, loss = 0.61679009\n",
            "Iteration 33, loss = 0.61647431\n",
            "Iteration 34, loss = 0.61611101\n",
            "Iteration 35, loss = 0.61590130\n",
            "Iteration 36, loss = 0.61599524\n",
            "Iteration 37, loss = 0.61548661\n",
            "Iteration 38, loss = 0.61535624\n",
            "Iteration 39, loss = 0.61488987\n",
            "Iteration 40, loss = 0.61483929\n",
            "Iteration 41, loss = 0.61490680\n",
            "Iteration 42, loss = 0.61492132\n",
            "Iteration 43, loss = 0.61406871\n",
            "Iteration 44, loss = 0.61444224\n",
            "Iteration 45, loss = 0.61362747\n",
            "Iteration 46, loss = 0.61382315\n",
            "Iteration 47, loss = 0.61342477\n",
            "Iteration 48, loss = 0.61323998\n",
            "Iteration 49, loss = 0.61305255\n",
            "Iteration 50, loss = 0.61317182\n",
            "Iteration 51, loss = 0.61286042\n",
            "Iteration 52, loss = 0.61241597\n",
            "Iteration 53, loss = 0.61223161\n",
            "Iteration 54, loss = 0.61265915\n",
            "Iteration 55, loss = 0.61186068\n",
            "Iteration 56, loss = 0.61157258\n",
            "Iteration 57, loss = 0.61178922\n",
            "Iteration 58, loss = 0.61146027\n",
            "Iteration 59, loss = 0.61125919\n",
            "Iteration 60, loss = 0.61115018\n",
            "Iteration 61, loss = 0.61079837\n",
            "Iteration 62, loss = 0.61094582\n",
            "Iteration 63, loss = 0.61038905\n",
            "Iteration 64, loss = 0.61011915\n",
            "Iteration 65, loss = 0.60991255\n",
            "Iteration 66, loss = 0.61027315\n",
            "Iteration 67, loss = 0.61020827\n",
            "Iteration 68, loss = 0.60991751\n",
            "Iteration 69, loss = 0.60929911\n",
            "Iteration 70, loss = 0.60926488\n",
            "Iteration 71, loss = 0.60873468\n",
            "Iteration 72, loss = 0.60884878\n",
            "Iteration 73, loss = 0.60862352\n",
            "Iteration 74, loss = 0.60846639\n",
            "Iteration 75, loss = 0.60803645\n",
            "Iteration 76, loss = 0.60791354\n",
            "Iteration 77, loss = 0.60799857\n",
            "Iteration 78, loss = 0.60801947\n",
            "Iteration 79, loss = 0.60756449\n",
            "Iteration 80, loss = 0.60725779\n",
            "Iteration 81, loss = 0.60702294\n",
            "Iteration 82, loss = 0.60691536\n",
            "Iteration 83, loss = 0.60667609\n",
            "Iteration 84, loss = 0.60632829\n",
            "Iteration 85, loss = 0.60677170\n",
            "Iteration 86, loss = 0.60584464\n",
            "Iteration 87, loss = 0.60573229\n",
            "Iteration 88, loss = 0.60538820\n",
            "Iteration 89, loss = 0.60512755\n",
            "Iteration 90, loss = 0.60512443\n",
            "Iteration 91, loss = 0.60517138\n",
            "Iteration 92, loss = 0.60438191\n",
            "Iteration 93, loss = 0.60491653\n",
            "Iteration 94, loss = 0.60444706\n",
            "Iteration 95, loss = 0.60398923\n",
            "Iteration 96, loss = 0.60395986\n",
            "Iteration 97, loss = 0.60355869\n",
            "Iteration 98, loss = 0.60368867\n",
            "Iteration 99, loss = 0.60332939\n",
            "Iteration 100, loss = 0.60294287\n",
            "Iteration 101, loss = 0.60276830\n",
            "Iteration 102, loss = 0.60289407\n",
            "Iteration 103, loss = 0.60260402\n",
            "Iteration 104, loss = 0.60203876\n",
            "Iteration 105, loss = 0.60192861\n",
            "Iteration 106, loss = 0.60183368\n",
            "Iteration 107, loss = 0.60123395\n",
            "Iteration 108, loss = 0.60146951\n",
            "Iteration 109, loss = 0.60089275\n",
            "Iteration 110, loss = 0.60069012\n",
            "Iteration 111, loss = 0.60056088\n",
            "Iteration 112, loss = 0.60057090\n",
            "Iteration 113, loss = 0.60021125\n",
            "Iteration 114, loss = 0.59989702\n",
            "Iteration 115, loss = 0.60019997\n",
            "Iteration 116, loss = 0.59981407\n",
            "Iteration 117, loss = 0.59951078\n",
            "Iteration 118, loss = 0.59959880\n",
            "Iteration 119, loss = 0.59898270\n",
            "Iteration 120, loss = 0.59878169\n",
            "Iteration 121, loss = 0.59867886\n",
            "Iteration 122, loss = 0.59825632\n",
            "Iteration 123, loss = 0.59849592\n",
            "Iteration 124, loss = 0.59837607\n",
            "Iteration 125, loss = 0.59930225\n",
            "Iteration 126, loss = 0.59777966\n",
            "Iteration 127, loss = 0.59826994\n",
            "Iteration 128, loss = 0.59698787\n",
            "Iteration 129, loss = 0.59898656\n",
            "Iteration 130, loss = 0.59737735\n",
            "Iteration 131, loss = 0.59733612\n",
            "Iteration 132, loss = 0.59684924\n",
            "Iteration 133, loss = 0.59657043\n",
            "Iteration 134, loss = 0.59656293\n",
            "Iteration 135, loss = 0.59639563\n",
            "Iteration 136, loss = 0.59588677\n",
            "Iteration 137, loss = 0.59619240\n",
            "Iteration 138, loss = 0.59603081\n",
            "Iteration 139, loss = 0.59525618\n",
            "Iteration 140, loss = 0.59499771\n",
            "Iteration 141, loss = 0.59573045\n",
            "Iteration 142, loss = 0.59472251\n",
            "Iteration 143, loss = 0.59463638\n",
            "Iteration 144, loss = 0.59469179\n",
            "Iteration 145, loss = 0.59468473\n",
            "Iteration 146, loss = 0.59391250\n",
            "Iteration 147, loss = 0.59491049\n",
            "Iteration 148, loss = 0.59463895\n",
            "Iteration 149, loss = 0.59374764\n",
            "Iteration 150, loss = 0.59435696\n",
            "Iteration 151, loss = 0.59540895\n",
            "Iteration 152, loss = 0.59314138\n",
            "Iteration 153, loss = 0.59408432\n",
            "Iteration 154, loss = 0.59252079\n",
            "Iteration 155, loss = 0.59404633\n",
            "Iteration 156, loss = 0.59295305\n",
            "Iteration 157, loss = 0.59253432\n",
            "Iteration 158, loss = 0.59183515\n",
            "Iteration 159, loss = 0.59291328\n",
            "Iteration 160, loss = 0.59178530\n",
            "Iteration 161, loss = 0.59189576\n",
            "Iteration 162, loss = 0.59173240\n",
            "Iteration 163, loss = 0.59152573\n",
            "Iteration 164, loss = 0.59085603\n",
            "Iteration 165, loss = 0.59134802\n",
            "Iteration 166, loss = 0.59075313\n",
            "Iteration 167, loss = 0.59049698\n",
            "Iteration 168, loss = 0.59011887\n",
            "Iteration 169, loss = 0.58976120\n",
            "Iteration 170, loss = 0.58986357\n",
            "Iteration 171, loss = 0.59042511\n",
            "Iteration 172, loss = 0.58932955\n",
            "Iteration 173, loss = 0.58890635\n",
            "Iteration 174, loss = 0.58884663\n",
            "Iteration 175, loss = 0.58873519\n",
            "Iteration 176, loss = 0.58900113\n",
            "Iteration 177, loss = 0.58879853\n",
            "Iteration 178, loss = 0.58824639\n",
            "Iteration 179, loss = 0.58867617\n",
            "Iteration 180, loss = 0.58805454\n",
            "Iteration 181, loss = 0.58833782\n",
            "Iteration 182, loss = 0.58799400\n",
            "Iteration 183, loss = 0.58833219\n",
            "Iteration 184, loss = 0.58722927\n",
            "Iteration 185, loss = 0.58685358\n",
            "Iteration 186, loss = 0.58714203\n",
            "Iteration 187, loss = 0.58714128\n",
            "Iteration 188, loss = 0.58715359\n",
            "Iteration 189, loss = 0.58613457\n",
            "Iteration 190, loss = 0.58700186\n",
            "Iteration 191, loss = 0.58636640\n",
            "Iteration 192, loss = 0.58574275\n",
            "Iteration 193, loss = 0.58534334\n",
            "Iteration 194, loss = 0.58515846\n",
            "Iteration 195, loss = 0.58615828\n",
            "Iteration 196, loss = 0.58501746\n",
            "Iteration 197, loss = 0.58461198\n",
            "Iteration 198, loss = 0.58447101\n",
            "Iteration 199, loss = 0.58433870\n",
            "Iteration 200, loss = 0.58406836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.65472092\n",
            "Iteration 2, loss = 0.63366369\n",
            "Iteration 3, loss = 0.62745346\n",
            "Iteration 4, loss = 0.62236451\n",
            "Iteration 5, loss = 0.62087965\n",
            "Iteration 6, loss = 0.61989394\n",
            "Iteration 7, loss = 0.61844709\n",
            "Iteration 8, loss = 0.61773212\n",
            "Iteration 9, loss = 0.61724920\n",
            "Iteration 10, loss = 0.61566424\n",
            "Iteration 11, loss = 0.61556790\n",
            "Iteration 12, loss = 0.61473915\n",
            "Iteration 13, loss = 0.61361755\n",
            "Iteration 14, loss = 0.61327040\n",
            "Iteration 15, loss = 0.61264795\n",
            "Iteration 16, loss = 0.61196943\n",
            "Iteration 17, loss = 0.61006199\n",
            "Iteration 18, loss = 0.60948700\n",
            "Iteration 19, loss = 0.60833781\n",
            "Iteration 20, loss = 0.60829631\n",
            "Iteration 21, loss = 0.60678465\n",
            "Iteration 22, loss = 0.60668397\n",
            "Iteration 23, loss = 0.60529924\n",
            "Iteration 24, loss = 0.60500820\n",
            "Iteration 25, loss = 0.60432747\n",
            "Iteration 26, loss = 0.60374432\n",
            "Iteration 27, loss = 0.60248028\n",
            "Iteration 28, loss = 0.60206519\n",
            "Iteration 29, loss = 0.60249208\n",
            "Iteration 30, loss = 0.60078308\n",
            "Iteration 31, loss = 0.60103092\n",
            "Iteration 32, loss = 0.60010418\n",
            "Iteration 33, loss = 0.59925814\n",
            "Iteration 34, loss = 0.59987225\n",
            "Iteration 35, loss = 0.59888510\n",
            "Iteration 36, loss = 0.59870471\n",
            "Iteration 37, loss = 0.59697212\n",
            "Iteration 38, loss = 0.59677473\n",
            "Iteration 39, loss = 0.59767314\n",
            "Iteration 40, loss = 0.59566256\n",
            "Iteration 41, loss = 0.59457112\n",
            "Iteration 42, loss = 0.59449536\n",
            "Iteration 43, loss = 0.59429048\n",
            "Iteration 44, loss = 0.59307645\n",
            "Iteration 45, loss = 0.59249185\n",
            "Iteration 46, loss = 0.59198086\n",
            "Iteration 47, loss = 0.59264395\n",
            "Iteration 48, loss = 0.59112420\n",
            "Iteration 49, loss = 0.58998352\n",
            "Iteration 50, loss = 0.59020147\n",
            "Iteration 51, loss = 0.58931025\n",
            "Iteration 52, loss = 0.58830066\n",
            "Iteration 53, loss = 0.58838932\n",
            "Iteration 54, loss = 0.58700097\n",
            "Iteration 55, loss = 0.58723731\n",
            "Iteration 56, loss = 0.58732748\n",
            "Iteration 57, loss = 0.58652569\n",
            "Iteration 58, loss = 0.58798061\n",
            "Iteration 59, loss = 0.58576665\n",
            "Iteration 60, loss = 0.58584824\n",
            "Iteration 61, loss = 0.58610306\n",
            "Iteration 62, loss = 0.58493385\n",
            "Iteration 63, loss = 0.58431894\n",
            "Iteration 64, loss = 0.58406565\n",
            "Iteration 65, loss = 0.58324898\n",
            "Iteration 66, loss = 0.58290357\n",
            "Iteration 67, loss = 0.58246126\n",
            "Iteration 68, loss = 0.58227820\n",
            "Iteration 69, loss = 0.58236693\n",
            "Iteration 70, loss = 0.58186609\n",
            "Iteration 71, loss = 0.58323551\n",
            "Iteration 72, loss = 0.58179387\n",
            "Iteration 73, loss = 0.58158301\n",
            "Iteration 74, loss = 0.58048263\n",
            "Iteration 75, loss = 0.57960137\n",
            "Iteration 76, loss = 0.57907811\n",
            "Iteration 77, loss = 0.58008799\n",
            "Iteration 78, loss = 0.57917756\n",
            "Iteration 79, loss = 0.57927931\n",
            "Iteration 80, loss = 0.57928517\n",
            "Iteration 81, loss = 0.57805693\n",
            "Iteration 82, loss = 0.57733997\n",
            "Iteration 83, loss = 0.57697823\n",
            "Iteration 84, loss = 0.58005183\n",
            "Iteration 85, loss = 0.57658035\n",
            "Iteration 86, loss = 0.58199354\n",
            "Iteration 87, loss = 0.57992237\n",
            "Iteration 88, loss = 0.57631379\n",
            "Iteration 89, loss = 0.57557859\n",
            "Iteration 90, loss = 0.57662007\n",
            "Iteration 91, loss = 0.57669814\n",
            "Iteration 92, loss = 0.57445848\n",
            "Iteration 93, loss = 0.57603142\n",
            "Iteration 94, loss = 0.57429591\n",
            "Iteration 95, loss = 0.57426057\n",
            "Iteration 96, loss = 0.57285505\n",
            "Iteration 97, loss = 0.57259762\n",
            "Iteration 98, loss = 0.57451639\n",
            "Iteration 99, loss = 0.57372191\n",
            "Iteration 100, loss = 0.57211319\n",
            "Iteration 101, loss = 0.57555776\n",
            "Iteration 102, loss = 0.57200225\n",
            "Iteration 103, loss = 0.57330153\n",
            "Iteration 104, loss = 0.57499934\n",
            "Iteration 105, loss = 0.57145660\n",
            "Iteration 106, loss = 0.57413969\n",
            "Iteration 107, loss = 0.57356461\n",
            "Iteration 108, loss = 0.56835027\n",
            "Iteration 109, loss = 0.56963656\n",
            "Iteration 110, loss = 0.56962044\n",
            "Iteration 111, loss = 0.57332175\n",
            "Iteration 112, loss = 0.56860629\n",
            "Iteration 113, loss = 0.56827018\n",
            "Iteration 114, loss = 0.56784999\n",
            "Iteration 115, loss = 0.56825173\n",
            "Iteration 116, loss = 0.56544379\n",
            "Iteration 117, loss = 0.56890825\n",
            "Iteration 118, loss = 0.56644132\n",
            "Iteration 119, loss = 0.56668812\n",
            "Iteration 120, loss = 0.56703661\n",
            "Iteration 121, loss = 0.56812731\n",
            "Iteration 122, loss = 0.56583477\n",
            "Iteration 123, loss = 0.56504552\n",
            "Iteration 124, loss = 0.56250238\n",
            "Iteration 125, loss = 0.56708265\n",
            "Iteration 126, loss = 0.56661612\n",
            "Iteration 127, loss = 0.56355959\n",
            "Iteration 128, loss = 0.56454978\n",
            "Iteration 129, loss = 0.56448840\n",
            "Iteration 130, loss = 0.56389104\n",
            "Iteration 131, loss = 0.56491084\n",
            "Iteration 132, loss = 0.56234530\n",
            "Iteration 133, loss = 0.56011973\n",
            "Iteration 134, loss = 0.56371410\n",
            "Iteration 135, loss = 0.56042627\n",
            "Iteration 136, loss = 0.55944936\n",
            "Iteration 137, loss = 0.55906329\n",
            "Iteration 138, loss = 0.55708780\n",
            "Iteration 139, loss = 0.55586801\n",
            "Iteration 140, loss = 0.55934621\n",
            "Iteration 141, loss = 0.55679047\n",
            "Iteration 142, loss = 0.55502438\n",
            "Iteration 143, loss = 0.55188017\n",
            "Iteration 144, loss = 0.55506892\n",
            "Iteration 145, loss = 0.55328040\n",
            "Iteration 146, loss = 0.55282906\n",
            "Iteration 147, loss = 0.55010450\n",
            "Iteration 148, loss = 0.55067984\n",
            "Iteration 149, loss = 0.54735496\n",
            "Iteration 150, loss = 0.54413990\n",
            "Iteration 151, loss = 0.54290607\n",
            "Iteration 152, loss = 0.54053209\n",
            "Iteration 153, loss = 0.54194776\n",
            "Iteration 154, loss = 0.54030868\n",
            "Iteration 155, loss = 0.53795971\n",
            "Iteration 156, loss = 0.53987116\n",
            "Iteration 157, loss = 0.54225775\n",
            "Iteration 158, loss = 0.54234308\n",
            "Iteration 159, loss = 0.53648422\n",
            "Iteration 160, loss = 0.53404117\n",
            "Iteration 161, loss = 0.53349272\n",
            "Iteration 162, loss = 0.52934722\n",
            "Iteration 163, loss = 0.53167949\n",
            "Iteration 164, loss = 0.52827464\n",
            "Iteration 165, loss = 0.53585087\n",
            "Iteration 166, loss = 0.52914916\n",
            "Iteration 167, loss = 0.53258776\n",
            "Iteration 168, loss = 0.52211375\n",
            "Iteration 169, loss = 0.53244962\n",
            "Iteration 170, loss = 0.53039083\n",
            "Iteration 171, loss = 0.52647401\n",
            "Iteration 172, loss = 0.53477654\n",
            "Iteration 173, loss = 0.52872804\n",
            "Iteration 174, loss = 0.53281382\n",
            "Iteration 175, loss = 0.52364337\n",
            "Iteration 176, loss = 0.53042171\n",
            "Iteration 177, loss = 0.52430998\n",
            "Iteration 178, loss = 0.51978383\n",
            "Iteration 179, loss = 0.52689743\n",
            "Iteration 180, loss = 0.52465497\n",
            "Iteration 181, loss = 0.51611378\n",
            "Iteration 182, loss = 0.51903181\n",
            "Iteration 183, loss = 0.51666851\n",
            "Iteration 184, loss = 0.51620957\n",
            "Iteration 185, loss = 0.51761360\n",
            "Iteration 186, loss = 0.51809281\n",
            "Iteration 187, loss = 0.51589567\n",
            "Iteration 188, loss = 0.51915838\n",
            "Iteration 189, loss = 0.52762480\n",
            "Iteration 190, loss = 0.51742621\n",
            "Iteration 191, loss = 0.51367756\n",
            "Iteration 192, loss = 0.51247381\n",
            "Iteration 193, loss = 0.52092207\n",
            "Iteration 194, loss = 0.50997980\n",
            "Iteration 195, loss = 0.51355755\n",
            "Iteration 196, loss = 0.50315797\n",
            "Iteration 197, loss = 0.52371952\n",
            "Iteration 198, loss = 0.50703562\n",
            "Iteration 199, loss = 0.51706471\n",
            "Iteration 200, loss = 0.50940528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.18306895\n",
            "Iteration 2, loss = 1.41266668\n",
            "Iteration 3, loss = 0.93524883\n",
            "Iteration 4, loss = 0.73244880\n",
            "Iteration 5, loss = 0.64684011\n",
            "Iteration 6, loss = 0.62036084\n",
            "Iteration 7, loss = 0.60437369\n",
            "Iteration 8, loss = 0.59510442\n",
            "Iteration 9, loss = 0.58867983\n",
            "Iteration 10, loss = 0.58391992\n",
            "Iteration 11, loss = 0.58006531\n",
            "Iteration 12, loss = 0.57810046\n",
            "Iteration 13, loss = 0.57654184\n",
            "Iteration 14, loss = 0.56671385\n",
            "Iteration 15, loss = 0.56693695\n",
            "Iteration 16, loss = 0.56314083\n",
            "Iteration 17, loss = 0.56025356\n",
            "Iteration 18, loss = 0.56032546\n",
            "Iteration 19, loss = 0.55644678\n",
            "Iteration 20, loss = 0.55374997\n",
            "Iteration 21, loss = 0.55135931\n",
            "Iteration 22, loss = 0.55181744\n",
            "Iteration 23, loss = 0.55471111\n",
            "Iteration 24, loss = 0.56029930\n",
            "Iteration 25, loss = 0.54433835\n",
            "Iteration 26, loss = 0.54344291\n",
            "Iteration 27, loss = 0.54625324\n",
            "Iteration 28, loss = 0.54059418\n",
            "Iteration 29, loss = 0.54203823\n",
            "Iteration 30, loss = 0.54251577\n",
            "Iteration 31, loss = 0.53490980\n",
            "Iteration 32, loss = 0.53572542\n",
            "Iteration 33, loss = 0.53234735\n",
            "Iteration 34, loss = 0.52902684\n",
            "Iteration 35, loss = 0.52534613\n",
            "Iteration 36, loss = 0.52700317\n",
            "Iteration 37, loss = 0.52664701\n",
            "Iteration 38, loss = 0.52650761\n",
            "Iteration 39, loss = 0.52585879\n",
            "Iteration 40, loss = 0.51616009\n",
            "Iteration 41, loss = 0.51762388\n",
            "Iteration 42, loss = 0.51611738\n",
            "Iteration 43, loss = 0.51048125\n",
            "Iteration 44, loss = 0.51723925\n",
            "Iteration 45, loss = 0.51714041\n",
            "Iteration 46, loss = 0.50830470\n",
            "Iteration 47, loss = 0.50332816\n",
            "Iteration 48, loss = 0.50158890\n",
            "Iteration 49, loss = 0.51009705\n",
            "Iteration 50, loss = 0.50599195\n",
            "Iteration 51, loss = 0.49491054\n",
            "Iteration 52, loss = 0.49819142\n",
            "Iteration 53, loss = 0.48963607\n",
            "Iteration 54, loss = 0.49365314\n",
            "Iteration 55, loss = 0.48754845\n",
            "Iteration 56, loss = 0.49063730\n",
            "Iteration 57, loss = 0.48749669\n",
            "Iteration 58, loss = 0.48732750\n",
            "Iteration 59, loss = 0.47907718\n",
            "Iteration 60, loss = 0.48620012\n",
            "Iteration 61, loss = 0.47997095\n",
            "Iteration 62, loss = 0.47876794\n",
            "Iteration 63, loss = 0.47413564\n",
            "Iteration 64, loss = 0.47436459\n",
            "Iteration 65, loss = 0.47192251\n",
            "Iteration 66, loss = 0.47980065\n",
            "Iteration 67, loss = 0.46952864\n",
            "Iteration 68, loss = 0.47064795\n",
            "Iteration 69, loss = 0.46465576\n",
            "Iteration 70, loss = 0.46334869\n",
            "Iteration 71, loss = 0.46135795\n",
            "Iteration 72, loss = 0.45922348\n",
            "Iteration 73, loss = 0.46053716\n",
            "Iteration 74, loss = 0.46938924\n",
            "Iteration 75, loss = 0.45784237\n",
            "Iteration 76, loss = 0.45997014\n",
            "Iteration 77, loss = 0.46201849\n",
            "Iteration 78, loss = 0.46355991\n",
            "Iteration 79, loss = 0.45328475\n",
            "Iteration 80, loss = 0.44866219\n",
            "Iteration 81, loss = 0.44750291\n",
            "Iteration 82, loss = 0.44924119\n",
            "Iteration 83, loss = 0.44589188\n",
            "Iteration 84, loss = 0.44425013\n",
            "Iteration 85, loss = 0.44390307\n",
            "Iteration 86, loss = 0.44199501\n",
            "Iteration 87, loss = 0.44237019\n",
            "Iteration 88, loss = 0.44174326\n",
            "Iteration 89, loss = 0.43939677\n",
            "Iteration 90, loss = 0.43594377\n",
            "Iteration 91, loss = 0.44261198\n",
            "Iteration 92, loss = 0.43396577\n",
            "Iteration 93, loss = 0.46234313\n",
            "Iteration 94, loss = 0.43812905\n",
            "Iteration 95, loss = 0.44017552\n",
            "Iteration 96, loss = 0.43860646\n",
            "Iteration 97, loss = 0.43502392\n",
            "Iteration 98, loss = 0.43255943\n",
            "Iteration 99, loss = 0.42712158\n",
            "Iteration 100, loss = 0.42793199\n",
            "Iteration 101, loss = 0.43091265\n",
            "Iteration 102, loss = 0.42400289\n",
            "Iteration 103, loss = 0.42447502\n",
            "Iteration 104, loss = 0.42214383\n",
            "Iteration 105, loss = 0.42469150\n",
            "Iteration 106, loss = 0.42418153\n",
            "Iteration 107, loss = 0.42235198\n",
            "Iteration 108, loss = 0.42001400\n",
            "Iteration 109, loss = 0.42194232\n",
            "Iteration 110, loss = 0.42675340\n",
            "Iteration 111, loss = 0.42463537\n",
            "Iteration 112, loss = 0.42379618\n",
            "Iteration 113, loss = 0.41521082\n",
            "Iteration 114, loss = 0.41514218\n",
            "Iteration 115, loss = 0.41372505\n",
            "Iteration 116, loss = 0.41785627\n",
            "Iteration 117, loss = 0.41939367\n",
            "Iteration 118, loss = 0.41405812\n",
            "Iteration 119, loss = 0.41051871\n",
            "Iteration 120, loss = 0.41618182\n",
            "Iteration 121, loss = 0.41664667\n",
            "Iteration 122, loss = 0.41548269\n",
            "Iteration 123, loss = 0.40997048\n",
            "Iteration 124, loss = 0.40687113\n",
            "Iteration 125, loss = 0.40702571\n",
            "Iteration 126, loss = 0.40711620\n",
            "Iteration 127, loss = 0.40659179\n",
            "Iteration 128, loss = 0.40476353\n",
            "Iteration 129, loss = 0.40787664\n",
            "Iteration 130, loss = 0.40372022\n",
            "Iteration 131, loss = 0.40539164\n",
            "Iteration 132, loss = 0.40589782\n",
            "Iteration 133, loss = 0.40988039\n",
            "Iteration 134, loss = 0.41462071\n",
            "Iteration 135, loss = 0.42111050\n",
            "Iteration 136, loss = 0.42637064\n",
            "Iteration 137, loss = 0.42521357\n",
            "Iteration 138, loss = 0.40472197\n",
            "Iteration 139, loss = 0.40859367\n",
            "Iteration 140, loss = 0.40364359\n",
            "Iteration 141, loss = 0.40027504\n",
            "Iteration 142, loss = 0.40113371\n",
            "Iteration 143, loss = 0.40017960\n",
            "Iteration 144, loss = 0.39883515\n",
            "Iteration 145, loss = 0.39968390\n",
            "Iteration 146, loss = 0.40096521\n",
            "Iteration 147, loss = 0.39912101\n",
            "Iteration 148, loss = 0.39835183\n",
            "Iteration 149, loss = 0.39728734\n",
            "Iteration 150, loss = 0.39698485\n",
            "Iteration 151, loss = 0.39484844\n",
            "Iteration 152, loss = 0.39755690\n",
            "Iteration 153, loss = 0.40030152\n",
            "Iteration 154, loss = 0.39668268\n",
            "Iteration 155, loss = 0.39660309\n",
            "Iteration 156, loss = 0.39823477\n",
            "Iteration 157, loss = 0.39517212\n",
            "Iteration 158, loss = 0.39606061\n",
            "Iteration 159, loss = 0.39158472\n",
            "Iteration 160, loss = 0.39328906\n",
            "Iteration 161, loss = 0.39526660\n",
            "Iteration 162, loss = 0.39275295\n",
            "Iteration 163, loss = 0.39452135\n",
            "Iteration 164, loss = 0.39311789\n",
            "Iteration 165, loss = 0.39833719\n",
            "Iteration 166, loss = 0.39250167\n",
            "Iteration 167, loss = 0.39989457\n",
            "Iteration 168, loss = 0.39447653\n",
            "Iteration 169, loss = 0.38985125\n",
            "Iteration 170, loss = 0.38717000\n",
            "Iteration 171, loss = 0.39203387\n",
            "Iteration 172, loss = 0.39798662\n",
            "Iteration 173, loss = 0.39483269\n",
            "Iteration 174, loss = 0.41490379\n",
            "Iteration 175, loss = 0.40645238\n",
            "Iteration 176, loss = 0.39858771\n",
            "Iteration 177, loss = 0.39361335\n",
            "Iteration 178, loss = 0.40686678\n",
            "Iteration 179, loss = 0.40161841\n",
            "Iteration 180, loss = 0.41109941\n",
            "Iteration 181, loss = 0.39432211\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.07433591\n",
            "Iteration 2, loss = 0.74109507\n",
            "Iteration 3, loss = 0.65525417\n",
            "Iteration 4, loss = 0.65808849\n",
            "Iteration 5, loss = 0.61926527\n",
            "Iteration 6, loss = 0.61816546\n",
            "Iteration 7, loss = 0.60049919\n",
            "Iteration 8, loss = 0.59822007\n",
            "Iteration 9, loss = 0.59289543\n",
            "Iteration 10, loss = 0.58789648\n",
            "Iteration 11, loss = 0.58822579\n",
            "Iteration 12, loss = 0.58423372\n",
            "Iteration 13, loss = 0.57851811\n",
            "Iteration 14, loss = 0.57722390\n",
            "Iteration 15, loss = 0.57540412\n",
            "Iteration 16, loss = 0.57512454\n",
            "Iteration 17, loss = 0.57593745\n",
            "Iteration 18, loss = 0.57094735\n",
            "Iteration 19, loss = 0.57162952\n",
            "Iteration 20, loss = 0.56015564\n",
            "Iteration 21, loss = 0.55378564\n",
            "Iteration 22, loss = 0.55742549\n",
            "Iteration 23, loss = 0.55293174\n",
            "Iteration 24, loss = 0.54870341\n",
            "Iteration 25, loss = 0.54373145\n",
            "Iteration 26, loss = 0.55042413\n",
            "Iteration 27, loss = 0.54550893\n",
            "Iteration 28, loss = 0.53479122\n",
            "Iteration 29, loss = 0.53883893\n",
            "Iteration 30, loss = 0.53683036\n",
            "Iteration 31, loss = 0.52674857\n",
            "Iteration 32, loss = 0.52916644\n",
            "Iteration 33, loss = 0.51984229\n",
            "Iteration 34, loss = 0.52271831\n",
            "Iteration 35, loss = 0.51398544\n",
            "Iteration 36, loss = 0.51931608\n",
            "Iteration 37, loss = 0.50311190\n",
            "Iteration 38, loss = 0.50173962\n",
            "Iteration 39, loss = 0.50627108\n",
            "Iteration 40, loss = 0.49463701\n",
            "Iteration 41, loss = 0.49043345\n",
            "Iteration 42, loss = 0.48383332\n",
            "Iteration 43, loss = 0.49220885\n",
            "Iteration 44, loss = 0.48624411\n",
            "Iteration 45, loss = 0.47813533\n",
            "Iteration 46, loss = 0.48849772\n",
            "Iteration 47, loss = 0.47772616\n",
            "Iteration 48, loss = 0.47181664\n",
            "Iteration 49, loss = 0.47127224\n",
            "Iteration 50, loss = 0.47058824\n",
            "Iteration 51, loss = 0.46277754\n",
            "Iteration 52, loss = 0.47224331\n",
            "Iteration 53, loss = 0.47903464\n",
            "Iteration 54, loss = 0.46036955\n",
            "Iteration 55, loss = 0.45045862\n",
            "Iteration 56, loss = 0.44920664\n",
            "Iteration 57, loss = 0.46070084\n",
            "Iteration 58, loss = 0.47081846\n",
            "Iteration 59, loss = 0.45497439\n",
            "Iteration 60, loss = 0.44420117\n",
            "Iteration 61, loss = 0.44696814\n",
            "Iteration 62, loss = 0.44253290\n",
            "Iteration 63, loss = 0.44994902\n",
            "Iteration 64, loss = 0.47895251\n",
            "Iteration 65, loss = 0.48619816\n",
            "Iteration 66, loss = 0.44977982\n",
            "Iteration 67, loss = 0.44151647\n",
            "Iteration 68, loss = 0.44130952\n",
            "Iteration 69, loss = 0.43819177\n",
            "Iteration 70, loss = 0.43382321\n",
            "Iteration 71, loss = 0.43106473\n",
            "Iteration 72, loss = 0.42530232\n",
            "Iteration 73, loss = 0.42385542\n",
            "Iteration 74, loss = 0.42465623\n",
            "Iteration 75, loss = 0.43402853\n",
            "Iteration 76, loss = 0.42205429\n",
            "Iteration 77, loss = 0.42238381\n",
            "Iteration 78, loss = 0.42547274\n",
            "Iteration 79, loss = 0.42137978\n",
            "Iteration 80, loss = 0.42542602\n",
            "Iteration 81, loss = 0.42333509\n",
            "Iteration 82, loss = 0.42170805\n",
            "Iteration 83, loss = 0.41891224\n",
            "Iteration 84, loss = 0.42014300\n",
            "Iteration 85, loss = 0.42446527\n",
            "Iteration 86, loss = 0.43226663\n",
            "Iteration 87, loss = 0.41763421\n",
            "Iteration 88, loss = 0.41076122\n",
            "Iteration 89, loss = 0.41710323\n",
            "Iteration 90, loss = 0.41281581\n",
            "Iteration 91, loss = 0.40676530\n",
            "Iteration 92, loss = 0.40799026\n",
            "Iteration 93, loss = 0.40713825\n",
            "Iteration 94, loss = 0.41128282\n",
            "Iteration 95, loss = 0.41055773\n",
            "Iteration 96, loss = 0.41135231\n",
            "Iteration 97, loss = 0.42023156\n",
            "Iteration 98, loss = 0.43340494\n",
            "Iteration 99, loss = 0.41226050\n",
            "Iteration 100, loss = 0.41132411\n",
            "Iteration 101, loss = 0.40887164\n",
            "Iteration 102, loss = 0.40839652\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.17752717\n",
            "Iteration 2, loss = 0.84142085\n",
            "Iteration 3, loss = 0.79844835\n",
            "Iteration 4, loss = 0.67413355\n",
            "Iteration 5, loss = 0.64976130\n",
            "Iteration 6, loss = 0.63195726\n",
            "Iteration 7, loss = 0.61052251\n",
            "Iteration 8, loss = 0.59775433\n",
            "Iteration 9, loss = 0.58967346\n",
            "Iteration 10, loss = 0.59586719\n",
            "Iteration 11, loss = 0.57770669\n",
            "Iteration 12, loss = 0.58103779\n",
            "Iteration 13, loss = 0.58085545\n",
            "Iteration 14, loss = 0.57153025\n",
            "Iteration 15, loss = 0.56776397\n",
            "Iteration 16, loss = 0.56535359\n",
            "Iteration 17, loss = 0.56572556\n",
            "Iteration 18, loss = 0.55711596\n",
            "Iteration 19, loss = 0.55647880\n",
            "Iteration 20, loss = 0.55946272\n",
            "Iteration 21, loss = 0.55358386\n",
            "Iteration 22, loss = 0.55371681\n",
            "Iteration 23, loss = 0.55250014\n",
            "Iteration 24, loss = 0.57027222\n",
            "Iteration 25, loss = 0.55562655\n",
            "Iteration 26, loss = 0.54994066\n",
            "Iteration 27, loss = 0.54597056\n",
            "Iteration 28, loss = 0.54536217\n",
            "Iteration 29, loss = 0.53563343\n",
            "Iteration 30, loss = 0.53410192\n",
            "Iteration 31, loss = 0.52172523\n",
            "Iteration 32, loss = 0.53775822\n",
            "Iteration 33, loss = 0.52416867\n",
            "Iteration 34, loss = 0.51580688\n",
            "Iteration 35, loss = 0.50281914\n",
            "Iteration 36, loss = 0.50660570\n",
            "Iteration 37, loss = 0.50009431\n",
            "Iteration 38, loss = 0.49675566\n",
            "Iteration 39, loss = 0.50331917\n",
            "Iteration 40, loss = 0.54162112\n",
            "Iteration 41, loss = 0.50426882\n",
            "Iteration 42, loss = 0.49239280\n",
            "Iteration 43, loss = 0.49510665\n",
            "Iteration 44, loss = 0.48727149\n",
            "Iteration 45, loss = 0.48450749\n",
            "Iteration 46, loss = 0.47763120\n",
            "Iteration 47, loss = 0.47287226\n",
            "Iteration 48, loss = 0.47635190\n",
            "Iteration 49, loss = 0.47166992\n",
            "Iteration 50, loss = 0.47162820\n",
            "Iteration 51, loss = 0.47011648\n",
            "Iteration 52, loss = 0.47699133\n",
            "Iteration 53, loss = 0.47746942\n",
            "Iteration 54, loss = 0.46322455\n",
            "Iteration 55, loss = 0.46387210\n",
            "Iteration 56, loss = 0.46885339\n",
            "Iteration 57, loss = 0.45617186\n",
            "Iteration 58, loss = 0.45809369\n",
            "Iteration 59, loss = 0.46829599\n",
            "Iteration 60, loss = 0.46363279\n",
            "Iteration 61, loss = 0.45940628\n",
            "Iteration 62, loss = 0.45363310\n",
            "Iteration 63, loss = 0.46235236\n",
            "Iteration 64, loss = 0.45622561\n",
            "Iteration 65, loss = 0.45820763\n",
            "Iteration 66, loss = 0.45205946\n",
            "Iteration 67, loss = 0.45488258\n",
            "Iteration 68, loss = 0.45712734\n",
            "Iteration 69, loss = 0.44865103\n",
            "Iteration 70, loss = 0.47887299\n",
            "Iteration 71, loss = 0.46576833\n",
            "Iteration 72, loss = 0.44561339\n",
            "Iteration 73, loss = 0.43991521\n",
            "Iteration 74, loss = 0.44379439\n",
            "Iteration 75, loss = 0.43355220\n",
            "Iteration 76, loss = 0.43932018\n",
            "Iteration 77, loss = 0.43114984\n",
            "Iteration 78, loss = 0.43861432\n",
            "Iteration 79, loss = 0.44329484\n",
            "Iteration 80, loss = 0.45512521\n",
            "Iteration 81, loss = 0.47058630\n",
            "Iteration 82, loss = 0.43938670\n",
            "Iteration 83, loss = 0.43025783\n",
            "Iteration 84, loss = 0.43752321\n",
            "Iteration 85, loss = 0.43319737\n",
            "Iteration 86, loss = 0.44378306\n",
            "Iteration 87, loss = 0.42301393\n",
            "Iteration 88, loss = 0.42909114\n",
            "Iteration 89, loss = 0.43178465\n",
            "Iteration 90, loss = 0.42397840\n",
            "Iteration 91, loss = 0.42910272\n",
            "Iteration 92, loss = 0.44717100\n",
            "Iteration 93, loss = 0.44353196\n",
            "Iteration 94, loss = 0.41732817\n",
            "Iteration 95, loss = 0.42493581\n",
            "Iteration 96, loss = 0.41957523\n",
            "Iteration 97, loss = 0.41724067\n",
            "Iteration 98, loss = 0.41784147\n",
            "Iteration 99, loss = 0.41836604\n",
            "Iteration 100, loss = 0.41892052\n",
            "Iteration 101, loss = 0.42307584\n",
            "Iteration 102, loss = 0.41398979\n",
            "Iteration 103, loss = 0.41402017\n",
            "Iteration 104, loss = 0.41594751\n",
            "Iteration 105, loss = 0.41375014\n",
            "Iteration 106, loss = 0.43154436\n",
            "Iteration 107, loss = 0.41719216\n",
            "Iteration 108, loss = 0.41693109\n",
            "Iteration 109, loss = 0.41992522\n",
            "Iteration 110, loss = 0.41231165\n",
            "Iteration 111, loss = 0.42265149\n",
            "Iteration 112, loss = 0.45553828\n",
            "Iteration 113, loss = 0.45655193\n",
            "Iteration 114, loss = 0.41511486\n",
            "Iteration 115, loss = 0.41501600\n",
            "Iteration 116, loss = 0.40647589\n",
            "Iteration 117, loss = 0.40913723\n",
            "Iteration 118, loss = 0.40835468\n",
            "Iteration 119, loss = 0.41029486\n",
            "Iteration 120, loss = 0.43028762\n",
            "Iteration 121, loss = 0.43179601\n",
            "Iteration 122, loss = 0.43366200\n",
            "Iteration 123, loss = 0.41780518\n",
            "Iteration 124, loss = 0.41723277\n",
            "Iteration 125, loss = 0.42130694\n",
            "Iteration 126, loss = 0.42207820\n",
            "Iteration 127, loss = 0.41683607\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.05471955\n",
            "Iteration 2, loss = 0.95006421\n",
            "Iteration 3, loss = 0.72365727\n",
            "Iteration 4, loss = 0.73772351\n",
            "Iteration 5, loss = 0.69359318\n",
            "Iteration 6, loss = 0.64899485\n",
            "Iteration 7, loss = 0.64471871\n",
            "Iteration 8, loss = 0.63247322\n",
            "Iteration 9, loss = 0.62692851\n",
            "Iteration 10, loss = 0.62637033\n",
            "Iteration 11, loss = 0.62811964\n",
            "Iteration 12, loss = 0.62554480\n",
            "Iteration 13, loss = 0.62015460\n",
            "Iteration 14, loss = 0.61390940\n",
            "Iteration 15, loss = 0.61217417\n",
            "Iteration 16, loss = 0.63062934\n",
            "Iteration 17, loss = 0.63294139\n",
            "Iteration 18, loss = 0.61156745\n",
            "Iteration 19, loss = 0.60647992\n",
            "Iteration 20, loss = 0.61569310\n",
            "Iteration 21, loss = 0.61055439\n",
            "Iteration 22, loss = 0.61972521\n",
            "Iteration 23, loss = 0.60779565\n",
            "Iteration 24, loss = 0.59277762\n",
            "Iteration 25, loss = 0.59646410\n",
            "Iteration 26, loss = 0.59386918\n",
            "Iteration 27, loss = 0.58730966\n",
            "Iteration 28, loss = 0.58132878\n",
            "Iteration 29, loss = 0.58278261\n",
            "Iteration 30, loss = 0.57964716\n",
            "Iteration 31, loss = 0.57668783\n",
            "Iteration 32, loss = 0.57990845\n",
            "Iteration 33, loss = 0.56802477\n",
            "Iteration 34, loss = 0.56418174\n",
            "Iteration 35, loss = 0.56659588\n",
            "Iteration 36, loss = 0.56792190\n",
            "Iteration 37, loss = 0.56552110\n",
            "Iteration 38, loss = 0.55152995\n",
            "Iteration 39, loss = 0.56773418\n",
            "Iteration 40, loss = 0.54396138\n",
            "Iteration 41, loss = 0.54559154\n",
            "Iteration 42, loss = 0.55299079\n",
            "Iteration 43, loss = 0.54110785\n",
            "Iteration 44, loss = 0.52491581\n",
            "Iteration 45, loss = 0.53379708\n",
            "Iteration 46, loss = 0.51376409\n",
            "Iteration 47, loss = 0.50821137\n",
            "Iteration 48, loss = 0.52329692\n",
            "Iteration 49, loss = 0.50304563\n",
            "Iteration 50, loss = 0.50064170\n",
            "Iteration 51, loss = 0.49630875\n",
            "Iteration 52, loss = 0.49090134\n",
            "Iteration 53, loss = 0.48449444\n",
            "Iteration 54, loss = 0.48297904\n",
            "Iteration 55, loss = 0.48574970\n",
            "Iteration 56, loss = 0.47954154\n",
            "Iteration 57, loss = 0.47591319\n",
            "Iteration 58, loss = 0.47663245\n",
            "Iteration 59, loss = 0.48502447\n",
            "Iteration 60, loss = 0.48364889\n",
            "Iteration 61, loss = 0.47379481\n",
            "Iteration 62, loss = 0.46577196\n",
            "Iteration 63, loss = 0.46595889\n",
            "Iteration 64, loss = 0.46632787\n",
            "Iteration 65, loss = 0.45847142\n",
            "Iteration 66, loss = 0.46031825\n",
            "Iteration 67, loss = 0.46511119\n",
            "Iteration 68, loss = 0.47052967\n",
            "Iteration 69, loss = 0.46461761\n",
            "Iteration 70, loss = 0.45347977\n",
            "Iteration 71, loss = 0.45623063\n",
            "Iteration 72, loss = 0.46129116\n",
            "Iteration 73, loss = 0.45919604\n",
            "Iteration 74, loss = 0.45580716\n",
            "Iteration 75, loss = 0.44925374\n",
            "Iteration 76, loss = 0.44881785\n",
            "Iteration 77, loss = 0.45921518\n",
            "Iteration 78, loss = 0.45976320\n",
            "Iteration 79, loss = 0.45379466\n",
            "Iteration 80, loss = 0.45003762\n",
            "Iteration 81, loss = 0.44518718\n",
            "Iteration 82, loss = 0.44510762\n",
            "Iteration 83, loss = 0.45009962\n",
            "Iteration 84, loss = 0.44526768\n",
            "Iteration 85, loss = 0.44015243\n",
            "Iteration 86, loss = 0.44854711\n",
            "Iteration 87, loss = 0.45520493\n",
            "Iteration 88, loss = 0.44563664\n",
            "Iteration 89, loss = 0.46722634\n",
            "Iteration 90, loss = 0.43989092\n",
            "Iteration 91, loss = 0.43716800\n",
            "Iteration 92, loss = 0.44815359\n",
            "Iteration 93, loss = 0.45374282\n",
            "Iteration 94, loss = 0.43620914\n",
            "Iteration 95, loss = 0.44517425\n",
            "Iteration 96, loss = 0.45059382\n",
            "Iteration 97, loss = 0.43550403\n",
            "Iteration 98, loss = 0.43399239\n",
            "Iteration 99, loss = 0.43759063\n",
            "Iteration 100, loss = 0.44488045\n",
            "Iteration 101, loss = 0.43448182\n",
            "Iteration 102, loss = 0.43065695\n",
            "Iteration 103, loss = 0.43318273\n",
            "Iteration 104, loss = 0.43390545\n",
            "Iteration 105, loss = 0.42952641\n",
            "Iteration 106, loss = 0.42767281\n",
            "Iteration 107, loss = 0.42700687\n",
            "Iteration 108, loss = 0.42777118\n",
            "Iteration 109, loss = 0.42783470\n",
            "Iteration 110, loss = 0.43442798\n",
            "Iteration 111, loss = 0.42758770\n",
            "Iteration 112, loss = 0.42959257\n",
            "Iteration 113, loss = 0.43376035\n",
            "Iteration 114, loss = 0.43186030\n",
            "Iteration 115, loss = 0.43188122\n",
            "Iteration 116, loss = 0.43631846\n",
            "Iteration 117, loss = 0.42729587\n",
            "Iteration 118, loss = 0.42650445\n",
            "Iteration 119, loss = 0.42425884\n",
            "Iteration 120, loss = 0.43448962\n",
            "Iteration 121, loss = 0.42553191\n",
            "Iteration 122, loss = 0.43725484\n",
            "Iteration 123, loss = 0.43063100\n",
            "Iteration 124, loss = 0.45797931\n",
            "Iteration 125, loss = 0.42781424\n",
            "Iteration 126, loss = 0.42729841\n",
            "Iteration 127, loss = 0.42811750\n",
            "Iteration 128, loss = 0.43082515\n",
            "Iteration 129, loss = 0.41790109\n",
            "Iteration 130, loss = 0.42726382\n",
            "Iteration 131, loss = 0.42237916\n",
            "Iteration 132, loss = 0.42581301\n",
            "Iteration 133, loss = 0.42294329\n",
            "Iteration 134, loss = 0.41945873\n",
            "Iteration 135, loss = 0.42722259\n",
            "Iteration 136, loss = 0.42219790\n",
            "Iteration 137, loss = 0.41735758\n",
            "Iteration 138, loss = 0.42663372\n",
            "Iteration 139, loss = 0.41873939\n",
            "Iteration 140, loss = 0.41951692\n",
            "Iteration 141, loss = 0.41691748\n",
            "Iteration 142, loss = 0.41652402\n",
            "Iteration 143, loss = 0.42036773\n",
            "Iteration 144, loss = 0.41816135\n",
            "Iteration 145, loss = 0.41432741\n",
            "Iteration 146, loss = 0.41488191\n",
            "Iteration 147, loss = 0.42111424\n",
            "Iteration 148, loss = 0.41876076\n",
            "Iteration 149, loss = 0.43058898\n",
            "Iteration 150, loss = 0.41488910\n",
            "Iteration 151, loss = 0.41999383\n",
            "Iteration 152, loss = 0.42886469\n",
            "Iteration 153, loss = 0.42168776\n",
            "Iteration 154, loss = 0.41820463\n",
            "Iteration 155, loss = 0.41595600\n",
            "Iteration 156, loss = 0.41446409\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 4.28922022\n",
            "Iteration 2, loss = 2.43504876\n",
            "Iteration 3, loss = 1.08327515\n",
            "Iteration 4, loss = 0.89747067\n",
            "Iteration 5, loss = 0.82084119\n",
            "Iteration 6, loss = 0.71960990\n",
            "Iteration 7, loss = 0.66366350\n",
            "Iteration 8, loss = 0.65414046\n",
            "Iteration 9, loss = 0.65114092\n",
            "Iteration 10, loss = 0.64141810\n",
            "Iteration 11, loss = 0.63593946\n",
            "Iteration 12, loss = 0.63369047\n",
            "Iteration 13, loss = 0.62952742\n",
            "Iteration 14, loss = 0.62591314\n",
            "Iteration 15, loss = 0.62367928\n",
            "Iteration 16, loss = 0.61995916\n",
            "Iteration 17, loss = 0.61706082\n",
            "Iteration 18, loss = 0.61430198\n",
            "Iteration 19, loss = 0.61307604\n",
            "Iteration 20, loss = 0.61038815\n",
            "Iteration 21, loss = 0.60786250\n",
            "Iteration 22, loss = 0.60628379\n",
            "Iteration 23, loss = 0.60568779\n",
            "Iteration 24, loss = 0.60309708\n",
            "Iteration 25, loss = 0.60510581\n",
            "Iteration 26, loss = 0.60200237\n",
            "Iteration 27, loss = 0.59836689\n",
            "Iteration 28, loss = 0.59473243\n",
            "Iteration 29, loss = 0.59287154\n",
            "Iteration 30, loss = 0.59520871\n",
            "Iteration 31, loss = 0.58941387\n",
            "Iteration 32, loss = 0.58978910\n",
            "Iteration 33, loss = 0.59107190\n",
            "Iteration 34, loss = 0.58478357\n",
            "Iteration 35, loss = 0.58486020\n",
            "Iteration 36, loss = 0.58121301\n",
            "Iteration 37, loss = 0.58040311\n",
            "Iteration 38, loss = 0.57924819\n",
            "Iteration 39, loss = 0.57288828\n",
            "Iteration 40, loss = 0.57408796\n",
            "Iteration 41, loss = 0.56996290\n",
            "Iteration 42, loss = 0.56965634\n",
            "Iteration 43, loss = 0.56834879\n",
            "Iteration 44, loss = 0.56476947\n",
            "Iteration 45, loss = 0.57140964\n",
            "Iteration 46, loss = 0.56265705\n",
            "Iteration 47, loss = 0.56135685\n",
            "Iteration 48, loss = 0.55840179\n",
            "Iteration 49, loss = 0.55812985\n",
            "Iteration 50, loss = 0.55183539\n",
            "Iteration 51, loss = 0.55213898\n",
            "Iteration 52, loss = 0.54707519\n",
            "Iteration 53, loss = 0.54982327\n",
            "Iteration 54, loss = 0.54257170\n",
            "Iteration 55, loss = 0.54304165\n",
            "Iteration 56, loss = 0.54122270\n",
            "Iteration 57, loss = 0.54122836\n",
            "Iteration 58, loss = 0.53810218\n",
            "Iteration 59, loss = 0.53427139\n",
            "Iteration 60, loss = 0.53483796\n",
            "Iteration 61, loss = 0.53102107\n",
            "Iteration 62, loss = 0.53108538\n",
            "Iteration 63, loss = 0.52710660\n",
            "Iteration 64, loss = 0.52814504\n",
            "Iteration 65, loss = 0.52737810\n",
            "Iteration 66, loss = 0.53255658\n",
            "Iteration 67, loss = 0.53186624\n",
            "Iteration 68, loss = 0.52260515\n",
            "Iteration 69, loss = 0.51215968\n",
            "Iteration 70, loss = 0.51573044\n",
            "Iteration 71, loss = 0.51225273\n",
            "Iteration 72, loss = 0.50914418\n",
            "Iteration 73, loss = 0.50830593\n",
            "Iteration 74, loss = 0.50647051\n",
            "Iteration 75, loss = 0.50493897\n",
            "Iteration 76, loss = 0.50287455\n",
            "Iteration 77, loss = 0.50213246\n",
            "Iteration 78, loss = 0.50077981\n",
            "Iteration 79, loss = 0.50002557\n",
            "Iteration 80, loss = 0.49368636\n",
            "Iteration 81, loss = 0.49580871\n",
            "Iteration 82, loss = 0.49378917\n",
            "Iteration 83, loss = 0.49161042\n",
            "Iteration 84, loss = 0.48880694\n",
            "Iteration 85, loss = 0.48758259\n",
            "Iteration 86, loss = 0.48645630\n",
            "Iteration 87, loss = 0.48712033\n",
            "Iteration 88, loss = 0.48902011\n",
            "Iteration 89, loss = 0.48767433\n",
            "Iteration 90, loss = 0.48158245\n",
            "Iteration 91, loss = 0.48544939\n",
            "Iteration 92, loss = 0.47865588\n",
            "Iteration 93, loss = 0.47606373\n",
            "Iteration 94, loss = 0.47580033\n",
            "Iteration 95, loss = 0.47093655\n",
            "Iteration 96, loss = 0.47035781\n",
            "Iteration 97, loss = 0.47289982\n",
            "Iteration 98, loss = 0.47478486\n",
            "Iteration 99, loss = 0.46815039\n",
            "Iteration 100, loss = 0.46593650\n",
            "Iteration 101, loss = 0.46291172\n",
            "Iteration 102, loss = 0.46097406\n",
            "Iteration 103, loss = 0.45842140\n",
            "Iteration 104, loss = 0.45803826\n",
            "Iteration 105, loss = 0.45851398\n",
            "Iteration 106, loss = 0.45927283\n",
            "Iteration 107, loss = 0.45201442\n",
            "Iteration 108, loss = 0.46127025\n",
            "Iteration 109, loss = 0.45767882\n",
            "Iteration 110, loss = 0.45701168\n",
            "Iteration 111, loss = 0.45065906\n",
            "Iteration 112, loss = 0.44871590\n",
            "Iteration 113, loss = 0.44969736\n",
            "Iteration 114, loss = 0.46514918\n",
            "Iteration 115, loss = 0.46163873\n",
            "Iteration 116, loss = 0.44856159\n",
            "Iteration 117, loss = 0.44492607\n",
            "Iteration 118, loss = 0.44620403\n",
            "Iteration 119, loss = 0.44169475\n",
            "Iteration 120, loss = 0.44011114\n",
            "Iteration 121, loss = 0.44208244\n",
            "Iteration 122, loss = 0.43353688\n",
            "Iteration 123, loss = 0.43442762\n",
            "Iteration 124, loss = 0.44696134\n",
            "Iteration 125, loss = 0.44830256\n",
            "Iteration 126, loss = 0.44226766\n",
            "Iteration 127, loss = 0.43985001\n",
            "Iteration 128, loss = 0.43437214\n",
            "Iteration 129, loss = 0.43280443\n",
            "Iteration 130, loss = 0.44135894\n",
            "Iteration 131, loss = 0.43098338\n",
            "Iteration 132, loss = 0.43719505\n",
            "Iteration 133, loss = 0.44437674\n",
            "Iteration 134, loss = 0.42581640\n",
            "Iteration 135, loss = 0.43716908\n",
            "Iteration 136, loss = 0.42993156\n",
            "Iteration 137, loss = 0.43172448\n",
            "Iteration 138, loss = 0.43911801\n",
            "Iteration 139, loss = 0.44203247\n",
            "Iteration 140, loss = 0.42924893\n",
            "Iteration 141, loss = 0.43146725\n",
            "Iteration 142, loss = 0.44075136\n",
            "Iteration 143, loss = 0.42555784\n",
            "Iteration 144, loss = 0.42829863\n",
            "Iteration 145, loss = 0.42684739\n",
            "Iteration 146, loss = 0.43621645\n",
            "Iteration 147, loss = 0.43437077\n",
            "Iteration 148, loss = 0.42986698\n",
            "Iteration 149, loss = 0.42921696\n",
            "Iteration 150, loss = 0.42933988\n",
            "Iteration 151, loss = 0.44719063\n",
            "Iteration 152, loss = 0.44215577\n",
            "Iteration 153, loss = 0.43231075\n",
            "Iteration 154, loss = 0.43954311\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68685061\n",
            "Iteration 2, loss = 0.61119974\n",
            "Iteration 3, loss = 0.60500004\n",
            "Iteration 4, loss = 0.58201049\n",
            "Iteration 5, loss = 0.52857269\n",
            "Iteration 6, loss = 0.52061977\n",
            "Iteration 7, loss = 0.53632536\n",
            "Iteration 8, loss = 0.51027499\n",
            "Iteration 9, loss = 0.48461066\n",
            "Iteration 10, loss = 0.46212074\n",
            "Iteration 11, loss = 0.46590293\n",
            "Iteration 12, loss = 0.46184635\n",
            "Iteration 13, loss = 0.46597279\n",
            "Iteration 14, loss = 0.44855528\n",
            "Iteration 15, loss = 0.47322687\n",
            "Iteration 16, loss = 0.44156661\n",
            "Iteration 17, loss = 0.47234102\n",
            "Iteration 18, loss = 0.52542894\n",
            "Iteration 19, loss = 0.45934506\n",
            "Iteration 20, loss = 0.43235990\n",
            "Iteration 21, loss = 0.42286569\n",
            "Iteration 22, loss = 0.44311753\n",
            "Iteration 23, loss = 0.41912272\n",
            "Iteration 24, loss = 0.42946530\n",
            "Iteration 25, loss = 0.43819608\n",
            "Iteration 26, loss = 0.41271884\n",
            "Iteration 27, loss = 0.42237246\n",
            "Iteration 28, loss = 0.41841377\n",
            "Iteration 29, loss = 0.40900770\n",
            "Iteration 30, loss = 0.42505086\n",
            "Iteration 31, loss = 0.41864803\n",
            "Iteration 32, loss = 0.41222359\n",
            "Iteration 33, loss = 0.40997549\n",
            "Iteration 34, loss = 0.40908611\n",
            "Iteration 35, loss = 0.40285355\n",
            "Iteration 36, loss = 0.40051056\n",
            "Iteration 37, loss = 0.41878319\n",
            "Iteration 38, loss = 0.44284074\n",
            "Iteration 39, loss = 0.40298339\n",
            "Iteration 40, loss = 0.41279636\n",
            "Iteration 41, loss = 0.39262914\n",
            "Iteration 42, loss = 0.41069873\n",
            "Iteration 43, loss = 0.40013620\n",
            "Iteration 44, loss = 0.40967296\n",
            "Iteration 45, loss = 0.39516030\n",
            "Iteration 46, loss = 0.38208883\n",
            "Iteration 47, loss = 0.41271623\n",
            "Iteration 48, loss = 0.38331171\n",
            "Iteration 49, loss = 0.39433696\n",
            "Iteration 50, loss = 0.38799175\n",
            "Iteration 51, loss = 0.38902108\n",
            "Iteration 52, loss = 0.37432759\n",
            "Iteration 53, loss = 0.38842680\n",
            "Iteration 54, loss = 0.38325782\n",
            "Iteration 55, loss = 0.39170784\n",
            "Iteration 56, loss = 0.38122824\n",
            "Iteration 57, loss = 0.40617215\n",
            "Iteration 58, loss = 0.38292210\n",
            "Iteration 59, loss = 0.38644837\n",
            "Iteration 60, loss = 0.37365872\n",
            "Iteration 61, loss = 0.37712853\n",
            "Iteration 62, loss = 0.37559221\n",
            "Iteration 63, loss = 0.38801039\n",
            "Iteration 64, loss = 0.36318939\n",
            "Iteration 65, loss = 0.36370713\n",
            "Iteration 66, loss = 0.36183709\n",
            "Iteration 67, loss = 0.35844145\n",
            "Iteration 68, loss = 0.41148815\n",
            "Iteration 69, loss = 0.36602051\n",
            "Iteration 70, loss = 0.36002297\n",
            "Iteration 71, loss = 0.36052263\n",
            "Iteration 72, loss = 0.35671807\n",
            "Iteration 73, loss = 0.38194095\n",
            "Iteration 74, loss = 0.36682784\n",
            "Iteration 75, loss = 0.37059731\n",
            "Iteration 76, loss = 0.36495000\n",
            "Iteration 77, loss = 0.38190140\n",
            "Iteration 78, loss = 0.37844912\n",
            "Iteration 79, loss = 0.35046134\n",
            "Iteration 80, loss = 0.35774163\n",
            "Iteration 81, loss = 0.35165689\n",
            "Iteration 82, loss = 0.35625834\n",
            "Iteration 83, loss = 0.34352297\n",
            "Iteration 84, loss = 0.36200388\n",
            "Iteration 85, loss = 0.37720040\n",
            "Iteration 86, loss = 0.35926030\n",
            "Iteration 87, loss = 0.35575538\n",
            "Iteration 88, loss = 0.35319089\n",
            "Iteration 89, loss = 0.34575929\n",
            "Iteration 90, loss = 0.36631608\n",
            "Iteration 91, loss = 0.34666976\n",
            "Iteration 92, loss = 0.36329433\n",
            "Iteration 93, loss = 0.35159287\n",
            "Iteration 94, loss = 0.36441291\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68256520\n",
            "Iteration 2, loss = 0.62747454\n",
            "Iteration 3, loss = 0.59549953\n",
            "Iteration 4, loss = 0.54967138\n",
            "Iteration 5, loss = 0.54231249\n",
            "Iteration 6, loss = 0.52611809\n",
            "Iteration 7, loss = 0.49782514\n",
            "Iteration 8, loss = 0.50423465\n",
            "Iteration 9, loss = 0.51593772\n",
            "Iteration 10, loss = 0.47043624\n",
            "Iteration 11, loss = 0.45923836\n",
            "Iteration 12, loss = 0.47310227\n",
            "Iteration 13, loss = 0.44662288\n",
            "Iteration 14, loss = 0.46826322\n",
            "Iteration 15, loss = 0.45190024\n",
            "Iteration 16, loss = 0.44268079\n",
            "Iteration 17, loss = 0.44994129\n",
            "Iteration 18, loss = 0.45847333\n",
            "Iteration 19, loss = 0.43051801\n",
            "Iteration 20, loss = 0.42556964\n",
            "Iteration 21, loss = 0.44239808\n",
            "Iteration 22, loss = 0.40806851\n",
            "Iteration 23, loss = 0.41182842\n",
            "Iteration 24, loss = 0.40861722\n",
            "Iteration 25, loss = 0.43237945\n",
            "Iteration 26, loss = 0.41316473\n",
            "Iteration 27, loss = 0.40421790\n",
            "Iteration 28, loss = 0.39343050\n",
            "Iteration 29, loss = 0.39367519\n",
            "Iteration 30, loss = 0.40012919\n",
            "Iteration 31, loss = 0.39375950\n",
            "Iteration 32, loss = 0.41073582\n",
            "Iteration 33, loss = 0.38205057\n",
            "Iteration 34, loss = 0.38363079\n",
            "Iteration 35, loss = 0.38805624\n",
            "Iteration 36, loss = 0.39077811\n",
            "Iteration 37, loss = 0.40608380\n",
            "Iteration 38, loss = 0.41630073\n",
            "Iteration 39, loss = 0.38097010\n",
            "Iteration 40, loss = 0.39378218\n",
            "Iteration 41, loss = 0.41462466\n",
            "Iteration 42, loss = 0.39925186\n",
            "Iteration 43, loss = 0.37250819\n",
            "Iteration 44, loss = 0.37627480\n",
            "Iteration 45, loss = 0.42769707\n",
            "Iteration 46, loss = 0.37983114\n",
            "Iteration 47, loss = 0.37875486\n",
            "Iteration 48, loss = 0.37322491\n",
            "Iteration 49, loss = 0.36064889\n",
            "Iteration 50, loss = 0.37707121\n",
            "Iteration 51, loss = 0.38519852\n",
            "Iteration 52, loss = 0.38532399\n",
            "Iteration 53, loss = 0.37137219\n",
            "Iteration 54, loss = 0.36057920\n",
            "Iteration 55, loss = 0.38150851\n",
            "Iteration 56, loss = 0.38706504\n",
            "Iteration 57, loss = 0.36220758\n",
            "Iteration 58, loss = 0.36851190\n",
            "Iteration 59, loss = 0.42115947\n",
            "Iteration 60, loss = 0.37845173\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70085523\n",
            "Iteration 2, loss = 0.62292529\n",
            "Iteration 3, loss = 0.58859411\n",
            "Iteration 4, loss = 0.54729388\n",
            "Iteration 5, loss = 0.55785922\n",
            "Iteration 6, loss = 0.51992628\n",
            "Iteration 7, loss = 0.52945646\n",
            "Iteration 8, loss = 0.48177584\n",
            "Iteration 9, loss = 0.48083291\n",
            "Iteration 10, loss = 0.48166400\n",
            "Iteration 11, loss = 0.46242796\n",
            "Iteration 12, loss = 0.44182553\n",
            "Iteration 13, loss = 0.42762928\n",
            "Iteration 14, loss = 0.45166290\n",
            "Iteration 15, loss = 0.43713473\n",
            "Iteration 16, loss = 0.43879733\n",
            "Iteration 17, loss = 0.43798302\n",
            "Iteration 18, loss = 0.41744391\n",
            "Iteration 19, loss = 0.44021789\n",
            "Iteration 20, loss = 0.41288491\n",
            "Iteration 21, loss = 0.42480770\n",
            "Iteration 22, loss = 0.40932543\n",
            "Iteration 23, loss = 0.41158121\n",
            "Iteration 24, loss = 0.46176894\n",
            "Iteration 25, loss = 0.40275944\n",
            "Iteration 26, loss = 0.40000925\n",
            "Iteration 27, loss = 0.40605158\n",
            "Iteration 28, loss = 0.40848820\n",
            "Iteration 29, loss = 0.40092744\n",
            "Iteration 30, loss = 0.40188468\n",
            "Iteration 31, loss = 0.40275270\n",
            "Iteration 32, loss = 0.38939834\n",
            "Iteration 33, loss = 0.40061509\n",
            "Iteration 34, loss = 0.39562118\n",
            "Iteration 35, loss = 0.39250746\n",
            "Iteration 36, loss = 0.39981177\n",
            "Iteration 37, loss = 0.42728235\n",
            "Iteration 38, loss = 0.42227700\n",
            "Iteration 39, loss = 0.39420394\n",
            "Iteration 40, loss = 0.39387653\n",
            "Iteration 41, loss = 0.39309118\n",
            "Iteration 42, loss = 0.38562145\n",
            "Iteration 43, loss = 0.38616028\n",
            "Iteration 44, loss = 0.38720982\n",
            "Iteration 45, loss = 0.38280879\n",
            "Iteration 46, loss = 0.39492877\n",
            "Iteration 47, loss = 0.38795849\n",
            "Iteration 48, loss = 0.37978923\n",
            "Iteration 49, loss = 0.37716295\n",
            "Iteration 50, loss = 0.37724688\n",
            "Iteration 51, loss = 0.36733238\n",
            "Iteration 52, loss = 0.37541585\n",
            "Iteration 53, loss = 0.37314341\n",
            "Iteration 54, loss = 0.39011007\n",
            "Iteration 55, loss = 0.40882454\n",
            "Iteration 56, loss = 0.38145998\n",
            "Iteration 57, loss = 0.39082893\n",
            "Iteration 58, loss = 0.36778204\n",
            "Iteration 59, loss = 0.37694934\n",
            "Iteration 60, loss = 0.37954451\n",
            "Iteration 61, loss = 0.38197393\n",
            "Iteration 62, loss = 0.36686273\n",
            "Iteration 63, loss = 0.39459539\n",
            "Iteration 64, loss = 0.38629755\n",
            "Iteration 65, loss = 0.38535723\n",
            "Iteration 66, loss = 0.36978379\n",
            "Iteration 67, loss = 0.35747723\n",
            "Iteration 68, loss = 0.36890386\n",
            "Iteration 69, loss = 0.35966877\n",
            "Iteration 70, loss = 0.36213877\n",
            "Iteration 71, loss = 0.37880013\n",
            "Iteration 72, loss = 0.39286654\n",
            "Iteration 73, loss = 0.36502860\n",
            "Iteration 74, loss = 0.36356309\n",
            "Iteration 75, loss = 0.36110013\n",
            "Iteration 76, loss = 0.36059762\n",
            "Iteration 77, loss = 0.35861254\n",
            "Iteration 78, loss = 0.34985615\n",
            "Iteration 79, loss = 0.36222819\n",
            "Iteration 80, loss = 0.35979648\n",
            "Iteration 81, loss = 0.35089865\n",
            "Iteration 82, loss = 0.34578947\n",
            "Iteration 83, loss = 0.35483886\n",
            "Iteration 84, loss = 0.35733066\n",
            "Iteration 85, loss = 0.34619253\n",
            "Iteration 86, loss = 0.34638814\n",
            "Iteration 87, loss = 0.34034938\n",
            "Iteration 88, loss = 0.35924155\n",
            "Iteration 89, loss = 0.37934804\n",
            "Iteration 90, loss = 0.35126473\n",
            "Iteration 91, loss = 0.34069914\n",
            "Iteration 92, loss = 0.34239266\n",
            "Iteration 93, loss = 0.33406683\n",
            "Iteration 94, loss = 0.35046120\n",
            "Iteration 95, loss = 0.33584345\n",
            "Iteration 96, loss = 0.38937278\n",
            "Iteration 97, loss = 0.36350349\n",
            "Iteration 98, loss = 0.35568997\n",
            "Iteration 99, loss = 0.34007672\n",
            "Iteration 100, loss = 0.36557603\n",
            "Iteration 101, loss = 0.34959892\n",
            "Iteration 102, loss = 0.33727111\n",
            "Iteration 103, loss = 0.33251862\n",
            "Iteration 104, loss = 0.33444358\n",
            "Iteration 105, loss = 0.32757308\n",
            "Iteration 106, loss = 0.34311773\n",
            "Iteration 107, loss = 0.33400432\n",
            "Iteration 108, loss = 0.32609758\n",
            "Iteration 109, loss = 0.34856904\n",
            "Iteration 110, loss = 0.32864294\n",
            "Iteration 111, loss = 0.32777207\n",
            "Iteration 112, loss = 0.32433388\n",
            "Iteration 113, loss = 0.32980059\n",
            "Iteration 114, loss = 0.33700293\n",
            "Iteration 115, loss = 0.32186877\n",
            "Iteration 116, loss = 0.31770107\n",
            "Iteration 117, loss = 0.32066391\n",
            "Iteration 118, loss = 0.33273303\n",
            "Iteration 119, loss = 0.32069061\n",
            "Iteration 120, loss = 0.32612417\n",
            "Iteration 121, loss = 0.32377464\n",
            "Iteration 122, loss = 0.31718431\n",
            "Iteration 123, loss = 0.31374411\n",
            "Iteration 124, loss = 0.31029292\n",
            "Iteration 125, loss = 0.31778190\n",
            "Iteration 126, loss = 0.32192458\n",
            "Iteration 127, loss = 0.31371113\n",
            "Iteration 128, loss = 0.31813690\n",
            "Iteration 129, loss = 0.31105417\n",
            "Iteration 130, loss = 0.31731846\n",
            "Iteration 131, loss = 0.31292281\n",
            "Iteration 132, loss = 0.31939307\n",
            "Iteration 133, loss = 0.35748708\n",
            "Iteration 134, loss = 0.31712991\n",
            "Iteration 135, loss = 0.31445416\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71212262\n",
            "Iteration 2, loss = 0.62406715\n",
            "Iteration 3, loss = 0.59924984\n",
            "Iteration 4, loss = 0.58978592\n",
            "Iteration 5, loss = 0.56161673\n",
            "Iteration 6, loss = 0.51767061\n",
            "Iteration 7, loss = 0.51829127\n",
            "Iteration 8, loss = 0.50538439\n",
            "Iteration 9, loss = 0.47639583\n",
            "Iteration 10, loss = 0.48289659\n",
            "Iteration 11, loss = 0.46717608\n",
            "Iteration 12, loss = 0.46165569\n",
            "Iteration 13, loss = 0.49222826\n",
            "Iteration 14, loss = 0.48253376\n",
            "Iteration 15, loss = 0.43310934\n",
            "Iteration 16, loss = 0.44006518\n",
            "Iteration 17, loss = 0.44777535\n",
            "Iteration 18, loss = 0.46818731\n",
            "Iteration 19, loss = 0.42831008\n",
            "Iteration 20, loss = 0.43900429\n",
            "Iteration 21, loss = 0.45178198\n",
            "Iteration 22, loss = 0.42740614\n",
            "Iteration 23, loss = 0.44585757\n",
            "Iteration 24, loss = 0.43142301\n",
            "Iteration 25, loss = 0.41848054\n",
            "Iteration 26, loss = 0.45468791\n",
            "Iteration 27, loss = 0.44893196\n",
            "Iteration 28, loss = 0.42627187\n",
            "Iteration 29, loss = 0.41466664\n",
            "Iteration 30, loss = 0.42965580\n",
            "Iteration 31, loss = 0.40303500\n",
            "Iteration 32, loss = 0.42871916\n",
            "Iteration 33, loss = 0.42183256\n",
            "Iteration 34, loss = 0.40210185\n",
            "Iteration 35, loss = 0.41047759\n",
            "Iteration 36, loss = 0.40611774\n",
            "Iteration 37, loss = 0.39252167\n",
            "Iteration 38, loss = 0.40614197\n",
            "Iteration 39, loss = 0.39936416\n",
            "Iteration 40, loss = 0.41411897\n",
            "Iteration 41, loss = 0.43472603\n",
            "Iteration 42, loss = 0.40167909\n",
            "Iteration 43, loss = 0.40307683\n",
            "Iteration 44, loss = 0.40061677\n",
            "Iteration 45, loss = 0.38849778\n",
            "Iteration 46, loss = 0.38762360\n",
            "Iteration 47, loss = 0.39141096\n",
            "Iteration 48, loss = 0.45755326\n",
            "Iteration 49, loss = 0.41527750\n",
            "Iteration 50, loss = 0.38933730\n",
            "Iteration 51, loss = 0.41146032\n",
            "Iteration 52, loss = 0.38716881\n",
            "Iteration 53, loss = 0.40687686\n",
            "Iteration 54, loss = 0.38748131\n",
            "Iteration 55, loss = 0.37056216\n",
            "Iteration 56, loss = 0.38775984\n",
            "Iteration 57, loss = 0.39814529\n",
            "Iteration 58, loss = 0.38436623\n",
            "Iteration 59, loss = 0.38368990\n",
            "Iteration 60, loss = 0.40832461\n",
            "Iteration 61, loss = 0.36904410\n",
            "Iteration 62, loss = 0.36710800\n",
            "Iteration 63, loss = 0.39406683\n",
            "Iteration 64, loss = 0.38855315\n",
            "Iteration 65, loss = 0.37037364\n",
            "Iteration 66, loss = 0.37149504\n",
            "Iteration 67, loss = 0.37530617\n",
            "Iteration 68, loss = 0.38618629\n",
            "Iteration 69, loss = 0.37233095\n",
            "Iteration 70, loss = 0.36452750\n",
            "Iteration 71, loss = 0.36697578\n",
            "Iteration 72, loss = 0.36740910\n",
            "Iteration 73, loss = 0.35136189\n",
            "Iteration 74, loss = 0.38267457\n",
            "Iteration 75, loss = 0.36375820\n",
            "Iteration 76, loss = 0.37542047\n",
            "Iteration 77, loss = 0.37120189\n",
            "Iteration 78, loss = 0.36932662\n",
            "Iteration 79, loss = 0.36853121\n",
            "Iteration 80, loss = 0.35062281\n",
            "Iteration 81, loss = 0.34290210\n",
            "Iteration 82, loss = 0.36558841\n",
            "Iteration 83, loss = 0.35694092\n",
            "Iteration 84, loss = 0.35982920\n",
            "Iteration 85, loss = 0.35047182\n",
            "Iteration 86, loss = 0.34383963\n",
            "Iteration 87, loss = 0.35572215\n",
            "Iteration 88, loss = 0.33556014\n",
            "Iteration 89, loss = 0.34452823\n",
            "Iteration 90, loss = 0.34744949\n",
            "Iteration 91, loss = 0.34857816\n",
            "Iteration 92, loss = 0.35049696\n",
            "Iteration 93, loss = 0.35714015\n",
            "Iteration 94, loss = 0.35378067\n",
            "Iteration 95, loss = 0.34698772\n",
            "Iteration 96, loss = 0.35141784\n",
            "Iteration 97, loss = 0.34807196\n",
            "Iteration 98, loss = 0.34854533\n",
            "Iteration 99, loss = 0.34497591\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.72080599\n",
            "Iteration 2, loss = 0.63524687\n",
            "Iteration 3, loss = 0.60263232\n",
            "Iteration 4, loss = 0.58329695\n",
            "Iteration 5, loss = 0.57601621\n",
            "Iteration 6, loss = 0.53566192\n",
            "Iteration 7, loss = 0.53551448\n",
            "Iteration 8, loss = 0.63161594\n",
            "Iteration 9, loss = 0.51298262\n",
            "Iteration 10, loss = 0.52082187\n",
            "Iteration 11, loss = 0.51075890\n",
            "Iteration 12, loss = 0.52447486\n",
            "Iteration 13, loss = 0.50881928\n",
            "Iteration 14, loss = 0.49507493\n",
            "Iteration 15, loss = 0.48053970\n",
            "Iteration 16, loss = 0.46048684\n",
            "Iteration 17, loss = 0.47803026\n",
            "Iteration 18, loss = 0.50398079\n",
            "Iteration 19, loss = 0.45061836\n",
            "Iteration 20, loss = 0.44891541\n",
            "Iteration 21, loss = 0.46780612\n",
            "Iteration 22, loss = 0.43056505\n",
            "Iteration 23, loss = 0.43535821\n",
            "Iteration 24, loss = 0.43820347\n",
            "Iteration 25, loss = 0.43060231\n",
            "Iteration 26, loss = 0.45840902\n",
            "Iteration 27, loss = 0.45027212\n",
            "Iteration 28, loss = 0.47536963\n",
            "Iteration 29, loss = 0.42540757\n",
            "Iteration 30, loss = 0.42144062\n",
            "Iteration 31, loss = 0.42235166\n",
            "Iteration 32, loss = 0.42083019\n",
            "Iteration 33, loss = 0.44153302\n",
            "Iteration 34, loss = 0.43098523\n",
            "Iteration 35, loss = 0.40853118\n",
            "Iteration 36, loss = 0.39738378\n",
            "Iteration 37, loss = 0.40430392\n",
            "Iteration 38, loss = 0.43432158\n",
            "Iteration 39, loss = 0.42737945\n",
            "Iteration 40, loss = 0.41106722\n",
            "Iteration 41, loss = 0.42043303\n",
            "Iteration 42, loss = 0.40676208\n",
            "Iteration 43, loss = 0.40467088\n",
            "Iteration 44, loss = 0.42049452\n",
            "Iteration 45, loss = 0.40344888\n",
            "Iteration 46, loss = 0.41021543\n",
            "Iteration 47, loss = 0.39382268\n",
            "Iteration 48, loss = 0.38915011\n",
            "Iteration 49, loss = 0.39813706\n",
            "Iteration 50, loss = 0.38331459\n",
            "Iteration 51, loss = 0.39279352\n",
            "Iteration 52, loss = 0.39992115\n",
            "Iteration 53, loss = 0.37947600\n",
            "Iteration 54, loss = 0.39311971\n",
            "Iteration 55, loss = 0.38801851\n",
            "Iteration 56, loss = 0.40081531\n",
            "Iteration 57, loss = 0.40205605\n",
            "Iteration 58, loss = 0.38582646\n",
            "Iteration 59, loss = 0.37724040\n",
            "Iteration 60, loss = 0.40759629\n",
            "Iteration 61, loss = 0.37892687\n",
            "Iteration 62, loss = 0.39126264\n",
            "Iteration 63, loss = 0.37512403\n",
            "Iteration 64, loss = 0.37263752\n",
            "Iteration 65, loss = 0.38646113\n",
            "Iteration 66, loss = 0.37670734\n",
            "Iteration 67, loss = 0.37757677\n",
            "Iteration 68, loss = 0.38213472\n",
            "Iteration 69, loss = 0.38360447\n",
            "Iteration 70, loss = 0.37988000\n",
            "Iteration 71, loss = 0.39034462\n",
            "Iteration 72, loss = 0.37894126\n",
            "Iteration 73, loss = 0.37214635\n",
            "Iteration 74, loss = 0.36653592\n",
            "Iteration 75, loss = 0.36736407\n",
            "Iteration 76, loss = 0.36094364\n",
            "Iteration 77, loss = 0.35909987\n",
            "Iteration 78, loss = 0.37029403\n",
            "Iteration 79, loss = 0.38797696\n",
            "Iteration 80, loss = 0.36474833\n",
            "Iteration 81, loss = 0.37361845\n",
            "Iteration 82, loss = 0.38943169\n",
            "Iteration 83, loss = 0.36956010\n",
            "Iteration 84, loss = 0.35996428\n",
            "Iteration 85, loss = 0.35422416\n",
            "Iteration 86, loss = 0.34551851\n",
            "Iteration 87, loss = 0.35266604\n",
            "Iteration 88, loss = 0.35261232\n",
            "Iteration 89, loss = 0.38608296\n",
            "Iteration 90, loss = 0.37408846\n",
            "Iteration 91, loss = 0.37535091\n",
            "Iteration 92, loss = 0.36031477\n",
            "Iteration 93, loss = 0.34782497\n",
            "Iteration 94, loss = 0.36245364\n",
            "Iteration 95, loss = 0.34664427\n",
            "Iteration 96, loss = 0.35813839\n",
            "Iteration 97, loss = 0.35420161\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67208206\n",
            "Iteration 2, loss = 0.66115721\n",
            "Iteration 3, loss = 0.65077460\n",
            "Iteration 4, loss = 0.64031603\n",
            "Iteration 5, loss = 0.62944405\n",
            "Iteration 6, loss = 0.62088750\n",
            "Iteration 7, loss = 0.61241522\n",
            "Iteration 8, loss = 0.60786586\n",
            "Iteration 9, loss = 0.60335448\n",
            "Iteration 10, loss = 0.60050249\n",
            "Iteration 11, loss = 0.59801951\n",
            "Iteration 12, loss = 0.59539368\n",
            "Iteration 13, loss = 0.59342096\n",
            "Iteration 14, loss = 0.59173881\n",
            "Iteration 15, loss = 0.58994787\n",
            "Iteration 16, loss = 0.58998519\n",
            "Iteration 17, loss = 0.58634415\n",
            "Iteration 18, loss = 0.58496670\n",
            "Iteration 19, loss = 0.58263809\n",
            "Iteration 20, loss = 0.58047447\n",
            "Iteration 21, loss = 0.57916073\n",
            "Iteration 22, loss = 0.57587036\n",
            "Iteration 23, loss = 0.57415703\n",
            "Iteration 24, loss = 0.57205261\n",
            "Iteration 25, loss = 0.56906289\n",
            "Iteration 26, loss = 0.56673996\n",
            "Iteration 27, loss = 0.56478988\n",
            "Iteration 28, loss = 0.56098515\n",
            "Iteration 29, loss = 0.55937233\n",
            "Iteration 30, loss = 0.55604265\n",
            "Iteration 31, loss = 0.55250253\n",
            "Iteration 32, loss = 0.55107417\n",
            "Iteration 33, loss = 0.54604050\n",
            "Iteration 34, loss = 0.54577948\n",
            "Iteration 35, loss = 0.53975655\n",
            "Iteration 36, loss = 0.53810252\n",
            "Iteration 37, loss = 0.53483659\n",
            "Iteration 38, loss = 0.53023265\n",
            "Iteration 39, loss = 0.52751032\n",
            "Iteration 40, loss = 0.52393026\n",
            "Iteration 41, loss = 0.52021475\n",
            "Iteration 42, loss = 0.51526034\n",
            "Iteration 43, loss = 0.51556670\n",
            "Iteration 44, loss = 0.50802506\n",
            "Iteration 45, loss = 0.50483624\n",
            "Iteration 46, loss = 0.50214987\n",
            "Iteration 47, loss = 0.49840196\n",
            "Iteration 48, loss = 0.49316338\n",
            "Iteration 49, loss = 0.49292400\n",
            "Iteration 50, loss = 0.49238584\n",
            "Iteration 51, loss = 0.48435078\n",
            "Iteration 52, loss = 0.48105584\n",
            "Iteration 53, loss = 0.47617817\n",
            "Iteration 54, loss = 0.47367196\n",
            "Iteration 55, loss = 0.47152391\n",
            "Iteration 56, loss = 0.46857195\n",
            "Iteration 57, loss = 0.46436304\n",
            "Iteration 58, loss = 0.46245736\n",
            "Iteration 59, loss = 0.45914420\n",
            "Iteration 60, loss = 0.45830299\n",
            "Iteration 61, loss = 0.45956084\n",
            "Iteration 62, loss = 0.45885116\n",
            "Iteration 63, loss = 0.45344631\n",
            "Iteration 64, loss = 0.45343855\n",
            "Iteration 65, loss = 0.45377776\n",
            "Iteration 66, loss = 0.45484241\n",
            "Iteration 67, loss = 0.44753335\n",
            "Iteration 68, loss = 0.44342664\n",
            "Iteration 69, loss = 0.44199827\n",
            "Iteration 70, loss = 0.44073443\n",
            "Iteration 71, loss = 0.44113761\n",
            "Iteration 72, loss = 0.43967499\n",
            "Iteration 73, loss = 0.43733273\n",
            "Iteration 74, loss = 0.43686852\n",
            "Iteration 75, loss = 0.43896409\n",
            "Iteration 76, loss = 0.43502533\n",
            "Iteration 77, loss = 0.43463687\n",
            "Iteration 78, loss = 0.43707200\n",
            "Iteration 79, loss = 0.43639543\n",
            "Iteration 80, loss = 0.43626871\n",
            "Iteration 81, loss = 0.43147446\n",
            "Iteration 82, loss = 0.43409514\n",
            "Iteration 83, loss = 0.43579764\n",
            "Iteration 84, loss = 0.43194350\n",
            "Iteration 85, loss = 0.43124706\n",
            "Iteration 86, loss = 0.43311568\n",
            "Iteration 87, loss = 0.42847316\n",
            "Iteration 88, loss = 0.42744459\n",
            "Iteration 89, loss = 0.42824909\n",
            "Iteration 90, loss = 0.42605549\n",
            "Iteration 91, loss = 0.42842428\n",
            "Iteration 92, loss = 0.42509198\n",
            "Iteration 93, loss = 0.42571130\n",
            "Iteration 94, loss = 0.42565030\n",
            "Iteration 95, loss = 0.42715536\n",
            "Iteration 96, loss = 0.42620869\n",
            "Iteration 97, loss = 0.42740132\n",
            "Iteration 98, loss = 0.42299115\n",
            "Iteration 99, loss = 0.42278166\n",
            "Iteration 100, loss = 0.42415935\n",
            "Iteration 101, loss = 0.42300268\n",
            "Iteration 102, loss = 0.42217934\n",
            "Iteration 103, loss = 0.42573571\n",
            "Iteration 104, loss = 0.42331793\n",
            "Iteration 105, loss = 0.42247960\n",
            "Iteration 106, loss = 0.42122158\n",
            "Iteration 107, loss = 0.41967668\n",
            "Iteration 108, loss = 0.41981526\n",
            "Iteration 109, loss = 0.42221132\n",
            "Iteration 110, loss = 0.42626476\n",
            "Iteration 111, loss = 0.41817219\n",
            "Iteration 112, loss = 0.41920671\n",
            "Iteration 113, loss = 0.42136004\n",
            "Iteration 114, loss = 0.41928785\n",
            "Iteration 115, loss = 0.41818272\n",
            "Iteration 116, loss = 0.41716403\n",
            "Iteration 117, loss = 0.41664826\n",
            "Iteration 118, loss = 0.42326853\n",
            "Iteration 119, loss = 0.43826614\n",
            "Iteration 120, loss = 0.42720843\n",
            "Iteration 121, loss = 0.43859478\n",
            "Iteration 122, loss = 0.43773539\n",
            "Iteration 123, loss = 0.41737733\n",
            "Iteration 124, loss = 0.41507501\n",
            "Iteration 125, loss = 0.41831980\n",
            "Iteration 126, loss = 0.41857230\n",
            "Iteration 127, loss = 0.41403645\n",
            "Iteration 128, loss = 0.41498953\n",
            "Iteration 129, loss = 0.41536877\n",
            "Iteration 130, loss = 0.41842565\n",
            "Iteration 131, loss = 0.41449671\n",
            "Iteration 132, loss = 0.41386020\n",
            "Iteration 133, loss = 0.41209289\n",
            "Iteration 134, loss = 0.41199694\n",
            "Iteration 135, loss = 0.41528075\n",
            "Iteration 136, loss = 0.41312581\n",
            "Iteration 137, loss = 0.41173719\n",
            "Iteration 138, loss = 0.41151362\n",
            "Iteration 139, loss = 0.41242896\n",
            "Iteration 140, loss = 0.41208198\n",
            "Iteration 141, loss = 0.41326960\n",
            "Iteration 142, loss = 0.40706760\n",
            "Iteration 143, loss = 0.41652407\n",
            "Iteration 144, loss = 0.41042252\n",
            "Iteration 145, loss = 0.40983533\n",
            "Iteration 146, loss = 0.41015072\n",
            "Iteration 147, loss = 0.40953220\n",
            "Iteration 148, loss = 0.40856532\n",
            "Iteration 149, loss = 0.40914984\n",
            "Iteration 150, loss = 0.41269418\n",
            "Iteration 151, loss = 0.41505884\n",
            "Iteration 152, loss = 0.42372907\n",
            "Iteration 153, loss = 0.41236415\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69305359\n",
            "Iteration 2, loss = 0.66788153\n",
            "Iteration 3, loss = 0.65805747\n",
            "Iteration 4, loss = 0.64739833\n",
            "Iteration 5, loss = 0.63772721\n",
            "Iteration 6, loss = 0.62836584\n",
            "Iteration 7, loss = 0.61954014\n",
            "Iteration 8, loss = 0.61213692\n",
            "Iteration 9, loss = 0.60733193\n",
            "Iteration 10, loss = 0.60321677\n",
            "Iteration 11, loss = 0.60039697\n",
            "Iteration 12, loss = 0.59781168\n",
            "Iteration 13, loss = 0.59582989\n",
            "Iteration 14, loss = 0.59462981\n",
            "Iteration 15, loss = 0.59236277\n",
            "Iteration 16, loss = 0.59169938\n",
            "Iteration 17, loss = 0.58879799\n",
            "Iteration 18, loss = 0.58773939\n",
            "Iteration 19, loss = 0.58599954\n",
            "Iteration 20, loss = 0.58380538\n",
            "Iteration 21, loss = 0.58453122\n",
            "Iteration 22, loss = 0.58023620\n",
            "Iteration 23, loss = 0.57921391\n",
            "Iteration 24, loss = 0.57612097\n",
            "Iteration 25, loss = 0.57438093\n",
            "Iteration 26, loss = 0.57222812\n",
            "Iteration 27, loss = 0.57009977\n",
            "Iteration 28, loss = 0.56953323\n",
            "Iteration 29, loss = 0.56550418\n",
            "Iteration 30, loss = 0.56449878\n",
            "Iteration 31, loss = 0.56314292\n",
            "Iteration 32, loss = 0.55985659\n",
            "Iteration 33, loss = 0.55766914\n",
            "Iteration 34, loss = 0.55572194\n",
            "Iteration 35, loss = 0.55313727\n",
            "Iteration 36, loss = 0.54981515\n",
            "Iteration 37, loss = 0.55026445\n",
            "Iteration 38, loss = 0.54522975\n",
            "Iteration 39, loss = 0.54308590\n",
            "Iteration 40, loss = 0.54013255\n",
            "Iteration 41, loss = 0.53740769\n",
            "Iteration 42, loss = 0.53518243\n",
            "Iteration 43, loss = 0.53135626\n",
            "Iteration 44, loss = 0.52984697\n",
            "Iteration 45, loss = 0.52945126\n",
            "Iteration 46, loss = 0.52250287\n",
            "Iteration 47, loss = 0.51980496\n",
            "Iteration 48, loss = 0.51746371\n",
            "Iteration 49, loss = 0.51426508\n",
            "Iteration 50, loss = 0.50936122\n",
            "Iteration 51, loss = 0.50684867\n",
            "Iteration 52, loss = 0.50297892\n",
            "Iteration 53, loss = 0.49967436\n",
            "Iteration 54, loss = 0.49710609\n",
            "Iteration 55, loss = 0.49199042\n",
            "Iteration 56, loss = 0.49070718\n",
            "Iteration 57, loss = 0.48749168\n",
            "Iteration 58, loss = 0.48209010\n",
            "Iteration 59, loss = 0.47883146\n",
            "Iteration 60, loss = 0.47540254\n",
            "Iteration 61, loss = 0.47257329\n",
            "Iteration 62, loss = 0.47011335\n",
            "Iteration 63, loss = 0.46863496\n",
            "Iteration 64, loss = 0.46507891\n",
            "Iteration 65, loss = 0.46086067\n",
            "Iteration 66, loss = 0.45838494\n",
            "Iteration 67, loss = 0.46252632\n",
            "Iteration 68, loss = 0.45613730\n",
            "Iteration 69, loss = 0.45653545\n",
            "Iteration 70, loss = 0.45580073\n",
            "Iteration 71, loss = 0.44711717\n",
            "Iteration 72, loss = 0.44087193\n",
            "Iteration 73, loss = 0.44535918\n",
            "Iteration 74, loss = 0.44261058\n",
            "Iteration 75, loss = 0.43834569\n",
            "Iteration 76, loss = 0.43628805\n",
            "Iteration 77, loss = 0.43485156\n",
            "Iteration 78, loss = 0.43290580\n",
            "Iteration 79, loss = 0.43180893\n",
            "Iteration 80, loss = 0.43338139\n",
            "Iteration 81, loss = 0.42999077\n",
            "Iteration 82, loss = 0.43690550\n",
            "Iteration 83, loss = 0.43251524\n",
            "Iteration 84, loss = 0.42891999\n",
            "Iteration 85, loss = 0.42958153\n",
            "Iteration 86, loss = 0.42423936\n",
            "Iteration 87, loss = 0.42336407\n",
            "Iteration 88, loss = 0.42213659\n",
            "Iteration 89, loss = 0.42129096\n",
            "Iteration 90, loss = 0.42138169\n",
            "Iteration 91, loss = 0.42384358\n",
            "Iteration 92, loss = 0.41991341\n",
            "Iteration 93, loss = 0.41934406\n",
            "Iteration 94, loss = 0.41978074\n",
            "Iteration 95, loss = 0.41851484\n",
            "Iteration 96, loss = 0.41774706\n",
            "Iteration 97, loss = 0.41939437\n",
            "Iteration 98, loss = 0.41696025\n",
            "Iteration 99, loss = 0.41568455\n",
            "Iteration 100, loss = 0.41721993\n",
            "Iteration 101, loss = 0.41703166\n",
            "Iteration 102, loss = 0.41761118\n",
            "Iteration 103, loss = 0.41709103\n",
            "Iteration 104, loss = 0.41342503\n",
            "Iteration 105, loss = 0.41641453\n",
            "Iteration 106, loss = 0.41390455\n",
            "Iteration 107, loss = 0.41684606\n",
            "Iteration 108, loss = 0.41214827\n",
            "Iteration 109, loss = 0.41419041\n",
            "Iteration 110, loss = 0.41179378\n",
            "Iteration 111, loss = 0.41109675\n",
            "Iteration 112, loss = 0.41260576\n",
            "Iteration 113, loss = 0.41536497\n",
            "Iteration 114, loss = 0.41123277\n",
            "Iteration 115, loss = 0.40840576\n",
            "Iteration 116, loss = 0.41198962\n",
            "Iteration 117, loss = 0.40675278\n",
            "Iteration 118, loss = 0.40969277\n",
            "Iteration 119, loss = 0.40828127\n",
            "Iteration 120, loss = 0.41094985\n",
            "Iteration 121, loss = 0.40944237\n",
            "Iteration 122, loss = 0.40827708\n",
            "Iteration 123, loss = 0.40993376\n",
            "Iteration 124, loss = 0.40675489\n",
            "Iteration 125, loss = 0.40898059\n",
            "Iteration 126, loss = 0.40612452\n",
            "Iteration 127, loss = 0.40518339\n",
            "Iteration 128, loss = 0.40700583\n",
            "Iteration 129, loss = 0.40827825\n",
            "Iteration 130, loss = 0.40761694\n",
            "Iteration 131, loss = 0.40301473\n",
            "Iteration 132, loss = 0.40991212\n",
            "Iteration 133, loss = 0.40553148\n",
            "Iteration 134, loss = 0.40579483\n",
            "Iteration 135, loss = 0.40352968\n",
            "Iteration 136, loss = 0.40419308\n",
            "Iteration 137, loss = 0.40344355\n",
            "Iteration 138, loss = 0.40277981\n",
            "Iteration 139, loss = 0.40397200\n",
            "Iteration 140, loss = 0.40340461\n",
            "Iteration 141, loss = 0.41070583\n",
            "Iteration 142, loss = 0.40384924\n",
            "Iteration 143, loss = 0.40269366\n",
            "Iteration 144, loss = 0.40117988\n",
            "Iteration 145, loss = 0.40169772\n",
            "Iteration 146, loss = 0.40225685\n",
            "Iteration 147, loss = 0.40257600\n",
            "Iteration 148, loss = 0.40165870\n",
            "Iteration 149, loss = 0.40326489\n",
            "Iteration 150, loss = 0.39967156\n",
            "Iteration 151, loss = 0.40100380\n",
            "Iteration 152, loss = 0.40362610\n",
            "Iteration 153, loss = 0.40897352\n",
            "Iteration 154, loss = 0.40219302\n",
            "Iteration 155, loss = 0.40222942\n",
            "Iteration 156, loss = 0.40139120\n",
            "Iteration 157, loss = 0.39879746\n",
            "Iteration 158, loss = 0.39958722\n",
            "Iteration 159, loss = 0.39962030\n",
            "Iteration 160, loss = 0.39900453\n",
            "Iteration 161, loss = 0.40138576\n",
            "Iteration 162, loss = 0.39974469\n",
            "Iteration 163, loss = 0.40118897\n",
            "Iteration 164, loss = 0.39837527\n",
            "Iteration 165, loss = 0.40204541\n",
            "Iteration 166, loss = 0.39873367\n",
            "Iteration 167, loss = 0.40167810\n",
            "Iteration 168, loss = 0.39620502\n",
            "Iteration 169, loss = 0.39743567\n",
            "Iteration 170, loss = 0.39678526\n",
            "Iteration 171, loss = 0.39589458\n",
            "Iteration 172, loss = 0.39786618\n",
            "Iteration 173, loss = 0.39729997\n",
            "Iteration 174, loss = 0.40484312\n",
            "Iteration 175, loss = 0.39741899\n",
            "Iteration 176, loss = 0.39760938\n",
            "Iteration 177, loss = 0.39474168\n",
            "Iteration 178, loss = 0.39944286\n",
            "Iteration 179, loss = 0.39550758\n",
            "Iteration 180, loss = 0.39531848\n",
            "Iteration 181, loss = 0.39532049\n",
            "Iteration 182, loss = 0.39622417\n",
            "Iteration 183, loss = 0.39418881\n",
            "Iteration 184, loss = 0.39438381\n",
            "Iteration 185, loss = 0.39924790\n",
            "Iteration 186, loss = 0.39952979\n",
            "Iteration 187, loss = 0.39348668\n",
            "Iteration 188, loss = 0.39445961\n",
            "Iteration 189, loss = 0.39656013\n",
            "Iteration 190, loss = 0.39546888\n",
            "Iteration 191, loss = 0.39343707\n",
            "Iteration 192, loss = 0.39360996\n",
            "Iteration 193, loss = 0.39430125\n",
            "Iteration 194, loss = 0.39227824\n",
            "Iteration 195, loss = 0.39377059\n",
            "Iteration 196, loss = 0.39706979\n",
            "Iteration 197, loss = 0.40042912\n",
            "Iteration 198, loss = 0.39434638\n",
            "Iteration 199, loss = 0.39343761\n",
            "Iteration 200, loss = 0.39598236\n",
            "Iteration 201, loss = 0.39942160\n",
            "Iteration 202, loss = 0.40222969\n",
            "Iteration 203, loss = 0.39809739\n",
            "Iteration 204, loss = 0.39331098\n",
            "Iteration 205, loss = 0.39513492\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.73860096\n",
            "Iteration 2, loss = 0.67588978\n",
            "Iteration 3, loss = 0.66082582\n",
            "Iteration 4, loss = 0.65366732\n",
            "Iteration 5, loss = 0.64400908\n",
            "Iteration 6, loss = 0.63349563\n",
            "Iteration 7, loss = 0.62670532\n",
            "Iteration 8, loss = 0.61983087\n",
            "Iteration 9, loss = 0.61401667\n",
            "Iteration 10, loss = 0.60854730\n",
            "Iteration 11, loss = 0.60435337\n",
            "Iteration 12, loss = 0.60149084\n",
            "Iteration 13, loss = 0.59951133\n",
            "Iteration 14, loss = 0.59766999\n",
            "Iteration 15, loss = 0.59466070\n",
            "Iteration 16, loss = 0.59259551\n",
            "Iteration 17, loss = 0.59043296\n",
            "Iteration 18, loss = 0.58801062\n",
            "Iteration 19, loss = 0.58543301\n",
            "Iteration 20, loss = 0.58304204\n",
            "Iteration 21, loss = 0.58071522\n",
            "Iteration 22, loss = 0.57851886\n",
            "Iteration 23, loss = 0.57586055\n",
            "Iteration 24, loss = 0.57418025\n",
            "Iteration 25, loss = 0.57160328\n",
            "Iteration 26, loss = 0.56894915\n",
            "Iteration 27, loss = 0.56691644\n",
            "Iteration 28, loss = 0.56419486\n",
            "Iteration 29, loss = 0.56290005\n",
            "Iteration 30, loss = 0.56032070\n",
            "Iteration 31, loss = 0.55802262\n",
            "Iteration 32, loss = 0.55411433\n",
            "Iteration 33, loss = 0.55133375\n",
            "Iteration 34, loss = 0.54805005\n",
            "Iteration 35, loss = 0.54442976\n",
            "Iteration 36, loss = 0.54126228\n",
            "Iteration 37, loss = 0.53759296\n",
            "Iteration 38, loss = 0.53492516\n",
            "Iteration 39, loss = 0.53013818\n",
            "Iteration 40, loss = 0.52693998\n",
            "Iteration 41, loss = 0.52225441\n",
            "Iteration 42, loss = 0.52010660\n",
            "Iteration 43, loss = 0.51444723\n",
            "Iteration 44, loss = 0.50942085\n",
            "Iteration 45, loss = 0.50960046\n",
            "Iteration 46, loss = 0.50321063\n",
            "Iteration 47, loss = 0.49780556\n",
            "Iteration 48, loss = 0.49246074\n",
            "Iteration 49, loss = 0.48873917\n",
            "Iteration 50, loss = 0.48397568\n",
            "Iteration 51, loss = 0.49065008\n",
            "Iteration 52, loss = 0.48546106\n",
            "Iteration 53, loss = 0.47568721\n",
            "Iteration 54, loss = 0.47360879\n",
            "Iteration 55, loss = 0.46786457\n",
            "Iteration 56, loss = 0.46439026\n",
            "Iteration 57, loss = 0.46103732\n",
            "Iteration 58, loss = 0.45823773\n",
            "Iteration 59, loss = 0.45902122\n",
            "Iteration 60, loss = 0.45496390\n",
            "Iteration 61, loss = 0.45336840\n",
            "Iteration 62, loss = 0.45194648\n",
            "Iteration 63, loss = 0.44233438\n",
            "Iteration 64, loss = 0.44082749\n",
            "Iteration 65, loss = 0.43929972\n",
            "Iteration 66, loss = 0.43796823\n",
            "Iteration 67, loss = 0.43959853\n",
            "Iteration 68, loss = 0.43388938\n",
            "Iteration 69, loss = 0.43046075\n",
            "Iteration 70, loss = 0.43090430\n",
            "Iteration 71, loss = 0.42789830\n",
            "Iteration 72, loss = 0.42705106\n",
            "Iteration 73, loss = 0.42718016\n",
            "Iteration 74, loss = 0.42888206\n",
            "Iteration 75, loss = 0.42481771\n",
            "Iteration 76, loss = 0.42069454\n",
            "Iteration 77, loss = 0.42193784\n",
            "Iteration 78, loss = 0.42151174\n",
            "Iteration 79, loss = 0.42676048\n",
            "Iteration 80, loss = 0.42458863\n",
            "Iteration 81, loss = 0.42254717\n",
            "Iteration 82, loss = 0.41567358\n",
            "Iteration 83, loss = 0.41587480\n",
            "Iteration 84, loss = 0.41644597\n",
            "Iteration 85, loss = 0.41557831\n",
            "Iteration 86, loss = 0.41684046\n",
            "Iteration 87, loss = 0.41207439\n",
            "Iteration 88, loss = 0.41076460\n",
            "Iteration 89, loss = 0.41155326\n",
            "Iteration 90, loss = 0.41074162\n",
            "Iteration 91, loss = 0.40980080\n",
            "Iteration 92, loss = 0.40937901\n",
            "Iteration 93, loss = 0.40894501\n",
            "Iteration 94, loss = 0.40899171\n",
            "Iteration 95, loss = 0.40821676\n",
            "Iteration 96, loss = 0.40917052\n",
            "Iteration 97, loss = 0.40601961\n",
            "Iteration 98, loss = 0.40680910\n",
            "Iteration 99, loss = 0.40651846\n",
            "Iteration 100, loss = 0.40675639\n",
            "Iteration 101, loss = 0.40341262\n",
            "Iteration 102, loss = 0.40375207\n",
            "Iteration 103, loss = 0.40401406\n",
            "Iteration 104, loss = 0.40258254\n",
            "Iteration 105, loss = 0.40375992\n",
            "Iteration 106, loss = 0.40406774\n",
            "Iteration 107, loss = 0.40304033\n",
            "Iteration 108, loss = 0.40113806\n",
            "Iteration 109, loss = 0.40095176\n",
            "Iteration 110, loss = 0.40355349\n",
            "Iteration 111, loss = 0.40137834\n",
            "Iteration 112, loss = 0.40091199\n",
            "Iteration 113, loss = 0.40502466\n",
            "Iteration 114, loss = 0.39994631\n",
            "Iteration 115, loss = 0.39970923\n",
            "Iteration 116, loss = 0.40198205\n",
            "Iteration 117, loss = 0.39973695\n",
            "Iteration 118, loss = 0.40033633\n",
            "Iteration 119, loss = 0.39987114\n",
            "Iteration 120, loss = 0.39976558\n",
            "Iteration 121, loss = 0.39689699\n",
            "Iteration 122, loss = 0.39616436\n",
            "Iteration 123, loss = 0.39872095\n",
            "Iteration 124, loss = 0.39766933\n",
            "Iteration 125, loss = 0.39958394\n",
            "Iteration 126, loss = 0.39768730\n",
            "Iteration 127, loss = 0.40149984\n",
            "Iteration 128, loss = 0.39713431\n",
            "Iteration 129, loss = 0.40263403\n",
            "Iteration 130, loss = 0.39662652\n",
            "Iteration 131, loss = 0.39520573\n",
            "Iteration 132, loss = 0.39621553\n",
            "Iteration 133, loss = 0.39374060\n",
            "Iteration 134, loss = 0.39750904\n",
            "Iteration 135, loss = 0.39423756\n",
            "Iteration 136, loss = 0.39189819\n",
            "Iteration 137, loss = 0.39231238\n",
            "Iteration 138, loss = 0.39477557\n",
            "Iteration 139, loss = 0.39667778\n",
            "Iteration 140, loss = 0.40141488\n",
            "Iteration 141, loss = 0.39387943\n",
            "Iteration 142, loss = 0.38963336\n",
            "Iteration 143, loss = 0.39424173\n",
            "Iteration 144, loss = 0.39100708\n",
            "Iteration 145, loss = 0.38994191\n",
            "Iteration 146, loss = 0.39354181\n",
            "Iteration 147, loss = 0.39002838\n",
            "Iteration 148, loss = 0.39384156\n",
            "Iteration 149, loss = 0.39042616\n",
            "Iteration 150, loss = 0.39339786\n",
            "Iteration 151, loss = 0.39230335\n",
            "Iteration 152, loss = 0.38927094\n",
            "Iteration 153, loss = 0.39066956\n",
            "Iteration 154, loss = 0.38843546\n",
            "Iteration 155, loss = 0.39101612\n",
            "Iteration 156, loss = 0.38744954\n",
            "Iteration 157, loss = 0.38833617\n",
            "Iteration 158, loss = 0.38762391\n",
            "Iteration 159, loss = 0.38702986\n",
            "Iteration 160, loss = 0.38927164\n",
            "Iteration 161, loss = 0.39069295\n",
            "Iteration 162, loss = 0.39897170\n",
            "Iteration 163, loss = 0.38785087\n",
            "Iteration 164, loss = 0.38674297\n",
            "Iteration 165, loss = 0.38566393\n",
            "Iteration 166, loss = 0.38690776\n",
            "Iteration 167, loss = 0.38844215\n",
            "Iteration 168, loss = 0.38880510\n",
            "Iteration 169, loss = 0.38934778\n",
            "Iteration 170, loss = 0.38735869\n",
            "Iteration 171, loss = 0.38750903\n",
            "Iteration 172, loss = 0.38328007\n",
            "Iteration 173, loss = 0.38964249\n",
            "Iteration 174, loss = 0.38804492\n",
            "Iteration 175, loss = 0.38432164\n",
            "Iteration 176, loss = 0.38488287\n",
            "Iteration 177, loss = 0.38388567\n",
            "Iteration 178, loss = 0.38308835\n",
            "Iteration 179, loss = 0.38358336\n",
            "Iteration 180, loss = 0.38418357\n",
            "Iteration 181, loss = 0.38382934\n",
            "Iteration 182, loss = 0.38224462\n",
            "Iteration 183, loss = 0.38395993\n",
            "Iteration 184, loss = 0.38152222\n",
            "Iteration 185, loss = 0.38425758\n",
            "Iteration 186, loss = 0.38141143\n",
            "Iteration 187, loss = 0.38741435\n",
            "Iteration 188, loss = 0.38432092\n",
            "Iteration 189, loss = 0.38164803\n",
            "Iteration 190, loss = 0.38064761\n",
            "Iteration 191, loss = 0.38205424\n",
            "Iteration 192, loss = 0.38594162\n",
            "Iteration 193, loss = 0.38281069\n",
            "Iteration 194, loss = 0.37986574\n",
            "Iteration 195, loss = 0.38438636\n",
            "Iteration 196, loss = 0.38892789\n",
            "Iteration 197, loss = 0.38195583\n",
            "Iteration 198, loss = 0.38040890\n",
            "Iteration 199, loss = 0.37912398\n",
            "Iteration 200, loss = 0.38098805\n",
            "Iteration 201, loss = 0.37937940\n",
            "Iteration 202, loss = 0.38324864\n",
            "Iteration 203, loss = 0.38187601\n",
            "Iteration 204, loss = 0.38045151\n",
            "Iteration 205, loss = 0.38872614\n",
            "Iteration 206, loss = 0.38316022\n",
            "Iteration 207, loss = 0.38056805\n",
            "Iteration 208, loss = 0.37789439\n",
            "Iteration 209, loss = 0.37836892\n",
            "Iteration 210, loss = 0.37717780\n",
            "Iteration 211, loss = 0.37755994\n",
            "Iteration 212, loss = 0.38037570\n",
            "Iteration 213, loss = 0.37746222\n",
            "Iteration 214, loss = 0.37884662\n",
            "Iteration 215, loss = 0.37703667\n",
            "Iteration 216, loss = 0.37686684\n",
            "Iteration 217, loss = 0.37849368\n",
            "Iteration 218, loss = 0.37504692\n",
            "Iteration 219, loss = 0.37639109\n",
            "Iteration 220, loss = 0.37529722\n",
            "Iteration 221, loss = 0.37536482\n",
            "Iteration 222, loss = 0.37788390\n",
            "Iteration 223, loss = 0.38024278\n",
            "Iteration 224, loss = 0.37673777\n",
            "Iteration 225, loss = 0.37506058\n",
            "Iteration 226, loss = 0.37416951\n",
            "Iteration 227, loss = 0.37637432\n",
            "Iteration 228, loss = 0.37623230\n",
            "Iteration 229, loss = 0.37472373\n",
            "Iteration 230, loss = 0.37515766\n",
            "Iteration 231, loss = 0.37504222\n",
            "Iteration 232, loss = 0.37437031\n",
            "Iteration 233, loss = 0.37314289\n",
            "Iteration 234, loss = 0.37412682\n",
            "Iteration 235, loss = 0.37550397\n",
            "Iteration 236, loss = 0.37323660\n",
            "Iteration 237, loss = 0.37375774\n",
            "Iteration 238, loss = 0.37304363\n",
            "Iteration 239, loss = 0.37460406\n",
            "Iteration 240, loss = 0.37243707\n",
            "Iteration 241, loss = 0.37344402\n",
            "Iteration 242, loss = 0.37289316\n",
            "Iteration 243, loss = 0.37129492\n",
            "Iteration 244, loss = 0.37255764\n",
            "Iteration 245, loss = 0.37323894\n",
            "Iteration 246, loss = 0.37194016\n",
            "Iteration 247, loss = 0.37141327\n",
            "Iteration 248, loss = 0.37188741\n",
            "Iteration 249, loss = 0.37460764\n",
            "Iteration 250, loss = 0.37581222\n",
            "Iteration 251, loss = 0.37034093\n",
            "Iteration 252, loss = 0.37064902\n",
            "Iteration 253, loss = 0.37356906\n",
            "Iteration 254, loss = 0.36977791\n",
            "Iteration 255, loss = 0.37102347\n",
            "Iteration 256, loss = 0.37206314\n",
            "Iteration 257, loss = 0.37209595\n",
            "Iteration 258, loss = 0.37380199\n",
            "Iteration 259, loss = 0.37732032\n",
            "Iteration 260, loss = 0.37510239\n",
            "Iteration 261, loss = 0.37925415\n",
            "Iteration 262, loss = 0.37131964\n",
            "Iteration 263, loss = 0.37055350\n",
            "Iteration 264, loss = 0.37070584\n",
            "Iteration 265, loss = 0.37251916\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68229105\n",
            "Iteration 2, loss = 0.66648121\n",
            "Iteration 3, loss = 0.65692658\n",
            "Iteration 4, loss = 0.64895435\n",
            "Iteration 5, loss = 0.64169155\n",
            "Iteration 6, loss = 0.63486552\n",
            "Iteration 7, loss = 0.63049134\n",
            "Iteration 8, loss = 0.62481201\n",
            "Iteration 9, loss = 0.62096998\n",
            "Iteration 10, loss = 0.61803742\n",
            "Iteration 11, loss = 0.61595628\n",
            "Iteration 12, loss = 0.61377948\n",
            "Iteration 13, loss = 0.61238230\n",
            "Iteration 14, loss = 0.61020822\n",
            "Iteration 15, loss = 0.60859167\n",
            "Iteration 16, loss = 0.60743997\n",
            "Iteration 17, loss = 0.60448651\n",
            "Iteration 18, loss = 0.60507652\n",
            "Iteration 19, loss = 0.60087617\n",
            "Iteration 20, loss = 0.59974876\n",
            "Iteration 21, loss = 0.59652947\n",
            "Iteration 22, loss = 0.59395232\n",
            "Iteration 23, loss = 0.59187082\n",
            "Iteration 24, loss = 0.58882775\n",
            "Iteration 25, loss = 0.58729953\n",
            "Iteration 26, loss = 0.58422950\n",
            "Iteration 27, loss = 0.58123720\n",
            "Iteration 28, loss = 0.57806664\n",
            "Iteration 29, loss = 0.57474864\n",
            "Iteration 30, loss = 0.57160917\n",
            "Iteration 31, loss = 0.56816049\n",
            "Iteration 32, loss = 0.56570713\n",
            "Iteration 33, loss = 0.56295270\n",
            "Iteration 34, loss = 0.55792087\n",
            "Iteration 35, loss = 0.55280091\n",
            "Iteration 36, loss = 0.54825603\n",
            "Iteration 37, loss = 0.55021108\n",
            "Iteration 38, loss = 0.54408507\n",
            "Iteration 39, loss = 0.53629289\n",
            "Iteration 40, loss = 0.53140243\n",
            "Iteration 41, loss = 0.52772447\n",
            "Iteration 42, loss = 0.52207565\n",
            "Iteration 43, loss = 0.51840808\n",
            "Iteration 44, loss = 0.51479336\n",
            "Iteration 45, loss = 0.51127361\n",
            "Iteration 46, loss = 0.50683240\n",
            "Iteration 47, loss = 0.50076106\n",
            "Iteration 48, loss = 0.49753461\n",
            "Iteration 49, loss = 0.49344100\n",
            "Iteration 50, loss = 0.48964933\n",
            "Iteration 51, loss = 0.48524424\n",
            "Iteration 52, loss = 0.48196289\n",
            "Iteration 53, loss = 0.47687036\n",
            "Iteration 54, loss = 0.47790633\n",
            "Iteration 55, loss = 0.47151635\n",
            "Iteration 56, loss = 0.46944642\n",
            "Iteration 57, loss = 0.46682976\n",
            "Iteration 58, loss = 0.46538023\n",
            "Iteration 59, loss = 0.46304008\n",
            "Iteration 60, loss = 0.46695971\n",
            "Iteration 61, loss = 0.46071473\n",
            "Iteration 62, loss = 0.45519232\n",
            "Iteration 63, loss = 0.45409164\n",
            "Iteration 64, loss = 0.45346807\n",
            "Iteration 65, loss = 0.44885518\n",
            "Iteration 66, loss = 0.45040177\n",
            "Iteration 67, loss = 0.44836688\n",
            "Iteration 68, loss = 0.44635893\n",
            "Iteration 69, loss = 0.44676356\n",
            "Iteration 70, loss = 0.44140243\n",
            "Iteration 71, loss = 0.44227318\n",
            "Iteration 72, loss = 0.44227594\n",
            "Iteration 73, loss = 0.43949057\n",
            "Iteration 74, loss = 0.43910891\n",
            "Iteration 75, loss = 0.44171786\n",
            "Iteration 76, loss = 0.43871079\n",
            "Iteration 77, loss = 0.43951994\n",
            "Iteration 78, loss = 0.43854649\n",
            "Iteration 79, loss = 0.43628928\n",
            "Iteration 80, loss = 0.43498558\n",
            "Iteration 81, loss = 0.43538106\n",
            "Iteration 82, loss = 0.43780879\n",
            "Iteration 83, loss = 0.43848553\n",
            "Iteration 84, loss = 0.44080564\n",
            "Iteration 85, loss = 0.43680348\n",
            "Iteration 86, loss = 0.43519188\n",
            "Iteration 87, loss = 0.43425178\n",
            "Iteration 88, loss = 0.43240103\n",
            "Iteration 89, loss = 0.43288078\n",
            "Iteration 90, loss = 0.43188214\n",
            "Iteration 91, loss = 0.42955750\n",
            "Iteration 92, loss = 0.42974061\n",
            "Iteration 93, loss = 0.43592343\n",
            "Iteration 94, loss = 0.43862729\n",
            "Iteration 95, loss = 0.43051395\n",
            "Iteration 96, loss = 0.43443421\n",
            "Iteration 97, loss = 0.43031703\n",
            "Iteration 98, loss = 0.42720361\n",
            "Iteration 99, loss = 0.43120305\n",
            "Iteration 100, loss = 0.42715108\n",
            "Iteration 101, loss = 0.42916976\n",
            "Iteration 102, loss = 0.43260218\n",
            "Iteration 103, loss = 0.42515994\n",
            "Iteration 104, loss = 0.42522803\n",
            "Iteration 105, loss = 0.42494328\n",
            "Iteration 106, loss = 0.42581805\n",
            "Iteration 107, loss = 0.42517204\n",
            "Iteration 108, loss = 0.42652768\n",
            "Iteration 109, loss = 0.42387860\n",
            "Iteration 110, loss = 0.42281003\n",
            "Iteration 111, loss = 0.42126538\n",
            "Iteration 112, loss = 0.42380439\n",
            "Iteration 113, loss = 0.42420845\n",
            "Iteration 114, loss = 0.42176069\n",
            "Iteration 115, loss = 0.42251759\n",
            "Iteration 116, loss = 0.42051433\n",
            "Iteration 117, loss = 0.42426642\n",
            "Iteration 118, loss = 0.42213360\n",
            "Iteration 119, loss = 0.42330831\n",
            "Iteration 120, loss = 0.41937688\n",
            "Iteration 121, loss = 0.42118502\n",
            "Iteration 122, loss = 0.41886877\n",
            "Iteration 123, loss = 0.42341042\n",
            "Iteration 124, loss = 0.42750709\n",
            "Iteration 125, loss = 0.42228362\n",
            "Iteration 126, loss = 0.42011408\n",
            "Iteration 127, loss = 0.41775992\n",
            "Iteration 128, loss = 0.41643302\n",
            "Iteration 129, loss = 0.41812777\n",
            "Iteration 130, loss = 0.41580097\n",
            "Iteration 131, loss = 0.41715051\n",
            "Iteration 132, loss = 0.41673057\n",
            "Iteration 133, loss = 0.41622457\n",
            "Iteration 134, loss = 0.41918227\n",
            "Iteration 135, loss = 0.41833462\n",
            "Iteration 136, loss = 0.41754840\n",
            "Iteration 137, loss = 0.41378666\n",
            "Iteration 138, loss = 0.41530780\n",
            "Iteration 139, loss = 0.42467034\n",
            "Iteration 140, loss = 0.42129055\n",
            "Iteration 141, loss = 0.41541299\n",
            "Iteration 142, loss = 0.41859733\n",
            "Iteration 143, loss = 0.41539563\n",
            "Iteration 144, loss = 0.41296732\n",
            "Iteration 145, loss = 0.41324730\n",
            "Iteration 146, loss = 0.41312039\n",
            "Iteration 147, loss = 0.41181948\n",
            "Iteration 148, loss = 0.41152262\n",
            "Iteration 149, loss = 0.41246893\n",
            "Iteration 150, loss = 0.40846161\n",
            "Iteration 151, loss = 0.41114500\n",
            "Iteration 152, loss = 0.41187665\n",
            "Iteration 153, loss = 0.41079883\n",
            "Iteration 154, loss = 0.41006474\n",
            "Iteration 155, loss = 0.41175064\n",
            "Iteration 156, loss = 0.40964335\n",
            "Iteration 157, loss = 0.40917510\n",
            "Iteration 158, loss = 0.40908740\n",
            "Iteration 159, loss = 0.41055870\n",
            "Iteration 160, loss = 0.40905062\n",
            "Iteration 161, loss = 0.41004344\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67878125\n",
            "Iteration 2, loss = 0.66433214\n",
            "Iteration 3, loss = 0.65686416\n",
            "Iteration 4, loss = 0.64749505\n",
            "Iteration 5, loss = 0.63922933\n",
            "Iteration 6, loss = 0.63240840\n",
            "Iteration 7, loss = 0.62659456\n",
            "Iteration 8, loss = 0.62183227\n",
            "Iteration 9, loss = 0.61907324\n",
            "Iteration 10, loss = 0.61591260\n",
            "Iteration 11, loss = 0.61417734\n",
            "Iteration 12, loss = 0.61254372\n",
            "Iteration 13, loss = 0.61069446\n",
            "Iteration 14, loss = 0.60959086\n",
            "Iteration 15, loss = 0.61008551\n",
            "Iteration 16, loss = 0.60793485\n",
            "Iteration 17, loss = 0.60541437\n",
            "Iteration 18, loss = 0.60389644\n",
            "Iteration 19, loss = 0.60320432\n",
            "Iteration 20, loss = 0.60043098\n",
            "Iteration 21, loss = 0.59879287\n",
            "Iteration 22, loss = 0.59693262\n",
            "Iteration 23, loss = 0.59476299\n",
            "Iteration 24, loss = 0.59325308\n",
            "Iteration 25, loss = 0.59105676\n",
            "Iteration 26, loss = 0.58898026\n",
            "Iteration 27, loss = 0.58634828\n",
            "Iteration 28, loss = 0.58402070\n",
            "Iteration 29, loss = 0.58205777\n",
            "Iteration 30, loss = 0.57909592\n",
            "Iteration 31, loss = 0.57679384\n",
            "Iteration 32, loss = 0.57414692\n",
            "Iteration 33, loss = 0.57020338\n",
            "Iteration 34, loss = 0.56801601\n",
            "Iteration 35, loss = 0.56416130\n",
            "Iteration 36, loss = 0.56132089\n",
            "Iteration 37, loss = 0.55705999\n",
            "Iteration 38, loss = 0.55362668\n",
            "Iteration 39, loss = 0.55366150\n",
            "Iteration 40, loss = 0.54692664\n",
            "Iteration 41, loss = 0.54273218\n",
            "Iteration 42, loss = 0.53935034\n",
            "Iteration 43, loss = 0.53569605\n",
            "Iteration 44, loss = 0.53156370\n",
            "Iteration 45, loss = 0.52640755\n",
            "Iteration 46, loss = 0.52244657\n",
            "Iteration 47, loss = 0.51800221\n",
            "Iteration 48, loss = 0.51367781\n",
            "Iteration 49, loss = 0.50829092\n",
            "Iteration 50, loss = 0.50677307\n",
            "Iteration 51, loss = 0.50067213\n",
            "Iteration 52, loss = 0.49757320\n",
            "Iteration 53, loss = 0.49698830\n",
            "Iteration 54, loss = 0.49353009\n",
            "Iteration 55, loss = 0.49618963\n",
            "Iteration 56, loss = 0.49532205\n",
            "Iteration 57, loss = 0.49928673\n",
            "Iteration 58, loss = 0.48424252\n",
            "Iteration 59, loss = 0.47884553\n",
            "Iteration 60, loss = 0.47124921\n",
            "Iteration 61, loss = 0.47446767\n",
            "Iteration 62, loss = 0.46984133\n",
            "Iteration 63, loss = 0.46759539\n",
            "Iteration 64, loss = 0.46543018\n",
            "Iteration 65, loss = 0.46180986\n",
            "Iteration 66, loss = 0.46207041\n",
            "Iteration 67, loss = 0.45739018\n",
            "Iteration 68, loss = 0.45766737\n",
            "Iteration 69, loss = 0.45371929\n",
            "Iteration 70, loss = 0.45576125\n",
            "Iteration 71, loss = 0.45465591\n",
            "Iteration 72, loss = 0.45384004\n",
            "Iteration 73, loss = 0.45140491\n",
            "Iteration 74, loss = 0.45095182\n",
            "Iteration 75, loss = 0.45179447\n",
            "Iteration 76, loss = 0.44674014\n",
            "Iteration 77, loss = 0.44854280\n",
            "Iteration 78, loss = 0.44813021\n",
            "Iteration 79, loss = 0.44411227\n",
            "Iteration 80, loss = 0.44305102\n",
            "Iteration 81, loss = 0.44155000\n",
            "Iteration 82, loss = 0.44147055\n",
            "Iteration 83, loss = 0.44236627\n",
            "Iteration 84, loss = 0.44274875\n",
            "Iteration 85, loss = 0.44369897\n",
            "Iteration 86, loss = 0.44294744\n",
            "Iteration 87, loss = 0.43945583\n",
            "Iteration 88, loss = 0.43873950\n",
            "Iteration 89, loss = 0.43889010\n",
            "Iteration 90, loss = 0.43894944\n",
            "Iteration 91, loss = 0.44209979\n",
            "Iteration 92, loss = 0.44776196\n",
            "Iteration 93, loss = 0.43899037\n",
            "Iteration 94, loss = 0.43544717\n",
            "Iteration 95, loss = 0.43635740\n",
            "Iteration 96, loss = 0.43642642\n",
            "Iteration 97, loss = 0.43344971\n",
            "Iteration 98, loss = 0.43413844\n",
            "Iteration 99, loss = 0.43356953\n",
            "Iteration 100, loss = 0.43333287\n",
            "Iteration 101, loss = 0.43185000\n",
            "Iteration 102, loss = 0.43292097\n",
            "Iteration 103, loss = 0.43414781\n",
            "Iteration 104, loss = 0.43028763\n",
            "Iteration 105, loss = 0.43580376\n",
            "Iteration 106, loss = 0.43030884\n",
            "Iteration 107, loss = 0.42977876\n",
            "Iteration 108, loss = 0.43716668\n",
            "Iteration 109, loss = 0.43448765\n",
            "Iteration 110, loss = 0.42900575\n",
            "Iteration 111, loss = 0.42945169\n",
            "Iteration 112, loss = 0.42859693\n",
            "Iteration 113, loss = 0.42951604\n",
            "Iteration 114, loss = 0.42952930\n",
            "Iteration 115, loss = 0.42741655\n",
            "Iteration 116, loss = 0.42757774\n",
            "Iteration 117, loss = 0.42940007\n",
            "Iteration 118, loss = 0.42729040\n",
            "Iteration 119, loss = 0.42802794\n",
            "Iteration 120, loss = 0.42856623\n",
            "Iteration 121, loss = 0.42857846\n",
            "Iteration 122, loss = 0.42967401\n",
            "Iteration 123, loss = 0.42898026\n",
            "Iteration 124, loss = 0.42604431\n",
            "Iteration 125, loss = 0.42442227\n",
            "Iteration 126, loss = 0.42690855\n",
            "Iteration 127, loss = 0.43008762\n",
            "Iteration 128, loss = 0.42610683\n",
            "Iteration 129, loss = 0.42364193\n",
            "Iteration 130, loss = 0.42350445\n",
            "Iteration 131, loss = 0.42790688\n",
            "Iteration 132, loss = 0.42435311\n",
            "Iteration 133, loss = 0.42512034\n",
            "Iteration 134, loss = 0.42328534\n",
            "Iteration 135, loss = 0.42405082\n",
            "Iteration 136, loss = 0.42436046\n",
            "Iteration 137, loss = 0.42276232\n",
            "Iteration 138, loss = 0.42311861\n",
            "Iteration 139, loss = 0.42740588\n",
            "Iteration 140, loss = 0.42253047\n",
            "Iteration 141, loss = 0.42098212\n",
            "Iteration 142, loss = 0.41976644\n",
            "Iteration 143, loss = 0.42253830\n",
            "Iteration 144, loss = 0.42101145\n",
            "Iteration 145, loss = 0.42561594\n",
            "Iteration 146, loss = 0.43195812\n",
            "Iteration 147, loss = 0.42052009\n",
            "Iteration 148, loss = 0.41906352\n",
            "Iteration 149, loss = 0.41887912\n",
            "Iteration 150, loss = 0.41936298\n",
            "Iteration 151, loss = 0.41879423\n",
            "Iteration 152, loss = 0.41870272\n",
            "Iteration 153, loss = 0.42024385\n",
            "Iteration 154, loss = 0.41980297\n",
            "Iteration 155, loss = 0.41611200\n",
            "Iteration 156, loss = 0.41961625\n",
            "Iteration 157, loss = 0.42008303\n",
            "Iteration 158, loss = 0.41619911\n",
            "Iteration 159, loss = 0.41851813\n",
            "Iteration 160, loss = 0.42093012\n",
            "Iteration 161, loss = 0.41782904\n",
            "Iteration 162, loss = 0.41496361\n",
            "Iteration 163, loss = 0.41811504\n",
            "Iteration 164, loss = 0.41699435\n",
            "Iteration 165, loss = 0.41649733\n",
            "Iteration 166, loss = 0.41442677\n",
            "Iteration 167, loss = 0.41526458\n",
            "Iteration 168, loss = 0.41329698\n",
            "Iteration 169, loss = 0.41430150\n",
            "Iteration 170, loss = 0.41346045\n",
            "Iteration 171, loss = 0.41326744\n",
            "Iteration 172, loss = 0.41166965\n",
            "Iteration 173, loss = 0.41242476\n",
            "Iteration 174, loss = 0.41328663\n",
            "Iteration 175, loss = 0.41528322\n",
            "Iteration 176, loss = 0.41099278\n",
            "Iteration 177, loss = 0.41264484\n",
            "Iteration 178, loss = 0.41229315\n",
            "Iteration 179, loss = 0.40997593\n",
            "Iteration 180, loss = 0.41019140\n",
            "Iteration 181, loss = 0.41045649\n",
            "Iteration 182, loss = 0.41212982\n",
            "Iteration 183, loss = 0.41236624\n",
            "Iteration 184, loss = 0.41311542\n",
            "Iteration 185, loss = 0.41173661\n",
            "Iteration 186, loss = 0.41208614\n",
            "Iteration 187, loss = 0.41265811\n",
            "Iteration 188, loss = 0.41018024\n",
            "Iteration 189, loss = 0.40919184\n",
            "Iteration 190, loss = 0.40905156\n",
            "Iteration 191, loss = 0.40958625\n",
            "Iteration 192, loss = 0.40719996\n",
            "Iteration 193, loss = 0.40794200\n",
            "Iteration 194, loss = 0.40727854\n",
            "Iteration 195, loss = 0.40737684\n",
            "Iteration 196, loss = 0.40809476\n",
            "Iteration 197, loss = 0.40737871\n",
            "Iteration 198, loss = 0.40671638\n",
            "Iteration 199, loss = 0.40918319\n",
            "Iteration 200, loss = 0.41515816\n",
            "Iteration 201, loss = 0.40889431\n",
            "Iteration 202, loss = 0.41009661\n",
            "Iteration 203, loss = 0.40470157\n",
            "Iteration 204, loss = 0.40615991\n",
            "Iteration 205, loss = 0.40723441\n",
            "Iteration 206, loss = 0.40473737\n",
            "Iteration 207, loss = 0.40569318\n",
            "Iteration 208, loss = 0.40645772\n",
            "Iteration 209, loss = 0.40420069\n",
            "Iteration 210, loss = 0.40751772\n",
            "Iteration 211, loss = 0.40414037\n",
            "Iteration 212, loss = 0.40536239\n",
            "Iteration 213, loss = 0.40528819\n",
            "Iteration 214, loss = 0.40356704\n",
            "Iteration 215, loss = 0.40442932\n",
            "Iteration 216, loss = 0.40213480\n",
            "Iteration 217, loss = 0.40512745\n",
            "Iteration 218, loss = 0.40306301\n",
            "Iteration 219, loss = 0.40562046\n",
            "Iteration 220, loss = 0.40218600\n",
            "Iteration 221, loss = 0.40146365\n",
            "Iteration 222, loss = 0.40122495\n",
            "Iteration 223, loss = 0.40127617\n",
            "Iteration 224, loss = 0.40050281\n",
            "Iteration 225, loss = 0.40276526\n",
            "Iteration 226, loss = 0.40270249\n",
            "Iteration 227, loss = 0.39979048\n",
            "Iteration 228, loss = 0.40089219\n",
            "Iteration 229, loss = 0.40014263\n",
            "Iteration 230, loss = 0.39993536\n",
            "Iteration 231, loss = 0.39817519\n",
            "Iteration 232, loss = 0.40227881\n",
            "Iteration 233, loss = 0.40194964\n",
            "Iteration 234, loss = 0.39981168\n",
            "Iteration 235, loss = 0.39901113\n",
            "Iteration 236, loss = 0.39832847\n",
            "Iteration 237, loss = 0.39896748\n",
            "Iteration 238, loss = 0.40039096\n",
            "Iteration 239, loss = 0.40071471\n",
            "Iteration 240, loss = 0.39856421\n",
            "Iteration 241, loss = 0.39777252\n",
            "Iteration 242, loss = 0.39701032\n",
            "Iteration 243, loss = 0.39731480\n",
            "Iteration 244, loss = 0.39832465\n",
            "Iteration 245, loss = 0.39840631\n",
            "Iteration 246, loss = 0.40117298\n",
            "Iteration 247, loss = 0.39795683\n",
            "Iteration 248, loss = 0.39809616\n",
            "Iteration 249, loss = 0.39545874\n",
            "Iteration 250, loss = 0.39550236\n",
            "Iteration 251, loss = 0.39591748\n",
            "Iteration 252, loss = 0.39651965\n",
            "Iteration 253, loss = 0.39723642\n",
            "Iteration 254, loss = 0.39738468\n",
            "Iteration 255, loss = 0.39706086\n",
            "Iteration 256, loss = 0.39549584\n",
            "Iteration 257, loss = 0.39428225\n",
            "Iteration 258, loss = 0.39580953\n",
            "Iteration 259, loss = 0.39439053\n",
            "Iteration 260, loss = 0.39374813\n",
            "Iteration 261, loss = 0.39778038\n",
            "Iteration 262, loss = 0.39770702\n",
            "Iteration 263, loss = 0.39737973\n",
            "Iteration 264, loss = 0.39491792\n",
            "Iteration 265, loss = 0.39468120\n",
            "Iteration 266, loss = 0.39177136\n",
            "Iteration 267, loss = 0.39108267\n",
            "Iteration 268, loss = 0.39117776\n",
            "Iteration 269, loss = 0.39150318\n",
            "Iteration 270, loss = 0.39356950\n",
            "Iteration 271, loss = 0.39735434\n",
            "Iteration 272, loss = 0.39700226\n",
            "Iteration 273, loss = 0.39205869\n",
            "Iteration 274, loss = 0.39229103\n",
            "Iteration 275, loss = 0.39111415\n",
            "Iteration 276, loss = 0.38977324\n",
            "Iteration 277, loss = 0.39000305\n",
            "Iteration 278, loss = 0.39127162\n",
            "Iteration 279, loss = 0.38960910\n",
            "Iteration 280, loss = 0.39065070\n",
            "Iteration 281, loss = 0.39010998\n",
            "Iteration 282, loss = 0.39459738\n",
            "Iteration 283, loss = 0.38907882\n",
            "Iteration 284, loss = 0.38799002\n",
            "Iteration 285, loss = 0.38899555\n",
            "Iteration 286, loss = 0.38864082\n",
            "Iteration 287, loss = 0.39030722\n",
            "Iteration 288, loss = 0.39064793\n",
            "Iteration 289, loss = 0.38745106\n",
            "Iteration 290, loss = 0.38854175\n",
            "Iteration 291, loss = 0.38621493\n",
            "Iteration 292, loss = 0.39334566\n",
            "Iteration 293, loss = 0.39690765\n",
            "Iteration 294, loss = 0.40005582\n",
            "Iteration 295, loss = 0.39039220\n",
            "Iteration 296, loss = 0.39151667\n",
            "Iteration 297, loss = 0.38804306\n",
            "Iteration 298, loss = 0.38691284\n",
            "Iteration 299, loss = 0.38691361\n",
            "Iteration 300, loss = 0.39629487\n",
            "Iteration 301, loss = 0.39662402\n",
            "Iteration 302, loss = 0.39108833\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.63668154\n",
            "Iteration 2, loss = 0.61030420\n",
            "Iteration 3, loss = 0.60295651\n",
            "Iteration 4, loss = 0.60076623\n",
            "Iteration 5, loss = 0.59771540\n",
            "Iteration 6, loss = 0.59150737\n",
            "Iteration 7, loss = 0.58676534\n",
            "Iteration 8, loss = 0.58321399\n",
            "Iteration 9, loss = 0.58410539\n",
            "Iteration 10, loss = 0.57642952\n",
            "Iteration 11, loss = 0.58490315\n",
            "Iteration 12, loss = 0.56689806\n",
            "Iteration 13, loss = 0.56026111\n",
            "Iteration 14, loss = 0.55224151\n",
            "Iteration 15, loss = 0.54528193\n",
            "Iteration 16, loss = 0.53666824\n",
            "Iteration 17, loss = 0.52589374\n",
            "Iteration 18, loss = 0.51654112\n",
            "Iteration 19, loss = 0.50827913\n",
            "Iteration 20, loss = 0.50369891\n",
            "Iteration 21, loss = 0.49429465\n",
            "Iteration 22, loss = 0.48841943\n",
            "Iteration 23, loss = 0.48717012\n",
            "Iteration 24, loss = 0.48577871\n",
            "Iteration 25, loss = 0.46984349\n",
            "Iteration 26, loss = 0.48183086\n",
            "Iteration 27, loss = 0.45777081\n",
            "Iteration 28, loss = 0.45732621\n",
            "Iteration 29, loss = 0.45870820\n",
            "Iteration 30, loss = 0.44471495\n",
            "Iteration 31, loss = 0.45849689\n",
            "Iteration 32, loss = 0.44799071\n",
            "Iteration 33, loss = 0.43977488\n",
            "Iteration 34, loss = 0.43693285\n",
            "Iteration 35, loss = 0.43076990\n",
            "Iteration 36, loss = 0.42584966\n",
            "Iteration 37, loss = 0.43280227\n",
            "Iteration 38, loss = 0.42665676\n",
            "Iteration 39, loss = 0.42373593\n",
            "Iteration 40, loss = 0.43107854\n",
            "Iteration 41, loss = 0.42424504\n",
            "Iteration 42, loss = 0.41736823\n",
            "Iteration 43, loss = 0.40560068\n",
            "Iteration 44, loss = 0.41479245\n",
            "Iteration 45, loss = 0.40269428\n",
            "Iteration 46, loss = 0.40905032\n",
            "Iteration 47, loss = 0.41840739\n",
            "Iteration 48, loss = 0.39757074\n",
            "Iteration 49, loss = 0.40727434\n",
            "Iteration 50, loss = 0.39781627\n",
            "Iteration 51, loss = 0.39652215\n",
            "Iteration 52, loss = 0.39582704\n",
            "Iteration 53, loss = 0.39846677\n",
            "Iteration 54, loss = 0.39203761\n",
            "Iteration 55, loss = 0.38629877\n",
            "Iteration 56, loss = 0.38962766\n",
            "Iteration 57, loss = 0.38555762\n",
            "Iteration 58, loss = 0.39373151\n",
            "Iteration 59, loss = 0.38489956\n",
            "Iteration 60, loss = 0.38338506\n",
            "Iteration 61, loss = 0.38493434\n",
            "Iteration 62, loss = 0.37649869\n",
            "Iteration 63, loss = 0.37976171\n",
            "Iteration 64, loss = 0.38528875\n",
            "Iteration 65, loss = 0.38158114\n",
            "Iteration 66, loss = 0.40341225\n",
            "Iteration 67, loss = 0.38779736\n",
            "Iteration 68, loss = 0.37512080\n",
            "Iteration 69, loss = 0.37775151\n",
            "Iteration 70, loss = 0.37694052\n",
            "Iteration 71, loss = 0.37733145\n",
            "Iteration 72, loss = 0.37063567\n",
            "Iteration 73, loss = 0.37613981\n",
            "Iteration 74, loss = 0.37583466\n",
            "Iteration 75, loss = 0.38668008\n",
            "Iteration 76, loss = 0.37412537\n",
            "Iteration 77, loss = 0.37313841\n",
            "Iteration 78, loss = 0.37027316\n",
            "Iteration 79, loss = 0.37211370\n",
            "Iteration 80, loss = 0.37665889\n",
            "Iteration 81, loss = 0.37503423\n",
            "Iteration 82, loss = 0.37087858\n",
            "Iteration 83, loss = 0.37176331\n",
            "Iteration 84, loss = 0.37013752\n",
            "Iteration 85, loss = 0.36913612\n",
            "Iteration 86, loss = 0.37318330\n",
            "Iteration 87, loss = 0.36561130\n",
            "Iteration 88, loss = 0.36646692\n",
            "Iteration 89, loss = 0.36790115\n",
            "Iteration 90, loss = 0.36909852\n",
            "Iteration 91, loss = 0.36523488\n",
            "Iteration 92, loss = 0.36611012\n",
            "Iteration 93, loss = 0.36843013\n",
            "Iteration 94, loss = 0.36647485\n",
            "Iteration 95, loss = 0.35812289\n",
            "Iteration 96, loss = 0.37278531\n",
            "Iteration 97, loss = 0.36314329\n",
            "Iteration 98, loss = 0.36887304\n",
            "Iteration 99, loss = 0.36509624\n",
            "Iteration 100, loss = 0.36605845\n",
            "Iteration 101, loss = 0.36437077\n",
            "Iteration 102, loss = 0.35964695\n",
            "Iteration 103, loss = 0.36431298\n",
            "Iteration 104, loss = 0.36562127\n",
            "Iteration 105, loss = 0.35972261\n",
            "Iteration 106, loss = 0.36187028\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61547623\n",
            "Iteration 2, loss = 0.59967068\n",
            "Iteration 3, loss = 0.59369502\n",
            "Iteration 4, loss = 0.58440196\n",
            "Iteration 5, loss = 0.57676773\n",
            "Iteration 6, loss = 0.56403499\n",
            "Iteration 7, loss = 0.55952998\n",
            "Iteration 8, loss = 0.54869999\n",
            "Iteration 9, loss = 0.54000958\n",
            "Iteration 10, loss = 0.53897325\n",
            "Iteration 11, loss = 0.52478130\n",
            "Iteration 12, loss = 0.52074201\n",
            "Iteration 13, loss = 0.50981538\n",
            "Iteration 14, loss = 0.49598870\n",
            "Iteration 15, loss = 0.49087912\n",
            "Iteration 16, loss = 0.48768339\n",
            "Iteration 17, loss = 0.47737426\n",
            "Iteration 18, loss = 0.47568005\n",
            "Iteration 19, loss = 0.47645452\n",
            "Iteration 20, loss = 0.47467367\n",
            "Iteration 21, loss = 0.47388219\n",
            "Iteration 22, loss = 0.46210389\n",
            "Iteration 23, loss = 0.45399795\n",
            "Iteration 24, loss = 0.45277329\n",
            "Iteration 25, loss = 0.45146198\n",
            "Iteration 26, loss = 0.45592163\n",
            "Iteration 27, loss = 0.44368159\n",
            "Iteration 28, loss = 0.44845657\n",
            "Iteration 29, loss = 0.44194590\n",
            "Iteration 30, loss = 0.42892902\n",
            "Iteration 31, loss = 0.43006445\n",
            "Iteration 32, loss = 0.42961943\n",
            "Iteration 33, loss = 0.43518828\n",
            "Iteration 34, loss = 0.42275224\n",
            "Iteration 35, loss = 0.42801801\n",
            "Iteration 36, loss = 0.42784314\n",
            "Iteration 37, loss = 0.41912289\n",
            "Iteration 38, loss = 0.41755886\n",
            "Iteration 39, loss = 0.42147049\n",
            "Iteration 40, loss = 0.42213261\n",
            "Iteration 41, loss = 0.41821215\n",
            "Iteration 42, loss = 0.40851165\n",
            "Iteration 43, loss = 0.40759308\n",
            "Iteration 44, loss = 0.40466436\n",
            "Iteration 45, loss = 0.40673385\n",
            "Iteration 46, loss = 0.40135884\n",
            "Iteration 47, loss = 0.39966010\n",
            "Iteration 48, loss = 0.40172452\n",
            "Iteration 49, loss = 0.41350221\n",
            "Iteration 50, loss = 0.40152208\n",
            "Iteration 51, loss = 0.39744744\n",
            "Iteration 52, loss = 0.38974436\n",
            "Iteration 53, loss = 0.39746414\n",
            "Iteration 54, loss = 0.39716789\n",
            "Iteration 55, loss = 0.39206683\n",
            "Iteration 56, loss = 0.40360211\n",
            "Iteration 57, loss = 0.40174468\n",
            "Iteration 58, loss = 0.39959954\n",
            "Iteration 59, loss = 0.39702608\n",
            "Iteration 60, loss = 0.38571221\n",
            "Iteration 61, loss = 0.38780727\n",
            "Iteration 62, loss = 0.38518476\n",
            "Iteration 63, loss = 0.38353673\n",
            "Iteration 64, loss = 0.37645777\n",
            "Iteration 65, loss = 0.37782868\n",
            "Iteration 66, loss = 0.37407269\n",
            "Iteration 67, loss = 0.38472966\n",
            "Iteration 68, loss = 0.38652215\n",
            "Iteration 69, loss = 0.38213045\n",
            "Iteration 70, loss = 0.38493893\n",
            "Iteration 71, loss = 0.38060263\n",
            "Iteration 72, loss = 0.37288014\n",
            "Iteration 73, loss = 0.37075668\n",
            "Iteration 74, loss = 0.37254228\n",
            "Iteration 75, loss = 0.36938116\n",
            "Iteration 76, loss = 0.36955491\n",
            "Iteration 77, loss = 0.36799195\n",
            "Iteration 78, loss = 0.36594179\n",
            "Iteration 79, loss = 0.37277694\n",
            "Iteration 80, loss = 0.36924426\n",
            "Iteration 81, loss = 0.36108675\n",
            "Iteration 82, loss = 0.36006210\n",
            "Iteration 83, loss = 0.36220196\n",
            "Iteration 84, loss = 0.35711544\n",
            "Iteration 85, loss = 0.35783076\n",
            "Iteration 86, loss = 0.36717694\n",
            "Iteration 87, loss = 0.39232259\n",
            "Iteration 88, loss = 0.36328079\n",
            "Iteration 89, loss = 0.35473002\n",
            "Iteration 90, loss = 0.34937583\n",
            "Iteration 91, loss = 0.36591623\n",
            "Iteration 92, loss = 0.36114291\n",
            "Iteration 93, loss = 0.37353527\n",
            "Iteration 94, loss = 0.35350219\n",
            "Iteration 95, loss = 0.38748012\n",
            "Iteration 96, loss = 0.37351473\n",
            "Iteration 97, loss = 0.35245176\n",
            "Iteration 98, loss = 0.35252539\n",
            "Iteration 99, loss = 0.34977595\n",
            "Iteration 100, loss = 0.35234131\n",
            "Iteration 101, loss = 0.34837194\n",
            "Iteration 102, loss = 0.34580740\n",
            "Iteration 103, loss = 0.34709913\n",
            "Iteration 104, loss = 0.34994921\n",
            "Iteration 105, loss = 0.34631757\n",
            "Iteration 106, loss = 0.36092270\n",
            "Iteration 107, loss = 0.34806070\n",
            "Iteration 108, loss = 0.34218385\n",
            "Iteration 109, loss = 0.33939967\n",
            "Iteration 110, loss = 0.34285873\n",
            "Iteration 111, loss = 0.36272927\n",
            "Iteration 112, loss = 0.35273074\n",
            "Iteration 113, loss = 0.34483147\n",
            "Iteration 114, loss = 0.34311546\n",
            "Iteration 115, loss = 0.35044940\n",
            "Iteration 116, loss = 0.34268129\n",
            "Iteration 117, loss = 0.33497178\n",
            "Iteration 118, loss = 0.33924022\n",
            "Iteration 119, loss = 0.34099752\n",
            "Iteration 120, loss = 0.34286372\n",
            "Iteration 121, loss = 0.33777494\n",
            "Iteration 122, loss = 0.34107204\n",
            "Iteration 123, loss = 0.34664231\n",
            "Iteration 124, loss = 0.33621998\n",
            "Iteration 125, loss = 0.33723897\n",
            "Iteration 126, loss = 0.33960704\n",
            "Iteration 127, loss = 0.36181606\n",
            "Iteration 128, loss = 0.34263485\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61151560\n",
            "Iteration 2, loss = 0.59942283\n",
            "Iteration 3, loss = 0.58377014\n",
            "Iteration 4, loss = 0.58183938\n",
            "Iteration 5, loss = 0.56797310\n",
            "Iteration 6, loss = 0.55541818\n",
            "Iteration 7, loss = 0.54624852\n",
            "Iteration 8, loss = 0.53381846\n",
            "Iteration 9, loss = 0.51549869\n",
            "Iteration 10, loss = 0.50515520\n",
            "Iteration 11, loss = 0.49936095\n",
            "Iteration 12, loss = 0.48432172\n",
            "Iteration 13, loss = 0.47850964\n",
            "Iteration 14, loss = 0.47043375\n",
            "Iteration 15, loss = 0.45964174\n",
            "Iteration 16, loss = 0.45296906\n",
            "Iteration 17, loss = 0.45952108\n",
            "Iteration 18, loss = 0.44500927\n",
            "Iteration 19, loss = 0.45259104\n",
            "Iteration 20, loss = 0.43765047\n",
            "Iteration 21, loss = 0.42841740\n",
            "Iteration 22, loss = 0.42810770\n",
            "Iteration 23, loss = 0.42894534\n",
            "Iteration 24, loss = 0.43255654\n",
            "Iteration 25, loss = 0.42550490\n",
            "Iteration 26, loss = 0.41612672\n",
            "Iteration 27, loss = 0.41818887\n",
            "Iteration 28, loss = 0.41600184\n",
            "Iteration 29, loss = 0.41486702\n",
            "Iteration 30, loss = 0.41406431\n",
            "Iteration 31, loss = 0.40830649\n",
            "Iteration 32, loss = 0.41179585\n",
            "Iteration 33, loss = 0.40247323\n",
            "Iteration 34, loss = 0.40934604\n",
            "Iteration 35, loss = 0.41031338\n",
            "Iteration 36, loss = 0.40461511\n",
            "Iteration 37, loss = 0.41002938\n",
            "Iteration 38, loss = 0.40359300\n",
            "Iteration 39, loss = 0.40042762\n",
            "Iteration 40, loss = 0.39887298\n",
            "Iteration 41, loss = 0.41575892\n",
            "Iteration 42, loss = 0.40942249\n",
            "Iteration 43, loss = 0.41250623\n",
            "Iteration 44, loss = 0.40852753\n",
            "Iteration 45, loss = 0.39417969\n",
            "Iteration 46, loss = 0.41259698\n",
            "Iteration 47, loss = 0.41062298\n",
            "Iteration 48, loss = 0.39057593\n",
            "Iteration 49, loss = 0.39342179\n",
            "Iteration 50, loss = 0.39069265\n",
            "Iteration 51, loss = 0.38352061\n",
            "Iteration 52, loss = 0.38837707\n",
            "Iteration 53, loss = 0.38790089\n",
            "Iteration 54, loss = 0.39573234\n",
            "Iteration 55, loss = 0.40109280\n",
            "Iteration 56, loss = 0.38785234\n",
            "Iteration 57, loss = 0.38602528\n",
            "Iteration 58, loss = 0.38260501\n",
            "Iteration 59, loss = 0.38103940\n",
            "Iteration 60, loss = 0.38986636\n",
            "Iteration 61, loss = 0.38571616\n",
            "Iteration 62, loss = 0.37810364\n",
            "Iteration 63, loss = 0.37755805\n",
            "Iteration 64, loss = 0.38190395\n",
            "Iteration 65, loss = 0.38362703\n",
            "Iteration 66, loss = 0.37860800\n",
            "Iteration 67, loss = 0.37975412\n",
            "Iteration 68, loss = 0.37493636\n",
            "Iteration 69, loss = 0.38556646\n",
            "Iteration 70, loss = 0.38030929\n",
            "Iteration 71, loss = 0.37332605\n",
            "Iteration 72, loss = 0.37557098\n",
            "Iteration 73, loss = 0.37556483\n",
            "Iteration 74, loss = 0.38177636\n",
            "Iteration 75, loss = 0.42156848\n",
            "Iteration 76, loss = 0.37786462\n",
            "Iteration 77, loss = 0.38298180\n",
            "Iteration 78, loss = 0.37164476\n",
            "Iteration 79, loss = 0.36808235\n",
            "Iteration 80, loss = 0.37036473\n",
            "Iteration 81, loss = 0.37433307\n",
            "Iteration 82, loss = 0.38128872\n",
            "Iteration 83, loss = 0.37960025\n",
            "Iteration 84, loss = 0.37316846\n",
            "Iteration 85, loss = 0.36846244\n",
            "Iteration 86, loss = 0.36844007\n",
            "Iteration 87, loss = 0.37068438\n",
            "Iteration 88, loss = 0.36537636\n",
            "Iteration 89, loss = 0.37033525\n",
            "Iteration 90, loss = 0.36491907\n",
            "Iteration 91, loss = 0.36245623\n",
            "Iteration 92, loss = 0.37284320\n",
            "Iteration 93, loss = 0.36857912\n",
            "Iteration 94, loss = 0.36141196\n",
            "Iteration 95, loss = 0.36333193\n",
            "Iteration 96, loss = 0.35941665\n",
            "Iteration 97, loss = 0.35869300\n",
            "Iteration 98, loss = 0.36931008\n",
            "Iteration 99, loss = 0.37400853\n",
            "Iteration 100, loss = 0.35746113\n",
            "Iteration 101, loss = 0.35803055\n",
            "Iteration 102, loss = 0.35760989\n",
            "Iteration 103, loss = 0.36071143\n",
            "Iteration 104, loss = 0.35435192\n",
            "Iteration 105, loss = 0.35765378\n",
            "Iteration 106, loss = 0.34786621\n",
            "Iteration 107, loss = 0.36059266\n",
            "Iteration 108, loss = 0.35544004\n",
            "Iteration 109, loss = 0.36476714\n",
            "Iteration 110, loss = 0.35280172\n",
            "Iteration 111, loss = 0.34698188\n",
            "Iteration 112, loss = 0.34906030\n",
            "Iteration 113, loss = 0.35144549\n",
            "Iteration 114, loss = 0.34665877\n",
            "Iteration 115, loss = 0.36543476\n",
            "Iteration 116, loss = 0.35578607\n",
            "Iteration 117, loss = 0.35122286\n",
            "Iteration 118, loss = 0.34733704\n",
            "Iteration 119, loss = 0.35395668\n",
            "Iteration 120, loss = 0.35240630\n",
            "Iteration 121, loss = 0.34885176\n",
            "Iteration 122, loss = 0.34188727\n",
            "Iteration 123, loss = 0.34864832\n",
            "Iteration 124, loss = 0.34694224\n",
            "Iteration 125, loss = 0.35440535\n",
            "Iteration 126, loss = 0.34618633\n",
            "Iteration 127, loss = 0.34618314\n",
            "Iteration 128, loss = 0.34556711\n",
            "Iteration 129, loss = 0.34766607\n",
            "Iteration 130, loss = 0.36215307\n",
            "Iteration 131, loss = 0.34914051\n",
            "Iteration 132, loss = 0.34390244\n",
            "Iteration 133, loss = 0.34182514\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.63347282\n",
            "Iteration 2, loss = 0.61665894\n",
            "Iteration 3, loss = 0.60830524\n",
            "Iteration 4, loss = 0.60085126\n",
            "Iteration 5, loss = 0.58861971\n",
            "Iteration 6, loss = 0.58309124\n",
            "Iteration 7, loss = 0.56883708\n",
            "Iteration 8, loss = 0.55831943\n",
            "Iteration 9, loss = 0.54024814\n",
            "Iteration 10, loss = 0.52758514\n",
            "Iteration 11, loss = 0.52362349\n",
            "Iteration 12, loss = 0.50908471\n",
            "Iteration 13, loss = 0.50217431\n",
            "Iteration 14, loss = 0.50022697\n",
            "Iteration 15, loss = 0.48452681\n",
            "Iteration 16, loss = 0.47578119\n",
            "Iteration 17, loss = 0.48045012\n",
            "Iteration 18, loss = 0.47616578\n",
            "Iteration 19, loss = 0.46746008\n",
            "Iteration 20, loss = 0.45519863\n",
            "Iteration 21, loss = 0.45967082\n",
            "Iteration 22, loss = 0.45362257\n",
            "Iteration 23, loss = 0.44356296\n",
            "Iteration 24, loss = 0.43468630\n",
            "Iteration 25, loss = 0.43261365\n",
            "Iteration 26, loss = 0.43242278\n",
            "Iteration 27, loss = 0.44016073\n",
            "Iteration 28, loss = 0.42003661\n",
            "Iteration 29, loss = 0.42358274\n",
            "Iteration 30, loss = 0.41902408\n",
            "Iteration 31, loss = 0.41481299\n",
            "Iteration 32, loss = 0.41319377\n",
            "Iteration 33, loss = 0.41878623\n",
            "Iteration 34, loss = 0.41469682\n",
            "Iteration 35, loss = 0.41160502\n",
            "Iteration 36, loss = 0.41241869\n",
            "Iteration 37, loss = 0.40727980\n",
            "Iteration 38, loss = 0.42701032\n",
            "Iteration 39, loss = 0.41209283\n",
            "Iteration 40, loss = 0.41478172\n",
            "Iteration 41, loss = 0.41304571\n",
            "Iteration 42, loss = 0.39616868\n",
            "Iteration 43, loss = 0.39702611\n",
            "Iteration 44, loss = 0.40178883\n",
            "Iteration 45, loss = 0.41306491\n",
            "Iteration 46, loss = 0.40029385\n",
            "Iteration 47, loss = 0.39943854\n",
            "Iteration 48, loss = 0.39785774\n",
            "Iteration 49, loss = 0.38697730\n",
            "Iteration 50, loss = 0.38659990\n",
            "Iteration 51, loss = 0.39498467\n",
            "Iteration 52, loss = 0.39258785\n",
            "Iteration 53, loss = 0.38678811\n",
            "Iteration 54, loss = 0.39276819\n",
            "Iteration 55, loss = 0.40340937\n",
            "Iteration 56, loss = 0.39499857\n",
            "Iteration 57, loss = 0.39082827\n",
            "Iteration 58, loss = 0.38695586\n",
            "Iteration 59, loss = 0.39216342\n",
            "Iteration 60, loss = 0.38334502\n",
            "Iteration 61, loss = 0.39511189\n",
            "Iteration 62, loss = 0.37718308\n",
            "Iteration 63, loss = 0.38889234\n",
            "Iteration 64, loss = 0.38158103\n",
            "Iteration 65, loss = 0.37672580\n",
            "Iteration 66, loss = 0.37837163\n",
            "Iteration 67, loss = 0.37884247\n",
            "Iteration 68, loss = 0.37996351\n",
            "Iteration 69, loss = 0.37316863\n",
            "Iteration 70, loss = 0.38784173\n",
            "Iteration 71, loss = 0.37032898\n",
            "Iteration 72, loss = 0.37782770\n",
            "Iteration 73, loss = 0.37490171\n",
            "Iteration 74, loss = 0.37463296\n",
            "Iteration 75, loss = 0.37933764\n",
            "Iteration 76, loss = 0.37989942\n",
            "Iteration 77, loss = 0.37601655\n",
            "Iteration 78, loss = 0.36448289\n",
            "Iteration 79, loss = 0.36696411\n",
            "Iteration 80, loss = 0.37648102\n",
            "Iteration 81, loss = 0.37145805\n",
            "Iteration 82, loss = 0.35203208\n",
            "Iteration 83, loss = 0.37580684\n",
            "Iteration 84, loss = 0.36344369\n",
            "Iteration 85, loss = 0.37415192\n",
            "Iteration 86, loss = 0.35932545\n",
            "Iteration 87, loss = 0.36951540\n",
            "Iteration 88, loss = 0.37849261\n",
            "Iteration 89, loss = 0.37596189\n",
            "Iteration 90, loss = 0.36073309\n",
            "Iteration 91, loss = 0.35785489\n",
            "Iteration 92, loss = 0.37656730\n",
            "Iteration 93, loss = 0.36862099\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62029776\n",
            "Iteration 2, loss = 0.60013733\n",
            "Iteration 3, loss = 0.59106086\n",
            "Iteration 4, loss = 0.57836953\n",
            "Iteration 5, loss = 0.58619752\n",
            "Iteration 6, loss = 0.56625407\n",
            "Iteration 7, loss = 0.55598557\n",
            "Iteration 8, loss = 0.54277790\n",
            "Iteration 9, loss = 0.52229673\n",
            "Iteration 10, loss = 0.52140119\n",
            "Iteration 11, loss = 0.50988229\n",
            "Iteration 12, loss = 0.49575659\n",
            "Iteration 13, loss = 0.47631490\n",
            "Iteration 14, loss = 0.47327407\n",
            "Iteration 15, loss = 0.46367069\n",
            "Iteration 16, loss = 0.46305017\n",
            "Iteration 17, loss = 0.45254286\n",
            "Iteration 18, loss = 0.45703881\n",
            "Iteration 19, loss = 0.45891789\n",
            "Iteration 20, loss = 0.44173635\n",
            "Iteration 21, loss = 0.44570456\n",
            "Iteration 22, loss = 0.43797327\n",
            "Iteration 23, loss = 0.43984649\n",
            "Iteration 24, loss = 0.44063469\n",
            "Iteration 25, loss = 0.44249446\n",
            "Iteration 26, loss = 0.42918130\n",
            "Iteration 27, loss = 0.44006438\n",
            "Iteration 28, loss = 0.42898548\n",
            "Iteration 29, loss = 0.42478292\n",
            "Iteration 30, loss = 0.42375656\n",
            "Iteration 31, loss = 0.42948280\n",
            "Iteration 32, loss = 0.44565423\n",
            "Iteration 33, loss = 0.41607587\n",
            "Iteration 34, loss = 0.42155886\n",
            "Iteration 35, loss = 0.42191899\n",
            "Iteration 36, loss = 0.40945393\n",
            "Iteration 37, loss = 0.42243636\n",
            "Iteration 38, loss = 0.41585607\n",
            "Iteration 39, loss = 0.41527780\n",
            "Iteration 40, loss = 0.40851023\n",
            "Iteration 41, loss = 0.41421024\n",
            "Iteration 42, loss = 0.40223641\n",
            "Iteration 43, loss = 0.41305816\n",
            "Iteration 44, loss = 0.40759708\n",
            "Iteration 45, loss = 0.40883728\n",
            "Iteration 46, loss = 0.41032820\n",
            "Iteration 47, loss = 0.39895831\n",
            "Iteration 48, loss = 0.40130768\n",
            "Iteration 49, loss = 0.40639871\n",
            "Iteration 50, loss = 0.40674511\n",
            "Iteration 51, loss = 0.40046018\n",
            "Iteration 52, loss = 0.39408415\n",
            "Iteration 53, loss = 0.39968060\n",
            "Iteration 54, loss = 0.39533127\n",
            "Iteration 55, loss = 0.39689320\n",
            "Iteration 56, loss = 0.40459964\n",
            "Iteration 57, loss = 0.39342811\n",
            "Iteration 58, loss = 0.39567051\n",
            "Iteration 59, loss = 0.39293313\n",
            "Iteration 60, loss = 0.39031069\n",
            "Iteration 61, loss = 0.39134075\n",
            "Iteration 62, loss = 0.38698347\n",
            "Iteration 63, loss = 0.39213983\n",
            "Iteration 64, loss = 0.39128541\n",
            "Iteration 65, loss = 0.39214725\n",
            "Iteration 66, loss = 0.39007600\n",
            "Iteration 67, loss = 0.38670892\n",
            "Iteration 68, loss = 0.38837086\n",
            "Iteration 69, loss = 0.40206558\n",
            "Iteration 70, loss = 0.39991963\n",
            "Iteration 71, loss = 0.38219306\n",
            "Iteration 72, loss = 0.38727755\n",
            "Iteration 73, loss = 0.38624569\n",
            "Iteration 74, loss = 0.38247646\n",
            "Iteration 75, loss = 0.38526272\n",
            "Iteration 76, loss = 0.38222989\n",
            "Iteration 77, loss = 0.38638525\n",
            "Iteration 78, loss = 0.38017018\n",
            "Iteration 79, loss = 0.38772318\n",
            "Iteration 80, loss = 0.39714426\n",
            "Iteration 81, loss = 0.38293941\n",
            "Iteration 82, loss = 0.38429098\n",
            "Iteration 83, loss = 0.38141446\n",
            "Iteration 84, loss = 0.38394443\n",
            "Iteration 85, loss = 0.37971121\n",
            "Iteration 86, loss = 0.36985229\n",
            "Iteration 87, loss = 0.38763031\n",
            "Iteration 88, loss = 0.38716430\n",
            "Iteration 89, loss = 0.38153656\n",
            "Iteration 90, loss = 0.37694221\n",
            "Iteration 91, loss = 0.39376421\n",
            "Iteration 92, loss = 0.37119402\n",
            "Iteration 93, loss = 0.37678177\n",
            "Iteration 94, loss = 0.37394291\n",
            "Iteration 95, loss = 0.37858356\n",
            "Iteration 96, loss = 0.37416406\n",
            "Iteration 97, loss = 0.37535061\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.79239293\n",
            "Iteration 2, loss = 0.75391165\n",
            "Iteration 3, loss = 0.71769580\n",
            "Iteration 4, loss = 0.69295260\n",
            "Iteration 5, loss = 0.68286593\n",
            "Iteration 6, loss = 0.67860956\n",
            "Iteration 7, loss = 0.67621973\n",
            "Iteration 8, loss = 0.67511008\n",
            "Iteration 9, loss = 0.67445983\n",
            "Iteration 10, loss = 0.67390668\n",
            "Iteration 11, loss = 0.67349978\n",
            "Iteration 12, loss = 0.67291947\n",
            "Iteration 13, loss = 0.67246305\n",
            "Iteration 14, loss = 0.67206199\n",
            "Iteration 15, loss = 0.67150262\n",
            "Iteration 16, loss = 0.67104444\n",
            "Iteration 17, loss = 0.67079017\n",
            "Iteration 18, loss = 0.67008511\n",
            "Iteration 19, loss = 0.66959195\n",
            "Iteration 20, loss = 0.66923485\n",
            "Iteration 21, loss = 0.66868560\n",
            "Iteration 22, loss = 0.66826444\n",
            "Iteration 23, loss = 0.66777593\n",
            "Iteration 24, loss = 0.66734521\n",
            "Iteration 25, loss = 0.66693738\n",
            "Iteration 26, loss = 0.66655675\n",
            "Iteration 27, loss = 0.66607635\n",
            "Iteration 28, loss = 0.66569128\n",
            "Iteration 29, loss = 0.66515714\n",
            "Iteration 30, loss = 0.66497474\n",
            "Iteration 31, loss = 0.66427183\n",
            "Iteration 32, loss = 0.66380233\n",
            "Iteration 33, loss = 0.66333723\n",
            "Iteration 34, loss = 0.66289236\n",
            "Iteration 35, loss = 0.66248759\n",
            "Iteration 36, loss = 0.66206225\n",
            "Iteration 37, loss = 0.66163041\n",
            "Iteration 38, loss = 0.66128498\n",
            "Iteration 39, loss = 0.66086112\n",
            "Iteration 40, loss = 0.66035451\n",
            "Iteration 41, loss = 0.65989327\n",
            "Iteration 42, loss = 0.65956909\n",
            "Iteration 43, loss = 0.65909221\n",
            "Iteration 44, loss = 0.65860144\n",
            "Iteration 45, loss = 0.65816349\n",
            "Iteration 46, loss = 0.65778774\n",
            "Iteration 47, loss = 0.65736265\n",
            "Iteration 48, loss = 0.65690024\n",
            "Iteration 49, loss = 0.65674345\n",
            "Iteration 50, loss = 0.65623479\n",
            "Iteration 51, loss = 0.65566518\n",
            "Iteration 52, loss = 0.65552442\n",
            "Iteration 53, loss = 0.65477063\n",
            "Iteration 54, loss = 0.65448810\n",
            "Iteration 55, loss = 0.65403773\n",
            "Iteration 56, loss = 0.65364651\n",
            "Iteration 57, loss = 0.65337037\n",
            "Iteration 58, loss = 0.65286642\n",
            "Iteration 59, loss = 0.65244382\n",
            "Iteration 60, loss = 0.65198672\n",
            "Iteration 61, loss = 0.65167314\n",
            "Iteration 62, loss = 0.65111778\n",
            "Iteration 63, loss = 0.65070342\n",
            "Iteration 64, loss = 0.65034706\n",
            "Iteration 65, loss = 0.64990185\n",
            "Iteration 66, loss = 0.64957658\n",
            "Iteration 67, loss = 0.64908116\n",
            "Iteration 68, loss = 0.64873049\n",
            "Iteration 69, loss = 0.64823904\n",
            "Iteration 70, loss = 0.64793542\n",
            "Iteration 71, loss = 0.64743354\n",
            "Iteration 72, loss = 0.64711163\n",
            "Iteration 73, loss = 0.64692350\n",
            "Iteration 74, loss = 0.64623942\n",
            "Iteration 75, loss = 0.64590695\n",
            "Iteration 76, loss = 0.64545451\n",
            "Iteration 77, loss = 0.64507219\n",
            "Iteration 78, loss = 0.64462049\n",
            "Iteration 79, loss = 0.64423591\n",
            "Iteration 80, loss = 0.64374792\n",
            "Iteration 81, loss = 0.64381540\n",
            "Iteration 82, loss = 0.64296999\n",
            "Iteration 83, loss = 0.64256333\n",
            "Iteration 84, loss = 0.64218667\n",
            "Iteration 85, loss = 0.64190683\n",
            "Iteration 86, loss = 0.64136020\n",
            "Iteration 87, loss = 0.64095513\n",
            "Iteration 88, loss = 0.64050372\n",
            "Iteration 89, loss = 0.64015381\n",
            "Iteration 90, loss = 0.63973238\n",
            "Iteration 91, loss = 0.63933984\n",
            "Iteration 92, loss = 0.63896696\n",
            "Iteration 93, loss = 0.63858348\n",
            "Iteration 94, loss = 0.63819669\n",
            "Iteration 95, loss = 0.63785828\n",
            "Iteration 96, loss = 0.63751909\n",
            "Iteration 97, loss = 0.63702808\n",
            "Iteration 98, loss = 0.63664209\n",
            "Iteration 99, loss = 0.63642776\n",
            "Iteration 100, loss = 0.63583250\n",
            "Iteration 101, loss = 0.63555331\n",
            "Iteration 102, loss = 0.63511533\n",
            "Iteration 103, loss = 0.63468075\n",
            "Iteration 104, loss = 0.63432640\n",
            "Iteration 105, loss = 0.63396772\n",
            "Iteration 106, loss = 0.63363553\n",
            "Iteration 107, loss = 0.63333790\n",
            "Iteration 108, loss = 0.63279096\n",
            "Iteration 109, loss = 0.63246075\n",
            "Iteration 110, loss = 0.63209020\n",
            "Iteration 111, loss = 0.63173357\n",
            "Iteration 112, loss = 0.63131253\n",
            "Iteration 113, loss = 0.63101177\n",
            "Iteration 114, loss = 0.63060920\n",
            "Iteration 115, loss = 0.63027835\n",
            "Iteration 116, loss = 0.63006841\n",
            "Iteration 117, loss = 0.62954123\n",
            "Iteration 118, loss = 0.62922165\n",
            "Iteration 119, loss = 0.62886782\n",
            "Iteration 120, loss = 0.62849449\n",
            "Iteration 121, loss = 0.62826927\n",
            "Iteration 122, loss = 0.62785128\n",
            "Iteration 123, loss = 0.62748278\n",
            "Iteration 124, loss = 0.62713120\n",
            "Iteration 125, loss = 0.62681995\n",
            "Iteration 126, loss = 0.62647177\n",
            "Iteration 127, loss = 0.62615524\n",
            "Iteration 128, loss = 0.62597185\n",
            "Iteration 129, loss = 0.62572648\n",
            "Iteration 130, loss = 0.62520894\n",
            "Iteration 131, loss = 0.62496199\n",
            "Iteration 132, loss = 0.62461625\n",
            "Iteration 133, loss = 0.62421912\n",
            "Iteration 134, loss = 0.62406622\n",
            "Iteration 135, loss = 0.62366730\n",
            "Iteration 136, loss = 0.62339626\n",
            "Iteration 137, loss = 0.62301118\n",
            "Iteration 138, loss = 0.62278003\n",
            "Iteration 139, loss = 0.62244885\n",
            "Iteration 140, loss = 0.62218197\n",
            "Iteration 141, loss = 0.62188178\n",
            "Iteration 142, loss = 0.62158813\n",
            "Iteration 143, loss = 0.62131004\n",
            "Iteration 144, loss = 0.62112737\n",
            "Iteration 145, loss = 0.62080741\n",
            "Iteration 146, loss = 0.62057456\n",
            "Iteration 147, loss = 0.62028687\n",
            "Iteration 148, loss = 0.62004941\n",
            "Iteration 149, loss = 0.61975679\n",
            "Iteration 150, loss = 0.61954907\n",
            "Iteration 151, loss = 0.61921298\n",
            "Iteration 152, loss = 0.61909149\n",
            "Iteration 153, loss = 0.61873365\n",
            "Iteration 154, loss = 0.61854269\n",
            "Iteration 155, loss = 0.61834063\n",
            "Iteration 156, loss = 0.61800684\n",
            "Iteration 157, loss = 0.61780241\n",
            "Iteration 158, loss = 0.61803495\n",
            "Iteration 159, loss = 0.61750893\n",
            "Iteration 160, loss = 0.61713263\n",
            "Iteration 161, loss = 0.61691515\n",
            "Iteration 162, loss = 0.61671772\n",
            "Iteration 163, loss = 0.61644996\n",
            "Iteration 164, loss = 0.61619783\n",
            "Iteration 165, loss = 0.61594917\n",
            "Iteration 166, loss = 0.61581778\n",
            "Iteration 167, loss = 0.61571329\n",
            "Iteration 168, loss = 0.61539363\n",
            "Iteration 169, loss = 0.61520243\n",
            "Iteration 170, loss = 0.61507825\n",
            "Iteration 171, loss = 0.61483441\n",
            "Iteration 172, loss = 0.61456351\n",
            "Iteration 173, loss = 0.61440138\n",
            "Iteration 174, loss = 0.61424058\n",
            "Iteration 175, loss = 0.61399635\n",
            "Iteration 176, loss = 0.61377471\n",
            "Iteration 177, loss = 0.61382402\n",
            "Iteration 178, loss = 0.61353101\n",
            "Iteration 179, loss = 0.61333325\n",
            "Iteration 180, loss = 0.61347955\n",
            "Iteration 181, loss = 0.61293143\n",
            "Iteration 182, loss = 0.61311648\n",
            "Iteration 183, loss = 0.61264762\n",
            "Iteration 184, loss = 0.61287485\n",
            "Iteration 185, loss = 0.61225707\n",
            "Iteration 186, loss = 0.61215657\n",
            "Iteration 187, loss = 0.61192377\n",
            "Iteration 188, loss = 0.61178500\n",
            "Iteration 189, loss = 0.61175006\n",
            "Iteration 190, loss = 0.61154920\n",
            "Iteration 191, loss = 0.61148224\n",
            "Iteration 192, loss = 0.61115953\n",
            "Iteration 193, loss = 0.61103183\n",
            "Iteration 194, loss = 0.61095971\n",
            "Iteration 195, loss = 0.61076383\n",
            "Iteration 196, loss = 0.61059689\n",
            "Iteration 197, loss = 0.61046623\n",
            "Iteration 198, loss = 0.61034128\n",
            "Iteration 199, loss = 0.61033064\n",
            "Iteration 200, loss = 0.61012701\n",
            "Iteration 1, loss = 0.67894648\n",
            "Iteration 2, loss = 0.67837429\n",
            "Iteration 3, loss = 0.67737120\n",
            "Iteration 4, loss = 0.67656697\n",
            "Iteration 5, loss = 0.67573473\n",
            "Iteration 6, loss = 0.67511751\n",
            "Iteration 7, loss = 0.67458690\n",
            "Iteration 8, loss = 0.67368605\n",
            "Iteration 9, loss = 0.67302646\n",
            "Iteration 10, loss = 0.67237698\n",
            "Iteration 11, loss = 0.67183574\n",
            "Iteration 12, loss = 0.67125463\n",
            "Iteration 13, loss = 0.67047355\n",
            "Iteration 14, loss = 0.66973981\n",
            "Iteration 15, loss = 0.66921020\n",
            "Iteration 16, loss = 0.66861966\n",
            "Iteration 17, loss = 0.66794926\n",
            "Iteration 18, loss = 0.66732200\n",
            "Iteration 19, loss = 0.66665230\n",
            "Iteration 20, loss = 0.66597856\n",
            "Iteration 21, loss = 0.66555467\n",
            "Iteration 22, loss = 0.66480737\n",
            "Iteration 23, loss = 0.66411840\n",
            "Iteration 24, loss = 0.66354365\n",
            "Iteration 25, loss = 0.66300971\n",
            "Iteration 26, loss = 0.66230547\n",
            "Iteration 27, loss = 0.66174139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 28, loss = 0.66112099\n",
            "Iteration 29, loss = 0.66052093\n",
            "Iteration 30, loss = 0.65995233\n",
            "Iteration 31, loss = 0.65935508\n",
            "Iteration 32, loss = 0.65881626\n",
            "Iteration 33, loss = 0.65818515\n",
            "Iteration 34, loss = 0.65776055\n",
            "Iteration 35, loss = 0.65728797\n",
            "Iteration 36, loss = 0.65645393\n",
            "Iteration 37, loss = 0.65593317\n",
            "Iteration 38, loss = 0.65557474\n",
            "Iteration 39, loss = 0.65486158\n",
            "Iteration 40, loss = 0.65424989\n",
            "Iteration 41, loss = 0.65380439\n",
            "Iteration 42, loss = 0.65307218\n",
            "Iteration 43, loss = 0.65251797\n",
            "Iteration 44, loss = 0.65200385\n",
            "Iteration 45, loss = 0.65138266\n",
            "Iteration 46, loss = 0.65091787\n",
            "Iteration 47, loss = 0.65050224\n",
            "Iteration 48, loss = 0.64999240\n",
            "Iteration 49, loss = 0.64938274\n",
            "Iteration 50, loss = 0.64877810\n",
            "Iteration 51, loss = 0.64827739\n",
            "Iteration 52, loss = 0.64777557\n",
            "Iteration 53, loss = 0.64726091\n",
            "Iteration 54, loss = 0.64678604\n",
            "Iteration 55, loss = 0.64640230\n",
            "Iteration 56, loss = 0.64582917\n",
            "Iteration 57, loss = 0.64525450\n",
            "Iteration 58, loss = 0.64481955\n",
            "Iteration 59, loss = 0.64431920\n",
            "Iteration 60, loss = 0.64387978\n",
            "Iteration 61, loss = 0.64340413\n",
            "Iteration 62, loss = 0.64289685\n",
            "Iteration 63, loss = 0.64247287\n",
            "Iteration 64, loss = 0.64191923\n",
            "Iteration 65, loss = 0.64147003\n",
            "Iteration 66, loss = 0.64101934\n",
            "Iteration 67, loss = 0.64050152\n",
            "Iteration 68, loss = 0.64013779\n",
            "Iteration 69, loss = 0.63962244\n",
            "Iteration 70, loss = 0.63917045\n",
            "Iteration 71, loss = 0.63871036\n",
            "Iteration 72, loss = 0.63832524\n",
            "Iteration 73, loss = 0.63787062\n",
            "Iteration 74, loss = 0.63738382\n",
            "Iteration 75, loss = 0.63689092\n",
            "Iteration 76, loss = 0.63653806\n",
            "Iteration 77, loss = 0.63604000\n",
            "Iteration 78, loss = 0.63561572\n",
            "Iteration 79, loss = 0.63515139\n",
            "Iteration 80, loss = 0.63470460\n",
            "Iteration 81, loss = 0.63432852\n",
            "Iteration 82, loss = 0.63401606\n",
            "Iteration 83, loss = 0.63347280\n",
            "Iteration 84, loss = 0.63305155\n",
            "Iteration 85, loss = 0.63262087\n",
            "Iteration 86, loss = 0.63216750\n",
            "Iteration 87, loss = 0.63179522\n",
            "Iteration 88, loss = 0.63141047\n",
            "Iteration 89, loss = 0.63114994\n",
            "Iteration 90, loss = 0.63069830\n",
            "Iteration 91, loss = 0.63026643\n",
            "Iteration 92, loss = 0.62983820\n",
            "Iteration 93, loss = 0.62950537\n",
            "Iteration 94, loss = 0.62913528\n",
            "Iteration 95, loss = 0.62871779\n",
            "Iteration 96, loss = 0.62833721\n",
            "Iteration 97, loss = 0.62816314\n",
            "Iteration 98, loss = 0.62760529\n",
            "Iteration 99, loss = 0.62725588\n",
            "Iteration 100, loss = 0.62692442\n",
            "Iteration 101, loss = 0.62658731\n",
            "Iteration 102, loss = 0.62627382\n",
            "Iteration 103, loss = 0.62589402\n",
            "Iteration 104, loss = 0.62553309\n",
            "Iteration 105, loss = 0.62517958\n",
            "Iteration 106, loss = 0.62485947\n",
            "Iteration 107, loss = 0.62453814\n",
            "Iteration 108, loss = 0.62426503\n",
            "Iteration 109, loss = 0.62388977\n",
            "Iteration 110, loss = 0.62351777\n",
            "Iteration 111, loss = 0.62319218\n",
            "Iteration 112, loss = 0.62288756\n",
            "Iteration 113, loss = 0.62257703\n",
            "Iteration 114, loss = 0.62238849\n",
            "Iteration 115, loss = 0.62200492\n",
            "Iteration 116, loss = 0.62174043\n",
            "Iteration 117, loss = 0.62148467\n",
            "Iteration 118, loss = 0.62119596\n",
            "Iteration 119, loss = 0.62085065\n",
            "Iteration 120, loss = 0.62069648\n",
            "Iteration 121, loss = 0.62029884\n",
            "Iteration 122, loss = 0.61994327\n",
            "Iteration 123, loss = 0.61974897\n",
            "Iteration 124, loss = 0.61950095\n",
            "Iteration 125, loss = 0.61917626\n",
            "Iteration 126, loss = 0.61895629\n",
            "Iteration 127, loss = 0.61867002\n",
            "Iteration 128, loss = 0.61844899\n",
            "Iteration 129, loss = 0.61843926\n",
            "Iteration 130, loss = 0.61806512\n",
            "Iteration 131, loss = 0.61781097\n",
            "Iteration 132, loss = 0.61742847\n",
            "Iteration 133, loss = 0.61726609\n",
            "Iteration 134, loss = 0.61700971\n",
            "Iteration 135, loss = 0.61688353\n",
            "Iteration 136, loss = 0.61661701\n",
            "Iteration 137, loss = 0.61657651\n",
            "Iteration 138, loss = 0.61614730\n",
            "Iteration 139, loss = 0.61593167\n",
            "Iteration 140, loss = 0.61575555\n",
            "Iteration 141, loss = 0.61556603\n",
            "Iteration 142, loss = 0.61540300\n",
            "Iteration 143, loss = 0.61522225\n",
            "Iteration 144, loss = 0.61502101\n",
            "Iteration 145, loss = 0.61480864\n",
            "Iteration 146, loss = 0.61456481\n",
            "Iteration 147, loss = 0.61435325\n",
            "Iteration 148, loss = 0.61422961\n",
            "Iteration 149, loss = 0.61397097\n",
            "Iteration 150, loss = 0.61382898\n",
            "Iteration 151, loss = 0.61362403\n",
            "Iteration 152, loss = 0.61354449\n",
            "Iteration 153, loss = 0.61334817\n",
            "Iteration 154, loss = 0.61312346\n",
            "Iteration 155, loss = 0.61301500\n",
            "Iteration 156, loss = 0.61283887\n",
            "Iteration 157, loss = 0.61258962\n",
            "Iteration 158, loss = 0.61245797\n",
            "Iteration 159, loss = 0.61226074\n",
            "Iteration 160, loss = 0.61212145\n",
            "Iteration 161, loss = 0.61200174\n",
            "Iteration 162, loss = 0.61187767\n",
            "Iteration 163, loss = 0.61166778\n",
            "Iteration 164, loss = 0.61153475\n",
            "Iteration 165, loss = 0.61142747\n",
            "Iteration 166, loss = 0.61124554\n",
            "Iteration 167, loss = 0.61111604\n",
            "Iteration 168, loss = 0.61092691\n",
            "Iteration 169, loss = 0.61081090\n",
            "Iteration 170, loss = 0.61069771\n",
            "Iteration 171, loss = 0.61061483\n",
            "Iteration 172, loss = 0.61039093\n",
            "Iteration 173, loss = 0.61029028\n",
            "Iteration 174, loss = 0.61017138\n",
            "Iteration 175, loss = 0.61005843\n",
            "Iteration 176, loss = 0.60995164\n",
            "Iteration 177, loss = 0.61002978\n",
            "Iteration 178, loss = 0.60971243\n",
            "Iteration 179, loss = 0.60957153\n",
            "Iteration 180, loss = 0.60946937\n",
            "Iteration 181, loss = 0.60930779\n",
            "Iteration 182, loss = 0.60925527\n",
            "Iteration 183, loss = 0.60907253\n",
            "Iteration 184, loss = 0.60912580\n",
            "Iteration 185, loss = 0.60881538\n",
            "Iteration 186, loss = 0.60874639\n",
            "Iteration 187, loss = 0.60863988\n",
            "Iteration 188, loss = 0.60850583\n",
            "Iteration 189, loss = 0.60846804\n",
            "Iteration 190, loss = 0.60833049\n",
            "Iteration 191, loss = 0.60824299\n",
            "Iteration 192, loss = 0.60812592\n",
            "Iteration 193, loss = 0.60817031\n",
            "Iteration 194, loss = 0.60796851\n",
            "Iteration 195, loss = 0.60785614\n",
            "Iteration 196, loss = 0.60774029\n",
            "Iteration 197, loss = 0.60800110\n",
            "Iteration 198, loss = 0.60756433\n",
            "Iteration 199, loss = 0.60757732\n",
            "Iteration 200, loss = 0.60740538\n",
            "Iteration 1, loss = 0.68339417\n",
            "Iteration 2, loss = 0.68245536\n",
            "Iteration 3, loss = 0.68097848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4, loss = 0.68002026\n",
            "Iteration 5, loss = 0.67957169\n",
            "Iteration 6, loss = 0.67891700\n",
            "Iteration 7, loss = 0.67849927\n",
            "Iteration 8, loss = 0.67811883\n",
            "Iteration 9, loss = 0.67775919\n",
            "Iteration 10, loss = 0.67750160\n",
            "Iteration 11, loss = 0.67702989\n",
            "Iteration 12, loss = 0.67680757\n",
            "Iteration 13, loss = 0.67628395\n",
            "Iteration 14, loss = 0.67587787\n",
            "Iteration 15, loss = 0.67550606\n",
            "Iteration 16, loss = 0.67534049\n",
            "Iteration 17, loss = 0.67492661\n",
            "Iteration 18, loss = 0.67446388\n",
            "Iteration 19, loss = 0.67418581\n",
            "Iteration 20, loss = 0.67378899\n",
            "Iteration 21, loss = 0.67350121\n",
            "Iteration 22, loss = 0.67310991\n",
            "Iteration 23, loss = 0.67288056\n",
            "Iteration 24, loss = 0.67241674\n",
            "Iteration 25, loss = 0.67205213\n",
            "Iteration 26, loss = 0.67169026\n",
            "Iteration 27, loss = 0.67146104\n",
            "Iteration 28, loss = 0.67113165\n",
            "Iteration 29, loss = 0.67079511\n",
            "Iteration 30, loss = 0.67061391\n",
            "Iteration 31, loss = 0.67020778\n",
            "Iteration 32, loss = 0.66975771\n",
            "Iteration 33, loss = 0.66947685\n",
            "Iteration 34, loss = 0.66906963\n",
            "Iteration 35, loss = 0.66878572\n",
            "Iteration 36, loss = 0.66846323\n",
            "Iteration 37, loss = 0.66813187\n",
            "Iteration 38, loss = 0.66774570\n",
            "Iteration 39, loss = 0.66739843\n",
            "Iteration 40, loss = 0.66714479\n",
            "Iteration 41, loss = 0.66675489\n",
            "Iteration 42, loss = 0.66650783\n",
            "Iteration 43, loss = 0.66622915\n",
            "Iteration 44, loss = 0.66582722\n",
            "Iteration 45, loss = 0.66550124\n",
            "Iteration 46, loss = 0.66517951\n",
            "Iteration 47, loss = 0.66502772\n",
            "Iteration 48, loss = 0.66450164\n",
            "Iteration 49, loss = 0.66417175\n",
            "Iteration 50, loss = 0.66394858\n",
            "Iteration 51, loss = 0.66358238\n",
            "Iteration 52, loss = 0.66330816\n",
            "Iteration 53, loss = 0.66295256\n",
            "Iteration 54, loss = 0.66262521\n",
            "Iteration 55, loss = 0.66231373\n",
            "Iteration 56, loss = 0.66230897\n",
            "Iteration 57, loss = 0.66167012\n",
            "Iteration 58, loss = 0.66137189\n",
            "Iteration 59, loss = 0.66095279\n",
            "Iteration 60, loss = 0.66062159\n",
            "Iteration 61, loss = 0.66041918\n",
            "Iteration 62, loss = 0.65995037\n",
            "Iteration 63, loss = 0.65973571\n",
            "Iteration 64, loss = 0.65941074\n",
            "Iteration 65, loss = 0.65905355\n",
            "Iteration 66, loss = 0.65865709\n",
            "Iteration 67, loss = 0.65834401\n",
            "Iteration 68, loss = 0.65802721\n",
            "Iteration 69, loss = 0.65770477\n",
            "Iteration 70, loss = 0.65738530\n",
            "Iteration 71, loss = 0.65702787\n",
            "Iteration 72, loss = 0.65678297\n",
            "Iteration 73, loss = 0.65642249\n",
            "Iteration 74, loss = 0.65610887\n",
            "Iteration 75, loss = 0.65578668\n",
            "Iteration 76, loss = 0.65544882\n",
            "Iteration 77, loss = 0.65524294\n",
            "Iteration 78, loss = 0.65499450\n",
            "Iteration 79, loss = 0.65450541\n",
            "Iteration 80, loss = 0.65412901\n",
            "Iteration 81, loss = 0.65381736\n",
            "Iteration 82, loss = 0.65345686\n",
            "Iteration 83, loss = 0.65312284\n",
            "Iteration 84, loss = 0.65277266\n",
            "Iteration 85, loss = 0.65248025\n",
            "Iteration 86, loss = 0.65215358\n",
            "Iteration 87, loss = 0.65176500\n",
            "Iteration 88, loss = 0.65156487\n",
            "Iteration 89, loss = 0.65118392\n",
            "Iteration 90, loss = 0.65086184\n",
            "Iteration 91, loss = 0.65048915\n",
            "Iteration 92, loss = 0.65015090\n",
            "Iteration 93, loss = 0.64985082\n",
            "Iteration 94, loss = 0.64948914\n",
            "Iteration 95, loss = 0.64924099\n",
            "Iteration 96, loss = 0.64883015\n",
            "Iteration 97, loss = 0.64852048\n",
            "Iteration 98, loss = 0.64816978\n",
            "Iteration 99, loss = 0.64788451\n",
            "Iteration 100, loss = 0.64749790\n",
            "Iteration 101, loss = 0.64723995\n",
            "Iteration 102, loss = 0.64690352\n",
            "Iteration 103, loss = 0.64651683\n",
            "Iteration 104, loss = 0.64643634\n",
            "Iteration 105, loss = 0.64594727\n",
            "Iteration 106, loss = 0.64566253\n",
            "Iteration 107, loss = 0.64521473\n",
            "Iteration 108, loss = 0.64495266\n",
            "Iteration 109, loss = 0.64456962\n",
            "Iteration 110, loss = 0.64424756\n",
            "Iteration 111, loss = 0.64385729\n",
            "Iteration 112, loss = 0.64364483\n",
            "Iteration 113, loss = 0.64323805\n",
            "Iteration 114, loss = 0.64286006\n",
            "Iteration 115, loss = 0.64250327\n",
            "Iteration 116, loss = 0.64224583\n",
            "Iteration 117, loss = 0.64186919\n",
            "Iteration 118, loss = 0.64154678\n",
            "Iteration 119, loss = 0.64129751\n",
            "Iteration 120, loss = 0.64080498\n",
            "Iteration 121, loss = 0.64052729\n",
            "Iteration 122, loss = 0.64023169\n",
            "Iteration 123, loss = 0.63992009\n",
            "Iteration 124, loss = 0.63953864\n",
            "Iteration 125, loss = 0.63926756\n",
            "Iteration 126, loss = 0.63888806\n",
            "Iteration 127, loss = 0.63864717\n",
            "Iteration 128, loss = 0.63812280\n",
            "Iteration 129, loss = 0.63789515\n",
            "Iteration 130, loss = 0.63750103\n",
            "Iteration 131, loss = 0.63709320\n",
            "Iteration 132, loss = 0.63681461\n",
            "Iteration 133, loss = 0.63642949\n",
            "Iteration 134, loss = 0.63615162\n",
            "Iteration 135, loss = 0.63583111\n",
            "Iteration 136, loss = 0.63541351\n",
            "Iteration 137, loss = 0.63509549\n",
            "Iteration 138, loss = 0.63495286\n",
            "Iteration 139, loss = 0.63445331\n",
            "Iteration 140, loss = 0.63404362\n",
            "Iteration 141, loss = 0.63369027\n",
            "Iteration 142, loss = 0.63340041\n",
            "Iteration 143, loss = 0.63313511\n",
            "Iteration 144, loss = 0.63267352\n",
            "Iteration 145, loss = 0.63252710\n",
            "Iteration 146, loss = 0.63212458\n",
            "Iteration 147, loss = 0.63178890\n",
            "Iteration 148, loss = 0.63145853\n",
            "Iteration 149, loss = 0.63117670\n",
            "Iteration 150, loss = 0.63080755\n",
            "Iteration 151, loss = 0.63054637\n",
            "Iteration 152, loss = 0.63016507\n",
            "Iteration 153, loss = 0.62993969\n",
            "Iteration 154, loss = 0.62952238\n",
            "Iteration 155, loss = 0.62919624\n",
            "Iteration 156, loss = 0.62890366\n",
            "Iteration 157, loss = 0.62861313\n",
            "Iteration 158, loss = 0.62827200\n",
            "Iteration 159, loss = 0.62801674\n",
            "Iteration 160, loss = 0.62765679\n",
            "Iteration 161, loss = 0.62743054\n",
            "Iteration 162, loss = 0.62705927\n",
            "Iteration 163, loss = 0.62672302\n",
            "Iteration 164, loss = 0.62650330\n",
            "Iteration 165, loss = 0.62612340\n",
            "Iteration 166, loss = 0.62590150\n",
            "Iteration 167, loss = 0.62555826\n",
            "Iteration 168, loss = 0.62526447\n",
            "Iteration 169, loss = 0.62486457\n",
            "Iteration 170, loss = 0.62466732\n",
            "Iteration 171, loss = 0.62431883\n",
            "Iteration 172, loss = 0.62412750\n",
            "Iteration 173, loss = 0.62380078\n",
            "Iteration 174, loss = 0.62347402\n",
            "Iteration 175, loss = 0.62311432\n",
            "Iteration 176, loss = 0.62296074\n",
            "Iteration 177, loss = 0.62256750\n",
            "Iteration 178, loss = 0.62231289\n",
            "Iteration 179, loss = 0.62202616\n",
            "Iteration 180, loss = 0.62187871\n",
            "Iteration 181, loss = 0.62146788\n",
            "Iteration 182, loss = 0.62129050\n",
            "Iteration 183, loss = 0.62102414\n",
            "Iteration 184, loss = 0.62073425\n",
            "Iteration 185, loss = 0.62038748\n",
            "Iteration 186, loss = 0.62028915\n",
            "Iteration 187, loss = 0.61997553\n",
            "Iteration 188, loss = 0.61962766\n",
            "Iteration 189, loss = 0.61975123\n",
            "Iteration 190, loss = 0.61916066\n",
            "Iteration 191, loss = 0.61899363\n",
            "Iteration 192, loss = 0.61880868\n",
            "Iteration 193, loss = 0.61856687\n",
            "Iteration 194, loss = 0.61822540\n",
            "Iteration 195, loss = 0.61798914\n",
            "Iteration 196, loss = 0.61772882\n",
            "Iteration 197, loss = 0.61762547\n",
            "Iteration 198, loss = 0.61749473\n",
            "Iteration 199, loss = 0.61715082\n",
            "Iteration 200, loss = 0.61690179\n",
            "Iteration 1, loss = 0.72439315\n",
            "Iteration 2, loss = 0.70856134\n",
            "Iteration 3, loss = 0.69280218\n",
            "Iteration 4, loss = 0.68363339\n",
            "Iteration 5, loss = 0.67924788\n",
            "Iteration 6, loss = 0.67622137\n",
            "Iteration 7, loss = 0.67459030\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8, loss = 0.67380963\n",
            "Iteration 9, loss = 0.67325597\n",
            "Iteration 10, loss = 0.67279303\n",
            "Iteration 11, loss = 0.67226856\n",
            "Iteration 12, loss = 0.67177410\n",
            "Iteration 13, loss = 0.67132677\n",
            "Iteration 14, loss = 0.67127320\n",
            "Iteration 15, loss = 0.67077087\n",
            "Iteration 16, loss = 0.67028026\n",
            "Iteration 17, loss = 0.66989947\n",
            "Iteration 18, loss = 0.66951914\n",
            "Iteration 19, loss = 0.66929850\n",
            "Iteration 20, loss = 0.66880758\n",
            "Iteration 21, loss = 0.66851054\n",
            "Iteration 22, loss = 0.66819407\n",
            "Iteration 23, loss = 0.66824992\n",
            "Iteration 24, loss = 0.66753693\n",
            "Iteration 25, loss = 0.66735464\n",
            "Iteration 26, loss = 0.66703331\n",
            "Iteration 27, loss = 0.66661966\n",
            "Iteration 28, loss = 0.66629695\n",
            "Iteration 29, loss = 0.66601211\n",
            "Iteration 30, loss = 0.66568277\n",
            "Iteration 31, loss = 0.66537588\n",
            "Iteration 32, loss = 0.66518467\n",
            "Iteration 33, loss = 0.66494110\n",
            "Iteration 34, loss = 0.66469666\n",
            "Iteration 35, loss = 0.66431594\n",
            "Iteration 36, loss = 0.66395854\n",
            "Iteration 37, loss = 0.66358988\n",
            "Iteration 38, loss = 0.66347587\n",
            "Iteration 39, loss = 0.66292252\n",
            "Iteration 40, loss = 0.66252556\n",
            "Iteration 41, loss = 0.66234389\n",
            "Iteration 42, loss = 0.66212023\n",
            "Iteration 43, loss = 0.66187000\n",
            "Iteration 44, loss = 0.66141187\n",
            "Iteration 45, loss = 0.66109211\n",
            "Iteration 46, loss = 0.66084240\n",
            "Iteration 47, loss = 0.66074520\n",
            "Iteration 48, loss = 0.66016153\n",
            "Iteration 49, loss = 0.65990907\n",
            "Iteration 50, loss = 0.65958439\n",
            "Iteration 51, loss = 0.65922189\n",
            "Iteration 52, loss = 0.65901001\n",
            "Iteration 53, loss = 0.65861993\n",
            "Iteration 54, loss = 0.65832292\n",
            "Iteration 55, loss = 0.65808365\n",
            "Iteration 56, loss = 0.65771055\n",
            "Iteration 57, loss = 0.65742014\n",
            "Iteration 58, loss = 0.65717141\n",
            "Iteration 59, loss = 0.65678446\n",
            "Iteration 60, loss = 0.65661612\n",
            "Iteration 61, loss = 0.65621743\n",
            "Iteration 62, loss = 0.65601543\n",
            "Iteration 63, loss = 0.65566319\n",
            "Iteration 64, loss = 0.65529594\n",
            "Iteration 65, loss = 0.65496341\n",
            "Iteration 66, loss = 0.65469516\n",
            "Iteration 67, loss = 0.65436806\n",
            "Iteration 68, loss = 0.65403911\n",
            "Iteration 69, loss = 0.65381561\n",
            "Iteration 70, loss = 0.65360059\n",
            "Iteration 71, loss = 0.65316477\n",
            "Iteration 72, loss = 0.65293290\n",
            "Iteration 73, loss = 0.65263028\n",
            "Iteration 74, loss = 0.65231260\n",
            "Iteration 75, loss = 0.65200404\n",
            "Iteration 76, loss = 0.65167949\n",
            "Iteration 77, loss = 0.65143945\n",
            "Iteration 78, loss = 0.65103886\n",
            "Iteration 79, loss = 0.65076935\n",
            "Iteration 80, loss = 0.65058355\n",
            "Iteration 81, loss = 0.65029063\n",
            "Iteration 82, loss = 0.65000221\n",
            "Iteration 83, loss = 0.64958306\n",
            "Iteration 84, loss = 0.64939490\n",
            "Iteration 85, loss = 0.64897700\n",
            "Iteration 86, loss = 0.64874179\n",
            "Iteration 87, loss = 0.64841997\n",
            "Iteration 88, loss = 0.64811528\n",
            "Iteration 89, loss = 0.64782313\n",
            "Iteration 90, loss = 0.64753827\n",
            "Iteration 91, loss = 0.64721219\n",
            "Iteration 92, loss = 0.64695727\n",
            "Iteration 93, loss = 0.64670284\n",
            "Iteration 94, loss = 0.64664537\n",
            "Iteration 95, loss = 0.64609673\n",
            "Iteration 96, loss = 0.64585839\n",
            "Iteration 97, loss = 0.64565290\n",
            "Iteration 98, loss = 0.64530761\n",
            "Iteration 99, loss = 0.64512105\n",
            "Iteration 100, loss = 0.64463984\n",
            "Iteration 101, loss = 0.64465652\n",
            "Iteration 102, loss = 0.64420263\n",
            "Iteration 103, loss = 0.64400395\n",
            "Iteration 104, loss = 0.64366605\n",
            "Iteration 105, loss = 0.64341307\n",
            "Iteration 106, loss = 0.64309151\n",
            "Iteration 107, loss = 0.64281021\n",
            "Iteration 108, loss = 0.64255642\n",
            "Iteration 109, loss = 0.64228587\n",
            "Iteration 110, loss = 0.64199236\n",
            "Iteration 111, loss = 0.64175873\n",
            "Iteration 112, loss = 0.64155063\n",
            "Iteration 113, loss = 0.64122649\n",
            "Iteration 114, loss = 0.64106175\n",
            "Iteration 115, loss = 0.64065213\n",
            "Iteration 116, loss = 0.64043410\n",
            "Iteration 117, loss = 0.64010751\n",
            "Iteration 118, loss = 0.63994498\n",
            "Iteration 119, loss = 0.63968568\n",
            "Iteration 120, loss = 0.63948914\n",
            "Iteration 121, loss = 0.63910878\n",
            "Iteration 122, loss = 0.63890085\n",
            "Iteration 123, loss = 0.63869006\n",
            "Iteration 124, loss = 0.63845465\n",
            "Iteration 125, loss = 0.63815842\n",
            "Iteration 126, loss = 0.63780969\n",
            "Iteration 127, loss = 0.63775681\n",
            "Iteration 128, loss = 0.63736754\n",
            "Iteration 129, loss = 0.63712270\n",
            "Iteration 130, loss = 0.63684807\n",
            "Iteration 131, loss = 0.63659467\n",
            "Iteration 132, loss = 0.63640342\n",
            "Iteration 133, loss = 0.63616920\n",
            "Iteration 134, loss = 0.63587556\n",
            "Iteration 135, loss = 0.63573935\n",
            "Iteration 136, loss = 0.63541537\n",
            "Iteration 137, loss = 0.63521895\n",
            "Iteration 138, loss = 0.63499725\n",
            "Iteration 139, loss = 0.63477447\n",
            "Iteration 140, loss = 0.63460708\n",
            "Iteration 141, loss = 0.63431740\n",
            "Iteration 142, loss = 0.63415312\n",
            "Iteration 143, loss = 0.63390215\n",
            "Iteration 144, loss = 0.63368149\n",
            "Iteration 145, loss = 0.63356004\n",
            "Iteration 146, loss = 0.63326147\n",
            "Iteration 147, loss = 0.63307142\n",
            "Iteration 148, loss = 0.63293909\n",
            "Iteration 149, loss = 0.63271337\n",
            "Iteration 150, loss = 0.63239059\n",
            "Iteration 151, loss = 0.63222607\n",
            "Iteration 152, loss = 0.63204162\n",
            "Iteration 153, loss = 0.63178469\n",
            "Iteration 154, loss = 0.63176814\n",
            "Iteration 155, loss = 0.63142980\n",
            "Iteration 156, loss = 0.63122874\n",
            "Iteration 157, loss = 0.63102234\n",
            "Iteration 158, loss = 0.63087167\n",
            "Iteration 159, loss = 0.63065954\n",
            "Iteration 160, loss = 0.63050432\n",
            "Iteration 161, loss = 0.63032581\n",
            "Iteration 162, loss = 0.63019036\n",
            "Iteration 163, loss = 0.62998213\n",
            "Iteration 164, loss = 0.62981809\n",
            "Iteration 165, loss = 0.62976171\n",
            "Iteration 166, loss = 0.62948429\n",
            "Iteration 167, loss = 0.62939241\n",
            "Iteration 168, loss = 0.62926091\n",
            "Iteration 169, loss = 0.62904984\n",
            "Iteration 170, loss = 0.62891295\n",
            "Iteration 171, loss = 0.62865940\n",
            "Iteration 172, loss = 0.62856764\n",
            "Iteration 173, loss = 0.62835833\n",
            "Iteration 174, loss = 0.62829853\n",
            "Iteration 175, loss = 0.62807122\n",
            "Iteration 176, loss = 0.62802102\n",
            "Iteration 177, loss = 0.62777652\n",
            "Iteration 178, loss = 0.62767865\n",
            "Iteration 179, loss = 0.62750571\n",
            "Iteration 180, loss = 0.62764769\n",
            "Iteration 181, loss = 0.62722901\n",
            "Iteration 182, loss = 0.62711477\n",
            "Iteration 183, loss = 0.62699174\n",
            "Iteration 184, loss = 0.62682918\n",
            "Iteration 185, loss = 0.62673594\n",
            "Iteration 186, loss = 0.62678102\n",
            "Iteration 187, loss = 0.62640834\n",
            "Iteration 188, loss = 0.62628350\n",
            "Iteration 189, loss = 0.62616808\n",
            "Iteration 190, loss = 0.62617314\n",
            "Iteration 191, loss = 0.62591748\n",
            "Iteration 192, loss = 0.62590189\n",
            "Iteration 193, loss = 0.62573574\n",
            "Iteration 194, loss = 0.62554286\n",
            "Iteration 195, loss = 0.62542696\n",
            "Iteration 196, loss = 0.62543976\n",
            "Iteration 197, loss = 0.62521728\n",
            "Iteration 198, loss = 0.62539316\n",
            "Iteration 199, loss = 0.62497700\n",
            "Iteration 200, loss = 0.62498101\n",
            "Iteration 1, loss = 0.71337106\n",
            "Iteration 2, loss = 0.70075482\n",
            "Iteration 3, loss = 0.68745093\n",
            "Iteration 4, loss = 0.68077470\n",
            "Iteration 5, loss = 0.67649679\n",
            "Iteration 6, loss = 0.67495452\n",
            "Iteration 7, loss = 0.67325420\n",
            "Iteration 8, loss = 0.67262637\n",
            "Iteration 9, loss = 0.67205280\n",
            "Iteration 10, loss = 0.67141544\n",
            "Iteration 11, loss = 0.67144364\n",
            "Iteration 12, loss = 0.67056088\n",
            "Iteration 13, loss = 0.67003084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 14, loss = 0.66956209\n",
            "Iteration 15, loss = 0.66897311\n",
            "Iteration 16, loss = 0.66831524\n",
            "Iteration 17, loss = 0.66775827\n",
            "Iteration 18, loss = 0.66714466\n",
            "Iteration 19, loss = 0.66681680\n",
            "Iteration 20, loss = 0.66618098\n",
            "Iteration 21, loss = 0.66568723\n",
            "Iteration 22, loss = 0.66520690\n",
            "Iteration 23, loss = 0.66468001\n",
            "Iteration 24, loss = 0.66419625\n",
            "Iteration 25, loss = 0.66375192\n",
            "Iteration 26, loss = 0.66332792\n",
            "Iteration 27, loss = 0.66293703\n",
            "Iteration 28, loss = 0.66245486\n",
            "Iteration 29, loss = 0.66204841\n",
            "Iteration 30, loss = 0.66157033\n",
            "Iteration 31, loss = 0.66114167\n",
            "Iteration 32, loss = 0.66073803\n",
            "Iteration 33, loss = 0.66033933\n",
            "Iteration 34, loss = 0.66005974\n",
            "Iteration 35, loss = 0.65960513\n",
            "Iteration 36, loss = 0.65913267\n",
            "Iteration 37, loss = 0.65873157\n",
            "Iteration 38, loss = 0.65835665\n",
            "Iteration 39, loss = 0.65790166\n",
            "Iteration 40, loss = 0.65754539\n",
            "Iteration 41, loss = 0.65730892\n",
            "Iteration 42, loss = 0.65684578\n",
            "Iteration 43, loss = 0.65643585\n",
            "Iteration 44, loss = 0.65615619\n",
            "Iteration 45, loss = 0.65580598\n",
            "Iteration 46, loss = 0.65535297\n",
            "Iteration 47, loss = 0.65504878\n",
            "Iteration 48, loss = 0.65466971\n",
            "Iteration 49, loss = 0.65434929\n",
            "Iteration 50, loss = 0.65395127\n",
            "Iteration 51, loss = 0.65364568\n",
            "Iteration 52, loss = 0.65341023\n",
            "Iteration 53, loss = 0.65296411\n",
            "Iteration 54, loss = 0.65263961\n",
            "Iteration 55, loss = 0.65244149\n",
            "Iteration 56, loss = 0.65194883\n",
            "Iteration 57, loss = 0.65171932\n",
            "Iteration 58, loss = 0.65126638\n",
            "Iteration 59, loss = 0.65093531\n",
            "Iteration 60, loss = 0.65062804\n",
            "Iteration 61, loss = 0.65024086\n",
            "Iteration 62, loss = 0.65005886\n",
            "Iteration 63, loss = 0.64980792\n",
            "Iteration 64, loss = 0.64919415\n",
            "Iteration 65, loss = 0.64907433\n",
            "Iteration 66, loss = 0.64869327\n",
            "Iteration 67, loss = 0.64833809\n",
            "Iteration 68, loss = 0.64798396\n",
            "Iteration 69, loss = 0.64772523\n",
            "Iteration 70, loss = 0.64737849\n",
            "Iteration 71, loss = 0.64716507\n",
            "Iteration 72, loss = 0.64679001\n",
            "Iteration 73, loss = 0.64651120\n",
            "Iteration 74, loss = 0.64631879\n",
            "Iteration 75, loss = 0.64588064\n",
            "Iteration 76, loss = 0.64564790\n",
            "Iteration 77, loss = 0.64535298\n",
            "Iteration 78, loss = 0.64513217\n",
            "Iteration 79, loss = 0.64486334\n",
            "Iteration 80, loss = 0.64442442\n",
            "Iteration 81, loss = 0.64417100\n",
            "Iteration 82, loss = 0.64386736\n",
            "Iteration 83, loss = 0.64361605\n",
            "Iteration 84, loss = 0.64330053\n",
            "Iteration 85, loss = 0.64302899\n",
            "Iteration 86, loss = 0.64270867\n",
            "Iteration 87, loss = 0.64245130\n",
            "Iteration 88, loss = 0.64220224\n",
            "Iteration 89, loss = 0.64196337\n",
            "Iteration 90, loss = 0.64161198\n",
            "Iteration 91, loss = 0.64130829\n",
            "Iteration 92, loss = 0.64108925\n",
            "Iteration 93, loss = 0.64077794\n",
            "Iteration 94, loss = 0.64050838\n",
            "Iteration 95, loss = 0.64026029\n",
            "Iteration 96, loss = 0.63997842\n",
            "Iteration 97, loss = 0.63978382\n",
            "Iteration 98, loss = 0.63951587\n",
            "Iteration 99, loss = 0.63925711\n",
            "Iteration 100, loss = 0.63898777\n",
            "Iteration 101, loss = 0.63867715\n",
            "Iteration 102, loss = 0.63848378\n",
            "Iteration 103, loss = 0.63820310\n",
            "Iteration 104, loss = 0.63798606\n",
            "Iteration 105, loss = 0.63776799\n",
            "Iteration 106, loss = 0.63752157\n",
            "Iteration 107, loss = 0.63722875\n",
            "Iteration 108, loss = 0.63701118\n",
            "Iteration 109, loss = 0.63674302\n",
            "Iteration 110, loss = 0.63659718\n",
            "Iteration 111, loss = 0.63628085\n",
            "Iteration 112, loss = 0.63606034\n",
            "Iteration 113, loss = 0.63585569\n",
            "Iteration 114, loss = 0.63567800\n",
            "Iteration 115, loss = 0.63538113\n",
            "Iteration 116, loss = 0.63520478\n",
            "Iteration 117, loss = 0.63494743\n",
            "Iteration 118, loss = 0.63488057\n",
            "Iteration 119, loss = 0.63449816\n",
            "Iteration 120, loss = 0.63434765\n",
            "Iteration 121, loss = 0.63405897\n",
            "Iteration 122, loss = 0.63386776\n",
            "Iteration 123, loss = 0.63358237\n",
            "Iteration 124, loss = 0.63338981\n",
            "Iteration 125, loss = 0.63311393\n",
            "Iteration 126, loss = 0.63290762\n",
            "Iteration 127, loss = 0.63268608\n",
            "Iteration 128, loss = 0.63251864\n",
            "Iteration 129, loss = 0.63231512\n",
            "Iteration 130, loss = 0.63209660\n",
            "Iteration 131, loss = 0.63189740\n",
            "Iteration 132, loss = 0.63178107\n",
            "Iteration 133, loss = 0.63158713\n",
            "Iteration 134, loss = 0.63138114\n",
            "Iteration 135, loss = 0.63120679\n",
            "Iteration 136, loss = 0.63112051\n",
            "Iteration 137, loss = 0.63077179\n",
            "Iteration 138, loss = 0.63068877\n",
            "Iteration 139, loss = 0.63038970\n",
            "Iteration 140, loss = 0.63019012\n",
            "Iteration 141, loss = 0.63007799\n",
            "Iteration 142, loss = 0.63006757\n",
            "Iteration 143, loss = 0.62972816\n",
            "Iteration 144, loss = 0.62949772\n",
            "Iteration 145, loss = 0.62953958\n",
            "Iteration 146, loss = 0.62912731\n",
            "Iteration 147, loss = 0.62894682\n",
            "Iteration 148, loss = 0.62891200\n",
            "Iteration 149, loss = 0.62865805\n",
            "Iteration 150, loss = 0.62848807\n",
            "Iteration 151, loss = 0.62834690\n",
            "Iteration 152, loss = 0.62817951\n",
            "Iteration 153, loss = 0.62807684\n",
            "Iteration 154, loss = 0.62792911\n",
            "Iteration 155, loss = 0.62772098\n",
            "Iteration 156, loss = 0.62756390\n",
            "Iteration 157, loss = 0.62735712\n",
            "Iteration 158, loss = 0.62730553\n",
            "Iteration 159, loss = 0.62725531\n",
            "Iteration 160, loss = 0.62691810\n",
            "Iteration 161, loss = 0.62676006\n",
            "Iteration 162, loss = 0.62662689\n",
            "Iteration 163, loss = 0.62655285\n",
            "Iteration 164, loss = 0.62658199\n",
            "Iteration 165, loss = 0.62627053\n",
            "Iteration 166, loss = 0.62612953\n",
            "Iteration 167, loss = 0.62599126\n",
            "Iteration 168, loss = 0.62585312\n",
            "Iteration 169, loss = 0.62577100\n",
            "Iteration 170, loss = 0.62557455\n",
            "Iteration 171, loss = 0.62545003\n",
            "Iteration 172, loss = 0.62535181\n",
            "Iteration 173, loss = 0.62532339\n",
            "Iteration 174, loss = 0.62517961\n",
            "Iteration 175, loss = 0.62500065\n",
            "Iteration 176, loss = 0.62496142\n",
            "Iteration 177, loss = 0.62480275\n",
            "Iteration 178, loss = 0.62466364\n",
            "Iteration 179, loss = 0.62477780\n",
            "Iteration 180, loss = 0.62442359\n",
            "Iteration 181, loss = 0.62453606\n",
            "Iteration 182, loss = 0.62426746\n",
            "Iteration 183, loss = 0.62421124\n",
            "Iteration 184, loss = 0.62412118\n",
            "Iteration 185, loss = 0.62401902\n",
            "Iteration 186, loss = 0.62394619\n",
            "Iteration 187, loss = 0.62380431\n",
            "Iteration 188, loss = 0.62377911\n",
            "Iteration 189, loss = 0.62361388\n",
            "Iteration 190, loss = 0.62356311\n",
            "Iteration 191, loss = 0.62341610\n",
            "Iteration 192, loss = 0.62357046\n",
            "Iteration 193, loss = 0.62332283\n",
            "Iteration 194, loss = 0.62319286\n",
            "Iteration 195, loss = 0.62304948\n",
            "Iteration 196, loss = 0.62306326\n",
            "Iteration 197, loss = 0.62294202\n",
            "Iteration 198, loss = 0.62278651\n",
            "Iteration 199, loss = 0.62271057\n",
            "Iteration 200, loss = 0.62265084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.61878447\n",
            "Iteration 2, loss = 0.61887495\n",
            "Iteration 3, loss = 0.60244253\n",
            "Iteration 4, loss = 0.60660162\n",
            "Iteration 5, loss = 0.60459320\n",
            "Iteration 6, loss = 0.59372735\n",
            "Iteration 7, loss = 0.59405265\n",
            "Iteration 8, loss = 0.58181732\n",
            "Iteration 9, loss = 0.56257034\n",
            "Iteration 10, loss = 0.55939967\n",
            "Iteration 11, loss = 0.55045645\n",
            "Iteration 12, loss = 0.54366766\n",
            "Iteration 13, loss = 0.53914053\n",
            "Iteration 14, loss = 0.53091953\n",
            "Iteration 15, loss = 0.52896735\n",
            "Iteration 16, loss = 0.53647010\n",
            "Iteration 17, loss = 0.50696950\n",
            "Iteration 18, loss = 0.50646520\n",
            "Iteration 19, loss = 0.49624379\n",
            "Iteration 20, loss = 0.47758854\n",
            "Iteration 21, loss = 0.48337387\n",
            "Iteration 22, loss = 0.46936741\n",
            "Iteration 23, loss = 0.47781889\n",
            "Iteration 24, loss = 0.48088704\n",
            "Iteration 25, loss = 0.46140297\n",
            "Iteration 26, loss = 0.45370415\n",
            "Iteration 27, loss = 0.45205608\n",
            "Iteration 28, loss = 0.43896603\n",
            "Iteration 29, loss = 0.44954764\n",
            "Iteration 30, loss = 0.43083827\n",
            "Iteration 31, loss = 0.43078393\n",
            "Iteration 32, loss = 0.43697783\n",
            "Iteration 33, loss = 0.43633623\n",
            "Iteration 34, loss = 0.41541236\n",
            "Iteration 35, loss = 0.47117423\n",
            "Iteration 36, loss = 0.43483115\n",
            "Iteration 37, loss = 0.42808792\n",
            "Iteration 38, loss = 0.43305880\n",
            "Iteration 39, loss = 0.43529370\n",
            "Iteration 40, loss = 0.40859415\n",
            "Iteration 41, loss = 0.42219439\n",
            "Iteration 42, loss = 0.43603909\n",
            "Iteration 43, loss = 0.43087232\n",
            "Iteration 44, loss = 0.41046513\n",
            "Iteration 45, loss = 0.40556116\n",
            "Iteration 46, loss = 0.40518428\n",
            "Iteration 47, loss = 0.40897847\n",
            "Iteration 48, loss = 0.40264743\n",
            "Iteration 49, loss = 0.40446854\n",
            "Iteration 50, loss = 0.40612307\n",
            "Iteration 51, loss = 0.41704358\n",
            "Iteration 52, loss = 0.40617547\n",
            "Iteration 53, loss = 0.39468620\n",
            "Iteration 54, loss = 0.40657065\n",
            "Iteration 55, loss = 0.40616946\n",
            "Iteration 56, loss = 0.40006485\n",
            "Iteration 57, loss = 0.39937953\n",
            "Iteration 58, loss = 0.40029260\n",
            "Iteration 59, loss = 0.40256761\n",
            "Iteration 60, loss = 0.40312811\n",
            "Iteration 61, loss = 0.39385496\n",
            "Iteration 62, loss = 0.39148113\n",
            "Iteration 63, loss = 0.38330470\n",
            "Iteration 64, loss = 0.39521802\n",
            "Iteration 65, loss = 0.39986997\n",
            "Iteration 66, loss = 0.39573069\n",
            "Iteration 67, loss = 0.38257938\n",
            "Iteration 68, loss = 0.38515749\n",
            "Iteration 69, loss = 0.38169468\n",
            "Iteration 70, loss = 0.40252368\n",
            "Iteration 71, loss = 0.40989105\n",
            "Iteration 72, loss = 0.38402690\n",
            "Iteration 73, loss = 0.37567536\n",
            "Iteration 74, loss = 0.37529348\n",
            "Iteration 75, loss = 0.37823250\n",
            "Iteration 76, loss = 0.37373178\n",
            "Iteration 77, loss = 0.37416459\n",
            "Iteration 78, loss = 0.37711938\n",
            "Iteration 79, loss = 0.38111353\n",
            "Iteration 80, loss = 0.38473626\n",
            "Iteration 81, loss = 0.37131896\n",
            "Iteration 82, loss = 0.37697932\n",
            "Iteration 83, loss = 0.38691226\n",
            "Iteration 84, loss = 0.37545413\n",
            "Iteration 85, loss = 0.37583507\n",
            "Iteration 86, loss = 0.36554563\n",
            "Iteration 87, loss = 0.36648894\n",
            "Iteration 88, loss = 0.36276811\n",
            "Iteration 89, loss = 0.37190445\n",
            "Iteration 90, loss = 0.36673723\n",
            "Iteration 91, loss = 0.36344598\n",
            "Iteration 92, loss = 0.36441847\n",
            "Iteration 93, loss = 0.39692146\n",
            "Iteration 94, loss = 0.38923714\n",
            "Iteration 95, loss = 0.39340355\n",
            "Iteration 96, loss = 0.38415648\n",
            "Iteration 97, loss = 0.36336123\n",
            "Iteration 98, loss = 0.37018664\n",
            "Iteration 99, loss = 0.36164236\n",
            "Iteration 100, loss = 0.36190614\n",
            "Iteration 101, loss = 0.36750813\n",
            "Iteration 102, loss = 0.36448546\n",
            "Iteration 103, loss = 0.36382766\n",
            "Iteration 104, loss = 0.39758813\n",
            "Iteration 105, loss = 0.37880473\n",
            "Iteration 106, loss = 0.37410321\n",
            "Iteration 107, loss = 0.36335768\n",
            "Iteration 108, loss = 0.36982632\n",
            "Iteration 109, loss = 0.36761419\n",
            "Iteration 110, loss = 0.36163531\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75608800\n",
            "Iteration 2, loss = 0.62727460\n",
            "Iteration 3, loss = 0.60568247\n",
            "Iteration 4, loss = 0.59272296\n",
            "Iteration 5, loss = 0.58225660\n",
            "Iteration 6, loss = 0.57547383\n",
            "Iteration 7, loss = 0.56973607\n",
            "Iteration 8, loss = 0.56157551\n",
            "Iteration 9, loss = 0.56589301\n",
            "Iteration 10, loss = 0.55824454\n",
            "Iteration 11, loss = 0.55543862\n",
            "Iteration 12, loss = 0.53823475\n",
            "Iteration 13, loss = 0.52757093\n",
            "Iteration 14, loss = 0.52968987\n",
            "Iteration 15, loss = 0.52698037\n",
            "Iteration 16, loss = 0.51734937\n",
            "Iteration 17, loss = 0.51666004\n",
            "Iteration 18, loss = 0.50538991\n",
            "Iteration 19, loss = 0.49527009\n",
            "Iteration 20, loss = 0.49082683\n",
            "Iteration 21, loss = 0.47798607\n",
            "Iteration 22, loss = 0.47282004\n",
            "Iteration 23, loss = 0.46760361\n",
            "Iteration 24, loss = 0.46080665\n",
            "Iteration 25, loss = 0.45732616\n",
            "Iteration 26, loss = 0.45088912\n",
            "Iteration 27, loss = 0.44889580\n",
            "Iteration 28, loss = 0.44762679\n",
            "Iteration 29, loss = 0.45147487\n",
            "Iteration 30, loss = 0.43706890\n",
            "Iteration 31, loss = 0.44163964\n",
            "Iteration 32, loss = 0.44301824\n",
            "Iteration 33, loss = 0.45051628\n",
            "Iteration 34, loss = 0.43643307\n",
            "Iteration 35, loss = 0.43510034\n",
            "Iteration 36, loss = 0.42992826\n",
            "Iteration 37, loss = 0.42160047\n",
            "Iteration 38, loss = 0.41691599\n",
            "Iteration 39, loss = 0.41165925\n",
            "Iteration 40, loss = 0.43006779\n",
            "Iteration 41, loss = 0.41332741\n",
            "Iteration 42, loss = 0.40953807\n",
            "Iteration 43, loss = 0.41058398\n",
            "Iteration 44, loss = 0.40239656\n",
            "Iteration 45, loss = 0.42226473\n",
            "Iteration 46, loss = 0.42282995\n",
            "Iteration 47, loss = 0.41772360\n",
            "Iteration 48, loss = 0.40621307\n",
            "Iteration 49, loss = 0.39733373\n",
            "Iteration 50, loss = 0.39187909\n",
            "Iteration 51, loss = 0.39259276\n",
            "Iteration 52, loss = 0.41599667\n",
            "Iteration 53, loss = 0.40613293\n",
            "Iteration 54, loss = 0.39430668\n",
            "Iteration 55, loss = 0.39349169\n",
            "Iteration 56, loss = 0.38797883\n",
            "Iteration 57, loss = 0.39784410\n",
            "Iteration 58, loss = 0.38385138\n",
            "Iteration 59, loss = 0.38896240\n",
            "Iteration 60, loss = 0.37963530\n",
            "Iteration 61, loss = 0.38149423\n",
            "Iteration 62, loss = 0.38064704\n",
            "Iteration 63, loss = 0.38051729\n",
            "Iteration 64, loss = 0.38850880\n",
            "Iteration 65, loss = 0.38759389\n",
            "Iteration 66, loss = 0.37662630\n",
            "Iteration 67, loss = 0.37948118\n",
            "Iteration 68, loss = 0.38814741\n",
            "Iteration 69, loss = 0.37605224\n",
            "Iteration 70, loss = 0.37721022\n",
            "Iteration 71, loss = 0.37133840\n",
            "Iteration 72, loss = 0.38454522\n",
            "Iteration 73, loss = 0.37878316\n",
            "Iteration 74, loss = 0.38666848\n",
            "Iteration 75, loss = 0.38642901\n",
            "Iteration 76, loss = 0.38180405\n",
            "Iteration 77, loss = 0.36652424\n",
            "Iteration 78, loss = 0.37212240\n",
            "Iteration 79, loss = 0.36271345\n",
            "Iteration 80, loss = 0.36685537\n",
            "Iteration 81, loss = 0.35928229\n",
            "Iteration 82, loss = 0.36616075\n",
            "Iteration 83, loss = 0.39896660\n",
            "Iteration 84, loss = 0.39133267\n",
            "Iteration 85, loss = 0.38151622\n",
            "Iteration 86, loss = 0.36200281\n",
            "Iteration 87, loss = 0.35694968\n",
            "Iteration 88, loss = 0.35713562\n",
            "Iteration 89, loss = 0.35229783\n",
            "Iteration 90, loss = 0.35294802\n",
            "Iteration 91, loss = 0.35259470\n",
            "Iteration 92, loss = 0.34842073\n",
            "Iteration 93, loss = 0.34774449\n",
            "Iteration 94, loss = 0.34752908\n",
            "Iteration 95, loss = 0.35723325\n",
            "Iteration 96, loss = 0.35236292\n",
            "Iteration 97, loss = 0.38760313\n",
            "Iteration 98, loss = 0.36979281\n",
            "Iteration 99, loss = 0.38685454\n",
            "Iteration 100, loss = 0.36284319\n",
            "Iteration 101, loss = 0.37849978\n",
            "Iteration 102, loss = 0.34472266\n",
            "Iteration 103, loss = 0.34985809\n",
            "Iteration 104, loss = 0.35531905\n",
            "Iteration 105, loss = 0.37200446\n",
            "Iteration 106, loss = 0.35661994\n",
            "Iteration 107, loss = 0.35413281\n",
            "Iteration 108, loss = 0.35430583\n",
            "Iteration 109, loss = 0.33864765\n",
            "Iteration 110, loss = 0.33881593\n",
            "Iteration 111, loss = 0.33815220\n",
            "Iteration 112, loss = 0.33473270\n",
            "Iteration 113, loss = 0.33817679\n",
            "Iteration 114, loss = 0.34081835\n",
            "Iteration 115, loss = 0.34860373\n",
            "Iteration 116, loss = 0.33762174\n",
            "Iteration 117, loss = 0.34011565\n",
            "Iteration 118, loss = 0.33808980\n",
            "Iteration 119, loss = 0.33780284\n",
            "Iteration 120, loss = 0.33409802\n",
            "Iteration 121, loss = 0.33091026\n",
            "Iteration 122, loss = 0.33434602\n",
            "Iteration 123, loss = 0.33842127\n",
            "Iteration 124, loss = 0.34448507\n",
            "Iteration 125, loss = 0.33662944\n",
            "Iteration 126, loss = 0.34131599\n",
            "Iteration 127, loss = 0.32948600\n",
            "Iteration 128, loss = 0.34573611\n",
            "Iteration 129, loss = 0.33287523\n",
            "Iteration 130, loss = 0.33864036\n",
            "Iteration 131, loss = 0.32926809\n",
            "Iteration 132, loss = 0.32704173\n",
            "Iteration 133, loss = 0.32886738\n",
            "Iteration 134, loss = 0.32145868\n",
            "Iteration 135, loss = 0.33257692\n",
            "Iteration 136, loss = 0.32882791\n",
            "Iteration 137, loss = 0.32977662\n",
            "Iteration 138, loss = 0.32656530\n",
            "Iteration 139, loss = 0.33125493\n",
            "Iteration 140, loss = 0.34007044\n",
            "Iteration 141, loss = 0.35325411\n",
            "Iteration 142, loss = 0.33456316\n",
            "Iteration 143, loss = 0.34014030\n",
            "Iteration 144, loss = 0.34280989\n",
            "Iteration 145, loss = 0.34958355\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62400737\n",
            "Iteration 2, loss = 0.60732908\n",
            "Iteration 3, loss = 0.58569959\n",
            "Iteration 4, loss = 0.58025631\n",
            "Iteration 5, loss = 0.55576447\n",
            "Iteration 6, loss = 0.54445644\n",
            "Iteration 7, loss = 0.53171840\n",
            "Iteration 8, loss = 0.52948858\n",
            "Iteration 9, loss = 0.51682129\n",
            "Iteration 10, loss = 0.50775600\n",
            "Iteration 11, loss = 0.49431160\n",
            "Iteration 12, loss = 0.49096692\n",
            "Iteration 13, loss = 0.48277289\n",
            "Iteration 14, loss = 0.48163034\n",
            "Iteration 15, loss = 0.50977924\n",
            "Iteration 16, loss = 0.48579675\n",
            "Iteration 17, loss = 0.46532397\n",
            "Iteration 18, loss = 0.45920143\n",
            "Iteration 19, loss = 0.44568612\n",
            "Iteration 20, loss = 0.43870237\n",
            "Iteration 21, loss = 0.44555614\n",
            "Iteration 22, loss = 0.45743040\n",
            "Iteration 23, loss = 0.44344149\n",
            "Iteration 24, loss = 0.43153019\n",
            "Iteration 25, loss = 0.42961691\n",
            "Iteration 26, loss = 0.42341765\n",
            "Iteration 27, loss = 0.41757029\n",
            "Iteration 28, loss = 0.42525634\n",
            "Iteration 29, loss = 0.42763172\n",
            "Iteration 30, loss = 0.42190112\n",
            "Iteration 31, loss = 0.42928988\n",
            "Iteration 32, loss = 0.40790336\n",
            "Iteration 33, loss = 0.41996918\n",
            "Iteration 34, loss = 0.41085239\n",
            "Iteration 35, loss = 0.41414139\n",
            "Iteration 36, loss = 0.40955442\n",
            "Iteration 37, loss = 0.41118205\n",
            "Iteration 38, loss = 0.41370783\n",
            "Iteration 39, loss = 0.41303227\n",
            "Iteration 40, loss = 0.41030254\n",
            "Iteration 41, loss = 0.41497129\n",
            "Iteration 42, loss = 0.40078186\n",
            "Iteration 43, loss = 0.40770863\n",
            "Iteration 44, loss = 0.40863688\n",
            "Iteration 45, loss = 0.40924232\n",
            "Iteration 46, loss = 0.39776348\n",
            "Iteration 47, loss = 0.41122084\n",
            "Iteration 48, loss = 0.39063027\n",
            "Iteration 49, loss = 0.39429044\n",
            "Iteration 50, loss = 0.40651125\n",
            "Iteration 51, loss = 0.40132653\n",
            "Iteration 52, loss = 0.40118420\n",
            "Iteration 53, loss = 0.41920436\n",
            "Iteration 54, loss = 0.39414879\n",
            "Iteration 55, loss = 0.40017881\n",
            "Iteration 56, loss = 0.38500865\n",
            "Iteration 57, loss = 0.39359254\n",
            "Iteration 58, loss = 0.38956244\n",
            "Iteration 59, loss = 0.38490648\n",
            "Iteration 60, loss = 0.38193929\n",
            "Iteration 61, loss = 0.38212894\n",
            "Iteration 62, loss = 0.38249697\n",
            "Iteration 63, loss = 0.38457205\n",
            "Iteration 64, loss = 0.37991064\n",
            "Iteration 65, loss = 0.38337249\n",
            "Iteration 66, loss = 0.42075626\n",
            "Iteration 67, loss = 0.40111649\n",
            "Iteration 68, loss = 0.39797654\n",
            "Iteration 69, loss = 0.39442662\n",
            "Iteration 70, loss = 0.37422514\n",
            "Iteration 71, loss = 0.38661236\n",
            "Iteration 72, loss = 0.37971994\n",
            "Iteration 73, loss = 0.37936766\n",
            "Iteration 74, loss = 0.37475022\n",
            "Iteration 75, loss = 0.37611468\n",
            "Iteration 76, loss = 0.39202650\n",
            "Iteration 77, loss = 0.36983499\n",
            "Iteration 78, loss = 0.37496868\n",
            "Iteration 79, loss = 0.38069512\n",
            "Iteration 80, loss = 0.36865444\n",
            "Iteration 81, loss = 0.37112862\n",
            "Iteration 82, loss = 0.36620299\n",
            "Iteration 83, loss = 0.36684891\n",
            "Iteration 84, loss = 0.37006266\n",
            "Iteration 85, loss = 0.36509442\n",
            "Iteration 86, loss = 0.36489615\n",
            "Iteration 87, loss = 0.36648228\n",
            "Iteration 88, loss = 0.36128349\n",
            "Iteration 89, loss = 0.36173059\n",
            "Iteration 90, loss = 0.39203316\n",
            "Iteration 91, loss = 0.36303811\n",
            "Iteration 92, loss = 0.36482280\n",
            "Iteration 93, loss = 0.36586959\n",
            "Iteration 94, loss = 0.36369080\n",
            "Iteration 95, loss = 0.35970776\n",
            "Iteration 96, loss = 0.36196900\n",
            "Iteration 97, loss = 0.37231203\n",
            "Iteration 98, loss = 0.37212393\n",
            "Iteration 99, loss = 0.36490505\n",
            "Iteration 100, loss = 0.36689461\n",
            "Iteration 101, loss = 0.35893205\n",
            "Iteration 102, loss = 0.35721024\n",
            "Iteration 103, loss = 0.35432946\n",
            "Iteration 104, loss = 0.39050443\n",
            "Iteration 105, loss = 0.35800851\n",
            "Iteration 106, loss = 0.35637419\n",
            "Iteration 107, loss = 0.35705391\n",
            "Iteration 108, loss = 0.37285570\n",
            "Iteration 109, loss = 0.36385518\n",
            "Iteration 110, loss = 0.36931174\n",
            "Iteration 111, loss = 0.36345761\n",
            "Iteration 112, loss = 0.35890014\n",
            "Iteration 113, loss = 0.35146573\n",
            "Iteration 114, loss = 0.35548964\n",
            "Iteration 115, loss = 0.35582004\n",
            "Iteration 116, loss = 0.36474409\n",
            "Iteration 117, loss = 0.36369230\n",
            "Iteration 118, loss = 0.36034394\n",
            "Iteration 119, loss = 0.34430597\n",
            "Iteration 120, loss = 0.35269817\n",
            "Iteration 121, loss = 0.34403199\n",
            "Iteration 122, loss = 0.35881213\n",
            "Iteration 123, loss = 0.34682694\n",
            "Iteration 124, loss = 0.35382489\n",
            "Iteration 125, loss = 0.34784118\n",
            "Iteration 126, loss = 0.34832612\n",
            "Iteration 127, loss = 0.34524518\n",
            "Iteration 128, loss = 0.34680808\n",
            "Iteration 129, loss = 0.34258254\n",
            "Iteration 130, loss = 0.35834693\n",
            "Iteration 131, loss = 0.35014011\n",
            "Iteration 132, loss = 0.34230962\n",
            "Iteration 133, loss = 0.35854126\n",
            "Iteration 134, loss = 0.35587269\n",
            "Iteration 135, loss = 0.36052864\n",
            "Iteration 136, loss = 0.44018031\n",
            "Iteration 137, loss = 0.38450870\n",
            "Iteration 138, loss = 0.36268246\n",
            "Iteration 139, loss = 0.34040959\n",
            "Iteration 140, loss = 0.33881784\n",
            "Iteration 141, loss = 0.33700047\n",
            "Iteration 142, loss = 0.34324529\n",
            "Iteration 143, loss = 0.33753539\n",
            "Iteration 144, loss = 0.33678003\n",
            "Iteration 145, loss = 0.33617534\n",
            "Iteration 146, loss = 0.34116794\n",
            "Iteration 147, loss = 0.33690050\n",
            "Iteration 148, loss = 0.33679414\n",
            "Iteration 149, loss = 0.34528780\n",
            "Iteration 150, loss = 0.33385313\n",
            "Iteration 151, loss = 0.36530378\n",
            "Iteration 152, loss = 0.34598010\n",
            "Iteration 153, loss = 0.33743236\n",
            "Iteration 154, loss = 0.34571160\n",
            "Iteration 155, loss = 0.33808017\n",
            "Iteration 156, loss = 0.33043102\n",
            "Iteration 157, loss = 0.33081126\n",
            "Iteration 158, loss = 0.33145324\n",
            "Iteration 159, loss = 0.33076783\n",
            "Iteration 160, loss = 0.34070851\n",
            "Iteration 161, loss = 0.36350678\n",
            "Iteration 162, loss = 0.35310679\n",
            "Iteration 163, loss = 0.33330131\n",
            "Iteration 164, loss = 0.33671384\n",
            "Iteration 165, loss = 0.33878462\n",
            "Iteration 166, loss = 0.32784556\n",
            "Iteration 167, loss = 0.33422591\n",
            "Iteration 168, loss = 0.33179975\n",
            "Iteration 169, loss = 0.34618502\n",
            "Iteration 170, loss = 0.34423218\n",
            "Iteration 171, loss = 0.34153282\n",
            "Iteration 172, loss = 0.33143469\n",
            "Iteration 173, loss = 0.33308676\n",
            "Iteration 174, loss = 0.33172187\n",
            "Iteration 175, loss = 0.32826039\n",
            "Iteration 176, loss = 0.32535491\n",
            "Iteration 177, loss = 0.32738590\n",
            "Iteration 178, loss = 0.33543461\n",
            "Iteration 179, loss = 0.35483734\n",
            "Iteration 180, loss = 0.34534664\n",
            "Iteration 181, loss = 0.33963143\n",
            "Iteration 182, loss = 0.33880474\n",
            "Iteration 183, loss = 0.33848271\n",
            "Iteration 184, loss = 0.35826484\n",
            "Iteration 185, loss = 0.34559588\n",
            "Iteration 186, loss = 0.36159899\n",
            "Iteration 187, loss = 0.33377867\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.63711974\n",
            "Iteration 2, loss = 0.61635466\n",
            "Iteration 3, loss = 0.61680077\n",
            "Iteration 4, loss = 0.62610996\n",
            "Iteration 5, loss = 0.59641823\n",
            "Iteration 6, loss = 0.59633718\n",
            "Iteration 7, loss = 0.59267260\n",
            "Iteration 8, loss = 0.57113934\n",
            "Iteration 9, loss = 0.56368114\n",
            "Iteration 10, loss = 0.55064480\n",
            "Iteration 11, loss = 0.54528607\n",
            "Iteration 12, loss = 0.55054699\n",
            "Iteration 13, loss = 0.52467971\n",
            "Iteration 14, loss = 0.51467360\n",
            "Iteration 15, loss = 0.50326150\n",
            "Iteration 16, loss = 0.52475446\n",
            "Iteration 17, loss = 0.50059300\n",
            "Iteration 18, loss = 0.49727405\n",
            "Iteration 19, loss = 0.48894576\n",
            "Iteration 20, loss = 0.47556661\n",
            "Iteration 21, loss = 0.46507505\n",
            "Iteration 22, loss = 0.46379437\n",
            "Iteration 23, loss = 0.46549303\n",
            "Iteration 24, loss = 0.46884723\n",
            "Iteration 25, loss = 0.45334636\n",
            "Iteration 26, loss = 0.44956777\n",
            "Iteration 27, loss = 0.44335876\n",
            "Iteration 28, loss = 0.45480624\n",
            "Iteration 29, loss = 0.44574332\n",
            "Iteration 30, loss = 0.44033287\n",
            "Iteration 31, loss = 0.43421547\n",
            "Iteration 32, loss = 0.43773529\n",
            "Iteration 33, loss = 0.43993703\n",
            "Iteration 34, loss = 0.43083776\n",
            "Iteration 35, loss = 0.43093044\n",
            "Iteration 36, loss = 0.42484467\n",
            "Iteration 37, loss = 0.42346675\n",
            "Iteration 38, loss = 0.41986439\n",
            "Iteration 39, loss = 0.41794192\n",
            "Iteration 40, loss = 0.42397190\n",
            "Iteration 41, loss = 0.41226083\n",
            "Iteration 42, loss = 0.41945626\n",
            "Iteration 43, loss = 0.40788095\n",
            "Iteration 44, loss = 0.41274181\n",
            "Iteration 45, loss = 0.42527683\n",
            "Iteration 46, loss = 0.42261927\n",
            "Iteration 47, loss = 0.40913441\n",
            "Iteration 48, loss = 0.40451941\n",
            "Iteration 49, loss = 0.39942356\n",
            "Iteration 50, loss = 0.44541395\n",
            "Iteration 51, loss = 0.43643848\n",
            "Iteration 52, loss = 0.42494103\n",
            "Iteration 53, loss = 0.40342238\n",
            "Iteration 54, loss = 0.39629666\n",
            "Iteration 55, loss = 0.40233257\n",
            "Iteration 56, loss = 0.41352291\n",
            "Iteration 57, loss = 0.44177035\n",
            "Iteration 58, loss = 0.39681127\n",
            "Iteration 59, loss = 0.41279273\n",
            "Iteration 60, loss = 0.39198994\n",
            "Iteration 61, loss = 0.39451795\n",
            "Iteration 62, loss = 0.39873956\n",
            "Iteration 63, loss = 0.39644788\n",
            "Iteration 64, loss = 0.39265387\n",
            "Iteration 65, loss = 0.39882371\n",
            "Iteration 66, loss = 0.39300934\n",
            "Iteration 67, loss = 0.38705009\n",
            "Iteration 68, loss = 0.37992546\n",
            "Iteration 69, loss = 0.38461582\n",
            "Iteration 70, loss = 0.38296709\n",
            "Iteration 71, loss = 0.39517115\n",
            "Iteration 72, loss = 0.40911676\n",
            "Iteration 73, loss = 0.38113248\n",
            "Iteration 74, loss = 0.37593247\n",
            "Iteration 75, loss = 0.37863109\n",
            "Iteration 76, loss = 0.37132674\n",
            "Iteration 77, loss = 0.37969770\n",
            "Iteration 78, loss = 0.38721200\n",
            "Iteration 79, loss = 0.37900403\n",
            "Iteration 80, loss = 0.37071146\n",
            "Iteration 81, loss = 0.38155451\n",
            "Iteration 82, loss = 0.38241280\n",
            "Iteration 83, loss = 0.37539268\n",
            "Iteration 84, loss = 0.39217052\n",
            "Iteration 85, loss = 0.38290212\n",
            "Iteration 86, loss = 0.37137914\n",
            "Iteration 87, loss = 0.37531201\n",
            "Iteration 88, loss = 0.37201124\n",
            "Iteration 89, loss = 0.38379942\n",
            "Iteration 90, loss = 0.38417010\n",
            "Iteration 91, loss = 0.39447414\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67374804\n",
            "Iteration 2, loss = 0.61619145\n",
            "Iteration 3, loss = 0.60974252\n",
            "Iteration 4, loss = 0.59474064\n",
            "Iteration 5, loss = 0.59299471\n",
            "Iteration 6, loss = 0.58013231\n",
            "Iteration 7, loss = 0.57850265\n",
            "Iteration 8, loss = 0.56407454\n",
            "Iteration 9, loss = 0.55458966\n",
            "Iteration 10, loss = 0.54440433\n",
            "Iteration 11, loss = 0.54957109\n",
            "Iteration 12, loss = 0.53588044\n",
            "Iteration 13, loss = 0.52397705\n",
            "Iteration 14, loss = 0.52654404\n",
            "Iteration 15, loss = 0.51229615\n",
            "Iteration 16, loss = 0.50542602\n",
            "Iteration 17, loss = 0.51037732\n",
            "Iteration 18, loss = 0.51861399\n",
            "Iteration 19, loss = 0.49372369\n",
            "Iteration 20, loss = 0.49647111\n",
            "Iteration 21, loss = 0.48409870\n",
            "Iteration 22, loss = 0.47126852\n",
            "Iteration 23, loss = 0.47205446\n",
            "Iteration 24, loss = 0.46533755\n",
            "Iteration 25, loss = 0.46667407\n",
            "Iteration 26, loss = 0.47257462\n",
            "Iteration 27, loss = 0.46261289\n",
            "Iteration 28, loss = 0.45295225\n",
            "Iteration 29, loss = 0.45429965\n",
            "Iteration 30, loss = 0.45441014\n",
            "Iteration 31, loss = 0.44778379\n",
            "Iteration 32, loss = 0.44801255\n",
            "Iteration 33, loss = 0.45283733\n",
            "Iteration 34, loss = 0.45275155\n",
            "Iteration 35, loss = 0.44697735\n",
            "Iteration 36, loss = 0.44150212\n",
            "Iteration 37, loss = 0.43902773\n",
            "Iteration 38, loss = 0.43745804\n",
            "Iteration 39, loss = 0.45856166\n",
            "Iteration 40, loss = 0.43521967\n",
            "Iteration 41, loss = 0.44400561\n",
            "Iteration 42, loss = 0.43934774\n",
            "Iteration 43, loss = 0.42846702\n",
            "Iteration 44, loss = 0.42704057\n",
            "Iteration 45, loss = 0.42615335\n",
            "Iteration 46, loss = 0.43734325\n",
            "Iteration 47, loss = 0.42020302\n",
            "Iteration 48, loss = 0.42287523\n",
            "Iteration 49, loss = 0.42348275\n",
            "Iteration 50, loss = 0.42488093\n",
            "Iteration 51, loss = 0.42087521\n",
            "Iteration 52, loss = 0.42454103\n",
            "Iteration 53, loss = 0.42518980\n",
            "Iteration 54, loss = 0.42520009\n",
            "Iteration 55, loss = 0.43208857\n",
            "Iteration 56, loss = 0.41216957\n",
            "Iteration 57, loss = 0.43159430\n",
            "Iteration 58, loss = 0.42749463\n",
            "Iteration 59, loss = 0.45319078\n",
            "Iteration 60, loss = 0.43350732\n",
            "Iteration 61, loss = 0.42733316\n",
            "Iteration 62, loss = 0.40610183\n",
            "Iteration 63, loss = 0.40620418\n",
            "Iteration 64, loss = 0.40201480\n",
            "Iteration 65, loss = 0.41163539\n",
            "Iteration 66, loss = 0.40247175\n",
            "Iteration 67, loss = 0.41221242\n",
            "Iteration 68, loss = 0.42606245\n",
            "Iteration 69, loss = 0.40868084\n",
            "Iteration 70, loss = 0.40561900\n",
            "Iteration 71, loss = 0.42890212\n",
            "Iteration 72, loss = 0.39852873\n",
            "Iteration 73, loss = 0.40044361\n",
            "Iteration 74, loss = 0.40719145\n",
            "Iteration 75, loss = 0.39825406\n",
            "Iteration 76, loss = 0.41142300\n",
            "Iteration 77, loss = 0.39262496\n",
            "Iteration 78, loss = 0.41683417\n",
            "Iteration 79, loss = 0.40761077\n",
            "Iteration 80, loss = 0.40876836\n",
            "Iteration 81, loss = 0.40080896\n",
            "Iteration 82, loss = 0.42399675\n",
            "Iteration 83, loss = 0.40483081\n",
            "Iteration 84, loss = 0.40043698\n",
            "Iteration 85, loss = 0.39952635\n",
            "Iteration 86, loss = 0.38709492\n",
            "Iteration 87, loss = 0.39587621\n",
            "Iteration 88, loss = 0.38248976\n",
            "Iteration 89, loss = 0.38826742\n",
            "Iteration 90, loss = 0.38209494\n",
            "Iteration 91, loss = 0.37922928\n",
            "Iteration 92, loss = 0.38646314\n",
            "Iteration 93, loss = 0.40291107\n",
            "Iteration 94, loss = 0.39118071\n",
            "Iteration 95, loss = 0.37665975\n",
            "Iteration 96, loss = 0.37972366\n",
            "Iteration 97, loss = 0.39338850\n",
            "Iteration 98, loss = 0.38486942\n",
            "Iteration 99, loss = 0.38505101\n",
            "Iteration 100, loss = 0.37625879\n",
            "Iteration 101, loss = 0.37737070\n",
            "Iteration 102, loss = 0.37954408\n",
            "Iteration 103, loss = 0.37115961\n",
            "Iteration 104, loss = 0.37895965\n",
            "Iteration 105, loss = 0.38222974\n",
            "Iteration 106, loss = 0.39666590\n",
            "Iteration 107, loss = 0.38991127\n",
            "Iteration 108, loss = 0.39570890\n",
            "Iteration 109, loss = 0.36767272\n",
            "Iteration 110, loss = 0.37763181\n",
            "Iteration 111, loss = 0.37338924\n",
            "Iteration 112, loss = 0.36754932\n",
            "Iteration 113, loss = 0.38159400\n",
            "Iteration 114, loss = 0.36063778\n",
            "Iteration 115, loss = 0.38283670\n",
            "Iteration 116, loss = 0.37107202\n",
            "Iteration 117, loss = 0.36852350\n",
            "Iteration 118, loss = 0.36700366\n",
            "Iteration 119, loss = 0.36756607\n",
            "Iteration 120, loss = 0.36077723\n",
            "Iteration 121, loss = 0.37293178\n",
            "Iteration 122, loss = 0.35926607\n",
            "Iteration 123, loss = 0.36297983\n",
            "Iteration 124, loss = 0.35358292\n",
            "Iteration 125, loss = 0.35821008\n",
            "Iteration 126, loss = 0.36846510\n",
            "Iteration 127, loss = 0.41362730\n",
            "Iteration 128, loss = 0.38946947\n",
            "Iteration 129, loss = 0.40047223\n",
            "Iteration 130, loss = 0.37598059\n",
            "Iteration 131, loss = 0.40591011\n",
            "Iteration 132, loss = 0.36536181\n",
            "Iteration 133, loss = 0.36196783\n",
            "Iteration 134, loss = 0.35492619\n",
            "Iteration 135, loss = 0.35956745\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.65409285\n",
            "Iteration 2, loss = 0.59827741\n",
            "Iteration 3, loss = 0.60350863\n",
            "Iteration 4, loss = 0.59712481\n",
            "Iteration 5, loss = 0.59084193\n",
            "Iteration 6, loss = 0.58725141\n",
            "Iteration 7, loss = 0.58077524\n",
            "Iteration 8, loss = 0.58150863\n",
            "Iteration 9, loss = 0.57363674\n",
            "Iteration 10, loss = 0.57449578\n",
            "Iteration 11, loss = 0.57984438\n",
            "Iteration 12, loss = 0.58266043\n",
            "Iteration 13, loss = 0.57583826\n",
            "Iteration 14, loss = 0.56796575\n",
            "Iteration 15, loss = 0.57297003\n",
            "Iteration 16, loss = 0.56242809\n",
            "Iteration 17, loss = 0.56284513\n",
            "Iteration 18, loss = 0.55801764\n",
            "Iteration 19, loss = 0.57191333\n",
            "Iteration 20, loss = 0.55736810\n",
            "Iteration 21, loss = 0.55407515\n",
            "Iteration 22, loss = 0.56181664\n",
            "Iteration 23, loss = 0.56598916\n",
            "Iteration 24, loss = 0.56697055\n",
            "Iteration 25, loss = 0.55980656\n",
            "Iteration 26, loss = 0.55800270\n",
            "Iteration 27, loss = 0.56596210\n",
            "Iteration 28, loss = 0.56387042\n",
            "Iteration 29, loss = 0.55543521\n",
            "Iteration 30, loss = 0.57096784\n",
            "Iteration 31, loss = 0.55945469\n",
            "Iteration 32, loss = 0.54504183\n",
            "Iteration 33, loss = 0.55302121\n",
            "Iteration 34, loss = 0.55875438\n",
            "Iteration 35, loss = 0.54757480\n",
            "Iteration 36, loss = 0.58876311\n",
            "Iteration 37, loss = 0.57711223\n",
            "Iteration 38, loss = 0.57820629\n",
            "Iteration 39, loss = 0.56740902\n",
            "Iteration 40, loss = 0.56429296\n",
            "Iteration 41, loss = 0.56446153\n",
            "Iteration 42, loss = 0.56205394\n",
            "Iteration 43, loss = 0.55467654\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.63058195\n",
            "Iteration 2, loss = 0.60856041\n",
            "Iteration 3, loss = 0.60242979\n",
            "Iteration 4, loss = 0.60050448\n",
            "Iteration 5, loss = 0.59689079\n",
            "Iteration 6, loss = 0.59543171\n",
            "Iteration 7, loss = 0.59462819\n",
            "Iteration 8, loss = 0.58937364\n",
            "Iteration 9, loss = 0.59581480\n",
            "Iteration 10, loss = 0.59194431\n",
            "Iteration 11, loss = 0.58607959\n",
            "Iteration 12, loss = 0.58496382\n",
            "Iteration 13, loss = 0.58513537\n",
            "Iteration 14, loss = 0.58451540\n",
            "Iteration 15, loss = 0.57598480\n",
            "Iteration 16, loss = 0.58130479\n",
            "Iteration 17, loss = 0.57855734\n",
            "Iteration 18, loss = 0.57653984\n",
            "Iteration 19, loss = 0.57997708\n",
            "Iteration 20, loss = 0.57756907\n",
            "Iteration 21, loss = 0.57274754\n",
            "Iteration 22, loss = 0.57131285\n",
            "Iteration 23, loss = 0.57217939\n",
            "Iteration 24, loss = 0.57040544\n",
            "Iteration 25, loss = 0.57202072\n",
            "Iteration 26, loss = 0.57251130\n",
            "Iteration 27, loss = 0.56894659\n",
            "Iteration 28, loss = 0.57200134\n",
            "Iteration 29, loss = 0.56356520\n",
            "Iteration 30, loss = 0.57573816\n",
            "Iteration 31, loss = 0.56606204\n",
            "Iteration 32, loss = 0.56202526\n",
            "Iteration 33, loss = 0.56409112\n",
            "Iteration 34, loss = 0.56342706\n",
            "Iteration 35, loss = 0.56455610\n",
            "Iteration 36, loss = 0.56957456\n",
            "Iteration 37, loss = 0.57118944\n",
            "Iteration 38, loss = 0.56669629\n",
            "Iteration 39, loss = 0.56290360\n",
            "Iteration 40, loss = 0.56081379\n",
            "Iteration 41, loss = 0.56836364\n",
            "Iteration 42, loss = 0.57956977\n",
            "Iteration 43, loss = 0.57036489\n",
            "Iteration 44, loss = 0.56054431\n",
            "Iteration 45, loss = 0.57041866\n",
            "Iteration 46, loss = 0.55996746\n",
            "Iteration 47, loss = 0.55840437\n",
            "Iteration 48, loss = 0.57576344\n",
            "Iteration 49, loss = 0.56864448\n",
            "Iteration 50, loss = 0.56178277\n",
            "Iteration 51, loss = 0.55974267\n",
            "Iteration 52, loss = 0.55621783\n",
            "Iteration 53, loss = 0.54834973\n",
            "Iteration 54, loss = 0.56303777\n",
            "Iteration 55, loss = 0.56338954\n",
            "Iteration 56, loss = 0.55291015\n",
            "Iteration 57, loss = 0.55679532\n",
            "Iteration 58, loss = 0.56562879\n",
            "Iteration 59, loss = 0.55113050\n",
            "Iteration 60, loss = 0.54711509\n",
            "Iteration 61, loss = 0.54812570\n",
            "Iteration 62, loss = 0.55718216\n",
            "Iteration 63, loss = 0.54244704\n",
            "Iteration 64, loss = 0.54427248\n",
            "Iteration 65, loss = 0.55503680\n",
            "Iteration 66, loss = 0.53945971\n",
            "Iteration 67, loss = 0.54981708\n",
            "Iteration 68, loss = 0.54705880\n",
            "Iteration 69, loss = 0.54256798\n",
            "Iteration 70, loss = 0.53907716\n",
            "Iteration 71, loss = 0.55406903\n",
            "Iteration 72, loss = 0.54773162\n",
            "Iteration 73, loss = 0.54195574\n",
            "Iteration 74, loss = 0.54559958\n",
            "Iteration 75, loss = 0.54877531\n",
            "Iteration 76, loss = 0.55322049\n",
            "Iteration 77, loss = 0.54372465\n",
            "Iteration 78, loss = 0.55224839\n",
            "Iteration 79, loss = 0.54420890\n",
            "Iteration 80, loss = 0.54517537\n",
            "Iteration 81, loss = 0.56080054\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.63055630\n",
            "Iteration 2, loss = 0.60651854\n",
            "Iteration 3, loss = 0.59357930\n",
            "Iteration 4, loss = 0.59999408\n",
            "Iteration 5, loss = 0.59949496\n",
            "Iteration 6, loss = 0.59680895\n",
            "Iteration 7, loss = 0.59533804\n",
            "Iteration 8, loss = 0.60020405\n",
            "Iteration 9, loss = 0.59243391\n",
            "Iteration 10, loss = 0.60009882\n",
            "Iteration 11, loss = 0.59922162\n",
            "Iteration 12, loss = 0.59697133\n",
            "Iteration 13, loss = 0.59495376\n",
            "Iteration 14, loss = 0.59580723\n",
            "Iteration 15, loss = 0.58976602\n",
            "Iteration 16, loss = 0.59151449\n",
            "Iteration 17, loss = 0.58543774\n",
            "Iteration 18, loss = 0.58707041\n",
            "Iteration 19, loss = 0.58458658\n",
            "Iteration 20, loss = 0.58726639\n",
            "Iteration 21, loss = 0.58773603\n",
            "Iteration 22, loss = 0.58707266\n",
            "Iteration 23, loss = 0.58518658\n",
            "Iteration 24, loss = 0.58209596\n",
            "Iteration 25, loss = 0.58611673\n",
            "Iteration 26, loss = 0.58115862\n",
            "Iteration 27, loss = 0.58394286\n",
            "Iteration 28, loss = 0.58454576\n",
            "Iteration 29, loss = 0.58173323\n",
            "Iteration 30, loss = 0.57504170\n",
            "Iteration 31, loss = 0.56983763\n",
            "Iteration 32, loss = 0.57264166\n",
            "Iteration 33, loss = 0.56840705\n",
            "Iteration 34, loss = 0.56762927\n",
            "Iteration 35, loss = 0.56918494\n",
            "Iteration 36, loss = 0.56788440\n",
            "Iteration 37, loss = 0.56766947\n",
            "Iteration 38, loss = 0.57532579\n",
            "Iteration 39, loss = 0.57249714\n",
            "Iteration 40, loss = 0.57144080\n",
            "Iteration 41, loss = 0.56132334\n",
            "Iteration 42, loss = 0.56874335\n",
            "Iteration 43, loss = 0.57004059\n",
            "Iteration 44, loss = 0.56958755\n",
            "Iteration 45, loss = 0.56553605\n",
            "Iteration 46, loss = 0.56597281\n",
            "Iteration 47, loss = 0.57766957\n",
            "Iteration 48, loss = 0.58936976\n",
            "Iteration 49, loss = 0.55889488\n",
            "Iteration 50, loss = 0.54264291\n",
            "Iteration 51, loss = 0.56859429\n",
            "Iteration 52, loss = 0.58819767\n",
            "Iteration 53, loss = 0.57258129\n",
            "Iteration 54, loss = 0.56973916\n",
            "Iteration 55, loss = 0.54220964\n",
            "Iteration 56, loss = 0.55579044\n",
            "Iteration 57, loss = 0.54040133\n",
            "Iteration 58, loss = 0.54269960\n",
            "Iteration 59, loss = 0.53783420\n",
            "Iteration 60, loss = 0.54176095\n",
            "Iteration 61, loss = 0.54508099\n",
            "Iteration 62, loss = 0.56354457\n",
            "Iteration 63, loss = 0.53535934\n",
            "Iteration 64, loss = 0.54600111\n",
            "Iteration 65, loss = 0.54365552\n",
            "Iteration 66, loss = 0.52693773\n",
            "Iteration 67, loss = 0.51699342\n",
            "Iteration 68, loss = 0.53609755\n",
            "Iteration 69, loss = 0.52954879\n",
            "Iteration 70, loss = 0.52883775\n",
            "Iteration 71, loss = 0.53583849\n",
            "Iteration 72, loss = 0.52975541\n",
            "Iteration 73, loss = 0.53906597\n",
            "Iteration 74, loss = 0.54947430\n",
            "Iteration 75, loss = 0.55893198\n",
            "Iteration 76, loss = 0.55904085\n",
            "Iteration 77, loss = 0.56553320\n",
            "Iteration 78, loss = 0.57630723\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64916201\n",
            "Iteration 2, loss = 0.62192399\n",
            "Iteration 3, loss = 0.61592519\n",
            "Iteration 4, loss = 0.61164623\n",
            "Iteration 5, loss = 0.61123798\n",
            "Iteration 6, loss = 0.61210229\n",
            "Iteration 7, loss = 0.61041182\n",
            "Iteration 8, loss = 0.59982653\n",
            "Iteration 9, loss = 0.60433450\n",
            "Iteration 10, loss = 0.60848855\n",
            "Iteration 11, loss = 0.60793301\n",
            "Iteration 12, loss = 0.59895948\n",
            "Iteration 13, loss = 0.60600469\n",
            "Iteration 14, loss = 0.60785404\n",
            "Iteration 15, loss = 0.59710865\n",
            "Iteration 16, loss = 0.59405653\n",
            "Iteration 17, loss = 0.59247024\n",
            "Iteration 18, loss = 0.58266661\n",
            "Iteration 19, loss = 0.58075648\n",
            "Iteration 20, loss = 0.59088419\n",
            "Iteration 21, loss = 0.57274112\n",
            "Iteration 22, loss = 0.58704748\n",
            "Iteration 23, loss = 0.58520987\n",
            "Iteration 24, loss = 0.59504260\n",
            "Iteration 25, loss = 0.57729209\n",
            "Iteration 26, loss = 0.59199513\n",
            "Iteration 27, loss = 0.57437245\n",
            "Iteration 28, loss = 0.57450225\n",
            "Iteration 29, loss = 0.58436765\n",
            "Iteration 30, loss = 0.57968022\n",
            "Iteration 31, loss = 0.58917771\n",
            "Iteration 32, loss = 0.57432215\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66292249\n",
            "Iteration 2, loss = 0.62462616\n",
            "Iteration 3, loss = 0.62348983\n",
            "Iteration 4, loss = 0.60826383\n",
            "Iteration 5, loss = 0.60491457\n",
            "Iteration 6, loss = 0.61551422\n",
            "Iteration 7, loss = 0.60370829\n",
            "Iteration 8, loss = 0.59862804\n",
            "Iteration 9, loss = 0.59370148\n",
            "Iteration 10, loss = 0.59477415\n",
            "Iteration 11, loss = 0.59336887\n",
            "Iteration 12, loss = 0.59095444\n",
            "Iteration 13, loss = 0.60874941\n",
            "Iteration 14, loss = 0.59221914\n",
            "Iteration 15, loss = 0.59127659\n",
            "Iteration 16, loss = 0.58623311\n",
            "Iteration 17, loss = 0.58201450\n",
            "Iteration 18, loss = 0.58888582\n",
            "Iteration 19, loss = 0.58455319\n",
            "Iteration 20, loss = 0.57535489\n",
            "Iteration 21, loss = 0.57492762\n",
            "Iteration 22, loss = 0.59248202\n",
            "Iteration 23, loss = 0.58121075\n",
            "Iteration 24, loss = 0.57090903\n",
            "Iteration 25, loss = 0.57864913\n",
            "Iteration 26, loss = 0.58675336\n",
            "Iteration 27, loss = 0.57157060\n",
            "Iteration 28, loss = 0.58633723\n",
            "Iteration 29, loss = 0.57613586\n",
            "Iteration 30, loss = 0.57372419\n",
            "Iteration 31, loss = 0.56320011\n",
            "Iteration 32, loss = 0.58011144\n",
            "Iteration 33, loss = 0.55725251\n",
            "Iteration 34, loss = 0.57210285\n",
            "Iteration 35, loss = 0.56745181\n",
            "Iteration 36, loss = 0.59160238\n",
            "Iteration 37, loss = 0.57394012\n",
            "Iteration 38, loss = 0.56769765\n",
            "Iteration 39, loss = 0.57765387\n",
            "Iteration 40, loss = 0.57419909\n",
            "Iteration 41, loss = 0.57233017\n",
            "Iteration 42, loss = 0.56829644\n",
            "Iteration 43, loss = 0.55834411\n",
            "Iteration 44, loss = 0.55771899\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.24795187\n",
            "Iteration 2, loss = 0.76132023\n",
            "Iteration 3, loss = 0.61309133\n",
            "Iteration 4, loss = 0.62341747\n",
            "Iteration 5, loss = 0.59565580\n",
            "Iteration 6, loss = 0.59333108\n",
            "Iteration 7, loss = 0.58863781\n",
            "Iteration 8, loss = 0.58660495\n",
            "Iteration 9, loss = 0.60011616\n",
            "Iteration 10, loss = 0.58138013\n",
            "Iteration 11, loss = 0.59219636\n",
            "Iteration 12, loss = 0.57813706\n",
            "Iteration 13, loss = 0.58832315\n",
            "Iteration 14, loss = 0.58588048\n",
            "Iteration 15, loss = 0.57617478\n",
            "Iteration 16, loss = 0.57961188\n",
            "Iteration 17, loss = 0.57852982\n",
            "Iteration 18, loss = 0.56872248\n",
            "Iteration 19, loss = 0.58658667\n",
            "Iteration 20, loss = 0.57861068\n",
            "Iteration 21, loss = 0.57096223\n",
            "Iteration 22, loss = 0.56269822\n",
            "Iteration 23, loss = 0.57032753\n",
            "Iteration 24, loss = 0.56724105\n",
            "Iteration 25, loss = 0.56245456\n",
            "Iteration 26, loss = 0.56542843\n",
            "Iteration 27, loss = 0.56267091\n",
            "Iteration 28, loss = 0.55180977\n",
            "Iteration 29, loss = 0.57712622\n",
            "Iteration 30, loss = 0.55175156\n",
            "Iteration 31, loss = 0.56326997\n",
            "Iteration 32, loss = 0.55401874\n",
            "Iteration 33, loss = 0.54872353\n",
            "Iteration 34, loss = 0.55704592\n",
            "Iteration 35, loss = 0.55953233\n",
            "Iteration 36, loss = 0.57022819\n",
            "Iteration 37, loss = 0.54600911\n",
            "Iteration 38, loss = 0.56048165\n",
            "Iteration 39, loss = 0.55158638\n",
            "Iteration 40, loss = 0.55474157\n",
            "Iteration 41, loss = 0.54284202\n",
            "Iteration 42, loss = 0.55007740\n",
            "Iteration 43, loss = 0.54277417\n",
            "Iteration 44, loss = 0.55194988\n",
            "Iteration 45, loss = 0.54261919\n",
            "Iteration 46, loss = 0.53117827\n",
            "Iteration 47, loss = 0.52040809\n",
            "Iteration 48, loss = 0.52365101\n",
            "Iteration 49, loss = 0.52250778\n",
            "Iteration 50, loss = 0.55051450\n",
            "Iteration 51, loss = 0.54718394\n",
            "Iteration 52, loss = 0.53186048\n",
            "Iteration 53, loss = 0.52287464\n",
            "Iteration 54, loss = 0.53756946\n",
            "Iteration 55, loss = 0.53758022\n",
            "Iteration 56, loss = 0.53710498\n",
            "Iteration 57, loss = 0.51205263\n",
            "Iteration 58, loss = 0.51503203\n",
            "Iteration 59, loss = 0.51226282\n",
            "Iteration 60, loss = 0.52757037\n",
            "Iteration 61, loss = 0.50248056\n",
            "Iteration 62, loss = 0.51790836\n",
            "Iteration 63, loss = 0.51103705\n",
            "Iteration 64, loss = 0.50790243\n",
            "Iteration 65, loss = 0.51035340\n",
            "Iteration 66, loss = 0.51100419\n",
            "Iteration 67, loss = 0.49540472\n",
            "Iteration 68, loss = 0.51717509\n",
            "Iteration 69, loss = 0.50071782\n",
            "Iteration 70, loss = 0.52191520\n",
            "Iteration 71, loss = 0.51569859\n",
            "Iteration 72, loss = 0.51528810\n",
            "Iteration 73, loss = 0.50598274\n",
            "Iteration 74, loss = 0.50150255\n",
            "Iteration 75, loss = 0.50519522\n",
            "Iteration 76, loss = 0.51785252\n",
            "Iteration 77, loss = 0.49358284\n",
            "Iteration 78, loss = 0.48384315\n",
            "Iteration 79, loss = 0.50822732\n",
            "Iteration 80, loss = 0.48930496\n",
            "Iteration 81, loss = 0.48459846\n",
            "Iteration 82, loss = 0.50493703\n",
            "Iteration 83, loss = 0.50342804\n",
            "Iteration 84, loss = 0.49664567\n",
            "Iteration 85, loss = 0.49809904\n",
            "Iteration 86, loss = 0.50589962\n",
            "Iteration 87, loss = 0.46770052\n",
            "Iteration 88, loss = 0.46824904\n",
            "Iteration 89, loss = 0.48291157\n",
            "Iteration 90, loss = 0.46652299\n",
            "Iteration 91, loss = 0.48331361\n",
            "Iteration 92, loss = 0.48535009\n",
            "Iteration 93, loss = 0.48820081\n",
            "Iteration 94, loss = 0.50462067\n",
            "Iteration 95, loss = 0.52675946\n",
            "Iteration 96, loss = 0.48589888\n",
            "Iteration 97, loss = 0.49381697\n",
            "Iteration 98, loss = 0.48992945\n",
            "Iteration 99, loss = 0.49279432\n",
            "Iteration 100, loss = 0.47928518\n",
            "Iteration 101, loss = 0.47028713\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.20044780\n",
            "Iteration 2, loss = 0.75619904\n",
            "Iteration 3, loss = 0.65286474\n",
            "Iteration 4, loss = 0.64006929\n",
            "Iteration 5, loss = 0.61463899\n",
            "Iteration 6, loss = 0.61934255\n",
            "Iteration 7, loss = 0.61184918\n",
            "Iteration 8, loss = 0.60241127\n",
            "Iteration 9, loss = 0.59580119\n",
            "Iteration 10, loss = 0.59264808\n",
            "Iteration 11, loss = 0.59984219\n",
            "Iteration 12, loss = 0.59976307\n",
            "Iteration 13, loss = 0.60297350\n",
            "Iteration 14, loss = 0.59265633\n",
            "Iteration 15, loss = 0.58403980\n",
            "Iteration 16, loss = 0.60151086\n",
            "Iteration 17, loss = 0.58019514\n",
            "Iteration 18, loss = 0.59164743\n",
            "Iteration 19, loss = 0.58885140\n",
            "Iteration 20, loss = 0.56968655\n",
            "Iteration 21, loss = 0.59983887\n",
            "Iteration 22, loss = 0.58168137\n",
            "Iteration 23, loss = 0.57655678\n",
            "Iteration 24, loss = 0.57923372\n",
            "Iteration 25, loss = 0.59280879\n",
            "Iteration 26, loss = 0.56778540\n",
            "Iteration 27, loss = 0.59139662\n",
            "Iteration 28, loss = 0.57746100\n",
            "Iteration 29, loss = 0.57472763\n",
            "Iteration 30, loss = 0.56415388\n",
            "Iteration 31, loss = 0.56485348\n",
            "Iteration 32, loss = 0.56357951\n",
            "Iteration 33, loss = 0.57527240\n",
            "Iteration 34, loss = 0.56299045\n",
            "Iteration 35, loss = 0.56389246\n",
            "Iteration 36, loss = 0.54646031\n",
            "Iteration 37, loss = 0.59756920\n",
            "Iteration 38, loss = 0.56659644\n",
            "Iteration 39, loss = 0.55749096\n",
            "Iteration 40, loss = 0.55726682\n",
            "Iteration 41, loss = 0.56458442\n",
            "Iteration 42, loss = 0.54904978\n",
            "Iteration 43, loss = 0.55414110\n",
            "Iteration 44, loss = 0.54549380\n",
            "Iteration 45, loss = 0.54351705\n",
            "Iteration 46, loss = 0.54907290\n",
            "Iteration 47, loss = 0.54537356\n",
            "Iteration 48, loss = 0.54477302\n",
            "Iteration 49, loss = 0.53810152\n",
            "Iteration 50, loss = 0.52721646\n",
            "Iteration 51, loss = 0.54192171\n",
            "Iteration 52, loss = 0.54205229\n",
            "Iteration 53, loss = 0.54164132\n",
            "Iteration 54, loss = 0.51678629\n",
            "Iteration 55, loss = 0.52820143\n",
            "Iteration 56, loss = 0.52241029\n",
            "Iteration 57, loss = 0.51883432\n",
            "Iteration 58, loss = 0.54811014\n",
            "Iteration 59, loss = 0.53450611\n",
            "Iteration 60, loss = 0.53565770\n",
            "Iteration 61, loss = 0.52420345\n",
            "Iteration 62, loss = 0.51914145\n",
            "Iteration 63, loss = 0.51655250\n",
            "Iteration 64, loss = 0.53354235\n",
            "Iteration 65, loss = 0.53029044\n",
            "Iteration 66, loss = 0.52264546\n",
            "Iteration 67, loss = 0.51038718\n",
            "Iteration 68, loss = 0.51551756\n",
            "Iteration 69, loss = 0.50770833\n",
            "Iteration 70, loss = 0.51657254\n",
            "Iteration 71, loss = 0.51956812\n",
            "Iteration 72, loss = 0.51629489\n",
            "Iteration 73, loss = 0.51876459\n",
            "Iteration 74, loss = 0.50759496\n",
            "Iteration 75, loss = 0.49477033\n",
            "Iteration 76, loss = 0.49826467\n",
            "Iteration 77, loss = 0.49499579\n",
            "Iteration 78, loss = 0.51025245\n",
            "Iteration 79, loss = 0.50984884\n",
            "Iteration 80, loss = 0.49912256\n",
            "Iteration 81, loss = 0.49345802\n",
            "Iteration 82, loss = 0.47998090\n",
            "Iteration 83, loss = 0.51065610\n",
            "Iteration 84, loss = 0.50544557\n",
            "Iteration 85, loss = 0.49232225\n",
            "Iteration 86, loss = 0.50626736\n",
            "Iteration 87, loss = 0.50224455\n",
            "Iteration 88, loss = 0.50371060\n",
            "Iteration 89, loss = 0.47847712\n",
            "Iteration 90, loss = 0.51135856\n",
            "Iteration 91, loss = 0.49478120\n",
            "Iteration 92, loss = 0.49079155\n",
            "Iteration 93, loss = 0.48016817\n",
            "Iteration 94, loss = 0.51707557\n",
            "Iteration 95, loss = 0.49334419\n",
            "Iteration 96, loss = 0.50347265\n",
            "Iteration 97, loss = 0.49079187\n",
            "Iteration 98, loss = 0.47772039\n",
            "Iteration 99, loss = 0.46396067\n",
            "Iteration 100, loss = 0.51985493\n",
            "Iteration 101, loss = 0.47936746\n",
            "Iteration 102, loss = 0.51075930\n",
            "Iteration 103, loss = 0.48502082\n",
            "Iteration 104, loss = 0.48157245\n",
            "Iteration 105, loss = 0.47570575\n",
            "Iteration 106, loss = 0.48854563\n",
            "Iteration 107, loss = 0.46385141\n",
            "Iteration 108, loss = 0.48889237\n",
            "Iteration 109, loss = 0.47906099\n",
            "Iteration 110, loss = 0.49747541\n",
            "Iteration 111, loss = 0.49807078\n",
            "Iteration 112, loss = 0.48217777\n",
            "Iteration 113, loss = 0.48337571\n",
            "Iteration 114, loss = 0.51207950\n",
            "Iteration 115, loss = 0.47471291\n",
            "Iteration 116, loss = 0.48638745\n",
            "Iteration 117, loss = 0.48427977\n",
            "Iteration 118, loss = 0.47653540\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.90806857\n",
            "Iteration 2, loss = 0.69676190\n",
            "Iteration 3, loss = 0.67379599\n",
            "Iteration 4, loss = 0.65658338\n",
            "Iteration 5, loss = 0.67522959\n",
            "Iteration 6, loss = 0.64469369\n",
            "Iteration 7, loss = 0.63819331\n",
            "Iteration 8, loss = 0.61584552\n",
            "Iteration 9, loss = 0.60999679\n",
            "Iteration 10, loss = 0.59306197\n",
            "Iteration 11, loss = 0.61446441\n",
            "Iteration 12, loss = 0.59016432\n",
            "Iteration 13, loss = 0.59458817\n",
            "Iteration 14, loss = 0.58436692\n",
            "Iteration 15, loss = 0.59735644\n",
            "Iteration 16, loss = 0.58249372\n",
            "Iteration 17, loss = 0.58603805\n",
            "Iteration 18, loss = 0.59022722\n",
            "Iteration 19, loss = 0.58837506\n",
            "Iteration 20, loss = 0.59008714\n",
            "Iteration 21, loss = 0.57932641\n",
            "Iteration 22, loss = 0.57731184\n",
            "Iteration 23, loss = 0.56547956\n",
            "Iteration 24, loss = 0.56409850\n",
            "Iteration 25, loss = 0.57202110\n",
            "Iteration 26, loss = 0.57501861\n",
            "Iteration 27, loss = 0.55872332\n",
            "Iteration 28, loss = 0.58582681\n",
            "Iteration 29, loss = 0.57899348\n",
            "Iteration 30, loss = 0.57339543\n",
            "Iteration 31, loss = 0.56754722\n",
            "Iteration 32, loss = 0.55251394\n",
            "Iteration 33, loss = 0.55137269\n",
            "Iteration 34, loss = 0.55069493\n",
            "Iteration 35, loss = 0.54444709\n",
            "Iteration 36, loss = 0.56417518\n",
            "Iteration 37, loss = 0.55056148\n",
            "Iteration 38, loss = 0.53879393\n",
            "Iteration 39, loss = 0.55060391\n",
            "Iteration 40, loss = 0.54035892\n",
            "Iteration 41, loss = 0.54991094\n",
            "Iteration 42, loss = 0.54272997\n",
            "Iteration 43, loss = 0.53986428\n",
            "Iteration 44, loss = 0.54300340\n",
            "Iteration 45, loss = 0.53216912\n",
            "Iteration 46, loss = 0.55652699\n",
            "Iteration 47, loss = 0.52811713\n",
            "Iteration 48, loss = 0.53900711\n",
            "Iteration 49, loss = 0.52396119\n",
            "Iteration 50, loss = 0.53208464\n",
            "Iteration 51, loss = 0.52033242\n",
            "Iteration 52, loss = 0.52018971\n",
            "Iteration 53, loss = 0.52228672\n",
            "Iteration 54, loss = 0.51265902\n",
            "Iteration 55, loss = 0.50496634\n",
            "Iteration 56, loss = 0.52555442\n",
            "Iteration 57, loss = 0.52020276\n",
            "Iteration 58, loss = 0.52463569\n",
            "Iteration 59, loss = 0.53109425\n",
            "Iteration 60, loss = 0.52089101\n",
            "Iteration 61, loss = 0.52639469\n",
            "Iteration 62, loss = 0.53701875\n",
            "Iteration 63, loss = 0.50996747\n",
            "Iteration 64, loss = 0.51225353\n",
            "Iteration 65, loss = 0.50004955\n",
            "Iteration 66, loss = 0.52022366\n",
            "Iteration 67, loss = 0.50606629\n",
            "Iteration 68, loss = 0.50452509\n",
            "Iteration 69, loss = 0.50481961\n",
            "Iteration 70, loss = 0.49160030\n",
            "Iteration 71, loss = 0.50354352\n",
            "Iteration 72, loss = 0.49809216\n",
            "Iteration 73, loss = 0.50448527\n",
            "Iteration 74, loss = 0.49427489\n",
            "Iteration 75, loss = 0.49455465\n",
            "Iteration 76, loss = 0.50727396\n",
            "Iteration 77, loss = 0.48806427\n",
            "Iteration 78, loss = 0.48065073\n",
            "Iteration 79, loss = 0.50869382\n",
            "Iteration 80, loss = 0.50506252\n",
            "Iteration 81, loss = 0.50021342\n",
            "Iteration 82, loss = 0.50883346\n",
            "Iteration 83, loss = 0.48924619\n",
            "Iteration 84, loss = 0.49454868\n",
            "Iteration 85, loss = 0.49128612\n",
            "Iteration 86, loss = 0.48863175\n",
            "Iteration 87, loss = 0.48860593\n",
            "Iteration 88, loss = 0.48108736\n",
            "Iteration 89, loss = 0.48091083\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.90865443\n",
            "Iteration 2, loss = 0.91332037\n",
            "Iteration 3, loss = 0.68948489\n",
            "Iteration 4, loss = 0.76480601\n",
            "Iteration 5, loss = 0.64593569\n",
            "Iteration 6, loss = 0.62545078\n",
            "Iteration 7, loss = 0.61378620\n",
            "Iteration 8, loss = 0.64710031\n",
            "Iteration 9, loss = 0.60339180\n",
            "Iteration 10, loss = 0.61434192\n",
            "Iteration 11, loss = 0.63062483\n",
            "Iteration 12, loss = 0.60480298\n",
            "Iteration 13, loss = 0.59800310\n",
            "Iteration 14, loss = 0.60061700\n",
            "Iteration 15, loss = 0.60040633\n",
            "Iteration 16, loss = 0.59181017\n",
            "Iteration 17, loss = 0.58708896\n",
            "Iteration 18, loss = 0.59808716\n",
            "Iteration 19, loss = 0.58490029\n",
            "Iteration 20, loss = 0.59129075\n",
            "Iteration 21, loss = 0.59056166\n",
            "Iteration 22, loss = 0.58008338\n",
            "Iteration 23, loss = 0.57871056\n",
            "Iteration 24, loss = 0.60290356\n",
            "Iteration 25, loss = 0.58379250\n",
            "Iteration 26, loss = 0.58191804\n",
            "Iteration 27, loss = 0.57709157\n",
            "Iteration 28, loss = 0.57876380\n",
            "Iteration 29, loss = 0.56745866\n",
            "Iteration 30, loss = 0.57032242\n",
            "Iteration 31, loss = 0.57522181\n",
            "Iteration 32, loss = 0.57778344\n",
            "Iteration 33, loss = 0.56657985\n",
            "Iteration 34, loss = 0.55642438\n",
            "Iteration 35, loss = 0.55747986\n",
            "Iteration 36, loss = 0.55401771\n",
            "Iteration 37, loss = 0.55659937\n",
            "Iteration 38, loss = 0.54827015\n",
            "Iteration 39, loss = 0.54639929\n",
            "Iteration 40, loss = 0.55322383\n",
            "Iteration 41, loss = 0.57798735\n",
            "Iteration 42, loss = 0.56182209\n",
            "Iteration 43, loss = 0.55483026\n",
            "Iteration 44, loss = 0.53743271\n",
            "Iteration 45, loss = 0.55463866\n",
            "Iteration 46, loss = 0.53021146\n",
            "Iteration 47, loss = 0.53452792\n",
            "Iteration 48, loss = 0.53766814\n",
            "Iteration 49, loss = 0.52968525\n",
            "Iteration 50, loss = 0.52325541\n",
            "Iteration 51, loss = 0.54278150\n",
            "Iteration 52, loss = 0.55108351\n",
            "Iteration 53, loss = 0.53768363\n",
            "Iteration 54, loss = 0.54680625\n",
            "Iteration 55, loss = 0.54348343\n",
            "Iteration 56, loss = 0.53166863\n",
            "Iteration 57, loss = 0.52440255\n",
            "Iteration 58, loss = 0.53224776\n",
            "Iteration 59, loss = 0.52106069\n",
            "Iteration 60, loss = 0.53084048\n",
            "Iteration 61, loss = 0.51770610\n",
            "Iteration 62, loss = 0.52060996\n",
            "Iteration 63, loss = 0.53151940\n",
            "Iteration 64, loss = 0.51713739\n",
            "Iteration 65, loss = 0.51698794\n",
            "Iteration 66, loss = 0.51243665\n",
            "Iteration 67, loss = 0.51020414\n",
            "Iteration 68, loss = 0.50808266\n",
            "Iteration 69, loss = 0.51442464\n",
            "Iteration 70, loss = 0.51691428\n",
            "Iteration 71, loss = 0.50987097\n",
            "Iteration 72, loss = 0.51468993\n",
            "Iteration 73, loss = 0.50733633\n",
            "Iteration 74, loss = 0.51546898\n",
            "Iteration 75, loss = 0.51049355\n",
            "Iteration 76, loss = 0.50811523\n",
            "Iteration 77, loss = 0.50419578\n",
            "Iteration 78, loss = 0.50715443\n",
            "Iteration 79, loss = 0.49662660\n",
            "Iteration 80, loss = 0.50423468\n",
            "Iteration 81, loss = 0.51045161\n",
            "Iteration 82, loss = 0.50707343\n",
            "Iteration 83, loss = 0.50213428\n",
            "Iteration 84, loss = 0.47356891\n",
            "Iteration 85, loss = 0.50427152\n",
            "Iteration 86, loss = 0.51221906\n",
            "Iteration 87, loss = 0.50242814\n",
            "Iteration 88, loss = 0.50277232\n",
            "Iteration 89, loss = 0.49296964\n",
            "Iteration 90, loss = 0.50511766\n",
            "Iteration 91, loss = 0.50366008\n",
            "Iteration 92, loss = 0.50855620\n",
            "Iteration 93, loss = 0.49094836\n",
            "Iteration 94, loss = 0.50545107\n",
            "Iteration 95, loss = 0.48667911\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.02525964\n",
            "Iteration 2, loss = 0.71737115\n",
            "Iteration 3, loss = 0.67490052\n",
            "Iteration 4, loss = 0.65379451\n",
            "Iteration 5, loss = 0.63922088\n",
            "Iteration 6, loss = 0.61982797\n",
            "Iteration 7, loss = 0.62184452\n",
            "Iteration 8, loss = 0.63861202\n",
            "Iteration 9, loss = 0.61972261\n",
            "Iteration 10, loss = 0.60145345\n",
            "Iteration 11, loss = 0.60469682\n",
            "Iteration 12, loss = 0.61474945\n",
            "Iteration 13, loss = 0.59814712\n",
            "Iteration 14, loss = 0.59799391\n",
            "Iteration 15, loss = 0.59935239\n",
            "Iteration 16, loss = 0.58260635\n",
            "Iteration 17, loss = 0.59443963\n",
            "Iteration 18, loss = 0.59128905\n",
            "Iteration 19, loss = 0.59790263\n",
            "Iteration 20, loss = 0.59761378\n",
            "Iteration 21, loss = 0.58782252\n",
            "Iteration 22, loss = 0.58452098\n",
            "Iteration 23, loss = 0.58662487\n",
            "Iteration 24, loss = 0.59093726\n",
            "Iteration 25, loss = 0.58778239\n",
            "Iteration 26, loss = 0.57553202\n",
            "Iteration 27, loss = 0.58323096\n",
            "Iteration 28, loss = 0.56913493\n",
            "Iteration 29, loss = 0.57970176\n",
            "Iteration 30, loss = 0.58102653\n",
            "Iteration 31, loss = 0.55693102\n",
            "Iteration 32, loss = 0.56392162\n",
            "Iteration 33, loss = 0.56126037\n",
            "Iteration 34, loss = 0.58241878\n",
            "Iteration 35, loss = 0.56951273\n",
            "Iteration 36, loss = 0.55862767\n",
            "Iteration 37, loss = 0.57449165\n",
            "Iteration 38, loss = 0.56921912\n",
            "Iteration 39, loss = 0.55642553\n",
            "Iteration 40, loss = 0.58204391\n",
            "Iteration 41, loss = 0.56314375\n",
            "Iteration 42, loss = 0.54432198\n",
            "Iteration 43, loss = 0.55795225\n",
            "Iteration 44, loss = 0.54449936\n",
            "Iteration 45, loss = 0.55353505\n",
            "Iteration 46, loss = 0.53485033\n",
            "Iteration 47, loss = 0.55360584\n",
            "Iteration 48, loss = 0.54868764\n",
            "Iteration 49, loss = 0.55349570\n",
            "Iteration 50, loss = 0.54050307\n",
            "Iteration 51, loss = 0.55756603\n",
            "Iteration 52, loss = 0.55237084\n",
            "Iteration 53, loss = 0.54084162\n",
            "Iteration 54, loss = 0.53428905\n",
            "Iteration 55, loss = 0.52878352\n",
            "Iteration 56, loss = 0.54873323\n",
            "Iteration 57, loss = 0.52814882\n",
            "Iteration 58, loss = 0.53095732\n",
            "Iteration 59, loss = 0.54329744\n",
            "Iteration 60, loss = 0.53413487\n",
            "Iteration 61, loss = 0.55173298\n",
            "Iteration 62, loss = 0.53084784\n",
            "Iteration 63, loss = 0.52110913\n",
            "Iteration 64, loss = 0.51690423\n",
            "Iteration 65, loss = 0.56601774\n",
            "Iteration 66, loss = 0.52498990\n",
            "Iteration 67, loss = 0.52749254\n",
            "Iteration 68, loss = 0.51481153\n",
            "Iteration 69, loss = 0.52032279\n",
            "Iteration 70, loss = 0.51237353\n",
            "Iteration 71, loss = 0.51547876\n",
            "Iteration 72, loss = 0.52853984\n",
            "Iteration 73, loss = 0.53570822\n",
            "Iteration 74, loss = 0.51420230\n",
            "Iteration 75, loss = 0.52180827\n",
            "Iteration 76, loss = 0.52337494\n",
            "Iteration 77, loss = 0.50214609\n",
            "Iteration 78, loss = 0.51161828\n",
            "Iteration 79, loss = 0.53315990\n",
            "Iteration 80, loss = 0.53139864\n",
            "Iteration 81, loss = 0.50966911\n",
            "Iteration 82, loss = 0.50848836\n",
            "Iteration 83, loss = 0.51185570\n",
            "Iteration 84, loss = 0.50949853\n",
            "Iteration 85, loss = 0.52223400\n",
            "Iteration 86, loss = 0.51317813\n",
            "Iteration 87, loss = 0.51228813\n",
            "Iteration 88, loss = 0.51803020\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.00829623\n",
            "Iteration 2, loss = 0.65131206\n",
            "Iteration 3, loss = 0.62246128\n",
            "Iteration 4, loss = 0.62429631\n",
            "Iteration 5, loss = 0.59426741\n",
            "Iteration 6, loss = 0.60698290\n",
            "Iteration 7, loss = 0.58492007\n",
            "Iteration 8, loss = 0.59083949\n",
            "Iteration 9, loss = 0.61109328\n",
            "Iteration 10, loss = 0.58875601\n",
            "Iteration 11, loss = 0.58739329\n",
            "Iteration 12, loss = 0.59196210\n",
            "Iteration 13, loss = 0.57228485\n",
            "Iteration 14, loss = 0.57196461\n",
            "Iteration 15, loss = 0.56559040\n",
            "Iteration 16, loss = 0.56755951\n",
            "Iteration 17, loss = 0.56637463\n",
            "Iteration 18, loss = 0.57218421\n",
            "Iteration 19, loss = 0.57609394\n",
            "Iteration 20, loss = 0.55086910\n",
            "Iteration 21, loss = 0.55533893\n",
            "Iteration 22, loss = 0.56835511\n",
            "Iteration 23, loss = 0.56145825\n",
            "Iteration 24, loss = 0.58749440\n",
            "Iteration 25, loss = 0.56029746\n",
            "Iteration 26, loss = 0.55775113\n",
            "Iteration 27, loss = 0.56344362\n",
            "Iteration 28, loss = 0.55856638\n",
            "Iteration 29, loss = 0.54247638\n",
            "Iteration 30, loss = 0.56691298\n",
            "Iteration 31, loss = 0.55178626\n",
            "Iteration 32, loss = 0.57759356\n",
            "Iteration 33, loss = 0.54900890\n",
            "Iteration 34, loss = 0.54538980\n",
            "Iteration 35, loss = 0.54076926\n",
            "Iteration 36, loss = 0.54927548\n",
            "Iteration 37, loss = 0.53774841\n",
            "Iteration 38, loss = 0.52995870\n",
            "Iteration 39, loss = 0.53790273\n",
            "Iteration 40, loss = 0.53847228\n",
            "Iteration 41, loss = 0.52896793\n",
            "Iteration 42, loss = 0.52615250\n",
            "Iteration 43, loss = 0.54194028\n",
            "Iteration 44, loss = 0.51897384\n",
            "Iteration 45, loss = 0.51336401\n",
            "Iteration 46, loss = 0.51017076\n",
            "Iteration 47, loss = 0.52533047\n",
            "Iteration 48, loss = 0.51745398\n",
            "Iteration 49, loss = 0.51023314\n",
            "Iteration 50, loss = 0.51483973\n",
            "Iteration 51, loss = 0.51346741\n",
            "Iteration 52, loss = 0.50930343\n",
            "Iteration 53, loss = 0.49874278\n",
            "Iteration 54, loss = 0.51214348\n",
            "Iteration 55, loss = 0.53323446\n",
            "Iteration 56, loss = 0.52189621\n",
            "Iteration 57, loss = 0.50124836\n",
            "Iteration 58, loss = 0.51634829\n",
            "Iteration 59, loss = 0.50380628\n",
            "Iteration 60, loss = 0.48878570\n",
            "Iteration 61, loss = 0.50158994\n",
            "Iteration 62, loss = 0.51093417\n",
            "Iteration 63, loss = 0.51061459\n",
            "Iteration 64, loss = 0.51605728\n",
            "Iteration 65, loss = 0.50535176\n",
            "Iteration 66, loss = 0.50109140\n",
            "Iteration 67, loss = 0.48634563\n",
            "Iteration 68, loss = 0.49372565\n",
            "Iteration 69, loss = 0.49229873\n",
            "Iteration 70, loss = 0.50092355\n",
            "Iteration 71, loss = 0.52059584\n",
            "Iteration 72, loss = 0.48618986\n",
            "Iteration 73, loss = 0.49410978\n",
            "Iteration 74, loss = 0.47310098\n",
            "Iteration 75, loss = 0.48009488\n",
            "Iteration 76, loss = 0.52882687\n",
            "Iteration 77, loss = 0.49309032\n",
            "Iteration 78, loss = 0.50786199\n",
            "Iteration 79, loss = 0.46622331\n",
            "Iteration 80, loss = 0.50255923\n",
            "Iteration 81, loss = 0.51063443\n",
            "Iteration 82, loss = 0.48769298\n",
            "Iteration 83, loss = 0.49665568\n",
            "Iteration 84, loss = 0.50396886\n",
            "Iteration 85, loss = 0.48590191\n",
            "Iteration 86, loss = 0.48146318\n",
            "Iteration 87, loss = 0.50658134\n",
            "Iteration 88, loss = 0.49969900\n",
            "Iteration 89, loss = 0.48590776\n",
            "Iteration 90, loss = 0.49927126\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.80298505\n",
            "Iteration 2, loss = 1.07104753\n",
            "Iteration 3, loss = 0.62049360\n",
            "Iteration 4, loss = 0.61081014\n",
            "Iteration 5, loss = 0.61310139\n",
            "Iteration 6, loss = 0.61461471\n",
            "Iteration 7, loss = 0.61122224\n",
            "Iteration 8, loss = 0.60028005\n",
            "Iteration 9, loss = 0.58872964\n",
            "Iteration 10, loss = 0.59326818\n",
            "Iteration 11, loss = 0.59205680\n",
            "Iteration 12, loss = 0.59397856\n",
            "Iteration 13, loss = 0.58504751\n",
            "Iteration 14, loss = 0.58119995\n",
            "Iteration 15, loss = 0.58264655\n",
            "Iteration 16, loss = 0.59164013\n",
            "Iteration 17, loss = 0.59773690\n",
            "Iteration 18, loss = 0.60196878\n",
            "Iteration 19, loss = 0.58377054\n",
            "Iteration 20, loss = 0.57479790\n",
            "Iteration 21, loss = 0.58953526\n",
            "Iteration 22, loss = 0.57469411\n",
            "Iteration 23, loss = 0.57640880\n",
            "Iteration 24, loss = 0.56935230\n",
            "Iteration 25, loss = 0.56549159\n",
            "Iteration 26, loss = 0.57000592\n",
            "Iteration 27, loss = 0.57156480\n",
            "Iteration 28, loss = 0.55991764\n",
            "Iteration 29, loss = 0.57203718\n",
            "Iteration 30, loss = 0.57845020\n",
            "Iteration 31, loss = 0.56124786\n",
            "Iteration 32, loss = 0.57313027\n",
            "Iteration 33, loss = 0.56155505\n",
            "Iteration 34, loss = 0.55730276\n",
            "Iteration 35, loss = 0.55494549\n",
            "Iteration 36, loss = 0.56054965\n",
            "Iteration 37, loss = 0.56330853\n",
            "Iteration 38, loss = 0.55011051\n",
            "Iteration 39, loss = 0.54750947\n",
            "Iteration 40, loss = 0.54975806\n",
            "Iteration 41, loss = 0.54479783\n",
            "Iteration 42, loss = 0.55545144\n",
            "Iteration 43, loss = 0.55470537\n",
            "Iteration 44, loss = 0.54032439\n",
            "Iteration 45, loss = 0.54181787\n",
            "Iteration 46, loss = 0.54423397\n",
            "Iteration 47, loss = 0.53720371\n",
            "Iteration 48, loss = 0.55413213\n",
            "Iteration 49, loss = 0.54836495\n",
            "Iteration 50, loss = 0.52697977\n",
            "Iteration 51, loss = 0.54425401\n",
            "Iteration 52, loss = 0.53270737\n",
            "Iteration 53, loss = 0.52741460\n",
            "Iteration 54, loss = 0.53497896\n",
            "Iteration 55, loss = 0.54875375\n",
            "Iteration 56, loss = 0.53691869\n",
            "Iteration 57, loss = 0.52493000\n",
            "Iteration 58, loss = 0.56317745\n",
            "Iteration 59, loss = 0.54406809\n",
            "Iteration 60, loss = 0.54952015\n",
            "Iteration 61, loss = 0.52938949\n",
            "Iteration 62, loss = 0.52436737\n",
            "Iteration 63, loss = 0.51806859\n",
            "Iteration 64, loss = 0.51602056\n",
            "Iteration 65, loss = 0.52511036\n",
            "Iteration 66, loss = 0.52927729\n",
            "Iteration 67, loss = 0.54498624\n",
            "Iteration 68, loss = 0.53320536\n",
            "Iteration 69, loss = 0.52869969\n",
            "Iteration 70, loss = 0.51390627\n",
            "Iteration 71, loss = 0.50844744\n",
            "Iteration 72, loss = 0.51146522\n",
            "Iteration 73, loss = 0.52111932\n",
            "Iteration 74, loss = 0.52407724\n",
            "Iteration 75, loss = 0.52789870\n",
            "Iteration 76, loss = 0.52305674\n",
            "Iteration 77, loss = 0.52071487\n",
            "Iteration 78, loss = 0.51875246\n",
            "Iteration 79, loss = 0.50391403\n",
            "Iteration 80, loss = 0.54602197\n",
            "Iteration 81, loss = 0.53414566\n",
            "Iteration 82, loss = 0.52592956\n",
            "Iteration 83, loss = 0.52358431\n",
            "Iteration 84, loss = 0.53607614\n",
            "Iteration 85, loss = 0.52440760\n",
            "Iteration 86, loss = 0.51908015\n",
            "Iteration 87, loss = 0.50598650\n",
            "Iteration 88, loss = 0.50788847\n",
            "Iteration 89, loss = 0.51782783\n",
            "Iteration 90, loss = 0.49744736\n",
            "Iteration 91, loss = 0.50720755\n",
            "Iteration 92, loss = 0.50208684\n",
            "Iteration 93, loss = 0.50260727\n",
            "Iteration 94, loss = 0.49839854\n",
            "Iteration 95, loss = 0.56499332\n",
            "Iteration 96, loss = 0.53025088\n",
            "Iteration 97, loss = 0.50943828\n",
            "Iteration 98, loss = 0.50983268\n",
            "Iteration 99, loss = 0.50130075\n",
            "Iteration 100, loss = 0.50540952\n",
            "Iteration 101, loss = 0.50165107\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.20669828\n",
            "Iteration 2, loss = 0.81231495\n",
            "Iteration 3, loss = 0.64758440\n",
            "Iteration 4, loss = 0.65403855\n",
            "Iteration 5, loss = 0.61929760\n",
            "Iteration 6, loss = 0.62299746\n",
            "Iteration 7, loss = 0.62084081\n",
            "Iteration 8, loss = 0.60530497\n",
            "Iteration 9, loss = 0.60194723\n",
            "Iteration 10, loss = 0.61616659\n",
            "Iteration 11, loss = 0.61162174\n",
            "Iteration 12, loss = 0.58395658\n",
            "Iteration 13, loss = 0.58968878\n",
            "Iteration 14, loss = 0.60644208\n",
            "Iteration 15, loss = 0.59029003\n",
            "Iteration 16, loss = 0.58446320\n",
            "Iteration 17, loss = 0.57713628\n",
            "Iteration 18, loss = 0.58481840\n",
            "Iteration 19, loss = 0.58553671\n",
            "Iteration 20, loss = 0.58145845\n",
            "Iteration 21, loss = 0.58370469\n",
            "Iteration 22, loss = 0.57404167\n",
            "Iteration 23, loss = 0.57558995\n",
            "Iteration 24, loss = 0.58949830\n",
            "Iteration 25, loss = 0.58496204\n",
            "Iteration 26, loss = 0.57723606\n",
            "Iteration 27, loss = 0.56452306\n",
            "Iteration 28, loss = 0.56392381\n",
            "Iteration 29, loss = 0.56631098\n",
            "Iteration 30, loss = 0.56681956\n",
            "Iteration 31, loss = 0.56013976\n",
            "Iteration 32, loss = 0.55097238\n",
            "Iteration 33, loss = 0.55925057\n",
            "Iteration 34, loss = 0.55549544\n",
            "Iteration 35, loss = 0.55450229\n",
            "Iteration 36, loss = 0.54643023\n",
            "Iteration 37, loss = 0.54308298\n",
            "Iteration 38, loss = 0.55149184\n",
            "Iteration 39, loss = 0.55650681\n",
            "Iteration 40, loss = 0.53842996\n",
            "Iteration 41, loss = 0.54098084\n",
            "Iteration 42, loss = 0.55248769\n",
            "Iteration 43, loss = 0.53297885\n",
            "Iteration 44, loss = 0.53516902\n",
            "Iteration 45, loss = 0.52584220\n",
            "Iteration 46, loss = 0.54372531\n",
            "Iteration 47, loss = 0.53894524\n",
            "Iteration 48, loss = 0.53232758\n",
            "Iteration 49, loss = 0.51512099\n",
            "Iteration 50, loss = 0.51547217\n",
            "Iteration 51, loss = 0.52564277\n",
            "Iteration 52, loss = 0.52468438\n",
            "Iteration 53, loss = 0.53197247\n",
            "Iteration 54, loss = 0.52261649\n",
            "Iteration 55, loss = 0.52265216\n",
            "Iteration 56, loss = 0.51065426\n",
            "Iteration 57, loss = 0.52257544\n",
            "Iteration 58, loss = 0.53915910\n",
            "Iteration 59, loss = 0.52161739\n",
            "Iteration 60, loss = 0.53386799\n",
            "Iteration 61, loss = 0.52507359\n",
            "Iteration 62, loss = 0.52524813\n",
            "Iteration 63, loss = 0.52106983\n",
            "Iteration 64, loss = 0.49962317\n",
            "Iteration 65, loss = 0.51864736\n",
            "Iteration 66, loss = 0.51374140\n",
            "Iteration 67, loss = 0.50743617\n",
            "Iteration 68, loss = 0.51529054\n",
            "Iteration 69, loss = 0.50599472\n",
            "Iteration 70, loss = 0.50852953\n",
            "Iteration 71, loss = 0.49827891\n",
            "Iteration 72, loss = 0.49413931\n",
            "Iteration 73, loss = 0.49432501\n",
            "Iteration 74, loss = 0.51113205\n",
            "Iteration 75, loss = 0.49550866\n",
            "Iteration 76, loss = 0.51139338\n",
            "Iteration 77, loss = 0.49538780\n",
            "Iteration 78, loss = 0.50172868\n",
            "Iteration 79, loss = 0.48916266\n",
            "Iteration 80, loss = 0.49634322\n",
            "Iteration 81, loss = 0.48555630\n",
            "Iteration 82, loss = 0.53640848\n",
            "Iteration 83, loss = 0.51328525\n",
            "Iteration 84, loss = 0.49509162\n",
            "Iteration 85, loss = 0.48509325\n",
            "Iteration 86, loss = 0.48332120\n",
            "Iteration 87, loss = 0.49393607\n",
            "Iteration 88, loss = 0.49855714\n",
            "Iteration 89, loss = 0.48409759\n",
            "Iteration 90, loss = 0.47922547\n",
            "Iteration 91, loss = 0.47946896\n",
            "Iteration 92, loss = 0.49652729\n",
            "Iteration 93, loss = 0.48107358\n",
            "Iteration 94, loss = 0.47618968\n",
            "Iteration 95, loss = 0.50238929\n",
            "Iteration 96, loss = 0.47190842\n",
            "Iteration 97, loss = 0.52086369\n",
            "Iteration 98, loss = 0.48527029\n",
            "Iteration 99, loss = 0.48113491\n",
            "Iteration 100, loss = 0.48290655\n",
            "Iteration 101, loss = 0.50711545\n",
            "Iteration 102, loss = 0.52088482\n",
            "Iteration 103, loss = 0.49676886\n",
            "Iteration 104, loss = 0.48012507\n",
            "Iteration 105, loss = 0.47118251\n",
            "Iteration 106, loss = 0.47433652\n",
            "Iteration 107, loss = 0.48168244\n",
            "Iteration 108, loss = 0.48616940\n",
            "Iteration 109, loss = 0.47447680\n",
            "Iteration 110, loss = 0.46494111\n",
            "Iteration 111, loss = 0.47834344\n",
            "Iteration 112, loss = 0.48166001\n",
            "Iteration 113, loss = 0.46463489\n",
            "Iteration 114, loss = 0.47058004\n",
            "Iteration 115, loss = 0.48335873\n",
            "Iteration 116, loss = 0.53356752\n",
            "Iteration 117, loss = 0.48498039\n",
            "Iteration 118, loss = 0.46733667\n",
            "Iteration 119, loss = 0.47110415\n",
            "Iteration 120, loss = 0.46795575\n",
            "Iteration 121, loss = 0.48936712\n",
            "Iteration 122, loss = 0.47478385\n",
            "Iteration 123, loss = 0.47086330\n",
            "Iteration 124, loss = 0.47654965\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.94647415\n",
            "Iteration 2, loss = 0.85635993\n",
            "Iteration 3, loss = 0.67219580\n",
            "Iteration 4, loss = 0.67616586\n",
            "Iteration 5, loss = 0.70041087\n",
            "Iteration 6, loss = 0.63800408\n",
            "Iteration 7, loss = 0.61766127\n",
            "Iteration 8, loss = 0.62443404\n",
            "Iteration 9, loss = 0.61213727\n",
            "Iteration 10, loss = 0.61253729\n",
            "Iteration 11, loss = 0.61174325\n",
            "Iteration 12, loss = 0.60973149\n",
            "Iteration 13, loss = 0.61913384\n",
            "Iteration 14, loss = 0.60929830\n",
            "Iteration 15, loss = 0.60448241\n",
            "Iteration 16, loss = 0.59996595\n",
            "Iteration 17, loss = 0.60650438\n",
            "Iteration 18, loss = 0.60066322\n",
            "Iteration 19, loss = 0.59924599\n",
            "Iteration 20, loss = 0.59512171\n",
            "Iteration 21, loss = 0.59630921\n",
            "Iteration 22, loss = 0.59053991\n",
            "Iteration 23, loss = 0.59223695\n",
            "Iteration 24, loss = 0.59079537\n",
            "Iteration 25, loss = 0.58818010\n",
            "Iteration 26, loss = 0.58587488\n",
            "Iteration 27, loss = 0.59378202\n",
            "Iteration 28, loss = 0.58447575\n",
            "Iteration 29, loss = 0.59424868\n",
            "Iteration 30, loss = 0.58564131\n",
            "Iteration 31, loss = 0.58261081\n",
            "Iteration 32, loss = 0.57939371\n",
            "Iteration 33, loss = 0.58025022\n",
            "Iteration 34, loss = 0.58131952\n",
            "Iteration 35, loss = 0.58635779\n",
            "Iteration 36, loss = 0.57418752\n",
            "Iteration 37, loss = 0.58052240\n",
            "Iteration 38, loss = 0.58007743\n",
            "Iteration 39, loss = 0.57713783\n",
            "Iteration 40, loss = 0.59144666\n",
            "Iteration 41, loss = 0.57055998\n",
            "Iteration 42, loss = 0.57804057\n",
            "Iteration 43, loss = 0.57069067\n",
            "Iteration 44, loss = 0.56238418\n",
            "Iteration 45, loss = 0.55943694\n",
            "Iteration 46, loss = 0.55046912\n",
            "Iteration 47, loss = 0.55893146\n",
            "Iteration 48, loss = 0.55797372\n",
            "Iteration 49, loss = 0.56959448\n",
            "Iteration 50, loss = 0.55871794\n",
            "Iteration 51, loss = 0.54925831\n",
            "Iteration 52, loss = 0.55180440\n",
            "Iteration 53, loss = 0.56223074\n",
            "Iteration 54, loss = 0.54099851\n",
            "Iteration 55, loss = 0.54541449\n",
            "Iteration 56, loss = 0.55361577\n",
            "Iteration 57, loss = 0.54426351\n",
            "Iteration 58, loss = 0.55934460\n",
            "Iteration 59, loss = 0.53506491\n",
            "Iteration 60, loss = 0.53606289\n",
            "Iteration 61, loss = 0.54761819\n",
            "Iteration 62, loss = 0.53847289\n",
            "Iteration 63, loss = 0.53321421\n",
            "Iteration 64, loss = 0.52879624\n",
            "Iteration 65, loss = 0.52739912\n",
            "Iteration 66, loss = 0.53710731\n",
            "Iteration 67, loss = 0.52218918\n",
            "Iteration 68, loss = 0.51710741\n",
            "Iteration 69, loss = 0.53286320\n",
            "Iteration 70, loss = 0.52192431\n",
            "Iteration 71, loss = 0.52744236\n",
            "Iteration 72, loss = 0.53068526\n",
            "Iteration 73, loss = 0.53007249\n",
            "Iteration 74, loss = 0.51066827\n",
            "Iteration 75, loss = 0.51295744\n",
            "Iteration 76, loss = 0.50770860\n",
            "Iteration 77, loss = 0.51447678\n",
            "Iteration 78, loss = 0.51989086\n",
            "Iteration 79, loss = 0.50795870\n",
            "Iteration 80, loss = 0.52787783\n",
            "Iteration 81, loss = 0.50291226\n",
            "Iteration 82, loss = 0.51885756\n",
            "Iteration 83, loss = 0.53743138\n",
            "Iteration 84, loss = 0.51088162\n",
            "Iteration 85, loss = 0.50403596\n",
            "Iteration 86, loss = 0.50401669\n",
            "Iteration 87, loss = 0.50922846\n",
            "Iteration 88, loss = 0.51137806\n",
            "Iteration 89, loss = 0.51608195\n",
            "Iteration 90, loss = 0.51081849\n",
            "Iteration 91, loss = 0.51787172\n",
            "Iteration 92, loss = 0.50849272\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75621590\n",
            "Iteration 2, loss = 0.75134220\n",
            "Iteration 3, loss = 0.79733325\n",
            "Iteration 4, loss = 0.65816218\n",
            "Iteration 5, loss = 0.63560317\n",
            "Iteration 6, loss = 0.63955375\n",
            "Iteration 7, loss = 0.63331258\n",
            "Iteration 8, loss = 0.60352237\n",
            "Iteration 9, loss = 0.60138251\n",
            "Iteration 10, loss = 0.59104331\n",
            "Iteration 11, loss = 0.60098748\n",
            "Iteration 12, loss = 0.58605086\n",
            "Iteration 13, loss = 0.59237914\n",
            "Iteration 14, loss = 0.61839237\n",
            "Iteration 15, loss = 0.59226926\n",
            "Iteration 16, loss = 0.60492170\n",
            "Iteration 17, loss = 0.58916147\n",
            "Iteration 18, loss = 0.58488175\n",
            "Iteration 19, loss = 0.58621239\n",
            "Iteration 20, loss = 0.60412641\n",
            "Iteration 21, loss = 0.59401297\n",
            "Iteration 22, loss = 0.58212621\n",
            "Iteration 23, loss = 0.60324514\n",
            "Iteration 24, loss = 0.57576992\n",
            "Iteration 25, loss = 0.57928251\n",
            "Iteration 26, loss = 0.58149498\n",
            "Iteration 27, loss = 0.57241260\n",
            "Iteration 28, loss = 0.56783327\n",
            "Iteration 29, loss = 0.57114622\n",
            "Iteration 30, loss = 0.57487647\n",
            "Iteration 31, loss = 0.56907522\n",
            "Iteration 32, loss = 0.57129618\n",
            "Iteration 33, loss = 0.57557929\n",
            "Iteration 34, loss = 0.57289466\n",
            "Iteration 35, loss = 0.56522581\n",
            "Iteration 36, loss = 0.55115207\n",
            "Iteration 37, loss = 0.55539657\n",
            "Iteration 38, loss = 0.57688410\n",
            "Iteration 39, loss = 0.56518608\n",
            "Iteration 40, loss = 0.56817823\n",
            "Iteration 41, loss = 0.54927525\n",
            "Iteration 42, loss = 0.55772980\n",
            "Iteration 43, loss = 0.54896839\n",
            "Iteration 44, loss = 0.55233027\n",
            "Iteration 45, loss = 0.56002079\n",
            "Iteration 46, loss = 0.54454120\n",
            "Iteration 47, loss = 0.54902438\n",
            "Iteration 48, loss = 0.55724900\n",
            "Iteration 49, loss = 0.54065728\n",
            "Iteration 50, loss = 0.54428609\n",
            "Iteration 51, loss = 0.54867844\n",
            "Iteration 52, loss = 0.52317254\n",
            "Iteration 53, loss = 0.53318181\n",
            "Iteration 54, loss = 0.53731398\n",
            "Iteration 55, loss = 0.53136472\n",
            "Iteration 56, loss = 0.55604500\n",
            "Iteration 57, loss = 0.52749713\n",
            "Iteration 58, loss = 0.54580218\n",
            "Iteration 59, loss = 0.53979142\n",
            "Iteration 60, loss = 0.53909958\n",
            "Iteration 61, loss = 0.52488178\n",
            "Iteration 62, loss = 0.52950334\n",
            "Iteration 63, loss = 0.51844662\n",
            "Iteration 64, loss = 0.52937310\n",
            "Iteration 65, loss = 0.53920711\n",
            "Iteration 66, loss = 0.52719058\n",
            "Iteration 67, loss = 0.53245303\n",
            "Iteration 68, loss = 0.52369259\n",
            "Iteration 69, loss = 0.52458770\n",
            "Iteration 70, loss = 0.53779093\n",
            "Iteration 71, loss = 0.51696698\n",
            "Iteration 72, loss = 0.52099526\n",
            "Iteration 73, loss = 0.52777443\n",
            "Iteration 74, loss = 0.51224865\n",
            "Iteration 75, loss = 0.54261628\n",
            "Iteration 76, loss = 0.52302856\n",
            "Iteration 77, loss = 0.51702045\n",
            "Iteration 78, loss = 0.50674510\n",
            "Iteration 79, loss = 0.54526514\n",
            "Iteration 80, loss = 0.52252359\n",
            "Iteration 81, loss = 0.50276937\n",
            "Iteration 82, loss = 0.51213335\n",
            "Iteration 83, loss = 0.52573242\n",
            "Iteration 84, loss = 0.51500047\n",
            "Iteration 85, loss = 0.50128427\n",
            "Iteration 86, loss = 0.51272711\n",
            "Iteration 87, loss = 0.50011120\n",
            "Iteration 88, loss = 0.49740726\n",
            "Iteration 89, loss = 0.48979239\n",
            "Iteration 90, loss = 0.53017584\n",
            "Iteration 91, loss = 0.53098504\n",
            "Iteration 92, loss = 0.49548670\n",
            "Iteration 93, loss = 0.50155646\n",
            "Iteration 94, loss = 0.50792297\n",
            "Iteration 95, loss = 0.49743609\n",
            "Iteration 96, loss = 0.49688789\n",
            "Iteration 97, loss = 0.51902253\n",
            "Iteration 98, loss = 0.52129040\n",
            "Iteration 99, loss = 0.51964118\n",
            "Iteration 100, loss = 0.49251938\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.09256964\n",
            "Iteration 2, loss = 0.82420467\n",
            "Iteration 3, loss = 0.67647512\n",
            "Iteration 4, loss = 0.76160338\n",
            "Iteration 5, loss = 0.61720762\n",
            "Iteration 6, loss = 0.58311492\n",
            "Iteration 7, loss = 0.55838161\n",
            "Iteration 8, loss = 0.55244143\n",
            "Iteration 9, loss = 0.57274917\n",
            "Iteration 10, loss = 0.55355751\n",
            "Iteration 11, loss = 0.56594529\n",
            "Iteration 12, loss = 0.55765098\n",
            "Iteration 13, loss = 0.53212592\n",
            "Iteration 14, loss = 0.53907877\n",
            "Iteration 15, loss = 0.53094450\n",
            "Iteration 16, loss = 0.50341415\n",
            "Iteration 17, loss = 0.51245314\n",
            "Iteration 18, loss = 0.51248363\n",
            "Iteration 19, loss = 0.51068303\n",
            "Iteration 20, loss = 0.51430837\n",
            "Iteration 21, loss = 0.53426222\n",
            "Iteration 22, loss = 0.49101527\n",
            "Iteration 23, loss = 0.47894731\n",
            "Iteration 24, loss = 0.51292985\n",
            "Iteration 25, loss = 0.51071082\n",
            "Iteration 26, loss = 0.49199967\n",
            "Iteration 27, loss = 0.48346487\n",
            "Iteration 28, loss = 0.51025714\n",
            "Iteration 29, loss = 0.49665598\n",
            "Iteration 30, loss = 0.49837485\n",
            "Iteration 31, loss = 0.54461524\n",
            "Iteration 32, loss = 0.45824110\n",
            "Iteration 33, loss = 0.44055476\n",
            "Iteration 34, loss = 0.46564696\n",
            "Iteration 35, loss = 0.52808796\n",
            "Iteration 36, loss = 0.47445518\n",
            "Iteration 37, loss = 0.45870338\n",
            "Iteration 38, loss = 0.44921093\n",
            "Iteration 39, loss = 0.48527104\n",
            "Iteration 40, loss = 0.44572787\n",
            "Iteration 41, loss = 0.42843843\n",
            "Iteration 42, loss = 0.44607005\n",
            "Iteration 43, loss = 0.44248719\n",
            "Iteration 44, loss = 0.44589221\n",
            "Iteration 45, loss = 0.51870849\n",
            "Iteration 46, loss = 0.61182423\n",
            "Iteration 47, loss = 0.49347514\n",
            "Iteration 48, loss = 0.45290196\n",
            "Iteration 49, loss = 0.42601917\n",
            "Iteration 50, loss = 0.42770056\n",
            "Iteration 51, loss = 0.42398593\n",
            "Iteration 52, loss = 0.42745199\n",
            "Iteration 53, loss = 0.41687631\n",
            "Iteration 54, loss = 0.41923010\n",
            "Iteration 55, loss = 0.46736496\n",
            "Iteration 56, loss = 0.44257989\n",
            "Iteration 57, loss = 0.40889190\n",
            "Iteration 58, loss = 0.43966190\n",
            "Iteration 59, loss = 0.42287259\n",
            "Iteration 60, loss = 0.44575851\n",
            "Iteration 61, loss = 0.43871761\n",
            "Iteration 62, loss = 0.41127450\n",
            "Iteration 63, loss = 0.41726998\n",
            "Iteration 64, loss = 0.39786226\n",
            "Iteration 65, loss = 0.40483505\n",
            "Iteration 66, loss = 0.40497960\n",
            "Iteration 67, loss = 0.43190438\n",
            "Iteration 68, loss = 0.46789383\n",
            "Iteration 69, loss = 0.40402689\n",
            "Iteration 70, loss = 0.41932598\n",
            "Iteration 71, loss = 0.48946177\n",
            "Iteration 72, loss = 0.49956536\n",
            "Iteration 73, loss = 0.45781385\n",
            "Iteration 74, loss = 0.43676975\n",
            "Iteration 75, loss = 0.40314619\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.76021514\n",
            "Iteration 2, loss = 0.69837443\n",
            "Iteration 3, loss = 0.62375186\n",
            "Iteration 4, loss = 0.56384351\n",
            "Iteration 5, loss = 0.57185643\n",
            "Iteration 6, loss = 0.67957163\n",
            "Iteration 7, loss = 0.70985404\n",
            "Iteration 8, loss = 0.82548391\n",
            "Iteration 9, loss = 0.67221376\n",
            "Iteration 10, loss = 0.59767057\n",
            "Iteration 11, loss = 0.55845066\n",
            "Iteration 12, loss = 0.52741659\n",
            "Iteration 13, loss = 0.53906305\n",
            "Iteration 14, loss = 0.52314524\n",
            "Iteration 15, loss = 0.54113636\n",
            "Iteration 16, loss = 0.55777704\n",
            "Iteration 17, loss = 0.54533434\n",
            "Iteration 18, loss = 0.52557538\n",
            "Iteration 19, loss = 0.62990489\n",
            "Iteration 20, loss = 0.50134983\n",
            "Iteration 21, loss = 0.49003234\n",
            "Iteration 22, loss = 0.47851483\n",
            "Iteration 23, loss = 0.46010772\n",
            "Iteration 24, loss = 0.49144793\n",
            "Iteration 25, loss = 0.53358179\n",
            "Iteration 26, loss = 0.47778863\n",
            "Iteration 27, loss = 0.49146779\n",
            "Iteration 28, loss = 0.47942753\n",
            "Iteration 29, loss = 0.44412107\n",
            "Iteration 30, loss = 0.45038681\n",
            "Iteration 31, loss = 0.46934073\n",
            "Iteration 32, loss = 0.49588908\n",
            "Iteration 33, loss = 0.43887774\n",
            "Iteration 34, loss = 0.44902373\n",
            "Iteration 35, loss = 0.43332095\n",
            "Iteration 36, loss = 0.45184714\n",
            "Iteration 37, loss = 0.46898183\n",
            "Iteration 38, loss = 0.46768338\n",
            "Iteration 39, loss = 0.44505970\n",
            "Iteration 40, loss = 0.44060106\n",
            "Iteration 41, loss = 0.43820152\n",
            "Iteration 42, loss = 0.43859266\n",
            "Iteration 43, loss = 0.45321684\n",
            "Iteration 44, loss = 0.47540562\n",
            "Iteration 45, loss = 0.45153878\n",
            "Iteration 46, loss = 0.41129176\n",
            "Iteration 47, loss = 0.40886788\n",
            "Iteration 48, loss = 0.43903339\n",
            "Iteration 49, loss = 0.44339126\n",
            "Iteration 50, loss = 0.47047755\n",
            "Iteration 51, loss = 0.43423685\n",
            "Iteration 52, loss = 0.40405501\n",
            "Iteration 53, loss = 0.43646385\n",
            "Iteration 54, loss = 0.42649929\n",
            "Iteration 55, loss = 0.43802508\n",
            "Iteration 56, loss = 0.47506288\n",
            "Iteration 57, loss = 0.41245854\n",
            "Iteration 58, loss = 0.41294883\n",
            "Iteration 59, loss = 0.39699088\n",
            "Iteration 60, loss = 0.41132444\n",
            "Iteration 61, loss = 0.44427370\n",
            "Iteration 62, loss = 0.42147105\n",
            "Iteration 63, loss = 0.41669501\n",
            "Iteration 64, loss = 0.40695406\n",
            "Iteration 65, loss = 0.39563318\n",
            "Iteration 66, loss = 0.41401644\n",
            "Iteration 67, loss = 0.40982891\n",
            "Iteration 68, loss = 0.40961721\n",
            "Iteration 69, loss = 0.38893906\n",
            "Iteration 70, loss = 0.38643930\n",
            "Iteration 71, loss = 0.40161369\n",
            "Iteration 72, loss = 0.39964406\n",
            "Iteration 73, loss = 0.38291306\n",
            "Iteration 74, loss = 0.38079007\n",
            "Iteration 75, loss = 0.38863290\n",
            "Iteration 76, loss = 0.40021814\n",
            "Iteration 77, loss = 0.42532065\n",
            "Iteration 78, loss = 0.42791913\n",
            "Iteration 79, loss = 0.40994503\n",
            "Iteration 80, loss = 0.39850548\n",
            "Iteration 81, loss = 0.43696189\n",
            "Iteration 82, loss = 0.38580953\n",
            "Iteration 83, loss = 0.37988088\n",
            "Iteration 84, loss = 0.41528319\n",
            "Iteration 85, loss = 0.66121481\n",
            "Iteration 86, loss = 0.52492516\n",
            "Iteration 87, loss = 0.51656763\n",
            "Iteration 88, loss = 0.45432098\n",
            "Iteration 89, loss = 0.40148303\n",
            "Iteration 90, loss = 0.39100534\n",
            "Iteration 91, loss = 0.39998856\n",
            "Iteration 92, loss = 0.39205064\n",
            "Iteration 93, loss = 0.39435461\n",
            "Iteration 94, loss = 0.42245650\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.90446490\n",
            "Iteration 2, loss = 0.64697230\n",
            "Iteration 3, loss = 0.62171628\n",
            "Iteration 4, loss = 0.61395789\n",
            "Iteration 5, loss = 0.58818622\n",
            "Iteration 6, loss = 0.64125754\n",
            "Iteration 7, loss = 0.65596397\n",
            "Iteration 8, loss = 0.63580152\n",
            "Iteration 9, loss = 0.57555037\n",
            "Iteration 10, loss = 0.55306867\n",
            "Iteration 11, loss = 0.53876370\n",
            "Iteration 12, loss = 0.59486394\n",
            "Iteration 13, loss = 0.61448265\n",
            "Iteration 14, loss = 0.56435224\n",
            "Iteration 15, loss = 0.53841295\n",
            "Iteration 16, loss = 0.53595275\n",
            "Iteration 17, loss = 0.62399033\n",
            "Iteration 18, loss = 0.56756232\n",
            "Iteration 19, loss = 0.53972977\n",
            "Iteration 20, loss = 0.47566959\n",
            "Iteration 21, loss = 0.47707566\n",
            "Iteration 22, loss = 0.48105263\n",
            "Iteration 23, loss = 0.46687858\n",
            "Iteration 24, loss = 0.50032482\n",
            "Iteration 25, loss = 0.54370837\n",
            "Iteration 26, loss = 0.48278330\n",
            "Iteration 27, loss = 0.48094045\n",
            "Iteration 28, loss = 0.44803164\n",
            "Iteration 29, loss = 0.45058898\n",
            "Iteration 30, loss = 0.45094760\n",
            "Iteration 31, loss = 0.44263200\n",
            "Iteration 32, loss = 0.46651902\n",
            "Iteration 33, loss = 0.47652434\n",
            "Iteration 34, loss = 0.49039582\n",
            "Iteration 35, loss = 0.44547696\n",
            "Iteration 36, loss = 0.44198410\n",
            "Iteration 37, loss = 0.47819179\n",
            "Iteration 38, loss = 0.51212419\n",
            "Iteration 39, loss = 0.49770554\n",
            "Iteration 40, loss = 0.43505233\n",
            "Iteration 41, loss = 0.45203968\n",
            "Iteration 42, loss = 0.42827684\n",
            "Iteration 43, loss = 0.42761492\n",
            "Iteration 44, loss = 0.43140592\n",
            "Iteration 45, loss = 0.43591700\n",
            "Iteration 46, loss = 0.41213884\n",
            "Iteration 47, loss = 0.41175695\n",
            "Iteration 48, loss = 0.44507321\n",
            "Iteration 49, loss = 0.48698906\n",
            "Iteration 50, loss = 0.44228082\n",
            "Iteration 51, loss = 0.42370552\n",
            "Iteration 52, loss = 0.49478071\n",
            "Iteration 53, loss = 0.48360742\n",
            "Iteration 54, loss = 0.49090364\n",
            "Iteration 55, loss = 0.48649005\n",
            "Iteration 56, loss = 0.45510931\n",
            "Iteration 57, loss = 0.44812319\n",
            "Iteration 58, loss = 0.43376013\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.18040566\n",
            "Iteration 2, loss = 0.75297902\n",
            "Iteration 3, loss = 0.62315357\n",
            "Iteration 4, loss = 0.63425565\n",
            "Iteration 5, loss = 0.59004851\n",
            "Iteration 6, loss = 0.66552624\n",
            "Iteration 7, loss = 0.77927965\n",
            "Iteration 8, loss = 0.62016563\n",
            "Iteration 9, loss = 0.56054555\n",
            "Iteration 10, loss = 0.56612942\n",
            "Iteration 11, loss = 0.61735438\n",
            "Iteration 12, loss = 0.55099778\n",
            "Iteration 13, loss = 0.55825730\n",
            "Iteration 14, loss = 0.57077528\n",
            "Iteration 15, loss = 0.56916108\n",
            "Iteration 16, loss = 0.54838276\n",
            "Iteration 17, loss = 0.59274613\n",
            "Iteration 18, loss = 0.54423466\n",
            "Iteration 19, loss = 0.55425091\n",
            "Iteration 20, loss = 0.53493237\n",
            "Iteration 21, loss = 0.50178212\n",
            "Iteration 22, loss = 0.48759079\n",
            "Iteration 23, loss = 0.52775186\n",
            "Iteration 24, loss = 0.47079313\n",
            "Iteration 25, loss = 0.45996213\n",
            "Iteration 26, loss = 0.46450435\n",
            "Iteration 27, loss = 0.58595104\n",
            "Iteration 28, loss = 0.54245614\n",
            "Iteration 29, loss = 0.53946268\n",
            "Iteration 30, loss = 0.57805704\n",
            "Iteration 31, loss = 0.49364039\n",
            "Iteration 32, loss = 0.44495633\n",
            "Iteration 33, loss = 0.44134136\n",
            "Iteration 34, loss = 0.45188359\n",
            "Iteration 35, loss = 0.45073408\n",
            "Iteration 36, loss = 0.49275710\n",
            "Iteration 37, loss = 0.45345578\n",
            "Iteration 38, loss = 0.47031003\n",
            "Iteration 39, loss = 0.52795763\n",
            "Iteration 40, loss = 0.50552614\n",
            "Iteration 41, loss = 0.48262476\n",
            "Iteration 42, loss = 0.42651189\n",
            "Iteration 43, loss = 0.46913667\n",
            "Iteration 44, loss = 0.43508657\n",
            "Iteration 45, loss = 0.43436977\n",
            "Iteration 46, loss = 0.45445140\n",
            "Iteration 47, loss = 0.47774447\n",
            "Iteration 48, loss = 0.46537505\n",
            "Iteration 49, loss = 0.46577385\n",
            "Iteration 50, loss = 0.46952466\n",
            "Iteration 51, loss = 0.42873813\n",
            "Iteration 52, loss = 0.41861200\n",
            "Iteration 53, loss = 0.41993771\n",
            "Iteration 54, loss = 0.44951379\n",
            "Iteration 55, loss = 0.46605440\n",
            "Iteration 56, loss = 0.46762704\n",
            "Iteration 57, loss = 0.45152089\n",
            "Iteration 58, loss = 0.45260006\n",
            "Iteration 59, loss = 0.49665021\n",
            "Iteration 60, loss = 0.47011551\n",
            "Iteration 61, loss = 0.43597115\n",
            "Iteration 62, loss = 0.54082898\n",
            "Iteration 63, loss = 0.44675807\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.02696032\n",
            "Iteration 2, loss = 0.82472028\n",
            "Iteration 3, loss = 0.68730916\n",
            "Iteration 4, loss = 0.62949611\n",
            "Iteration 5, loss = 0.61802867\n",
            "Iteration 6, loss = 0.60261668\n",
            "Iteration 7, loss = 0.57542881\n",
            "Iteration 8, loss = 0.58573697\n",
            "Iteration 9, loss = 0.59127704\n",
            "Iteration 10, loss = 0.58177605\n",
            "Iteration 11, loss = 0.57295488\n",
            "Iteration 12, loss = 0.60051333\n",
            "Iteration 13, loss = 0.66696772\n",
            "Iteration 14, loss = 0.64779739\n",
            "Iteration 15, loss = 0.57644271\n",
            "Iteration 16, loss = 0.55255113\n",
            "Iteration 17, loss = 0.53104794\n",
            "Iteration 18, loss = 0.52951689\n",
            "Iteration 19, loss = 0.53375561\n",
            "Iteration 20, loss = 0.50937986\n",
            "Iteration 21, loss = 0.52171571\n",
            "Iteration 22, loss = 0.54038841\n",
            "Iteration 23, loss = 0.50988825\n",
            "Iteration 24, loss = 0.52161932\n",
            "Iteration 25, loss = 0.66835568\n",
            "Iteration 26, loss = 0.64639944\n",
            "Iteration 27, loss = 0.49727403\n",
            "Iteration 28, loss = 0.49897796\n",
            "Iteration 29, loss = 0.47119779\n",
            "Iteration 30, loss = 0.47283605\n",
            "Iteration 31, loss = 0.48397679\n",
            "Iteration 32, loss = 0.45845063\n",
            "Iteration 33, loss = 0.46618841\n",
            "Iteration 34, loss = 0.48604435\n",
            "Iteration 35, loss = 0.51421902\n",
            "Iteration 36, loss = 0.48911617\n",
            "Iteration 37, loss = 0.49359027\n",
            "Iteration 38, loss = 0.49694787\n",
            "Iteration 39, loss = 0.47854102\n",
            "Iteration 40, loss = 0.44992184\n",
            "Iteration 41, loss = 0.45194385\n",
            "Iteration 42, loss = 0.44574463\n",
            "Iteration 43, loss = 0.49367229\n",
            "Iteration 44, loss = 0.49247871\n",
            "Iteration 45, loss = 0.47354923\n",
            "Iteration 46, loss = 0.43368915\n",
            "Iteration 47, loss = 0.45671076\n",
            "Iteration 48, loss = 0.44750342\n",
            "Iteration 49, loss = 0.44354843\n",
            "Iteration 50, loss = 0.42913944\n",
            "Iteration 51, loss = 0.45178823\n",
            "Iteration 52, loss = 0.55754442\n",
            "Iteration 53, loss = 0.48598551\n",
            "Iteration 54, loss = 0.48265334\n",
            "Iteration 55, loss = 0.49567754\n",
            "Iteration 56, loss = 0.45279127\n",
            "Iteration 57, loss = 0.42977616\n",
            "Iteration 58, loss = 0.44915031\n",
            "Iteration 59, loss = 0.44334687\n",
            "Iteration 60, loss = 0.43337336\n",
            "Iteration 61, loss = 0.46789835\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.09740859\n",
            "Iteration 2, loss = 1.01287423\n",
            "Iteration 3, loss = 0.96806362\n",
            "Iteration 4, loss = 0.71662592\n",
            "Iteration 5, loss = 0.63027830\n",
            "Iteration 6, loss = 0.58625611\n",
            "Iteration 7, loss = 0.57886055\n",
            "Iteration 8, loss = 0.56890425\n",
            "Iteration 9, loss = 0.56466709\n",
            "Iteration 10, loss = 0.56289436\n",
            "Iteration 11, loss = 0.56003922\n",
            "Iteration 12, loss = 0.54986271\n",
            "Iteration 13, loss = 0.55244560\n",
            "Iteration 14, loss = 0.54331970\n",
            "Iteration 15, loss = 0.53872308\n",
            "Iteration 16, loss = 0.53453757\n",
            "Iteration 17, loss = 0.54079794\n",
            "Iteration 18, loss = 0.55471795\n",
            "Iteration 19, loss = 0.52924482\n",
            "Iteration 20, loss = 0.51607680\n",
            "Iteration 21, loss = 0.51210387\n",
            "Iteration 22, loss = 0.52694570\n",
            "Iteration 23, loss = 0.51726881\n",
            "Iteration 24, loss = 0.51226528\n",
            "Iteration 25, loss = 0.50430911\n",
            "Iteration 26, loss = 0.50651875\n",
            "Iteration 27, loss = 0.50331492\n",
            "Iteration 28, loss = 0.50571545\n",
            "Iteration 29, loss = 0.54193969\n",
            "Iteration 30, loss = 0.50515114\n",
            "Iteration 31, loss = 0.49248590\n",
            "Iteration 32, loss = 0.52725713\n",
            "Iteration 33, loss = 0.50525074\n",
            "Iteration 34, loss = 0.48954276\n",
            "Iteration 35, loss = 0.48319156\n",
            "Iteration 36, loss = 0.51344080\n",
            "Iteration 37, loss = 0.50342900\n",
            "Iteration 38, loss = 0.50482261\n",
            "Iteration 39, loss = 0.48332781\n",
            "Iteration 40, loss = 0.51173934\n",
            "Iteration 41, loss = 0.49989784\n",
            "Iteration 42, loss = 0.48644771\n",
            "Iteration 43, loss = 0.48179551\n",
            "Iteration 44, loss = 0.47810424\n",
            "Iteration 45, loss = 0.46316387\n",
            "Iteration 46, loss = 0.46737055\n",
            "Iteration 47, loss = 0.46297499\n",
            "Iteration 48, loss = 0.47174798\n",
            "Iteration 49, loss = 0.48550591\n",
            "Iteration 50, loss = 0.48371437\n",
            "Iteration 51, loss = 0.47515396\n",
            "Iteration 52, loss = 0.47845428\n",
            "Iteration 53, loss = 0.48452316\n",
            "Iteration 54, loss = 0.54067827\n",
            "Iteration 55, loss = 0.48618417\n",
            "Iteration 56, loss = 0.47680397\n",
            "Iteration 57, loss = 0.45337181\n",
            "Iteration 58, loss = 0.44814040\n",
            "Iteration 59, loss = 0.44938119\n",
            "Iteration 60, loss = 0.46370807\n",
            "Iteration 61, loss = 0.44271797\n",
            "Iteration 62, loss = 0.45460498\n",
            "Iteration 63, loss = 0.47350620\n",
            "Iteration 64, loss = 0.46277792\n",
            "Iteration 65, loss = 0.44736441\n",
            "Iteration 66, loss = 0.44013057\n",
            "Iteration 67, loss = 0.44800572\n",
            "Iteration 68, loss = 0.45113974\n",
            "Iteration 69, loss = 0.45213105\n",
            "Iteration 70, loss = 0.45915354\n",
            "Iteration 71, loss = 0.44566634\n",
            "Iteration 72, loss = 0.45432970\n",
            "Iteration 73, loss = 0.50379994\n",
            "Iteration 74, loss = 0.47551216\n",
            "Iteration 75, loss = 0.47114393\n",
            "Iteration 76, loss = 0.44790643\n",
            "Iteration 77, loss = 0.44415525\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.21138093\n",
            "Iteration 2, loss = 0.81391207\n",
            "Iteration 3, loss = 0.83124331\n",
            "Iteration 4, loss = 0.62062742\n",
            "Iteration 5, loss = 0.61498476\n",
            "Iteration 6, loss = 0.59842655\n",
            "Iteration 7, loss = 0.57817472\n",
            "Iteration 8, loss = 0.57176958\n",
            "Iteration 9, loss = 0.55973998\n",
            "Iteration 10, loss = 0.56063298\n",
            "Iteration 11, loss = 0.55799411\n",
            "Iteration 12, loss = 0.56114098\n",
            "Iteration 13, loss = 0.55212582\n",
            "Iteration 14, loss = 0.56640221\n",
            "Iteration 15, loss = 0.56576682\n",
            "Iteration 16, loss = 0.54924790\n",
            "Iteration 17, loss = 0.55523892\n",
            "Iteration 18, loss = 0.53984901\n",
            "Iteration 19, loss = 0.53920715\n",
            "Iteration 20, loss = 0.54492585\n",
            "Iteration 21, loss = 0.52106920\n",
            "Iteration 22, loss = 0.51702373\n",
            "Iteration 23, loss = 0.51213783\n",
            "Iteration 24, loss = 0.50875644\n",
            "Iteration 25, loss = 0.51030223\n",
            "Iteration 26, loss = 0.52226065\n",
            "Iteration 27, loss = 0.51251082\n",
            "Iteration 28, loss = 0.49950699\n",
            "Iteration 29, loss = 0.49808099\n",
            "Iteration 30, loss = 0.50140594\n",
            "Iteration 31, loss = 0.49791851\n",
            "Iteration 32, loss = 0.49212113\n",
            "Iteration 33, loss = 0.48718673\n",
            "Iteration 34, loss = 0.49104544\n",
            "Iteration 35, loss = 0.47992902\n",
            "Iteration 36, loss = 0.48104725\n",
            "Iteration 37, loss = 0.48855757\n",
            "Iteration 38, loss = 0.47074607\n",
            "Iteration 39, loss = 0.46838234\n",
            "Iteration 40, loss = 0.46841139\n",
            "Iteration 41, loss = 0.46129252\n",
            "Iteration 42, loss = 0.47852972\n",
            "Iteration 43, loss = 0.47409968\n",
            "Iteration 44, loss = 0.47750533\n",
            "Iteration 45, loss = 0.47362977\n",
            "Iteration 46, loss = 0.45858225\n",
            "Iteration 47, loss = 0.46468163\n",
            "Iteration 48, loss = 0.45066257\n",
            "Iteration 49, loss = 0.43863628\n",
            "Iteration 50, loss = 0.44970149\n",
            "Iteration 51, loss = 0.43678214\n",
            "Iteration 52, loss = 0.43786663\n",
            "Iteration 53, loss = 0.43539256\n",
            "Iteration 54, loss = 0.43568420\n",
            "Iteration 55, loss = 0.44427056\n",
            "Iteration 56, loss = 0.44902800\n",
            "Iteration 57, loss = 0.44679029\n",
            "Iteration 58, loss = 0.43230599\n",
            "Iteration 59, loss = 0.45181416\n",
            "Iteration 60, loss = 0.43504752\n",
            "Iteration 61, loss = 0.45140969\n",
            "Iteration 62, loss = 0.42427153\n",
            "Iteration 63, loss = 0.42719662\n",
            "Iteration 64, loss = 0.42630665\n",
            "Iteration 65, loss = 0.42653010\n",
            "Iteration 66, loss = 0.42475627\n",
            "Iteration 67, loss = 0.42645227\n",
            "Iteration 68, loss = 0.42457900\n",
            "Iteration 69, loss = 0.41920827\n",
            "Iteration 70, loss = 0.42961555\n",
            "Iteration 71, loss = 0.41739209\n",
            "Iteration 72, loss = 0.42875708\n",
            "Iteration 73, loss = 0.42064671\n",
            "Iteration 74, loss = 0.42169193\n",
            "Iteration 75, loss = 0.41750348\n",
            "Iteration 76, loss = 0.41179294\n",
            "Iteration 77, loss = 0.43070918\n",
            "Iteration 78, loss = 0.43762511\n",
            "Iteration 79, loss = 0.48473015\n",
            "Iteration 80, loss = 0.42253904\n",
            "Iteration 81, loss = 0.42206208\n",
            "Iteration 82, loss = 0.42042583\n",
            "Iteration 83, loss = 0.42008007\n",
            "Iteration 84, loss = 0.41664927\n",
            "Iteration 85, loss = 0.41745659\n",
            "Iteration 86, loss = 0.41028572\n",
            "Iteration 87, loss = 0.41868746\n",
            "Iteration 88, loss = 0.42932769\n",
            "Iteration 89, loss = 0.43957608\n",
            "Iteration 90, loss = 0.44005716\n",
            "Iteration 91, loss = 0.42433832\n",
            "Iteration 92, loss = 0.41231874\n",
            "Iteration 93, loss = 0.43248557\n",
            "Iteration 94, loss = 0.46381166\n",
            "Iteration 95, loss = 0.44164919\n",
            "Iteration 96, loss = 0.41557110\n",
            "Iteration 97, loss = 0.40382643\n",
            "Iteration 98, loss = 0.40839547\n",
            "Iteration 99, loss = 0.41171052\n",
            "Iteration 100, loss = 0.41183929\n",
            "Iteration 101, loss = 0.40759917\n",
            "Iteration 102, loss = 0.40829561\n",
            "Iteration 103, loss = 0.40021937\n",
            "Iteration 104, loss = 0.41050219\n",
            "Iteration 105, loss = 0.43135028\n",
            "Iteration 106, loss = 0.41791561\n",
            "Iteration 107, loss = 0.40072090\n",
            "Iteration 108, loss = 0.39878279\n",
            "Iteration 109, loss = 0.40738251\n",
            "Iteration 110, loss = 0.40863197\n",
            "Iteration 111, loss = 0.43599874\n",
            "Iteration 112, loss = 0.41127014\n",
            "Iteration 113, loss = 0.40100713\n",
            "Iteration 114, loss = 0.40315764\n",
            "Iteration 115, loss = 0.39879342\n",
            "Iteration 116, loss = 0.39636013\n",
            "Iteration 117, loss = 0.39786736\n",
            "Iteration 118, loss = 0.39292472\n",
            "Iteration 119, loss = 0.39163249\n",
            "Iteration 120, loss = 0.41201664\n",
            "Iteration 121, loss = 0.40906123\n",
            "Iteration 122, loss = 0.40738567\n",
            "Iteration 123, loss = 0.39363534\n",
            "Iteration 124, loss = 0.40354889\n",
            "Iteration 125, loss = 0.40782103\n",
            "Iteration 126, loss = 0.40875647\n",
            "Iteration 127, loss = 0.41889785\n",
            "Iteration 128, loss = 0.39907594\n",
            "Iteration 129, loss = 0.40604417\n",
            "Iteration 130, loss = 0.41130745\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.48019403\n",
            "Iteration 2, loss = 0.91733364\n",
            "Iteration 3, loss = 0.68942414\n",
            "Iteration 4, loss = 0.63828179\n",
            "Iteration 5, loss = 0.59567716\n",
            "Iteration 6, loss = 0.60735899\n",
            "Iteration 7, loss = 0.58319953\n",
            "Iteration 8, loss = 0.57538162\n",
            "Iteration 9, loss = 0.57456828\n",
            "Iteration 10, loss = 0.56421255\n",
            "Iteration 11, loss = 0.56463406\n",
            "Iteration 12, loss = 0.55848103\n",
            "Iteration 13, loss = 0.55002407\n",
            "Iteration 14, loss = 0.54656452\n",
            "Iteration 15, loss = 0.55292516\n",
            "Iteration 16, loss = 0.54405536\n",
            "Iteration 17, loss = 0.53526969\n",
            "Iteration 18, loss = 0.53255942\n",
            "Iteration 19, loss = 0.52706020\n",
            "Iteration 20, loss = 0.53618682\n",
            "Iteration 21, loss = 0.54765705\n",
            "Iteration 22, loss = 0.51993677\n",
            "Iteration 23, loss = 0.52371705\n",
            "Iteration 24, loss = 0.51158117\n",
            "Iteration 25, loss = 0.50228976\n",
            "Iteration 26, loss = 0.49990104\n",
            "Iteration 27, loss = 0.50614598\n",
            "Iteration 28, loss = 0.49989845\n",
            "Iteration 29, loss = 0.49331123\n",
            "Iteration 30, loss = 0.48706598\n",
            "Iteration 31, loss = 0.49287303\n",
            "Iteration 32, loss = 0.48542538\n",
            "Iteration 33, loss = 0.48767536\n",
            "Iteration 34, loss = 0.48058031\n",
            "Iteration 35, loss = 0.48085265\n",
            "Iteration 36, loss = 0.47055383\n",
            "Iteration 37, loss = 0.48184350\n",
            "Iteration 38, loss = 0.47670400\n",
            "Iteration 39, loss = 0.46670293\n",
            "Iteration 40, loss = 0.48577080\n",
            "Iteration 41, loss = 0.47373775\n",
            "Iteration 42, loss = 0.46669647\n",
            "Iteration 43, loss = 0.49538897\n",
            "Iteration 44, loss = 0.46485122\n",
            "Iteration 45, loss = 0.46379559\n",
            "Iteration 46, loss = 0.45298401\n",
            "Iteration 47, loss = 0.46779796\n",
            "Iteration 48, loss = 0.44859132\n",
            "Iteration 49, loss = 0.44792586\n",
            "Iteration 50, loss = 0.45565912\n",
            "Iteration 51, loss = 0.48566230\n",
            "Iteration 52, loss = 0.48092937\n",
            "Iteration 53, loss = 0.46594797\n",
            "Iteration 54, loss = 0.43207177\n",
            "Iteration 55, loss = 0.44116834\n",
            "Iteration 56, loss = 0.46616100\n",
            "Iteration 57, loss = 0.44518982\n",
            "Iteration 58, loss = 0.43289933\n",
            "Iteration 59, loss = 0.45101472\n",
            "Iteration 60, loss = 0.43575410\n",
            "Iteration 61, loss = 0.42402959\n",
            "Iteration 62, loss = 0.42806770\n",
            "Iteration 63, loss = 0.43328541\n",
            "Iteration 64, loss = 0.43350373\n",
            "Iteration 65, loss = 0.42385283\n",
            "Iteration 66, loss = 0.42953943\n",
            "Iteration 67, loss = 0.43143089\n",
            "Iteration 68, loss = 0.42464025\n",
            "Iteration 69, loss = 0.42055115\n",
            "Iteration 70, loss = 0.42066146\n",
            "Iteration 71, loss = 0.41428673\n",
            "Iteration 72, loss = 0.41917997\n",
            "Iteration 73, loss = 0.42559737\n",
            "Iteration 74, loss = 0.41894131\n",
            "Iteration 75, loss = 0.41266406\n",
            "Iteration 76, loss = 0.41334936\n",
            "Iteration 77, loss = 0.42070669\n",
            "Iteration 78, loss = 0.41557809\n",
            "Iteration 79, loss = 0.41696306\n",
            "Iteration 80, loss = 0.42370952\n",
            "Iteration 81, loss = 0.42593901\n",
            "Iteration 82, loss = 0.40527842\n",
            "Iteration 83, loss = 0.41748354\n",
            "Iteration 84, loss = 0.41010314\n",
            "Iteration 85, loss = 0.40645738\n",
            "Iteration 86, loss = 0.41474440\n",
            "Iteration 87, loss = 0.44024341\n",
            "Iteration 88, loss = 0.40258003\n",
            "Iteration 89, loss = 0.42465790\n",
            "Iteration 90, loss = 0.41517795\n",
            "Iteration 91, loss = 0.40713607\n",
            "Iteration 92, loss = 0.42190109\n",
            "Iteration 93, loss = 0.40153284\n",
            "Iteration 94, loss = 0.40366349\n",
            "Iteration 95, loss = 0.40161806\n",
            "Iteration 96, loss = 0.40332381\n",
            "Iteration 97, loss = 0.40276128\n",
            "Iteration 98, loss = 0.40533253\n",
            "Iteration 99, loss = 0.42646420\n",
            "Iteration 100, loss = 0.40851447\n",
            "Iteration 101, loss = 0.41488149\n",
            "Iteration 102, loss = 0.42177226\n",
            "Iteration 103, loss = 0.40397681\n",
            "Iteration 104, loss = 0.40428269\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.50181975\n",
            "Iteration 2, loss = 0.96648628\n",
            "Iteration 3, loss = 0.69317453\n",
            "Iteration 4, loss = 0.67976548\n",
            "Iteration 5, loss = 0.63169704\n",
            "Iteration 6, loss = 0.62290696\n",
            "Iteration 7, loss = 0.62400517\n",
            "Iteration 8, loss = 0.61644070\n",
            "Iteration 9, loss = 0.60029398\n",
            "Iteration 10, loss = 0.60765937\n",
            "Iteration 11, loss = 0.65667000\n",
            "Iteration 12, loss = 0.62913835\n",
            "Iteration 13, loss = 0.59678261\n",
            "Iteration 14, loss = 0.59660007\n",
            "Iteration 15, loss = 0.58932860\n",
            "Iteration 16, loss = 0.58762384\n",
            "Iteration 17, loss = 0.58157533\n",
            "Iteration 18, loss = 0.58633820\n",
            "Iteration 19, loss = 0.61493638\n",
            "Iteration 20, loss = 0.67674655\n",
            "Iteration 21, loss = 0.60386168\n",
            "Iteration 22, loss = 0.59684505\n",
            "Iteration 23, loss = 0.56323031\n",
            "Iteration 24, loss = 0.59000417\n",
            "Iteration 25, loss = 0.58680872\n",
            "Iteration 26, loss = 0.55900189\n",
            "Iteration 27, loss = 0.54751692\n",
            "Iteration 28, loss = 0.54766482\n",
            "Iteration 29, loss = 0.54720741\n",
            "Iteration 30, loss = 0.54992314\n",
            "Iteration 31, loss = 0.56181123\n",
            "Iteration 32, loss = 0.55722734\n",
            "Iteration 33, loss = 0.53949033\n",
            "Iteration 34, loss = 0.53568331\n",
            "Iteration 35, loss = 0.52484468\n",
            "Iteration 36, loss = 0.52908335\n",
            "Iteration 37, loss = 0.52897555\n",
            "Iteration 38, loss = 0.52143028\n",
            "Iteration 39, loss = 0.52391937\n",
            "Iteration 40, loss = 0.53529408\n",
            "Iteration 41, loss = 0.53792145\n",
            "Iteration 42, loss = 0.51915756\n",
            "Iteration 43, loss = 0.50304188\n",
            "Iteration 44, loss = 0.49957157\n",
            "Iteration 45, loss = 0.49632429\n",
            "Iteration 46, loss = 0.49571292\n",
            "Iteration 47, loss = 0.49914219\n",
            "Iteration 48, loss = 0.48534716\n",
            "Iteration 49, loss = 0.49926096\n",
            "Iteration 50, loss = 0.48828020\n",
            "Iteration 51, loss = 0.49612178\n",
            "Iteration 52, loss = 0.49064691\n",
            "Iteration 53, loss = 0.47946774\n",
            "Iteration 54, loss = 0.47907314\n",
            "Iteration 55, loss = 0.49301398\n",
            "Iteration 56, loss = 0.47960972\n",
            "Iteration 57, loss = 0.46979664\n",
            "Iteration 58, loss = 0.46099047\n",
            "Iteration 59, loss = 0.46649183\n",
            "Iteration 60, loss = 0.46322631\n",
            "Iteration 61, loss = 0.46101287\n",
            "Iteration 62, loss = 0.46027314\n",
            "Iteration 63, loss = 0.45758220\n",
            "Iteration 64, loss = 0.46023761\n",
            "Iteration 65, loss = 0.45606960\n",
            "Iteration 66, loss = 0.46016431\n",
            "Iteration 67, loss = 0.45737685\n",
            "Iteration 68, loss = 0.45063329\n",
            "Iteration 69, loss = 0.44818295\n",
            "Iteration 70, loss = 0.44505861\n",
            "Iteration 71, loss = 0.44157048\n",
            "Iteration 72, loss = 0.44440920\n",
            "Iteration 73, loss = 0.44250978\n",
            "Iteration 74, loss = 0.45457557\n",
            "Iteration 75, loss = 0.43469434\n",
            "Iteration 76, loss = 0.44312132\n",
            "Iteration 77, loss = 0.43636918\n",
            "Iteration 78, loss = 0.43950673\n",
            "Iteration 79, loss = 0.44022453\n",
            "Iteration 80, loss = 0.47144739\n",
            "Iteration 81, loss = 0.45293135\n",
            "Iteration 82, loss = 0.43466202\n",
            "Iteration 83, loss = 0.43106179\n",
            "Iteration 84, loss = 0.43694741\n",
            "Iteration 85, loss = 0.44680131\n",
            "Iteration 86, loss = 0.42763407\n",
            "Iteration 87, loss = 0.43488963\n",
            "Iteration 88, loss = 0.42294874\n",
            "Iteration 89, loss = 0.42459616\n",
            "Iteration 90, loss = 0.42584904\n",
            "Iteration 91, loss = 0.43819649\n",
            "Iteration 92, loss = 0.43496832\n",
            "Iteration 93, loss = 0.45548295\n",
            "Iteration 94, loss = 0.43355730\n",
            "Iteration 95, loss = 0.46041211\n",
            "Iteration 96, loss = 0.43497296\n",
            "Iteration 97, loss = 0.42794844\n",
            "Iteration 98, loss = 0.42641077\n",
            "Iteration 99, loss = 0.42196921\n",
            "Iteration 100, loss = 0.43057687\n",
            "Iteration 101, loss = 0.41985842\n",
            "Iteration 102, loss = 0.44129121\n",
            "Iteration 103, loss = 0.43121308\n",
            "Iteration 104, loss = 0.43525182\n",
            "Iteration 105, loss = 0.43557955\n",
            "Iteration 106, loss = 0.42220460\n",
            "Iteration 107, loss = 0.42142496\n",
            "Iteration 108, loss = 0.42412223\n",
            "Iteration 109, loss = 0.41949274\n",
            "Iteration 110, loss = 0.41735816\n",
            "Iteration 111, loss = 0.42657830\n",
            "Iteration 112, loss = 0.53006869\n",
            "Iteration 113, loss = 0.45479511\n",
            "Iteration 114, loss = 0.43500246\n",
            "Iteration 115, loss = 0.43417081\n",
            "Iteration 116, loss = 0.43795769\n",
            "Iteration 117, loss = 0.43116773\n",
            "Iteration 118, loss = 0.42510861\n",
            "Iteration 119, loss = 0.41538075\n",
            "Iteration 120, loss = 0.42322550\n",
            "Iteration 121, loss = 0.41398626\n",
            "Iteration 122, loss = 0.41928373\n",
            "Iteration 123, loss = 0.41304146\n",
            "Iteration 124, loss = 0.40817210\n",
            "Iteration 125, loss = 0.41464950\n",
            "Iteration 126, loss = 0.41324475\n",
            "Iteration 127, loss = 0.40801165\n",
            "Iteration 128, loss = 0.40689102\n",
            "Iteration 129, loss = 0.40907420\n",
            "Iteration 130, loss = 0.41533756\n",
            "Iteration 131, loss = 0.40601810\n",
            "Iteration 132, loss = 0.40370481\n",
            "Iteration 133, loss = 0.40265442\n",
            "Iteration 134, loss = 0.41138163\n",
            "Iteration 135, loss = 0.40681628\n",
            "Iteration 136, loss = 0.40534897\n",
            "Iteration 137, loss = 0.40048354\n",
            "Iteration 138, loss = 0.40767686\n",
            "Iteration 139, loss = 0.40502045\n",
            "Iteration 140, loss = 0.40858734\n",
            "Iteration 141, loss = 0.41527294\n",
            "Iteration 142, loss = 0.40308721\n",
            "Iteration 143, loss = 0.40848070\n",
            "Iteration 144, loss = 0.42466012\n",
            "Iteration 145, loss = 0.44571772\n",
            "Iteration 146, loss = 0.40671417\n",
            "Iteration 147, loss = 0.41974186\n",
            "Iteration 148, loss = 0.39883143\n",
            "Iteration 149, loss = 0.39626402\n",
            "Iteration 150, loss = 0.40292579\n",
            "Iteration 151, loss = 0.39978017\n",
            "Iteration 152, loss = 0.40252995\n",
            "Iteration 153, loss = 0.40404058\n",
            "Iteration 154, loss = 0.39842377\n",
            "Iteration 155, loss = 0.39426630\n",
            "Iteration 156, loss = 0.40123835\n",
            "Iteration 157, loss = 0.44504068\n",
            "Iteration 158, loss = 0.44504874\n",
            "Iteration 159, loss = 0.43068219\n",
            "Iteration 160, loss = 0.41972849\n",
            "Iteration 161, loss = 0.41313529\n",
            "Iteration 162, loss = 0.39575465\n",
            "Iteration 163, loss = 0.39797338\n",
            "Iteration 164, loss = 0.39622802\n",
            "Iteration 165, loss = 0.39810792\n",
            "Iteration 166, loss = 0.39344411\n",
            "Iteration 167, loss = 0.39588439\n",
            "Iteration 168, loss = 0.39426682\n",
            "Iteration 169, loss = 0.39970581\n",
            "Iteration 170, loss = 0.40135239\n",
            "Iteration 171, loss = 0.39514867\n",
            "Iteration 172, loss = 0.40503914\n",
            "Iteration 173, loss = 0.39301599\n",
            "Iteration 174, loss = 0.39504935\n",
            "Iteration 175, loss = 0.39453319\n",
            "Iteration 176, loss = 0.38723197\n",
            "Iteration 177, loss = 0.39138294\n",
            "Iteration 178, loss = 0.39967405\n",
            "Iteration 179, loss = 0.39061162\n",
            "Iteration 180, loss = 0.38825333\n",
            "Iteration 181, loss = 0.39449718\n",
            "Iteration 182, loss = 0.39210721\n",
            "Iteration 183, loss = 0.39820019\n",
            "Iteration 184, loss = 0.39265432\n",
            "Iteration 185, loss = 0.39069347\n",
            "Iteration 186, loss = 0.39090124\n",
            "Iteration 187, loss = 0.39246598\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.96523684\n",
            "Iteration 2, loss = 0.93878844\n",
            "Iteration 3, loss = 0.84388142\n",
            "Iteration 4, loss = 0.80633083\n",
            "Iteration 5, loss = 0.67086344\n",
            "Iteration 6, loss = 0.65440327\n",
            "Iteration 7, loss = 0.63178228\n",
            "Iteration 8, loss = 0.61734618\n",
            "Iteration 9, loss = 0.61027918\n",
            "Iteration 10, loss = 0.61018414\n",
            "Iteration 11, loss = 0.60138221\n",
            "Iteration 12, loss = 0.59853954\n",
            "Iteration 13, loss = 0.59543720\n",
            "Iteration 14, loss = 0.59356988\n",
            "Iteration 15, loss = 0.59598320\n",
            "Iteration 16, loss = 0.58786134\n",
            "Iteration 17, loss = 0.58377013\n",
            "Iteration 18, loss = 0.62731678\n",
            "Iteration 19, loss = 0.60064242\n",
            "Iteration 20, loss = 0.59903382\n",
            "Iteration 21, loss = 0.57837284\n",
            "Iteration 22, loss = 0.57381592\n",
            "Iteration 23, loss = 0.57702085\n",
            "Iteration 24, loss = 0.56372506\n",
            "Iteration 25, loss = 0.57598698\n",
            "Iteration 26, loss = 0.56103128\n",
            "Iteration 27, loss = 0.57140222\n",
            "Iteration 28, loss = 0.55239588\n",
            "Iteration 29, loss = 0.55511125\n",
            "Iteration 30, loss = 0.55276437\n",
            "Iteration 31, loss = 0.54687921\n",
            "Iteration 32, loss = 0.54996473\n",
            "Iteration 33, loss = 0.54303782\n",
            "Iteration 34, loss = 0.53630109\n",
            "Iteration 35, loss = 0.53552559\n",
            "Iteration 36, loss = 0.53211503\n",
            "Iteration 37, loss = 0.53414926\n",
            "Iteration 38, loss = 0.52711410\n",
            "Iteration 39, loss = 0.52476372\n",
            "Iteration 40, loss = 0.51568466\n",
            "Iteration 41, loss = 0.51578720\n",
            "Iteration 42, loss = 0.53009475\n",
            "Iteration 43, loss = 0.51367730\n",
            "Iteration 44, loss = 0.52729509\n",
            "Iteration 45, loss = 0.49982738\n",
            "Iteration 46, loss = 0.50763969\n",
            "Iteration 47, loss = 0.49986037\n",
            "Iteration 48, loss = 0.49667000\n",
            "Iteration 49, loss = 0.49081074\n",
            "Iteration 50, loss = 0.49289596\n",
            "Iteration 51, loss = 0.48691902\n",
            "Iteration 52, loss = 0.48806583\n",
            "Iteration 53, loss = 0.48310093\n",
            "Iteration 54, loss = 0.49485040\n",
            "Iteration 55, loss = 0.48113728\n",
            "Iteration 56, loss = 0.47766823\n",
            "Iteration 57, loss = 0.47618085\n",
            "Iteration 58, loss = 0.48490434\n",
            "Iteration 59, loss = 0.47160251\n",
            "Iteration 60, loss = 0.46986155\n",
            "Iteration 61, loss = 0.47004845\n",
            "Iteration 62, loss = 0.46954067\n",
            "Iteration 63, loss = 0.47610904\n",
            "Iteration 64, loss = 0.47872715\n",
            "Iteration 65, loss = 0.50404311\n",
            "Iteration 66, loss = 0.48794421\n",
            "Iteration 67, loss = 0.48567054\n",
            "Iteration 68, loss = 0.49929350\n",
            "Iteration 69, loss = 0.47755897\n",
            "Iteration 70, loss = 0.47090492\n",
            "Iteration 71, loss = 0.45452855\n",
            "Iteration 72, loss = 0.45198812\n",
            "Iteration 73, loss = 0.45772718\n",
            "Iteration 74, loss = 0.45047353\n",
            "Iteration 75, loss = 0.46771701\n",
            "Iteration 76, loss = 0.46107597\n",
            "Iteration 77, loss = 0.46472436\n",
            "Iteration 78, loss = 0.46402222\n",
            "Iteration 79, loss = 0.44175021\n",
            "Iteration 80, loss = 0.45397924\n",
            "Iteration 81, loss = 0.45263215\n",
            "Iteration 82, loss = 0.44502861\n",
            "Iteration 83, loss = 0.44529028\n",
            "Iteration 84, loss = 0.44180752\n",
            "Iteration 85, loss = 0.44573676\n",
            "Iteration 86, loss = 0.44083250\n",
            "Iteration 87, loss = 0.45193425\n",
            "Iteration 88, loss = 0.46016673\n",
            "Iteration 89, loss = 0.44302506\n",
            "Iteration 90, loss = 0.44102218\n",
            "Iteration 91, loss = 0.43957259\n",
            "Iteration 92, loss = 0.43382026\n",
            "Iteration 93, loss = 0.47186632\n",
            "Iteration 94, loss = 0.46284015\n",
            "Iteration 95, loss = 0.45546953\n",
            "Iteration 96, loss = 0.43542056\n",
            "Iteration 97, loss = 0.44406084\n",
            "Iteration 98, loss = 0.46468575\n",
            "Iteration 99, loss = 0.44447084\n",
            "Iteration 100, loss = 0.44405650\n",
            "Iteration 101, loss = 0.45541344\n",
            "Iteration 102, loss = 0.42963661\n",
            "Iteration 103, loss = 0.43704610\n",
            "Iteration 104, loss = 0.43629670\n",
            "Iteration 105, loss = 0.43307361\n",
            "Iteration 106, loss = 0.42929249\n",
            "Iteration 107, loss = 0.43621050\n",
            "Iteration 108, loss = 0.45397333\n",
            "Iteration 109, loss = 0.44329430\n",
            "Iteration 110, loss = 0.44256065\n",
            "Iteration 111, loss = 0.43624066\n",
            "Iteration 112, loss = 0.42616215\n",
            "Iteration 113, loss = 0.43194445\n",
            "Iteration 114, loss = 0.42759820\n",
            "Iteration 115, loss = 0.43963394\n",
            "Iteration 116, loss = 0.43773144\n",
            "Iteration 117, loss = 0.42683665\n",
            "Iteration 118, loss = 0.42799779\n",
            "Iteration 119, loss = 0.44066696\n",
            "Iteration 120, loss = 0.45557103\n",
            "Iteration 121, loss = 0.42569071\n",
            "Iteration 122, loss = 0.42615584\n",
            "Iteration 123, loss = 0.42958545\n",
            "Iteration 124, loss = 0.43191220\n",
            "Iteration 125, loss = 0.43597605\n",
            "Iteration 126, loss = 0.45704550\n",
            "Iteration 127, loss = 0.43552140\n",
            "Iteration 128, loss = 0.42756978\n",
            "Iteration 129, loss = 0.42828179\n",
            "Iteration 130, loss = 0.43691104\n",
            "Iteration 131, loss = 0.43546060\n",
            "Iteration 132, loss = 0.43392048\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.06494625\n",
            "Iteration 2, loss = 0.77732937\n",
            "Iteration 3, loss = 0.63923700\n",
            "Iteration 4, loss = 0.62698775\n",
            "Iteration 5, loss = 0.57932119\n",
            "Iteration 6, loss = 0.58814014\n",
            "Iteration 7, loss = 0.62122030\n",
            "Iteration 8, loss = 0.56825096\n",
            "Iteration 9, loss = 0.55988850\n",
            "Iteration 10, loss = 0.55618666\n",
            "Iteration 11, loss = 0.55021227\n",
            "Iteration 12, loss = 0.54835229\n",
            "Iteration 13, loss = 0.54726911\n",
            "Iteration 14, loss = 0.57485915\n",
            "Iteration 15, loss = 0.58855849\n",
            "Iteration 16, loss = 0.60296228\n",
            "Iteration 17, loss = 0.52011387\n",
            "Iteration 18, loss = 0.52618523\n",
            "Iteration 19, loss = 0.52233071\n",
            "Iteration 20, loss = 0.50559761\n",
            "Iteration 21, loss = 0.50051214\n",
            "Iteration 22, loss = 0.50334242\n",
            "Iteration 23, loss = 0.49814205\n",
            "Iteration 24, loss = 0.50252522\n",
            "Iteration 25, loss = 0.48825441\n",
            "Iteration 26, loss = 0.47333248\n",
            "Iteration 27, loss = 0.51548443\n",
            "Iteration 28, loss = 0.48982811\n",
            "Iteration 29, loss = 0.46605857\n",
            "Iteration 30, loss = 0.46620327\n",
            "Iteration 31, loss = 0.47741041\n",
            "Iteration 32, loss = 0.47708577\n",
            "Iteration 33, loss = 0.46080231\n",
            "Iteration 34, loss = 0.45337132\n",
            "Iteration 35, loss = 0.44865167\n",
            "Iteration 36, loss = 0.44846065\n",
            "Iteration 37, loss = 0.44751565\n",
            "Iteration 38, loss = 0.48260150\n",
            "Iteration 39, loss = 0.48039905\n",
            "Iteration 40, loss = 0.47016315\n",
            "Iteration 41, loss = 0.44740122\n",
            "Iteration 42, loss = 0.48081424\n",
            "Iteration 43, loss = 0.48590333\n",
            "Iteration 44, loss = 0.44426348\n",
            "Iteration 45, loss = 0.44022666\n",
            "Iteration 46, loss = 0.41631785\n",
            "Iteration 47, loss = 0.43123905\n",
            "Iteration 48, loss = 0.42346138\n",
            "Iteration 49, loss = 0.43314901\n",
            "Iteration 50, loss = 0.44016570\n",
            "Iteration 51, loss = 0.43615463\n",
            "Iteration 52, loss = 0.41482828\n",
            "Iteration 53, loss = 0.42315923\n",
            "Iteration 54, loss = 0.42171274\n",
            "Iteration 55, loss = 0.42319409\n",
            "Iteration 56, loss = 0.41644387\n",
            "Iteration 57, loss = 0.45757792\n",
            "Iteration 58, loss = 0.45450915\n",
            "Iteration 59, loss = 0.42678366\n",
            "Iteration 60, loss = 0.41736927\n",
            "Iteration 61, loss = 0.40761209\n",
            "Iteration 62, loss = 0.40461965\n",
            "Iteration 63, loss = 0.42910929\n",
            "Iteration 64, loss = 0.42989431\n",
            "Iteration 65, loss = 0.40166980\n",
            "Iteration 66, loss = 0.41084624\n",
            "Iteration 67, loss = 0.40582994\n",
            "Iteration 68, loss = 0.40840447\n",
            "Iteration 69, loss = 0.41183614\n",
            "Iteration 70, loss = 0.41706549\n",
            "Iteration 71, loss = 0.40914359\n",
            "Iteration 72, loss = 0.44336261\n",
            "Iteration 73, loss = 0.44000328\n",
            "Iteration 74, loss = 0.41830593\n",
            "Iteration 75, loss = 0.42389313\n",
            "Iteration 76, loss = 0.40870051\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.06374711\n",
            "Iteration 2, loss = 0.70147919\n",
            "Iteration 3, loss = 0.61074095\n",
            "Iteration 4, loss = 0.60378668\n",
            "Iteration 5, loss = 0.56983526\n",
            "Iteration 6, loss = 0.58085293\n",
            "Iteration 7, loss = 0.56153626\n",
            "Iteration 8, loss = 0.56118337\n",
            "Iteration 9, loss = 0.54943464\n",
            "Iteration 10, loss = 0.54477049\n",
            "Iteration 11, loss = 0.56200731\n",
            "Iteration 12, loss = 0.54777593\n",
            "Iteration 13, loss = 0.54043637\n",
            "Iteration 14, loss = 0.52449921\n",
            "Iteration 15, loss = 0.54272225\n",
            "Iteration 16, loss = 0.52286462\n",
            "Iteration 17, loss = 0.51214692\n",
            "Iteration 18, loss = 0.53195850\n",
            "Iteration 19, loss = 0.53543792\n",
            "Iteration 20, loss = 0.49576771\n",
            "Iteration 21, loss = 0.49301431\n",
            "Iteration 22, loss = 0.51746576\n",
            "Iteration 23, loss = 0.52975520\n",
            "Iteration 24, loss = 0.54707156\n",
            "Iteration 25, loss = 0.51269880\n",
            "Iteration 26, loss = 0.47994421\n",
            "Iteration 27, loss = 0.47625610\n",
            "Iteration 28, loss = 0.46111917\n",
            "Iteration 29, loss = 0.47591848\n",
            "Iteration 30, loss = 0.47583229\n",
            "Iteration 31, loss = 0.47373020\n",
            "Iteration 32, loss = 0.46124840\n",
            "Iteration 33, loss = 0.45013730\n",
            "Iteration 34, loss = 0.47634573\n",
            "Iteration 35, loss = 0.45515723\n",
            "Iteration 36, loss = 0.44030059\n",
            "Iteration 37, loss = 0.44598023\n",
            "Iteration 38, loss = 0.45098988\n",
            "Iteration 39, loss = 0.48020326\n",
            "Iteration 40, loss = 0.45786346\n",
            "Iteration 41, loss = 0.43648440\n",
            "Iteration 42, loss = 0.44947973\n",
            "Iteration 43, loss = 0.45601964\n",
            "Iteration 44, loss = 0.43098727\n",
            "Iteration 45, loss = 0.43774775\n",
            "Iteration 46, loss = 0.50651205\n",
            "Iteration 47, loss = 0.49979053\n",
            "Iteration 48, loss = 0.47641222\n",
            "Iteration 49, loss = 0.48804760\n",
            "Iteration 50, loss = 0.46760716\n",
            "Iteration 51, loss = 0.44409907\n",
            "Iteration 52, loss = 0.43248101\n",
            "Iteration 53, loss = 0.42366333\n",
            "Iteration 54, loss = 0.43127893\n",
            "Iteration 55, loss = 0.42662144\n",
            "Iteration 56, loss = 0.42395785\n",
            "Iteration 57, loss = 0.43603586\n",
            "Iteration 58, loss = 0.50046051\n",
            "Iteration 59, loss = 0.44053333\n",
            "Iteration 60, loss = 0.52809175\n",
            "Iteration 61, loss = 0.48098000\n",
            "Iteration 62, loss = 0.41143237\n",
            "Iteration 63, loss = 0.40929102\n",
            "Iteration 64, loss = 0.42432564\n",
            "Iteration 65, loss = 0.47530755\n",
            "Iteration 66, loss = 0.46693007\n",
            "Iteration 67, loss = 0.45813439\n",
            "Iteration 68, loss = 0.45174809\n",
            "Iteration 69, loss = 0.46110744\n",
            "Iteration 70, loss = 0.42481750\n",
            "Iteration 71, loss = 0.43268884\n",
            "Iteration 72, loss = 0.42873851\n",
            "Iteration 73, loss = 0.41417166\n",
            "Iteration 74, loss = 0.42730600\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.82827548\n",
            "Iteration 2, loss = 0.71209615\n",
            "Iteration 3, loss = 0.61436395\n",
            "Iteration 4, loss = 0.60686675\n",
            "Iteration 5, loss = 0.59915421\n",
            "Iteration 6, loss = 0.61088049\n",
            "Iteration 7, loss = 0.59665117\n",
            "Iteration 8, loss = 0.58937711\n",
            "Iteration 9, loss = 0.57355586\n",
            "Iteration 10, loss = 0.59845063\n",
            "Iteration 11, loss = 0.55836253\n",
            "Iteration 12, loss = 0.54060619\n",
            "Iteration 13, loss = 0.56767773\n",
            "Iteration 14, loss = 0.53997637\n",
            "Iteration 15, loss = 0.53488451\n",
            "Iteration 16, loss = 0.55675674\n",
            "Iteration 17, loss = 0.53286981\n",
            "Iteration 18, loss = 0.51221472\n",
            "Iteration 19, loss = 0.52419627\n",
            "Iteration 20, loss = 0.53016467\n",
            "Iteration 21, loss = 0.52509473\n",
            "Iteration 22, loss = 0.50853155\n",
            "Iteration 23, loss = 0.51222066\n",
            "Iteration 24, loss = 0.49974499\n",
            "Iteration 25, loss = 0.49469416\n",
            "Iteration 26, loss = 0.48649699\n",
            "Iteration 27, loss = 0.49523953\n",
            "Iteration 28, loss = 0.47844332\n",
            "Iteration 29, loss = 0.46696136\n",
            "Iteration 30, loss = 0.47422698\n",
            "Iteration 31, loss = 0.47153400\n",
            "Iteration 32, loss = 0.47101467\n",
            "Iteration 33, loss = 0.47401163\n",
            "Iteration 34, loss = 0.48032856\n",
            "Iteration 35, loss = 0.48682722\n",
            "Iteration 36, loss = 0.48281737\n",
            "Iteration 37, loss = 0.48666070\n",
            "Iteration 38, loss = 0.47667819\n",
            "Iteration 39, loss = 0.49298830\n",
            "Iteration 40, loss = 0.44032383\n",
            "Iteration 41, loss = 0.44638481\n",
            "Iteration 42, loss = 0.44046070\n",
            "Iteration 43, loss = 0.43597235\n",
            "Iteration 44, loss = 0.43854421\n",
            "Iteration 45, loss = 0.43367664\n",
            "Iteration 46, loss = 0.44047812\n",
            "Iteration 47, loss = 0.44631824\n",
            "Iteration 48, loss = 0.45782121\n",
            "Iteration 49, loss = 0.47085904\n",
            "Iteration 50, loss = 0.44578443\n",
            "Iteration 51, loss = 0.43607558\n",
            "Iteration 52, loss = 0.42763258\n",
            "Iteration 53, loss = 0.43231985\n",
            "Iteration 54, loss = 0.42780064\n",
            "Iteration 55, loss = 0.41919200\n",
            "Iteration 56, loss = 0.45234920\n",
            "Iteration 57, loss = 0.44082962\n",
            "Iteration 58, loss = 0.42493396\n",
            "Iteration 59, loss = 0.41492175\n",
            "Iteration 60, loss = 0.42165163\n",
            "Iteration 61, loss = 0.41752438\n",
            "Iteration 62, loss = 0.41020651\n",
            "Iteration 63, loss = 0.43493046\n",
            "Iteration 64, loss = 0.42706070\n",
            "Iteration 65, loss = 0.45877236\n",
            "Iteration 66, loss = 0.42723694\n",
            "Iteration 67, loss = 0.42901147\n",
            "Iteration 68, loss = 0.41641890\n",
            "Iteration 69, loss = 0.41922723\n",
            "Iteration 70, loss = 0.42678999\n",
            "Iteration 71, loss = 0.41998132\n",
            "Iteration 72, loss = 0.41633256\n",
            "Iteration 73, loss = 0.39526473\n",
            "Iteration 74, loss = 0.39704886\n",
            "Iteration 75, loss = 0.39619864\n",
            "Iteration 76, loss = 0.39679898\n",
            "Iteration 77, loss = 0.40221207\n",
            "Iteration 78, loss = 0.40496865\n",
            "Iteration 79, loss = 0.39333036\n",
            "Iteration 80, loss = 0.42535698\n",
            "Iteration 81, loss = 0.40185719\n",
            "Iteration 82, loss = 0.41430406\n",
            "Iteration 83, loss = 0.42848471\n",
            "Iteration 84, loss = 0.41771966\n",
            "Iteration 85, loss = 0.44972588\n",
            "Iteration 86, loss = 0.40664629\n",
            "Iteration 87, loss = 0.43451157\n",
            "Iteration 88, loss = 0.42607915\n",
            "Iteration 89, loss = 0.41651984\n",
            "Iteration 90, loss = 0.45670465\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.51773721\n",
            "Iteration 2, loss = 0.96638446\n",
            "Iteration 3, loss = 0.78620617\n",
            "Iteration 4, loss = 0.76316424\n",
            "Iteration 5, loss = 0.70952151\n",
            "Iteration 6, loss = 0.61988249\n",
            "Iteration 7, loss = 0.60147151\n",
            "Iteration 8, loss = 0.59397196\n",
            "Iteration 9, loss = 0.61271220\n",
            "Iteration 10, loss = 0.60503141\n",
            "Iteration 11, loss = 0.60118742\n",
            "Iteration 12, loss = 0.58208076\n",
            "Iteration 13, loss = 0.56668863\n",
            "Iteration 14, loss = 0.57471687\n",
            "Iteration 15, loss = 0.54808023\n",
            "Iteration 16, loss = 0.53927797\n",
            "Iteration 17, loss = 0.53317643\n",
            "Iteration 18, loss = 0.53055815\n",
            "Iteration 19, loss = 0.52229119\n",
            "Iteration 20, loss = 0.52219873\n",
            "Iteration 21, loss = 0.52146564\n",
            "Iteration 22, loss = 0.54071604\n",
            "Iteration 23, loss = 0.52530560\n",
            "Iteration 24, loss = 0.52802734\n",
            "Iteration 25, loss = 0.52471155\n",
            "Iteration 26, loss = 0.51628162\n",
            "Iteration 27, loss = 0.50121597\n",
            "Iteration 28, loss = 0.50812561\n",
            "Iteration 29, loss = 0.48870710\n",
            "Iteration 30, loss = 0.49690779\n",
            "Iteration 31, loss = 0.50313973\n",
            "Iteration 32, loss = 0.50737021\n",
            "Iteration 33, loss = 0.51255332\n",
            "Iteration 34, loss = 0.50825777\n",
            "Iteration 35, loss = 0.46712590\n",
            "Iteration 36, loss = 0.46259056\n",
            "Iteration 37, loss = 0.46690306\n",
            "Iteration 38, loss = 0.45888093\n",
            "Iteration 39, loss = 0.46599169\n",
            "Iteration 40, loss = 0.48459614\n",
            "Iteration 41, loss = 0.50623605\n",
            "Iteration 42, loss = 0.46024100\n",
            "Iteration 43, loss = 0.44582398\n",
            "Iteration 44, loss = 0.46771909\n",
            "Iteration 45, loss = 0.54060347\n",
            "Iteration 46, loss = 0.46122456\n",
            "Iteration 47, loss = 0.45948858\n",
            "Iteration 48, loss = 0.46320851\n",
            "Iteration 49, loss = 0.46286560\n",
            "Iteration 50, loss = 0.52643994\n",
            "Iteration 51, loss = 0.46309030\n",
            "Iteration 52, loss = 0.48099252\n",
            "Iteration 53, loss = 0.44605146\n",
            "Iteration 54, loss = 0.43897642\n",
            "Iteration 55, loss = 0.44124966\n",
            "Iteration 56, loss = 0.45430595\n",
            "Iteration 57, loss = 0.44841764\n",
            "Iteration 58, loss = 0.43394348\n",
            "Iteration 59, loss = 0.43937604\n",
            "Iteration 60, loss = 0.43045693\n",
            "Iteration 61, loss = 0.50344594\n",
            "Iteration 62, loss = 0.48127733\n",
            "Iteration 63, loss = 0.44355815\n",
            "Iteration 64, loss = 0.43070559\n",
            "Iteration 65, loss = 0.42702108\n",
            "Iteration 66, loss = 0.42715376\n",
            "Iteration 67, loss = 0.41930808\n",
            "Iteration 68, loss = 0.43252995\n",
            "Iteration 69, loss = 0.42810494\n",
            "Iteration 70, loss = 0.43964214\n",
            "Iteration 71, loss = 0.45761899\n",
            "Iteration 72, loss = 0.50091268\n",
            "Iteration 73, loss = 0.43492521\n",
            "Iteration 74, loss = 0.41521997\n",
            "Iteration 75, loss = 0.40860624\n",
            "Iteration 76, loss = 0.43303459\n",
            "Iteration 77, loss = 0.44133996\n",
            "Iteration 78, loss = 0.41715185\n",
            "Iteration 79, loss = 0.44889611\n",
            "Iteration 80, loss = 0.42053453\n",
            "Iteration 81, loss = 0.41887360\n",
            "Iteration 82, loss = 0.41104616\n",
            "Iteration 83, loss = 0.43150277\n",
            "Iteration 84, loss = 0.41725905\n",
            "Iteration 85, loss = 0.41022027\n",
            "Iteration 86, loss = 0.43117752\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71473978\n",
            "Iteration 2, loss = 0.66385401\n",
            "Iteration 3, loss = 0.63126851\n",
            "Iteration 4, loss = 0.62494564\n",
            "Iteration 5, loss = 0.58787411\n",
            "Iteration 6, loss = 0.58239369\n",
            "Iteration 7, loss = 0.58715409\n",
            "Iteration 8, loss = 0.58685225\n",
            "Iteration 9, loss = 0.58066794\n",
            "Iteration 10, loss = 0.60175649\n",
            "Iteration 11, loss = 0.59574190\n",
            "Iteration 12, loss = 0.56799854\n",
            "Iteration 13, loss = 0.56635964\n",
            "Iteration 14, loss = 0.55047482\n",
            "Iteration 15, loss = 0.55384277\n",
            "Iteration 16, loss = 0.54960614\n",
            "Iteration 17, loss = 0.56393658\n",
            "Iteration 18, loss = 0.55898665\n",
            "Iteration 19, loss = 0.53080733\n",
            "Iteration 20, loss = 0.51720816\n",
            "Iteration 21, loss = 0.51140312\n",
            "Iteration 22, loss = 0.51573572\n",
            "Iteration 23, loss = 0.50748341\n",
            "Iteration 24, loss = 0.51254640\n",
            "Iteration 25, loss = 0.50082169\n",
            "Iteration 26, loss = 0.48909598\n",
            "Iteration 27, loss = 0.49705108\n",
            "Iteration 28, loss = 0.51860479\n",
            "Iteration 29, loss = 0.60215449\n",
            "Iteration 30, loss = 0.54645476\n",
            "Iteration 31, loss = 0.53628992\n",
            "Iteration 32, loss = 0.57371457\n",
            "Iteration 33, loss = 0.50114493\n",
            "Iteration 34, loss = 0.47265455\n",
            "Iteration 35, loss = 0.51081360\n",
            "Iteration 36, loss = 0.47465550\n",
            "Iteration 37, loss = 0.46665669\n",
            "Iteration 38, loss = 0.45783406\n",
            "Iteration 39, loss = 0.45952096\n",
            "Iteration 40, loss = 0.46608325\n",
            "Iteration 41, loss = 0.47715542\n",
            "Iteration 42, loss = 0.46994447\n",
            "Iteration 43, loss = 0.46046729\n",
            "Iteration 44, loss = 0.46854667\n",
            "Iteration 45, loss = 0.46215137\n",
            "Iteration 46, loss = 0.46177715\n",
            "Iteration 47, loss = 0.45870327\n",
            "Iteration 48, loss = 0.44629937\n",
            "Iteration 49, loss = 0.43839676\n",
            "Iteration 50, loss = 0.43550863\n",
            "Iteration 51, loss = 0.44208272\n",
            "Iteration 52, loss = 0.44952872\n",
            "Iteration 53, loss = 0.46678949\n",
            "Iteration 54, loss = 0.44960150\n",
            "Iteration 55, loss = 0.46383489\n",
            "Iteration 56, loss = 0.46073138\n",
            "Iteration 57, loss = 0.42956871\n",
            "Iteration 58, loss = 0.43070666\n",
            "Iteration 59, loss = 0.42854272\n",
            "Iteration 60, loss = 0.42577123\n",
            "Iteration 61, loss = 0.42001368\n",
            "Iteration 62, loss = 0.42445348\n",
            "Iteration 63, loss = 0.43535797\n",
            "Iteration 64, loss = 0.42939878\n",
            "Iteration 65, loss = 0.41462833\n",
            "Iteration 66, loss = 0.41826117\n",
            "Iteration 67, loss = 0.42045251\n",
            "Iteration 68, loss = 0.41238661\n",
            "Iteration 69, loss = 0.41682620\n",
            "Iteration 70, loss = 0.43798228\n",
            "Iteration 71, loss = 0.44316717\n",
            "Iteration 72, loss = 0.43432869\n",
            "Iteration 73, loss = 0.44314853\n",
            "Iteration 74, loss = 0.44001890\n",
            "Iteration 75, loss = 0.42720678\n",
            "Iteration 76, loss = 0.41372688\n",
            "Iteration 77, loss = 0.40908486\n",
            "Iteration 78, loss = 0.41166621\n",
            "Iteration 79, loss = 0.41201163\n",
            "Iteration 80, loss = 0.41479075\n",
            "Iteration 81, loss = 0.42272409\n",
            "Iteration 82, loss = 0.41968861\n",
            "Iteration 83, loss = 0.41665558\n",
            "Iteration 84, loss = 0.42222398\n",
            "Iteration 85, loss = 0.42422867\n",
            "Iteration 86, loss = 0.41380507\n",
            "Iteration 87, loss = 0.41134268\n",
            "Iteration 88, loss = 0.42676294\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68051881\n",
            "Iteration 2, loss = 0.67514023\n",
            "Iteration 3, loss = 0.67395147\n",
            "Iteration 4, loss = 0.67225937\n",
            "Iteration 5, loss = 0.67121018\n",
            "Iteration 6, loss = 0.66973581\n",
            "Iteration 7, loss = 0.66861005\n",
            "Iteration 8, loss = 0.66757481\n",
            "Iteration 9, loss = 0.66683760\n",
            "Iteration 10, loss = 0.66563682\n",
            "Iteration 11, loss = 0.66460084\n",
            "Iteration 12, loss = 0.66353434\n",
            "Iteration 13, loss = 0.66263689\n",
            "Iteration 14, loss = 0.66124455\n",
            "Iteration 15, loss = 0.66015025\n",
            "Iteration 16, loss = 0.65842841\n",
            "Iteration 17, loss = 0.65731552\n",
            "Iteration 18, loss = 0.65659259\n",
            "Iteration 19, loss = 0.65492835\n",
            "Iteration 20, loss = 0.65333384\n",
            "Iteration 21, loss = 0.65213688\n",
            "Iteration 22, loss = 0.65056026\n",
            "Iteration 23, loss = 0.64879597\n",
            "Iteration 24, loss = 0.64775829\n",
            "Iteration 25, loss = 0.64603327\n",
            "Iteration 26, loss = 0.64476539\n",
            "Iteration 27, loss = 0.64318898\n",
            "Iteration 28, loss = 0.64176382\n",
            "Iteration 29, loss = 0.64027785\n",
            "Iteration 30, loss = 0.63841480\n",
            "Iteration 31, loss = 0.63715731\n",
            "Iteration 32, loss = 0.63577769\n",
            "Iteration 33, loss = 0.63449193\n",
            "Iteration 34, loss = 0.63266700\n",
            "Iteration 35, loss = 0.63117912\n",
            "Iteration 36, loss = 0.63087262\n",
            "Iteration 37, loss = 0.62956210\n",
            "Iteration 38, loss = 0.62800742\n",
            "Iteration 39, loss = 0.62653575\n",
            "Iteration 40, loss = 0.62512118\n",
            "Iteration 41, loss = 0.62391989\n",
            "Iteration 42, loss = 0.62277551\n",
            "Iteration 43, loss = 0.62207017\n",
            "Iteration 44, loss = 0.62112204\n",
            "Iteration 45, loss = 0.62035699\n",
            "Iteration 46, loss = 0.62015916\n",
            "Iteration 47, loss = 0.61857999\n",
            "Iteration 48, loss = 0.61755703\n",
            "Iteration 49, loss = 0.61630773\n",
            "Iteration 50, loss = 0.61592947\n",
            "Iteration 51, loss = 0.61531996\n",
            "Iteration 52, loss = 0.61451469\n",
            "Iteration 53, loss = 0.61506538\n",
            "Iteration 54, loss = 0.61298141\n",
            "Iteration 55, loss = 0.61223496\n",
            "Iteration 56, loss = 0.61189262\n",
            "Iteration 57, loss = 0.61090096\n",
            "Iteration 58, loss = 0.61030385\n",
            "Iteration 59, loss = 0.61029041\n",
            "Iteration 60, loss = 0.60972640\n",
            "Iteration 61, loss = 0.60923801\n",
            "Iteration 62, loss = 0.60812263\n",
            "Iteration 63, loss = 0.60786335\n",
            "Iteration 64, loss = 0.60762823\n",
            "Iteration 65, loss = 0.60724630\n",
            "Iteration 66, loss = 0.60623315\n",
            "Iteration 67, loss = 0.60630689\n",
            "Iteration 68, loss = 0.60560813\n",
            "Iteration 69, loss = 0.60600351\n",
            "Iteration 70, loss = 0.60496450\n",
            "Iteration 71, loss = 0.60532527\n",
            "Iteration 72, loss = 0.60447637\n",
            "Iteration 73, loss = 0.60440229\n",
            "Iteration 74, loss = 0.60374404\n",
            "Iteration 75, loss = 0.60384284\n",
            "Iteration 76, loss = 0.60282494\n",
            "Iteration 77, loss = 0.60432827\n",
            "Iteration 78, loss = 0.60267927\n",
            "Iteration 79, loss = 0.60250192\n",
            "Iteration 80, loss = 0.60211180\n",
            "Iteration 81, loss = 0.60246613\n",
            "Iteration 82, loss = 0.60202426\n",
            "Iteration 83, loss = 0.60216981\n",
            "Iteration 84, loss = 0.60091004\n",
            "Iteration 85, loss = 0.60145403\n",
            "Iteration 86, loss = 0.60073449\n",
            "Iteration 87, loss = 0.60112597\n",
            "Iteration 88, loss = 0.60089930\n",
            "Iteration 89, loss = 0.60040470\n",
            "Iteration 90, loss = 0.60019317\n",
            "Iteration 91, loss = 0.60062558\n",
            "Iteration 92, loss = 0.59957800\n",
            "Iteration 93, loss = 0.59957816\n",
            "Iteration 94, loss = 0.60078115\n",
            "Iteration 95, loss = 0.59957214\n",
            "Iteration 96, loss = 0.59968984\n",
            "Iteration 97, loss = 0.59943593\n",
            "Iteration 98, loss = 0.59946824\n",
            "Iteration 99, loss = 0.59973753\n",
            "Iteration 100, loss = 0.59917808\n",
            "Iteration 101, loss = 0.59914985\n",
            "Iteration 102, loss = 0.59947229\n",
            "Iteration 103, loss = 0.59843646\n",
            "Iteration 104, loss = 0.59927704\n",
            "Iteration 105, loss = 0.59903430\n",
            "Iteration 106, loss = 0.59847886\n",
            "Iteration 107, loss = 0.59888051\n",
            "Iteration 108, loss = 0.59801893\n",
            "Iteration 109, loss = 0.59800279\n",
            "Iteration 110, loss = 0.59787977\n",
            "Iteration 111, loss = 0.59741589\n",
            "Iteration 112, loss = 0.59780200\n",
            "Iteration 113, loss = 0.59718010\n",
            "Iteration 114, loss = 0.59791922\n",
            "Iteration 115, loss = 0.59869111\n",
            "Iteration 116, loss = 0.59720326\n",
            "Iteration 117, loss = 0.59693527\n",
            "Iteration 118, loss = 0.59752932\n",
            "Iteration 119, loss = 0.59775148\n",
            "Iteration 120, loss = 0.59731811\n",
            "Iteration 121, loss = 0.59685180\n",
            "Iteration 122, loss = 0.59688734\n",
            "Iteration 123, loss = 0.59642068\n",
            "Iteration 124, loss = 0.59653068\n",
            "Iteration 125, loss = 0.59634382\n",
            "Iteration 126, loss = 0.59636682\n",
            "Iteration 127, loss = 0.59627849\n",
            "Iteration 128, loss = 0.59580091\n",
            "Iteration 129, loss = 0.59610268\n",
            "Iteration 130, loss = 0.59622049\n",
            "Iteration 131, loss = 0.59600482\n",
            "Iteration 132, loss = 0.59536854\n",
            "Iteration 133, loss = 0.59533592\n",
            "Iteration 134, loss = 0.59572586\n",
            "Iteration 135, loss = 0.59598762\n",
            "Iteration 136, loss = 0.59634037\n",
            "Iteration 137, loss = 0.59497871\n",
            "Iteration 138, loss = 0.59523036\n",
            "Iteration 139, loss = 0.59589230\n",
            "Iteration 140, loss = 0.59472224\n",
            "Iteration 141, loss = 0.59482279\n",
            "Iteration 142, loss = 0.59518235\n",
            "Iteration 143, loss = 0.59497443\n",
            "Iteration 144, loss = 0.59474747\n",
            "Iteration 145, loss = 0.59424084\n",
            "Iteration 146, loss = 0.59459765\n",
            "Iteration 147, loss = 0.59461246\n",
            "Iteration 148, loss = 0.59440753\n",
            "Iteration 149, loss = 0.59441012\n",
            "Iteration 150, loss = 0.59450772\n",
            "Iteration 151, loss = 0.59380102\n",
            "Iteration 152, loss = 0.59571724\n",
            "Iteration 153, loss = 0.59469989\n",
            "Iteration 154, loss = 0.59307387\n",
            "Iteration 155, loss = 0.59432829\n",
            "Iteration 156, loss = 0.59412657\n",
            "Iteration 157, loss = 0.59431002\n",
            "Iteration 158, loss = 0.59366830\n",
            "Iteration 159, loss = 0.59287133\n",
            "Iteration 160, loss = 0.59262621\n",
            "Iteration 161, loss = 0.59315147\n",
            "Iteration 162, loss = 0.59321062\n",
            "Iteration 163, loss = 0.59249328\n",
            "Iteration 164, loss = 0.59365880\n",
            "Iteration 165, loss = 0.59351499\n",
            "Iteration 166, loss = 0.59256746\n",
            "Iteration 167, loss = 0.59355158\n",
            "Iteration 168, loss = 0.59315332\n",
            "Iteration 169, loss = 0.59229814\n",
            "Iteration 170, loss = 0.59195494\n",
            "Iteration 171, loss = 0.59235369\n",
            "Iteration 172, loss = 0.59196788\n",
            "Iteration 173, loss = 0.59210138\n",
            "Iteration 174, loss = 0.59211767\n",
            "Iteration 175, loss = 0.59164751\n",
            "Iteration 176, loss = 0.59079433\n",
            "Iteration 177, loss = 0.59158497\n",
            "Iteration 178, loss = 0.59076015\n",
            "Iteration 179, loss = 0.59052146\n",
            "Iteration 180, loss = 0.59056958\n",
            "Iteration 181, loss = 0.58991718\n",
            "Iteration 182, loss = 0.58998031\n",
            "Iteration 183, loss = 0.58987450\n",
            "Iteration 184, loss = 0.58900249\n",
            "Iteration 185, loss = 0.58893947\n",
            "Iteration 186, loss = 0.59006567\n",
            "Iteration 187, loss = 0.58938894\n",
            "Iteration 188, loss = 0.58896630\n",
            "Iteration 189, loss = 0.58913124\n",
            "Iteration 190, loss = 0.58860030\n",
            "Iteration 191, loss = 0.58876171\n",
            "Iteration 192, loss = 0.58809417\n",
            "Iteration 193, loss = 0.58769534\n",
            "Iteration 194, loss = 0.58761743\n",
            "Iteration 195, loss = 0.58806744\n",
            "Iteration 196, loss = 0.58728019\n",
            "Iteration 197, loss = 0.58752370\n",
            "Iteration 198, loss = 0.58784942\n",
            "Iteration 199, loss = 0.58638469\n",
            "Iteration 200, loss = 0.58800145\n",
            "Iteration 1, loss = 0.67528105\n",
            "Iteration 2, loss = 0.67215869\n",
            "Iteration 3, loss = 0.67020976\n",
            "Iteration 4, loss = 0.66891440\n",
            "Iteration 5, loss = 0.66725428\n",
            "Iteration 6, loss = 0.66582940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 7, loss = 0.66437581\n",
            "Iteration 8, loss = 0.66322329\n",
            "Iteration 9, loss = 0.66220517\n",
            "Iteration 10, loss = 0.66085106\n",
            "Iteration 11, loss = 0.65919423\n",
            "Iteration 12, loss = 0.65849264\n",
            "Iteration 13, loss = 0.65659777\n",
            "Iteration 14, loss = 0.65567639\n",
            "Iteration 15, loss = 0.65456769\n",
            "Iteration 16, loss = 0.65297232\n",
            "Iteration 17, loss = 0.65118844\n",
            "Iteration 18, loss = 0.65027932\n",
            "Iteration 19, loss = 0.64876971\n",
            "Iteration 20, loss = 0.64783219\n",
            "Iteration 21, loss = 0.64574612\n",
            "Iteration 22, loss = 0.64504885\n",
            "Iteration 23, loss = 0.64309229\n",
            "Iteration 24, loss = 0.64141474\n",
            "Iteration 25, loss = 0.64025378\n",
            "Iteration 26, loss = 0.63865198\n",
            "Iteration 27, loss = 0.63728754\n",
            "Iteration 28, loss = 0.63582069\n",
            "Iteration 29, loss = 0.63416833\n",
            "Iteration 30, loss = 0.63261103\n",
            "Iteration 31, loss = 0.63139850\n",
            "Iteration 32, loss = 0.62987811\n",
            "Iteration 33, loss = 0.62853664\n",
            "Iteration 34, loss = 0.62741970\n",
            "Iteration 35, loss = 0.62589367\n",
            "Iteration 36, loss = 0.62482650\n",
            "Iteration 37, loss = 0.62323758\n",
            "Iteration 38, loss = 0.62245659\n",
            "Iteration 39, loss = 0.62352842\n",
            "Iteration 40, loss = 0.62044868\n",
            "Iteration 41, loss = 0.61965284\n",
            "Iteration 42, loss = 0.61841425\n",
            "Iteration 43, loss = 0.61822453\n",
            "Iteration 44, loss = 0.61701117\n",
            "Iteration 45, loss = 0.61589014\n",
            "Iteration 46, loss = 0.61548368\n",
            "Iteration 47, loss = 0.61451895\n",
            "Iteration 48, loss = 0.61394178\n",
            "Iteration 49, loss = 0.61319691\n",
            "Iteration 50, loss = 0.61268288\n",
            "Iteration 51, loss = 0.61202412\n",
            "Iteration 52, loss = 0.61171889\n",
            "Iteration 53, loss = 0.61105139\n",
            "Iteration 54, loss = 0.61073006\n",
            "Iteration 55, loss = 0.60963606\n",
            "Iteration 56, loss = 0.60944685\n",
            "Iteration 57, loss = 0.60859083\n",
            "Iteration 58, loss = 0.60818739\n",
            "Iteration 59, loss = 0.60763709\n",
            "Iteration 60, loss = 0.60761435\n",
            "Iteration 61, loss = 0.60723895\n",
            "Iteration 62, loss = 0.60678877\n",
            "Iteration 63, loss = 0.60725616\n",
            "Iteration 64, loss = 0.60608082\n",
            "Iteration 65, loss = 0.60616243\n",
            "Iteration 66, loss = 0.60579744\n",
            "Iteration 67, loss = 0.60491650\n",
            "Iteration 68, loss = 0.60455671\n",
            "Iteration 69, loss = 0.60486539\n",
            "Iteration 70, loss = 0.60415662\n",
            "Iteration 71, loss = 0.60426238\n",
            "Iteration 72, loss = 0.60349350\n",
            "Iteration 73, loss = 0.60338889\n",
            "Iteration 74, loss = 0.60311723\n",
            "Iteration 75, loss = 0.60307192\n",
            "Iteration 76, loss = 0.60307920\n",
            "Iteration 77, loss = 0.60248025\n",
            "Iteration 78, loss = 0.60318223\n",
            "Iteration 79, loss = 0.60181865\n",
            "Iteration 80, loss = 0.60147030\n",
            "Iteration 81, loss = 0.60164063\n",
            "Iteration 82, loss = 0.60125295\n",
            "Iteration 83, loss = 0.60168445\n",
            "Iteration 84, loss = 0.60135597\n",
            "Iteration 85, loss = 0.60087569\n",
            "Iteration 86, loss = 0.60076314\n",
            "Iteration 87, loss = 0.60049261\n",
            "Iteration 88, loss = 0.60088344\n",
            "Iteration 89, loss = 0.60008178\n",
            "Iteration 90, loss = 0.60041431\n",
            "Iteration 91, loss = 0.60189323\n",
            "Iteration 92, loss = 0.60003515\n",
            "Iteration 93, loss = 0.59956285\n",
            "Iteration 94, loss = 0.59974811\n",
            "Iteration 95, loss = 0.60049448\n",
            "Iteration 96, loss = 0.59991289\n",
            "Iteration 97, loss = 0.59905643\n",
            "Iteration 98, loss = 0.59895729\n",
            "Iteration 99, loss = 0.59912999\n",
            "Iteration 100, loss = 0.59930784\n",
            "Iteration 101, loss = 0.59849255\n",
            "Iteration 102, loss = 0.59910800\n",
            "Iteration 103, loss = 0.59810825\n",
            "Iteration 104, loss = 0.59784269\n",
            "Iteration 105, loss = 0.59779778\n",
            "Iteration 106, loss = 0.59782712\n",
            "Iteration 107, loss = 0.59827120\n",
            "Iteration 108, loss = 0.59804491\n",
            "Iteration 109, loss = 0.59795081\n",
            "Iteration 110, loss = 0.59798896\n",
            "Iteration 111, loss = 0.59697071\n",
            "Iteration 112, loss = 0.59683742\n",
            "Iteration 113, loss = 0.59694907\n",
            "Iteration 114, loss = 0.59677746\n",
            "Iteration 115, loss = 0.59644202\n",
            "Iteration 116, loss = 0.59703991\n",
            "Iteration 117, loss = 0.59690336\n",
            "Iteration 118, loss = 0.59589822\n",
            "Iteration 119, loss = 0.59574994\n",
            "Iteration 120, loss = 0.59563509\n",
            "Iteration 121, loss = 0.59521253\n",
            "Iteration 122, loss = 0.59505770\n",
            "Iteration 123, loss = 0.59602787\n",
            "Iteration 124, loss = 0.59604358\n",
            "Iteration 125, loss = 0.59453205\n",
            "Iteration 126, loss = 0.59496017\n",
            "Iteration 127, loss = 0.59483270\n",
            "Iteration 128, loss = 0.59498635\n",
            "Iteration 129, loss = 0.59347237\n",
            "Iteration 130, loss = 0.59391209\n",
            "Iteration 131, loss = 0.59312975\n",
            "Iteration 132, loss = 0.59298844\n",
            "Iteration 133, loss = 0.59370340\n",
            "Iteration 134, loss = 0.59294016\n",
            "Iteration 135, loss = 0.59259336\n",
            "Iteration 136, loss = 0.59267502\n",
            "Iteration 137, loss = 0.59291619\n",
            "Iteration 138, loss = 0.59202840\n",
            "Iteration 139, loss = 0.59234154\n",
            "Iteration 140, loss = 0.59179368\n",
            "Iteration 141, loss = 0.59255792\n",
            "Iteration 142, loss = 0.59088176\n",
            "Iteration 143, loss = 0.59159199\n",
            "Iteration 144, loss = 0.59057494\n",
            "Iteration 145, loss = 0.59060898\n",
            "Iteration 146, loss = 0.59045753\n",
            "Iteration 147, loss = 0.59087604\n",
            "Iteration 148, loss = 0.59019269\n",
            "Iteration 149, loss = 0.59147132\n",
            "Iteration 150, loss = 0.58990628\n",
            "Iteration 151, loss = 0.59005342\n",
            "Iteration 152, loss = 0.58995113\n",
            "Iteration 153, loss = 0.59056219\n",
            "Iteration 154, loss = 0.58893089\n",
            "Iteration 155, loss = 0.58895258\n",
            "Iteration 156, loss = 0.58940691\n",
            "Iteration 157, loss = 0.58835837\n",
            "Iteration 158, loss = 0.58910171\n",
            "Iteration 159, loss = 0.59054568\n",
            "Iteration 160, loss = 0.58905125\n",
            "Iteration 161, loss = 0.58946190\n",
            "Iteration 162, loss = 0.58886576\n",
            "Iteration 163, loss = 0.58810876\n",
            "Iteration 164, loss = 0.58934480\n",
            "Iteration 165, loss = 0.58741342\n",
            "Iteration 166, loss = 0.58969663\n",
            "Iteration 167, loss = 0.58700263\n",
            "Iteration 168, loss = 0.58703041\n",
            "Iteration 169, loss = 0.58754522\n",
            "Iteration 170, loss = 0.58726612\n",
            "Iteration 171, loss = 0.58650811\n",
            "Iteration 172, loss = 0.58668711\n",
            "Iteration 173, loss = 0.58715853\n",
            "Iteration 174, loss = 0.58651543\n",
            "Iteration 175, loss = 0.58616582\n",
            "Iteration 176, loss = 0.58700363\n",
            "Iteration 177, loss = 0.58644591\n",
            "Iteration 178, loss = 0.58546994\n",
            "Iteration 179, loss = 0.58550845\n",
            "Iteration 180, loss = 0.58517457\n",
            "Iteration 181, loss = 0.58588753\n",
            "Iteration 182, loss = 0.58489166\n",
            "Iteration 183, loss = 0.58490924\n",
            "Iteration 184, loss = 0.58465887\n",
            "Iteration 185, loss = 0.58426044\n",
            "Iteration 186, loss = 0.58446586\n",
            "Iteration 187, loss = 0.58396998\n",
            "Iteration 188, loss = 0.58341145\n",
            "Iteration 189, loss = 0.58388472\n",
            "Iteration 190, loss = 0.58356869\n",
            "Iteration 191, loss = 0.58370915\n",
            "Iteration 192, loss = 0.58519139\n",
            "Iteration 193, loss = 0.58391990\n",
            "Iteration 194, loss = 0.58242232\n",
            "Iteration 195, loss = 0.58366487\n",
            "Iteration 196, loss = 0.58224346\n",
            "Iteration 197, loss = 0.58406317\n",
            "Iteration 198, loss = 0.58277153\n",
            "Iteration 199, loss = 0.58193717\n",
            "Iteration 200, loss = 0.58386065\n",
            "Iteration 1, loss = 0.78098959\n",
            "Iteration 2, loss = 0.67993274\n",
            "Iteration 3, loss = 0.67002503\n",
            "Iteration 4, loss = 0.66873702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5, loss = 0.66686229\n",
            "Iteration 6, loss = 0.66508934\n",
            "Iteration 7, loss = 0.66294710\n",
            "Iteration 8, loss = 0.66152175\n",
            "Iteration 9, loss = 0.66027544\n",
            "Iteration 10, loss = 0.65860857\n",
            "Iteration 11, loss = 0.65764861\n",
            "Iteration 12, loss = 0.65591119\n",
            "Iteration 13, loss = 0.65449708\n",
            "Iteration 14, loss = 0.65317039\n",
            "Iteration 15, loss = 0.65197328\n",
            "Iteration 16, loss = 0.65050327\n",
            "Iteration 17, loss = 0.64947458\n",
            "Iteration 18, loss = 0.64766172\n",
            "Iteration 19, loss = 0.64628640\n",
            "Iteration 20, loss = 0.64572030\n",
            "Iteration 21, loss = 0.64415964\n",
            "Iteration 22, loss = 0.64228629\n",
            "Iteration 23, loss = 0.64143780\n",
            "Iteration 24, loss = 0.64005679\n",
            "Iteration 25, loss = 0.63870364\n",
            "Iteration 26, loss = 0.63664071\n",
            "Iteration 27, loss = 0.63528643\n",
            "Iteration 28, loss = 0.63432747\n",
            "Iteration 29, loss = 0.63254006\n",
            "Iteration 30, loss = 0.63134564\n",
            "Iteration 31, loss = 0.63108156\n",
            "Iteration 32, loss = 0.62943220\n",
            "Iteration 33, loss = 0.62755532\n",
            "Iteration 34, loss = 0.62668572\n",
            "Iteration 35, loss = 0.62504865\n",
            "Iteration 36, loss = 0.62395455\n",
            "Iteration 37, loss = 0.62307408\n",
            "Iteration 38, loss = 0.62210776\n",
            "Iteration 39, loss = 0.62110384\n",
            "Iteration 40, loss = 0.61977771\n",
            "Iteration 41, loss = 0.61920000\n",
            "Iteration 42, loss = 0.61813983\n",
            "Iteration 43, loss = 0.61732397\n",
            "Iteration 44, loss = 0.61612091\n",
            "Iteration 45, loss = 0.61561754\n",
            "Iteration 46, loss = 0.61443256\n",
            "Iteration 47, loss = 0.61372874\n",
            "Iteration 48, loss = 0.61350632\n",
            "Iteration 49, loss = 0.61220858\n",
            "Iteration 50, loss = 0.61191522\n",
            "Iteration 51, loss = 0.61201398\n",
            "Iteration 52, loss = 0.61068471\n",
            "Iteration 53, loss = 0.61001533\n",
            "Iteration 54, loss = 0.60918079\n",
            "Iteration 55, loss = 0.60880084\n",
            "Iteration 56, loss = 0.60829445\n",
            "Iteration 57, loss = 0.60850037\n",
            "Iteration 58, loss = 0.60812943\n",
            "Iteration 59, loss = 0.60693612\n",
            "Iteration 60, loss = 0.60753334\n",
            "Iteration 61, loss = 0.60653000\n",
            "Iteration 62, loss = 0.60569149\n",
            "Iteration 63, loss = 0.60537878\n",
            "Iteration 64, loss = 0.60495346\n",
            "Iteration 65, loss = 0.60519070\n",
            "Iteration 66, loss = 0.60395581\n",
            "Iteration 67, loss = 0.60459902\n",
            "Iteration 68, loss = 0.60375442\n",
            "Iteration 69, loss = 0.60377992\n",
            "Iteration 70, loss = 0.60320768\n",
            "Iteration 71, loss = 0.60387285\n",
            "Iteration 72, loss = 0.60266275\n",
            "Iteration 73, loss = 0.60289745\n",
            "Iteration 74, loss = 0.60305212\n",
            "Iteration 75, loss = 0.60238452\n",
            "Iteration 76, loss = 0.60315525\n",
            "Iteration 77, loss = 0.60101843\n",
            "Iteration 78, loss = 0.60157976\n",
            "Iteration 79, loss = 0.60052841\n",
            "Iteration 80, loss = 0.59983204\n",
            "Iteration 81, loss = 0.59999735\n",
            "Iteration 82, loss = 0.59967393\n",
            "Iteration 83, loss = 0.60150599\n",
            "Iteration 84, loss = 0.59845356\n",
            "Iteration 85, loss = 0.59975616\n",
            "Iteration 86, loss = 0.59828147\n",
            "Iteration 87, loss = 0.59903879\n",
            "Iteration 88, loss = 0.59999854\n",
            "Iteration 89, loss = 0.59793008\n",
            "Iteration 90, loss = 0.59725634\n",
            "Iteration 91, loss = 0.59669601\n",
            "Iteration 92, loss = 0.59686711\n",
            "Iteration 93, loss = 0.59630451\n",
            "Iteration 94, loss = 0.59677729\n",
            "Iteration 95, loss = 0.59594651\n",
            "Iteration 96, loss = 0.59551183\n",
            "Iteration 97, loss = 0.59503248\n",
            "Iteration 98, loss = 0.59625444\n",
            "Iteration 99, loss = 0.59528093\n",
            "Iteration 100, loss = 0.59451145\n",
            "Iteration 101, loss = 0.59340467\n",
            "Iteration 102, loss = 0.59566421\n",
            "Iteration 103, loss = 0.59401180\n",
            "Iteration 104, loss = 0.59258375\n",
            "Iteration 105, loss = 0.59154430\n",
            "Iteration 106, loss = 0.59224413\n",
            "Iteration 107, loss = 0.59113404\n",
            "Iteration 108, loss = 0.59045288\n",
            "Iteration 109, loss = 0.59073923\n",
            "Iteration 110, loss = 0.59027074\n",
            "Iteration 111, loss = 0.59009379\n",
            "Iteration 112, loss = 0.58975291\n",
            "Iteration 113, loss = 0.58904043\n",
            "Iteration 114, loss = 0.58898655\n",
            "Iteration 115, loss = 0.58971310\n",
            "Iteration 116, loss = 0.58790998\n",
            "Iteration 117, loss = 0.58779183\n",
            "Iteration 118, loss = 0.58760577\n",
            "Iteration 119, loss = 0.58741141\n",
            "Iteration 120, loss = 0.58921018\n",
            "Iteration 121, loss = 0.58732406\n",
            "Iteration 122, loss = 0.58622789\n",
            "Iteration 123, loss = 0.58787776\n",
            "Iteration 124, loss = 0.58551036\n",
            "Iteration 125, loss = 0.58580133\n",
            "Iteration 126, loss = 0.58662937\n",
            "Iteration 127, loss = 0.58517743\n",
            "Iteration 128, loss = 0.58662165\n",
            "Iteration 129, loss = 0.58474080\n",
            "Iteration 130, loss = 0.58377278\n",
            "Iteration 131, loss = 0.58423626\n",
            "Iteration 132, loss = 0.58292699\n",
            "Iteration 133, loss = 0.58293582\n",
            "Iteration 134, loss = 0.58452744\n",
            "Iteration 135, loss = 0.58326100\n",
            "Iteration 136, loss = 0.58201999\n",
            "Iteration 137, loss = 0.58184022\n",
            "Iteration 138, loss = 0.58242569\n",
            "Iteration 139, loss = 0.58055765\n",
            "Iteration 140, loss = 0.58035638\n",
            "Iteration 141, loss = 0.58080439\n",
            "Iteration 142, loss = 0.58087980\n",
            "Iteration 143, loss = 0.58168337\n",
            "Iteration 144, loss = 0.58046877\n",
            "Iteration 145, loss = 0.58015445\n",
            "Iteration 146, loss = 0.57905476\n",
            "Iteration 147, loss = 0.57863095\n",
            "Iteration 148, loss = 0.58143512\n",
            "Iteration 149, loss = 0.58079533\n",
            "Iteration 150, loss = 0.57946908\n",
            "Iteration 151, loss = 0.57832372\n",
            "Iteration 152, loss = 0.57718827\n",
            "Iteration 153, loss = 0.58303750\n",
            "Iteration 154, loss = 0.58066591\n",
            "Iteration 155, loss = 0.57943681\n",
            "Iteration 156, loss = 0.57652222\n",
            "Iteration 157, loss = 0.57561694\n",
            "Iteration 158, loss = 0.57518497\n",
            "Iteration 159, loss = 0.57541667\n",
            "Iteration 160, loss = 0.57631352\n",
            "Iteration 161, loss = 0.57497337\n",
            "Iteration 162, loss = 0.57387144\n",
            "Iteration 163, loss = 0.57447652\n",
            "Iteration 164, loss = 0.57294771\n",
            "Iteration 165, loss = 0.57414181\n",
            "Iteration 166, loss = 0.57507559\n",
            "Iteration 167, loss = 0.57270273\n",
            "Iteration 168, loss = 0.57318728\n",
            "Iteration 169, loss = 0.57182945\n",
            "Iteration 170, loss = 0.57120471\n",
            "Iteration 171, loss = 0.57329676\n",
            "Iteration 172, loss = 0.57310626\n",
            "Iteration 173, loss = 0.57042087\n",
            "Iteration 174, loss = 0.57072637\n",
            "Iteration 175, loss = 0.57062320\n",
            "Iteration 176, loss = 0.57293364\n",
            "Iteration 177, loss = 0.56994322\n",
            "Iteration 178, loss = 0.57057274\n",
            "Iteration 179, loss = 0.57076419\n",
            "Iteration 180, loss = 0.57151507\n",
            "Iteration 181, loss = 0.56885148\n",
            "Iteration 182, loss = 0.56764909\n",
            "Iteration 183, loss = 0.56730420\n",
            "Iteration 184, loss = 0.56926395\n",
            "Iteration 185, loss = 0.57060439\n",
            "Iteration 186, loss = 0.56713394\n",
            "Iteration 187, loss = 0.56763219\n",
            "Iteration 188, loss = 0.56910467\n",
            "Iteration 189, loss = 0.56769640\n",
            "Iteration 190, loss = 0.56944614\n",
            "Iteration 191, loss = 0.56620044\n",
            "Iteration 192, loss = 0.56412379\n",
            "Iteration 193, loss = 0.56633920\n",
            "Iteration 194, loss = 0.56552557\n",
            "Iteration 195, loss = 0.56515591\n",
            "Iteration 196, loss = 0.56495413\n",
            "Iteration 197, loss = 0.56325885\n",
            "Iteration 198, loss = 0.56496383\n",
            "Iteration 199, loss = 0.56323123\n",
            "Iteration 200, loss = 0.56041170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.67449943\n",
            "Iteration 2, loss = 0.66848053\n",
            "Iteration 3, loss = 0.66763377\n",
            "Iteration 4, loss = 0.66598740\n",
            "Iteration 5, loss = 0.66497615\n",
            "Iteration 6, loss = 0.66446126\n",
            "Iteration 7, loss = 0.66378672\n",
            "Iteration 8, loss = 0.66254979\n",
            "Iteration 9, loss = 0.66157077\n",
            "Iteration 10, loss = 0.66080261\n",
            "Iteration 11, loss = 0.65994431\n",
            "Iteration 12, loss = 0.65862602\n",
            "Iteration 13, loss = 0.65835734\n",
            "Iteration 14, loss = 0.65791297\n",
            "Iteration 15, loss = 0.65636965\n",
            "Iteration 16, loss = 0.65488856\n",
            "Iteration 17, loss = 0.65463144\n",
            "Iteration 18, loss = 0.65318146\n",
            "Iteration 19, loss = 0.65221919\n",
            "Iteration 20, loss = 0.65150761\n",
            "Iteration 21, loss = 0.65064150\n",
            "Iteration 22, loss = 0.64955656\n",
            "Iteration 23, loss = 0.64859163\n",
            "Iteration 24, loss = 0.64780016\n",
            "Iteration 25, loss = 0.64670036\n",
            "Iteration 26, loss = 0.64605943\n",
            "Iteration 27, loss = 0.64512784\n",
            "Iteration 28, loss = 0.64416675\n",
            "Iteration 29, loss = 0.64298702\n",
            "Iteration 30, loss = 0.64267933\n",
            "Iteration 31, loss = 0.64156685\n",
            "Iteration 32, loss = 0.64061717\n",
            "Iteration 33, loss = 0.63974877\n",
            "Iteration 34, loss = 0.63847652\n",
            "Iteration 35, loss = 0.63791389\n",
            "Iteration 36, loss = 0.63706966\n",
            "Iteration 37, loss = 0.63596885\n",
            "Iteration 38, loss = 0.63536374\n",
            "Iteration 39, loss = 0.63496675\n",
            "Iteration 40, loss = 0.63378358\n",
            "Iteration 41, loss = 0.63280043\n",
            "Iteration 42, loss = 0.63311896\n",
            "Iteration 43, loss = 0.63180983\n",
            "Iteration 44, loss = 0.63092504\n",
            "Iteration 45, loss = 0.63068945\n",
            "Iteration 46, loss = 0.62993905\n",
            "Iteration 47, loss = 0.62888352\n",
            "Iteration 48, loss = 0.62856569\n",
            "Iteration 49, loss = 0.62808113\n",
            "Iteration 50, loss = 0.62738938\n",
            "Iteration 51, loss = 0.62707726\n",
            "Iteration 52, loss = 0.62678395\n",
            "Iteration 53, loss = 0.62591653\n",
            "Iteration 54, loss = 0.62588235\n",
            "Iteration 55, loss = 0.62515883\n",
            "Iteration 56, loss = 0.62479314\n",
            "Iteration 57, loss = 0.62441154\n",
            "Iteration 58, loss = 0.62412330\n",
            "Iteration 59, loss = 0.62391975\n",
            "Iteration 60, loss = 0.62308488\n",
            "Iteration 61, loss = 0.62307981\n",
            "Iteration 62, loss = 0.62234723\n",
            "Iteration 63, loss = 0.62207848\n",
            "Iteration 64, loss = 0.62225061\n",
            "Iteration 65, loss = 0.62236240\n",
            "Iteration 66, loss = 0.62078046\n",
            "Iteration 67, loss = 0.62106828\n",
            "Iteration 68, loss = 0.62027044\n",
            "Iteration 69, loss = 0.62051458\n",
            "Iteration 70, loss = 0.61981056\n",
            "Iteration 71, loss = 0.61998191\n",
            "Iteration 72, loss = 0.62021965\n",
            "Iteration 73, loss = 0.61990191\n",
            "Iteration 74, loss = 0.61921264\n",
            "Iteration 75, loss = 0.61926904\n",
            "Iteration 76, loss = 0.61892828\n",
            "Iteration 77, loss = 0.61874702\n",
            "Iteration 78, loss = 0.61804020\n",
            "Iteration 79, loss = 0.61803775\n",
            "Iteration 80, loss = 0.61786173\n",
            "Iteration 81, loss = 0.61711735\n",
            "Iteration 82, loss = 0.61634909\n",
            "Iteration 83, loss = 0.61622338\n",
            "Iteration 84, loss = 0.61597909\n",
            "Iteration 85, loss = 0.61613352\n",
            "Iteration 86, loss = 0.61576377\n",
            "Iteration 87, loss = 0.61583704\n",
            "Iteration 88, loss = 0.61508869\n",
            "Iteration 89, loss = 0.61459624\n",
            "Iteration 90, loss = 0.61526409\n",
            "Iteration 91, loss = 0.61384709\n",
            "Iteration 92, loss = 0.61448469\n",
            "Iteration 93, loss = 0.61346671\n",
            "Iteration 94, loss = 0.61319002\n",
            "Iteration 95, loss = 0.61312764\n",
            "Iteration 96, loss = 0.61377774\n",
            "Iteration 97, loss = 0.61371432\n",
            "Iteration 98, loss = 0.61393381\n",
            "Iteration 99, loss = 0.61296632\n",
            "Iteration 100, loss = 0.61228367\n",
            "Iteration 101, loss = 0.61214594\n",
            "Iteration 102, loss = 0.61284608\n",
            "Iteration 103, loss = 0.61246671\n",
            "Iteration 104, loss = 0.61146231\n",
            "Iteration 105, loss = 0.61128626\n",
            "Iteration 106, loss = 0.61072599\n",
            "Iteration 107, loss = 0.61204142\n",
            "Iteration 108, loss = 0.61099700\n",
            "Iteration 109, loss = 0.61001363\n",
            "Iteration 110, loss = 0.61153016\n",
            "Iteration 111, loss = 0.61049375\n",
            "Iteration 112, loss = 0.60953682\n",
            "Iteration 113, loss = 0.60946107\n",
            "Iteration 114, loss = 0.60994315\n",
            "Iteration 115, loss = 0.60879255\n",
            "Iteration 116, loss = 0.60879171\n",
            "Iteration 117, loss = 0.60874073\n",
            "Iteration 118, loss = 0.60850574\n",
            "Iteration 119, loss = 0.60801995\n",
            "Iteration 120, loss = 0.60774637\n",
            "Iteration 121, loss = 0.60777450\n",
            "Iteration 122, loss = 0.60805772\n",
            "Iteration 123, loss = 0.60717923\n",
            "Iteration 124, loss = 0.60695481\n",
            "Iteration 125, loss = 0.60629935\n",
            "Iteration 126, loss = 0.60668352\n",
            "Iteration 127, loss = 0.60909387\n",
            "Iteration 128, loss = 0.60703192\n",
            "Iteration 129, loss = 0.60708785\n",
            "Iteration 130, loss = 0.60712858\n",
            "Iteration 131, loss = 0.60675931\n",
            "Iteration 132, loss = 0.60611910\n",
            "Iteration 133, loss = 0.60511854\n",
            "Iteration 134, loss = 0.60485590\n",
            "Iteration 135, loss = 0.60486654\n",
            "Iteration 136, loss = 0.60403608\n",
            "Iteration 137, loss = 0.60550497\n",
            "Iteration 138, loss = 0.60494121\n",
            "Iteration 139, loss = 0.60518953\n",
            "Iteration 140, loss = 0.60534193\n",
            "Iteration 141, loss = 0.60454263\n",
            "Iteration 142, loss = 0.60504482\n",
            "Iteration 143, loss = 0.60344984\n",
            "Iteration 144, loss = 0.60603193\n",
            "Iteration 145, loss = 0.60260652\n",
            "Iteration 146, loss = 0.60300588\n",
            "Iteration 147, loss = 0.60244571\n",
            "Iteration 148, loss = 0.60353045\n",
            "Iteration 149, loss = 0.60199093\n",
            "Iteration 150, loss = 0.60384466\n",
            "Iteration 151, loss = 0.60427103\n",
            "Iteration 152, loss = 0.60290083\n",
            "Iteration 153, loss = 0.60123095\n",
            "Iteration 154, loss = 0.60122489\n",
            "Iteration 155, loss = 0.60235136\n",
            "Iteration 156, loss = 0.60015131\n",
            "Iteration 157, loss = 0.60114173\n",
            "Iteration 158, loss = 0.60006986\n",
            "Iteration 159, loss = 0.59973612\n",
            "Iteration 160, loss = 0.59937242\n",
            "Iteration 161, loss = 0.59935156\n",
            "Iteration 162, loss = 0.59910411\n",
            "Iteration 163, loss = 0.59932009\n",
            "Iteration 164, loss = 0.59912299\n",
            "Iteration 165, loss = 0.59898608\n",
            "Iteration 166, loss = 0.59836306\n",
            "Iteration 167, loss = 0.60018319\n",
            "Iteration 168, loss = 0.59895050\n",
            "Iteration 169, loss = 0.59754002\n",
            "Iteration 170, loss = 0.59724881\n",
            "Iteration 171, loss = 0.59762123\n",
            "Iteration 172, loss = 0.59866989\n",
            "Iteration 173, loss = 0.60172277\n",
            "Iteration 174, loss = 0.59730285\n",
            "Iteration 175, loss = 0.59752275\n",
            "Iteration 176, loss = 0.59676306\n",
            "Iteration 177, loss = 0.59574590\n",
            "Iteration 178, loss = 0.59551252\n",
            "Iteration 179, loss = 0.59548782\n",
            "Iteration 180, loss = 0.59627493\n",
            "Iteration 181, loss = 0.59640097\n",
            "Iteration 182, loss = 0.59534385\n",
            "Iteration 183, loss = 0.59702632\n",
            "Iteration 184, loss = 0.59509171\n",
            "Iteration 185, loss = 0.59491510\n",
            "Iteration 186, loss = 0.59689485\n",
            "Iteration 187, loss = 0.59372370\n",
            "Iteration 188, loss = 0.59378797\n",
            "Iteration 189, loss = 0.59461300\n",
            "Iteration 190, loss = 0.59376346\n",
            "Iteration 191, loss = 0.59219458\n",
            "Iteration 192, loss = 0.59365471\n",
            "Iteration 193, loss = 0.59251676\n",
            "Iteration 194, loss = 0.59354903\n",
            "Iteration 195, loss = 0.59383343\n",
            "Iteration 196, loss = 0.59361502\n",
            "Iteration 197, loss = 0.59049545\n",
            "Iteration 198, loss = 0.59221700\n",
            "Iteration 199, loss = 0.59321924\n",
            "Iteration 200, loss = 0.59308262\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.75993516\n",
            "Iteration 2, loss = 0.68593660\n",
            "Iteration 3, loss = 0.67814027\n",
            "Iteration 4, loss = 0.67673269\n",
            "Iteration 5, loss = 0.67491120\n",
            "Iteration 6, loss = 0.67458407\n",
            "Iteration 7, loss = 0.67403760\n",
            "Iteration 8, loss = 0.67290300\n",
            "Iteration 9, loss = 0.67218210\n",
            "Iteration 10, loss = 0.67144451\n",
            "Iteration 11, loss = 0.67044451\n",
            "Iteration 12, loss = 0.67030472\n",
            "Iteration 13, loss = 0.66911553\n",
            "Iteration 14, loss = 0.66839585\n",
            "Iteration 15, loss = 0.66794639\n",
            "Iteration 16, loss = 0.66736443\n",
            "Iteration 17, loss = 0.66556792\n",
            "Iteration 18, loss = 0.66693441\n",
            "Iteration 19, loss = 0.66490552\n",
            "Iteration 20, loss = 0.66327514\n",
            "Iteration 21, loss = 0.66227452\n",
            "Iteration 22, loss = 0.66156851\n",
            "Iteration 23, loss = 0.66021515\n",
            "Iteration 24, loss = 0.65919337\n",
            "Iteration 25, loss = 0.65822316\n",
            "Iteration 26, loss = 0.65738522\n",
            "Iteration 27, loss = 0.65637744\n",
            "Iteration 28, loss = 0.65521858\n",
            "Iteration 29, loss = 0.65397427\n",
            "Iteration 30, loss = 0.65340665\n",
            "Iteration 31, loss = 0.65185288\n",
            "Iteration 32, loss = 0.65084881\n",
            "Iteration 33, loss = 0.65002713\n",
            "Iteration 34, loss = 0.64922816\n",
            "Iteration 35, loss = 0.64759296\n",
            "Iteration 36, loss = 0.64723675\n",
            "Iteration 37, loss = 0.64595847\n",
            "Iteration 38, loss = 0.64473912\n",
            "Iteration 39, loss = 0.64379384\n",
            "Iteration 40, loss = 0.64357224\n",
            "Iteration 41, loss = 0.64313792\n",
            "Iteration 42, loss = 0.64198691\n",
            "Iteration 43, loss = 0.64024793\n",
            "Iteration 44, loss = 0.63903259\n",
            "Iteration 45, loss = 0.63840762\n",
            "Iteration 46, loss = 0.63718172\n",
            "Iteration 47, loss = 0.63668769\n",
            "Iteration 48, loss = 0.63592371\n",
            "Iteration 49, loss = 0.63460040\n",
            "Iteration 50, loss = 0.63382415\n",
            "Iteration 51, loss = 0.63383773\n",
            "Iteration 52, loss = 0.63262116\n",
            "Iteration 53, loss = 0.63163027\n",
            "Iteration 54, loss = 0.63138493\n",
            "Iteration 55, loss = 0.63140482\n",
            "Iteration 56, loss = 0.62966251\n",
            "Iteration 57, loss = 0.62916903\n",
            "Iteration 58, loss = 0.62901962\n",
            "Iteration 59, loss = 0.62810420\n",
            "Iteration 60, loss = 0.62779809\n",
            "Iteration 61, loss = 0.62730607\n",
            "Iteration 62, loss = 0.62680083\n",
            "Iteration 63, loss = 0.62622543\n",
            "Iteration 64, loss = 0.62655663\n",
            "Iteration 65, loss = 0.62551802\n",
            "Iteration 66, loss = 0.62591645\n",
            "Iteration 67, loss = 0.62470815\n",
            "Iteration 68, loss = 0.62552652\n",
            "Iteration 69, loss = 0.62471828\n",
            "Iteration 70, loss = 0.62379655\n",
            "Iteration 71, loss = 0.62362553\n",
            "Iteration 72, loss = 0.62372460\n",
            "Iteration 73, loss = 0.62343968\n",
            "Iteration 74, loss = 0.62320186\n",
            "Iteration 75, loss = 0.62282628\n",
            "Iteration 76, loss = 0.62277595\n",
            "Iteration 77, loss = 0.62217815\n",
            "Iteration 78, loss = 0.62211836\n",
            "Iteration 79, loss = 0.62203515\n",
            "Iteration 80, loss = 0.62208077\n",
            "Iteration 81, loss = 0.62205105\n",
            "Iteration 82, loss = 0.62197071\n",
            "Iteration 83, loss = 0.62166764\n",
            "Iteration 84, loss = 0.62170184\n",
            "Iteration 85, loss = 0.62111767\n",
            "Iteration 86, loss = 0.62096982\n",
            "Iteration 87, loss = 0.62134733\n",
            "Iteration 88, loss = 0.62114827\n",
            "Iteration 89, loss = 0.62139350\n",
            "Iteration 90, loss = 0.62054417\n",
            "Iteration 91, loss = 0.62088651\n",
            "Iteration 92, loss = 0.62031602\n",
            "Iteration 93, loss = 0.62079066\n",
            "Iteration 94, loss = 0.62037046\n",
            "Iteration 95, loss = 0.61998384\n",
            "Iteration 96, loss = 0.62000626\n",
            "Iteration 97, loss = 0.62089379\n",
            "Iteration 98, loss = 0.61985871\n",
            "Iteration 99, loss = 0.61998123\n",
            "Iteration 100, loss = 0.61989347\n",
            "Iteration 101, loss = 0.61946053\n",
            "Iteration 102, loss = 0.62001612\n",
            "Iteration 103, loss = 0.62001522\n",
            "Iteration 104, loss = 0.62009143\n",
            "Iteration 105, loss = 0.61956565\n",
            "Iteration 106, loss = 0.61948957\n",
            "Iteration 107, loss = 0.61946693\n",
            "Iteration 108, loss = 0.61934427\n",
            "Iteration 109, loss = 0.61877882\n",
            "Iteration 110, loss = 0.61913354\n",
            "Iteration 111, loss = 0.61931076\n",
            "Iteration 112, loss = 0.61893211\n",
            "Iteration 113, loss = 0.61854847\n",
            "Iteration 114, loss = 0.61883351\n",
            "Iteration 115, loss = 0.61852093\n",
            "Iteration 116, loss = 0.61827333\n",
            "Iteration 117, loss = 0.61852206\n",
            "Iteration 118, loss = 0.61911872\n",
            "Iteration 119, loss = 0.61883534\n",
            "Iteration 120, loss = 0.62000935\n",
            "Iteration 121, loss = 0.61853122\n",
            "Iteration 122, loss = 0.61768903\n",
            "Iteration 123, loss = 0.61797163\n",
            "Iteration 124, loss = 0.61792738\n",
            "Iteration 125, loss = 0.61716837\n",
            "Iteration 126, loss = 0.61754446\n",
            "Iteration 127, loss = 0.61792133\n",
            "Iteration 128, loss = 0.61760638\n",
            "Iteration 129, loss = 0.61756628\n",
            "Iteration 130, loss = 0.61696230\n",
            "Iteration 131, loss = 0.61694862\n",
            "Iteration 132, loss = 0.61648166\n",
            "Iteration 133, loss = 0.61687825\n",
            "Iteration 134, loss = 0.61670606\n",
            "Iteration 135, loss = 0.61777629\n",
            "Iteration 136, loss = 0.61680268\n",
            "Iteration 137, loss = 0.61688970\n",
            "Iteration 138, loss = 0.61692184\n",
            "Iteration 139, loss = 0.61650849\n",
            "Iteration 140, loss = 0.61628705\n",
            "Iteration 141, loss = 0.61603609\n",
            "Iteration 142, loss = 0.61586777\n",
            "Iteration 143, loss = 0.61642088\n",
            "Iteration 144, loss = 0.61573074\n",
            "Iteration 145, loss = 0.61644347\n",
            "Iteration 146, loss = 0.61633240\n",
            "Iteration 147, loss = 0.61621670\n",
            "Iteration 148, loss = 0.61522590\n",
            "Iteration 149, loss = 0.61530464\n",
            "Iteration 150, loss = 0.61550265\n",
            "Iteration 151, loss = 0.61509955\n",
            "Iteration 152, loss = 0.61522927\n",
            "Iteration 153, loss = 0.61504513\n",
            "Iteration 154, loss = 0.61468327\n",
            "Iteration 155, loss = 0.61459683\n",
            "Iteration 156, loss = 0.61480900\n",
            "Iteration 157, loss = 0.61476104\n",
            "Iteration 158, loss = 0.61561707\n",
            "Iteration 159, loss = 0.61428851\n",
            "Iteration 160, loss = 0.61415335\n",
            "Iteration 161, loss = 0.61403741\n",
            "Iteration 162, loss = 0.61437510\n",
            "Iteration 163, loss = 0.61374243\n",
            "Iteration 164, loss = 0.61480046\n",
            "Iteration 165, loss = 0.61356899\n",
            "Iteration 166, loss = 0.61369563\n",
            "Iteration 167, loss = 0.61375535\n",
            "Iteration 168, loss = 0.61346134\n",
            "Iteration 169, loss = 0.61423861\n",
            "Iteration 170, loss = 0.61376410\n",
            "Iteration 171, loss = 0.61350305\n",
            "Iteration 172, loss = 0.61276117\n",
            "Iteration 173, loss = 0.61392098\n",
            "Iteration 174, loss = 0.61323082\n",
            "Iteration 175, loss = 0.61213883\n",
            "Iteration 176, loss = 0.61290436\n",
            "Iteration 177, loss = 0.61267355\n",
            "Iteration 178, loss = 0.61193311\n",
            "Iteration 179, loss = 0.61173968\n",
            "Iteration 180, loss = 0.61248228\n",
            "Iteration 181, loss = 0.61250152\n",
            "Iteration 182, loss = 0.61190411\n",
            "Iteration 183, loss = 0.61193339\n",
            "Iteration 184, loss = 0.61138529\n",
            "Iteration 185, loss = 0.61090586\n",
            "Iteration 186, loss = 0.61087353\n",
            "Iteration 187, loss = 0.61102786\n",
            "Iteration 188, loss = 0.61101823\n",
            "Iteration 189, loss = 0.61051924\n",
            "Iteration 190, loss = 0.61042661\n",
            "Iteration 191, loss = 0.61038015\n",
            "Iteration 192, loss = 0.61098502\n",
            "Iteration 193, loss = 0.61036689\n",
            "Iteration 194, loss = 0.61091187\n",
            "Iteration 195, loss = 0.61010078\n",
            "Iteration 196, loss = 0.60979861\n",
            "Iteration 197, loss = 0.60896063\n",
            "Iteration 198, loss = 0.61034229\n",
            "Iteration 199, loss = 0.60941421\n",
            "Iteration 200, loss = 0.60960638\n",
            "Iteration 1, loss = 0.71341066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss = 0.67282791\n",
            "Iteration 3, loss = 0.64266860\n",
            "Iteration 4, loss = 0.57666251\n",
            "Iteration 5, loss = 0.62663713\n",
            "Iteration 6, loss = 0.63056532\n",
            "Iteration 7, loss = 0.72019296\n",
            "Iteration 8, loss = 0.62663524\n",
            "Iteration 9, loss = 0.65558305\n",
            "Iteration 10, loss = 0.61826653\n",
            "Iteration 11, loss = 0.53746831\n",
            "Iteration 12, loss = 0.66218005\n",
            "Iteration 13, loss = 0.53004940\n",
            "Iteration 14, loss = 0.52711010\n",
            "Iteration 15, loss = 0.53320055\n",
            "Iteration 16, loss = 0.49390997\n",
            "Iteration 17, loss = 0.54795774\n",
            "Iteration 18, loss = 0.46978119\n",
            "Iteration 19, loss = 0.52253048\n",
            "Iteration 20, loss = 0.47026609\n",
            "Iteration 21, loss = 0.53183456\n",
            "Iteration 22, loss = 0.49430728\n",
            "Iteration 23, loss = 0.46222032\n",
            "Iteration 24, loss = 0.48644131\n",
            "Iteration 25, loss = 0.48947858\n",
            "Iteration 26, loss = 0.52338563\n",
            "Iteration 27, loss = 0.44770238\n",
            "Iteration 28, loss = 0.44183126\n",
            "Iteration 29, loss = 0.45801602\n",
            "Iteration 30, loss = 0.44533549\n",
            "Iteration 31, loss = 0.45671885\n",
            "Iteration 32, loss = 0.48317336\n",
            "Iteration 33, loss = 0.46327302\n",
            "Iteration 34, loss = 0.46060754\n",
            "Iteration 35, loss = 0.45530801\n",
            "Iteration 36, loss = 0.44453549\n",
            "Iteration 37, loss = 0.45856568\n",
            "Iteration 38, loss = 0.45361384\n",
            "Iteration 39, loss = 0.45626945\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.73464951\n",
            "Iteration 2, loss = 0.60068595\n",
            "Iteration 3, loss = 0.63470472\n",
            "Iteration 4, loss = 0.72818756\n",
            "Iteration 5, loss = 0.55480449\n",
            "Iteration 6, loss = 0.62629294\n",
            "Iteration 7, loss = 0.53473239\n",
            "Iteration 8, loss = 0.58291693\n",
            "Iteration 9, loss = 0.57145339\n",
            "Iteration 10, loss = 0.50539905\n",
            "Iteration 11, loss = 0.49599086\n",
            "Iteration 12, loss = 0.50080057\n",
            "Iteration 13, loss = 0.53825235\n",
            "Iteration 14, loss = 0.52462290\n",
            "Iteration 15, loss = 0.49139853\n",
            "Iteration 16, loss = 0.52156655\n",
            "Iteration 17, loss = 0.52606963\n",
            "Iteration 18, loss = 0.51286549\n",
            "Iteration 19, loss = 0.46615653\n",
            "Iteration 20, loss = 0.44968146\n",
            "Iteration 21, loss = 0.48845690\n",
            "Iteration 22, loss = 0.43810762\n",
            "Iteration 23, loss = 0.46050310\n",
            "Iteration 24, loss = 0.44664939\n",
            "Iteration 25, loss = 0.53906138\n",
            "Iteration 26, loss = 0.46200269\n",
            "Iteration 27, loss = 0.51714601\n",
            "Iteration 28, loss = 0.46400478\n",
            "Iteration 29, loss = 0.45890625\n",
            "Iteration 30, loss = 0.45423574\n",
            "Iteration 31, loss = 0.44779151\n",
            "Iteration 32, loss = 0.43763687\n",
            "Iteration 33, loss = 0.45889192\n",
            "Iteration 34, loss = 0.43367776\n",
            "Iteration 35, loss = 0.45204847\n",
            "Iteration 36, loss = 0.42408531\n",
            "Iteration 37, loss = 0.43052547\n",
            "Iteration 38, loss = 0.47796108\n",
            "Iteration 39, loss = 0.48019954\n",
            "Iteration 40, loss = 0.42656378\n",
            "Iteration 41, loss = 0.46994634\n",
            "Iteration 42, loss = 0.41955891\n",
            "Iteration 43, loss = 0.42751462\n",
            "Iteration 44, loss = 0.41889845\n",
            "Iteration 45, loss = 0.42473544\n",
            "Iteration 46, loss = 0.43621516\n",
            "Iteration 47, loss = 0.40403473\n",
            "Iteration 48, loss = 0.40821703\n",
            "Iteration 49, loss = 0.41998085\n",
            "Iteration 50, loss = 0.41426440\n",
            "Iteration 51, loss = 0.39331926\n",
            "Iteration 52, loss = 0.44094863\n",
            "Iteration 53, loss = 0.45886169\n",
            "Iteration 54, loss = 0.39821532\n",
            "Iteration 55, loss = 0.40240051\n",
            "Iteration 56, loss = 0.44405744\n",
            "Iteration 57, loss = 0.39339892\n",
            "Iteration 58, loss = 0.44219100\n",
            "Iteration 59, loss = 0.40594056\n",
            "Iteration 60, loss = 0.39901085\n",
            "Iteration 61, loss = 0.41685120\n",
            "Iteration 62, loss = 0.39336759\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.89708188\n",
            "Iteration 2, loss = 0.67355228\n",
            "Iteration 3, loss = 0.61954122\n",
            "Iteration 4, loss = 0.67398601\n",
            "Iteration 5, loss = 0.55618568\n",
            "Iteration 6, loss = 0.65541799\n",
            "Iteration 7, loss = 0.56273426\n",
            "Iteration 8, loss = 0.58783581\n",
            "Iteration 9, loss = 0.64750984\n",
            "Iteration 10, loss = 0.65557028\n",
            "Iteration 11, loss = 0.53832164\n",
            "Iteration 12, loss = 0.50368222\n",
            "Iteration 13, loss = 0.51804429\n",
            "Iteration 14, loss = 0.57304750\n",
            "Iteration 15, loss = 0.46188010\n",
            "Iteration 16, loss = 0.47408769\n",
            "Iteration 17, loss = 0.50391858\n",
            "Iteration 18, loss = 0.46791845\n",
            "Iteration 19, loss = 0.49855451\n",
            "Iteration 20, loss = 0.50176062\n",
            "Iteration 21, loss = 0.53236098\n",
            "Iteration 22, loss = 0.46886016\n",
            "Iteration 23, loss = 0.45405469\n",
            "Iteration 24, loss = 0.46910101\n",
            "Iteration 25, loss = 0.48963231\n",
            "Iteration 26, loss = 0.56303433\n",
            "Iteration 27, loss = 0.47584920\n",
            "Iteration 28, loss = 0.43922886\n",
            "Iteration 29, loss = 0.46364720\n",
            "Iteration 30, loss = 0.44890527\n",
            "Iteration 31, loss = 0.43279120\n",
            "Iteration 32, loss = 0.45965639\n",
            "Iteration 33, loss = 0.44780992\n",
            "Iteration 34, loss = 0.42749052\n",
            "Iteration 35, loss = 0.47784201\n",
            "Iteration 36, loss = 0.43820695\n",
            "Iteration 37, loss = 0.42855230\n",
            "Iteration 38, loss = 0.50330360\n",
            "Iteration 39, loss = 0.42504727\n",
            "Iteration 40, loss = 0.46487413\n",
            "Iteration 41, loss = 0.44778391\n",
            "Iteration 42, loss = 0.43358118\n",
            "Iteration 43, loss = 0.42615672\n",
            "Iteration 44, loss = 0.48396711\n",
            "Iteration 45, loss = 0.44452524\n",
            "Iteration 46, loss = 0.46130581\n",
            "Iteration 47, loss = 0.42523981\n",
            "Iteration 48, loss = 0.43562610\n",
            "Iteration 49, loss = 0.43673671\n",
            "Iteration 50, loss = 0.42544381\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.82737861\n",
            "Iteration 2, loss = 0.72386855\n",
            "Iteration 3, loss = 0.73416612\n",
            "Iteration 4, loss = 0.69946262\n",
            "Iteration 5, loss = 0.71063345\n",
            "Iteration 6, loss = 0.58178822\n",
            "Iteration 7, loss = 0.65261627\n",
            "Iteration 8, loss = 0.59508228\n",
            "Iteration 9, loss = 0.69482627\n",
            "Iteration 10, loss = 0.61377043\n",
            "Iteration 11, loss = 0.54076138\n",
            "Iteration 12, loss = 0.54025381\n",
            "Iteration 13, loss = 0.51769965\n",
            "Iteration 14, loss = 0.51795177\n",
            "Iteration 15, loss = 0.51529493\n",
            "Iteration 16, loss = 0.51933980\n",
            "Iteration 17, loss = 0.51119210\n",
            "Iteration 18, loss = 0.58443211\n",
            "Iteration 19, loss = 0.51306349\n",
            "Iteration 20, loss = 0.53529421\n",
            "Iteration 21, loss = 0.50564019\n",
            "Iteration 22, loss = 0.51494936\n",
            "Iteration 23, loss = 0.46646161\n",
            "Iteration 24, loss = 0.49155670\n",
            "Iteration 25, loss = 0.48423452\n",
            "Iteration 26, loss = 0.48637097\n",
            "Iteration 27, loss = 0.48476503\n",
            "Iteration 28, loss = 0.51637758\n",
            "Iteration 29, loss = 0.46626769\n",
            "Iteration 30, loss = 0.44946908\n",
            "Iteration 31, loss = 0.49212582\n",
            "Iteration 32, loss = 0.45040026\n",
            "Iteration 33, loss = 0.45136229\n",
            "Iteration 34, loss = 0.50819704\n",
            "Iteration 35, loss = 0.43775764\n",
            "Iteration 36, loss = 0.46690962\n",
            "Iteration 37, loss = 0.45860592\n",
            "Iteration 38, loss = 0.42823462\n",
            "Iteration 39, loss = 0.45242898\n",
            "Iteration 40, loss = 0.52324830\n",
            "Iteration 41, loss = 0.44602314\n",
            "Iteration 42, loss = 0.46443672\n",
            "Iteration 43, loss = 0.43724261\n",
            "Iteration 44, loss = 0.45832565\n",
            "Iteration 45, loss = 0.44081802\n",
            "Iteration 46, loss = 0.48773631\n",
            "Iteration 47, loss = 0.45491844\n",
            "Iteration 48, loss = 0.42877484\n",
            "Iteration 49, loss = 0.42994975\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.79177653\n",
            "Iteration 2, loss = 0.76321140\n",
            "Iteration 3, loss = 0.69788361\n",
            "Iteration 4, loss = 0.67139323\n",
            "Iteration 5, loss = 0.71620084\n",
            "Iteration 6, loss = 0.65984684\n",
            "Iteration 7, loss = 0.61744922\n",
            "Iteration 8, loss = 0.60032437\n",
            "Iteration 9, loss = 0.59888209\n",
            "Iteration 10, loss = 0.67318096\n",
            "Iteration 11, loss = 0.74144994\n",
            "Iteration 12, loss = 0.53002449\n",
            "Iteration 13, loss = 0.52921408\n",
            "Iteration 14, loss = 0.60140874\n",
            "Iteration 15, loss = 0.54814392\n",
            "Iteration 16, loss = 0.49525457\n",
            "Iteration 17, loss = 0.50555721\n",
            "Iteration 18, loss = 0.50613565\n",
            "Iteration 19, loss = 0.51924010\n",
            "Iteration 20, loss = 0.52125125\n",
            "Iteration 21, loss = 0.47521135\n",
            "Iteration 22, loss = 0.55891631\n",
            "Iteration 23, loss = 0.49379405\n",
            "Iteration 24, loss = 0.52535628\n",
            "Iteration 25, loss = 0.46822091\n",
            "Iteration 26, loss = 0.46675416\n",
            "Iteration 27, loss = 0.46690274\n",
            "Iteration 28, loss = 0.47854026\n",
            "Iteration 29, loss = 0.49159070\n",
            "Iteration 30, loss = 0.52759642\n",
            "Iteration 31, loss = 0.50077758\n",
            "Iteration 32, loss = 0.52154508\n",
            "Iteration 33, loss = 0.52009563\n",
            "Iteration 34, loss = 0.53725729\n",
            "Iteration 35, loss = 0.48345633\n",
            "Iteration 36, loss = 0.43562010\n",
            "Iteration 37, loss = 0.47340011\n",
            "Iteration 38, loss = 0.48382866\n",
            "Iteration 39, loss = 0.47156712\n",
            "Iteration 40, loss = 0.45731994\n",
            "Iteration 41, loss = 0.50198538\n",
            "Iteration 42, loss = 0.44399744\n",
            "Iteration 43, loss = 0.43663240\n",
            "Iteration 44, loss = 0.44501550\n",
            "Iteration 45, loss = 0.45964608\n",
            "Iteration 46, loss = 0.45570697\n",
            "Iteration 47, loss = 0.47330781\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68138192\n",
            "Iteration 2, loss = 0.67580742\n",
            "Iteration 3, loss = 0.67008713\n",
            "Iteration 4, loss = 0.66614835\n",
            "Iteration 5, loss = 0.66299155\n",
            "Iteration 6, loss = 0.65823917\n",
            "Iteration 7, loss = 0.65415970\n",
            "Iteration 8, loss = 0.64961405\n",
            "Iteration 9, loss = 0.65018942\n",
            "Iteration 10, loss = 0.64247555\n",
            "Iteration 11, loss = 0.63941570\n",
            "Iteration 12, loss = 0.63651164\n",
            "Iteration 13, loss = 0.63328772\n",
            "Iteration 14, loss = 0.63063838\n",
            "Iteration 15, loss = 0.62763042\n",
            "Iteration 16, loss = 0.62519651\n",
            "Iteration 17, loss = 0.62194914\n",
            "Iteration 18, loss = 0.62014551\n",
            "Iteration 19, loss = 0.61973836\n",
            "Iteration 20, loss = 0.61530893\n",
            "Iteration 21, loss = 0.61455281\n",
            "Iteration 22, loss = 0.61413050\n",
            "Iteration 23, loss = 0.61575584\n",
            "Iteration 24, loss = 0.61064936\n",
            "Iteration 25, loss = 0.61187355\n",
            "Iteration 26, loss = 0.60987119\n",
            "Iteration 27, loss = 0.60848404\n",
            "Iteration 28, loss = 0.60803871\n",
            "Iteration 29, loss = 0.60659988\n",
            "Iteration 30, loss = 0.60768044\n",
            "Iteration 31, loss = 0.60636234\n",
            "Iteration 32, loss = 0.60473234\n",
            "Iteration 33, loss = 0.60447341\n",
            "Iteration 34, loss = 0.60633568\n",
            "Iteration 35, loss = 0.60386304\n",
            "Iteration 36, loss = 0.60284068\n",
            "Iteration 37, loss = 0.60327359\n",
            "Iteration 38, loss = 0.60344644\n",
            "Iteration 39, loss = 0.60428080\n",
            "Iteration 40, loss = 0.60232036\n",
            "Iteration 41, loss = 0.60178612\n",
            "Iteration 42, loss = 0.60325960\n",
            "Iteration 43, loss = 0.60185105\n",
            "Iteration 44, loss = 0.60195494\n",
            "Iteration 45, loss = 0.60143505\n",
            "Iteration 46, loss = 0.60043216\n",
            "Iteration 47, loss = 0.60098375\n",
            "Iteration 48, loss = 0.60012464\n",
            "Iteration 49, loss = 0.60103387\n",
            "Iteration 50, loss = 0.59977316\n",
            "Iteration 51, loss = 0.59981205\n",
            "Iteration 52, loss = 0.60003566\n",
            "Iteration 53, loss = 0.59881642\n",
            "Iteration 54, loss = 0.59907326\n",
            "Iteration 55, loss = 0.60041830\n",
            "Iteration 56, loss = 0.59926721\n",
            "Iteration 57, loss = 0.59794994\n",
            "Iteration 58, loss = 0.60031707\n",
            "Iteration 59, loss = 0.59617973\n",
            "Iteration 60, loss = 0.59792155\n",
            "Iteration 61, loss = 0.59916526\n",
            "Iteration 62, loss = 0.59691857\n",
            "Iteration 63, loss = 0.59639210\n",
            "Iteration 64, loss = 0.59829477\n",
            "Iteration 65, loss = 0.59863863\n",
            "Iteration 66, loss = 0.59858025\n",
            "Iteration 67, loss = 0.59759106\n",
            "Iteration 68, loss = 0.59760314\n",
            "Iteration 69, loss = 0.59545279\n",
            "Iteration 70, loss = 0.59554941\n",
            "Iteration 71, loss = 0.59781761\n",
            "Iteration 72, loss = 0.59606995\n",
            "Iteration 73, loss = 0.59540699\n",
            "Iteration 74, loss = 0.59561680\n",
            "Iteration 75, loss = 0.59483035\n",
            "Iteration 76, loss = 0.59439541\n",
            "Iteration 77, loss = 0.59420329\n",
            "Iteration 78, loss = 0.59510019\n",
            "Iteration 79, loss = 0.59329694\n",
            "Iteration 80, loss = 0.59310252\n",
            "Iteration 81, loss = 0.59280862\n",
            "Iteration 82, loss = 0.59292029\n",
            "Iteration 83, loss = 0.59410651\n",
            "Iteration 84, loss = 0.59350219\n",
            "Iteration 85, loss = 0.59399088\n",
            "Iteration 86, loss = 0.59277319\n",
            "Iteration 87, loss = 0.59165452\n",
            "Iteration 88, loss = 0.59315673\n",
            "Iteration 89, loss = 0.59264073\n",
            "Iteration 90, loss = 0.59068653\n",
            "Iteration 91, loss = 0.59186954\n",
            "Iteration 92, loss = 0.59059394\n",
            "Iteration 93, loss = 0.59035095\n",
            "Iteration 94, loss = 0.59261254\n",
            "Iteration 95, loss = 0.59045746\n",
            "Iteration 96, loss = 0.58995606\n",
            "Iteration 97, loss = 0.58929395\n",
            "Iteration 98, loss = 0.58873869\n",
            "Iteration 99, loss = 0.59132249\n",
            "Iteration 100, loss = 0.58945995\n",
            "Iteration 101, loss = 0.59122240\n",
            "Iteration 102, loss = 0.58938131\n",
            "Iteration 103, loss = 0.58872226\n",
            "Iteration 104, loss = 0.58932595\n",
            "Iteration 105, loss = 0.58800887\n",
            "Iteration 106, loss = 0.58751521\n",
            "Iteration 107, loss = 0.58673481\n",
            "Iteration 108, loss = 0.58787906\n",
            "Iteration 109, loss = 0.58771284\n",
            "Iteration 110, loss = 0.58591275\n",
            "Iteration 111, loss = 0.58545911\n",
            "Iteration 112, loss = 0.58620730\n",
            "Iteration 113, loss = 0.58502250\n",
            "Iteration 114, loss = 0.58639250\n",
            "Iteration 115, loss = 0.58498670\n",
            "Iteration 116, loss = 0.58747028\n",
            "Iteration 117, loss = 0.58466602\n",
            "Iteration 118, loss = 0.58359621\n",
            "Iteration 119, loss = 0.58592859\n",
            "Iteration 120, loss = 0.58511267\n",
            "Iteration 121, loss = 0.58490672\n",
            "Iteration 122, loss = 0.58457616\n",
            "Iteration 123, loss = 0.58486561\n",
            "Iteration 124, loss = 0.58110476\n",
            "Iteration 125, loss = 0.58693722\n",
            "Iteration 126, loss = 0.58326764\n",
            "Iteration 127, loss = 0.58237909\n",
            "Iteration 128, loss = 0.58199625\n",
            "Iteration 129, loss = 0.58431520\n",
            "Iteration 130, loss = 0.58442354\n",
            "Iteration 131, loss = 0.58058967\n",
            "Iteration 132, loss = 0.58205036\n",
            "Iteration 133, loss = 0.58116560\n",
            "Iteration 134, loss = 0.58138942\n",
            "Iteration 135, loss = 0.58207275\n",
            "Iteration 136, loss = 0.57938420\n",
            "Iteration 137, loss = 0.58096295\n",
            "Iteration 138, loss = 0.58007158\n",
            "Iteration 139, loss = 0.57899690\n",
            "Iteration 140, loss = 0.58007853\n",
            "Iteration 141, loss = 0.57858931\n",
            "Iteration 142, loss = 0.57603618\n",
            "Iteration 143, loss = 0.58177789\n",
            "Iteration 144, loss = 0.57629424\n",
            "Iteration 145, loss = 0.57755005\n",
            "Iteration 146, loss = 0.58279635\n",
            "Iteration 147, loss = 0.57570237\n",
            "Iteration 148, loss = 0.57668977\n",
            "Iteration 149, loss = 0.57802007\n",
            "Iteration 150, loss = 0.57825773\n",
            "Iteration 151, loss = 0.57668820\n",
            "Iteration 152, loss = 0.57343388\n",
            "Iteration 153, loss = 0.57824125\n",
            "Iteration 154, loss = 0.57407353\n",
            "Iteration 155, loss = 0.57393331\n",
            "Iteration 156, loss = 0.56992237\n",
            "Iteration 157, loss = 0.57373043\n",
            "Iteration 158, loss = 0.57661262\n",
            "Iteration 159, loss = 0.57541155\n",
            "Iteration 160, loss = 0.57437430\n",
            "Iteration 161, loss = 0.57650698\n",
            "Iteration 162, loss = 0.57092115\n",
            "Iteration 163, loss = 0.57873593\n",
            "Iteration 164, loss = 0.57859326\n",
            "Iteration 165, loss = 0.57102815\n",
            "Iteration 166, loss = 0.57262296\n",
            "Iteration 167, loss = 0.57085311\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70324719\n",
            "Iteration 2, loss = 0.67058692\n",
            "Iteration 3, loss = 0.66613392\n",
            "Iteration 4, loss = 0.66256138\n",
            "Iteration 5, loss = 0.65995965\n",
            "Iteration 6, loss = 0.65709849\n",
            "Iteration 7, loss = 0.65347076\n",
            "Iteration 8, loss = 0.64834301\n",
            "Iteration 9, loss = 0.64498654\n",
            "Iteration 10, loss = 0.64121615\n",
            "Iteration 11, loss = 0.63838956\n",
            "Iteration 12, loss = 0.63570305\n",
            "Iteration 13, loss = 0.63323588\n",
            "Iteration 14, loss = 0.62976123\n",
            "Iteration 15, loss = 0.62740031\n",
            "Iteration 16, loss = 0.62705268\n",
            "Iteration 17, loss = 0.62301563\n",
            "Iteration 18, loss = 0.62026684\n",
            "Iteration 19, loss = 0.61862157\n",
            "Iteration 20, loss = 0.61838940\n",
            "Iteration 21, loss = 0.61791899\n",
            "Iteration 22, loss = 0.61408523\n",
            "Iteration 23, loss = 0.61414588\n",
            "Iteration 24, loss = 0.61195107\n",
            "Iteration 25, loss = 0.61178142\n",
            "Iteration 26, loss = 0.61071814\n",
            "Iteration 27, loss = 0.61025907\n",
            "Iteration 28, loss = 0.60941665\n",
            "Iteration 29, loss = 0.60926895\n",
            "Iteration 30, loss = 0.60690191\n",
            "Iteration 31, loss = 0.60830221\n",
            "Iteration 32, loss = 0.60974742\n",
            "Iteration 33, loss = 0.60854111\n",
            "Iteration 34, loss = 0.60633626\n",
            "Iteration 35, loss = 0.60579401\n",
            "Iteration 36, loss = 0.60734043\n",
            "Iteration 37, loss = 0.60584158\n",
            "Iteration 38, loss = 0.60503644\n",
            "Iteration 39, loss = 0.60475508\n",
            "Iteration 40, loss = 0.60715978\n",
            "Iteration 41, loss = 0.60581029\n",
            "Iteration 42, loss = 0.60588545\n",
            "Iteration 43, loss = 0.60436979\n",
            "Iteration 44, loss = 0.60431828\n",
            "Iteration 45, loss = 0.60302790\n",
            "Iteration 46, loss = 0.60397670\n",
            "Iteration 47, loss = 0.60351112\n",
            "Iteration 48, loss = 0.60253168\n",
            "Iteration 49, loss = 0.60340246\n",
            "Iteration 50, loss = 0.60250922\n",
            "Iteration 51, loss = 0.60318548\n",
            "Iteration 52, loss = 0.60257160\n",
            "Iteration 53, loss = 0.60181744\n",
            "Iteration 54, loss = 0.60217364\n",
            "Iteration 55, loss = 0.60083640\n",
            "Iteration 56, loss = 0.60207472\n",
            "Iteration 57, loss = 0.60109693\n",
            "Iteration 58, loss = 0.60196972\n",
            "Iteration 59, loss = 0.60019836\n",
            "Iteration 60, loss = 0.60010517\n",
            "Iteration 61, loss = 0.60062844\n",
            "Iteration 62, loss = 0.60008882\n",
            "Iteration 63, loss = 0.60052232\n",
            "Iteration 64, loss = 0.60117272\n",
            "Iteration 65, loss = 0.60023702\n",
            "Iteration 66, loss = 0.59887553\n",
            "Iteration 67, loss = 0.59942040\n",
            "Iteration 68, loss = 0.60055879\n",
            "Iteration 69, loss = 0.60057898\n",
            "Iteration 70, loss = 0.59902390\n",
            "Iteration 71, loss = 0.59862293\n",
            "Iteration 72, loss = 0.59768170\n",
            "Iteration 73, loss = 0.59775940\n",
            "Iteration 74, loss = 0.59811147\n",
            "Iteration 75, loss = 0.59754734\n",
            "Iteration 76, loss = 0.59687222\n",
            "Iteration 77, loss = 0.59709791\n",
            "Iteration 78, loss = 0.59762017\n",
            "Iteration 79, loss = 0.59732781\n",
            "Iteration 80, loss = 0.59594222\n",
            "Iteration 81, loss = 0.59610719\n",
            "Iteration 82, loss = 0.59687202\n",
            "Iteration 83, loss = 0.59528747\n",
            "Iteration 84, loss = 0.59658754\n",
            "Iteration 85, loss = 0.59563649\n",
            "Iteration 86, loss = 0.59552194\n",
            "Iteration 87, loss = 0.59657796\n",
            "Iteration 88, loss = 0.59577592\n",
            "Iteration 89, loss = 0.59430759\n",
            "Iteration 90, loss = 0.59412763\n",
            "Iteration 91, loss = 0.59415789\n",
            "Iteration 92, loss = 0.59356063\n",
            "Iteration 93, loss = 0.59643998\n",
            "Iteration 94, loss = 0.59525134\n",
            "Iteration 95, loss = 0.59389720\n",
            "Iteration 96, loss = 0.59537825\n",
            "Iteration 97, loss = 0.59533909\n",
            "Iteration 98, loss = 0.59423104\n",
            "Iteration 99, loss = 0.59389790\n",
            "Iteration 100, loss = 0.59413218\n",
            "Iteration 101, loss = 0.59418457\n",
            "Iteration 102, loss = 0.59270893\n",
            "Iteration 103, loss = 0.59232207\n",
            "Iteration 104, loss = 0.59172038\n",
            "Iteration 105, loss = 0.59364653\n",
            "Iteration 106, loss = 0.59194919\n",
            "Iteration 107, loss = 0.59076051\n",
            "Iteration 108, loss = 0.59501172\n",
            "Iteration 109, loss = 0.59123571\n",
            "Iteration 110, loss = 0.59133879\n",
            "Iteration 111, loss = 0.59152188\n",
            "Iteration 112, loss = 0.59346594\n",
            "Iteration 113, loss = 0.59129941\n",
            "Iteration 114, loss = 0.59053668\n",
            "Iteration 115, loss = 0.59029890\n",
            "Iteration 116, loss = 0.59046838\n",
            "Iteration 117, loss = 0.59052453\n",
            "Iteration 118, loss = 0.59415769\n",
            "Iteration 119, loss = 0.59103241\n",
            "Iteration 120, loss = 0.59225563\n",
            "Iteration 121, loss = 0.59102897\n",
            "Iteration 122, loss = 0.59077655\n",
            "Iteration 123, loss = 0.58886613\n",
            "Iteration 124, loss = 0.58981018\n",
            "Iteration 125, loss = 0.58767127\n",
            "Iteration 126, loss = 0.59037649\n",
            "Iteration 127, loss = 0.58855799\n",
            "Iteration 128, loss = 0.58845079\n",
            "Iteration 129, loss = 0.58670826\n",
            "Iteration 130, loss = 0.58847661\n",
            "Iteration 131, loss = 0.58928316\n",
            "Iteration 132, loss = 0.58880838\n",
            "Iteration 133, loss = 0.59128145\n",
            "Iteration 134, loss = 0.58888393\n",
            "Iteration 135, loss = 0.58767215\n",
            "Iteration 136, loss = 0.58613620\n",
            "Iteration 137, loss = 0.58697460\n",
            "Iteration 138, loss = 0.58581750\n",
            "Iteration 139, loss = 0.58598540\n",
            "Iteration 140, loss = 0.58622785\n",
            "Iteration 141, loss = 0.58700162\n",
            "Iteration 142, loss = 0.58572070\n",
            "Iteration 143, loss = 0.58612989\n",
            "Iteration 144, loss = 0.58624331\n",
            "Iteration 145, loss = 0.58671927\n",
            "Iteration 146, loss = 0.58845349\n",
            "Iteration 147, loss = 0.58571094\n",
            "Iteration 148, loss = 0.58552959\n",
            "Iteration 149, loss = 0.58430239\n",
            "Iteration 150, loss = 0.58527643\n",
            "Iteration 151, loss = 0.58527197\n",
            "Iteration 152, loss = 0.58655175\n",
            "Iteration 153, loss = 0.58485079\n",
            "Iteration 154, loss = 0.58431976\n",
            "Iteration 155, loss = 0.58576812\n",
            "Iteration 156, loss = 0.58356024\n",
            "Iteration 157, loss = 0.58352930\n",
            "Iteration 158, loss = 0.58323321\n",
            "Iteration 159, loss = 0.58830089\n",
            "Iteration 160, loss = 0.58315817\n",
            "Iteration 161, loss = 0.58469313\n",
            "Iteration 162, loss = 0.58422810\n",
            "Iteration 163, loss = 0.58444170\n",
            "Iteration 164, loss = 0.58142050\n",
            "Iteration 165, loss = 0.58410107\n",
            "Iteration 166, loss = 0.58062160\n",
            "Iteration 167, loss = 0.58336892\n",
            "Iteration 168, loss = 0.58156939\n",
            "Iteration 169, loss = 0.58037711\n",
            "Iteration 170, loss = 0.58517088\n",
            "Iteration 171, loss = 0.58070129\n",
            "Iteration 172, loss = 0.58277414\n",
            "Iteration 173, loss = 0.58217801\n",
            "Iteration 174, loss = 0.57928618\n",
            "Iteration 175, loss = 0.58026453\n",
            "Iteration 176, loss = 0.57794392\n",
            "Iteration 177, loss = 0.58019917\n",
            "Iteration 178, loss = 0.57996785\n",
            "Iteration 179, loss = 0.58223403\n",
            "Iteration 180, loss = 0.58001102\n",
            "Iteration 181, loss = 0.57926432\n",
            "Iteration 182, loss = 0.57994072\n",
            "Iteration 183, loss = 0.57828903\n",
            "Iteration 184, loss = 0.58067416\n",
            "Iteration 185, loss = 0.57768272\n",
            "Iteration 186, loss = 0.57770727\n",
            "Iteration 187, loss = 0.57482334\n",
            "Iteration 188, loss = 0.57717696\n",
            "Iteration 189, loss = 0.57581151\n",
            "Iteration 190, loss = 0.57921674\n",
            "Iteration 191, loss = 0.57611095\n",
            "Iteration 192, loss = 0.57615246\n",
            "Iteration 193, loss = 0.57532935\n",
            "Iteration 194, loss = 0.57236493\n",
            "Iteration 195, loss = 0.57636425\n",
            "Iteration 196, loss = 0.57740399\n",
            "Iteration 197, loss = 0.57503040\n",
            "Iteration 198, loss = 0.57603158\n",
            "Iteration 199, loss = 0.57619994\n",
            "Iteration 200, loss = 0.57832592\n",
            "Iteration 201, loss = 0.57215443\n",
            "Iteration 202, loss = 0.57345937\n",
            "Iteration 203, loss = 0.57612621\n",
            "Iteration 204, loss = 0.57221864\n",
            "Iteration 205, loss = 0.57612157\n",
            "Iteration 206, loss = 0.57340540\n",
            "Iteration 207, loss = 0.56951868\n",
            "Iteration 208, loss = 0.56862504\n",
            "Iteration 209, loss = 0.57201085\n",
            "Iteration 210, loss = 0.57047835\n",
            "Iteration 211, loss = 0.57540081\n",
            "Iteration 212, loss = 0.57240478\n",
            "Iteration 213, loss = 0.57093924\n",
            "Iteration 214, loss = 0.56891878\n",
            "Iteration 215, loss = 0.57066282\n",
            "Iteration 216, loss = 0.57131199\n",
            "Iteration 217, loss = 0.57415131\n",
            "Iteration 218, loss = 0.57237047\n",
            "Iteration 219, loss = 0.57298405\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67784282\n",
            "Iteration 2, loss = 0.67223857\n",
            "Iteration 3, loss = 0.66711381\n",
            "Iteration 4, loss = 0.66404642\n",
            "Iteration 5, loss = 0.66318490\n",
            "Iteration 6, loss = 0.65921116\n",
            "Iteration 7, loss = 0.65816434\n",
            "Iteration 8, loss = 0.65624033\n",
            "Iteration 9, loss = 0.65125973\n",
            "Iteration 10, loss = 0.64821530\n",
            "Iteration 11, loss = 0.64521782\n",
            "Iteration 12, loss = 0.64190348\n",
            "Iteration 13, loss = 0.63959041\n",
            "Iteration 14, loss = 0.63647061\n",
            "Iteration 15, loss = 0.63393517\n",
            "Iteration 16, loss = 0.63144075\n",
            "Iteration 17, loss = 0.63030986\n",
            "Iteration 18, loss = 0.62818955\n",
            "Iteration 19, loss = 0.62579660\n",
            "Iteration 20, loss = 0.62218764\n",
            "Iteration 21, loss = 0.62350543\n",
            "Iteration 22, loss = 0.62025620\n",
            "Iteration 23, loss = 0.61921058\n",
            "Iteration 24, loss = 0.61981219\n",
            "Iteration 25, loss = 0.61802076\n",
            "Iteration 26, loss = 0.61750564\n",
            "Iteration 27, loss = 0.61445460\n",
            "Iteration 28, loss = 0.61610839\n",
            "Iteration 29, loss = 0.61467548\n",
            "Iteration 30, loss = 0.61215511\n",
            "Iteration 31, loss = 0.61136897\n",
            "Iteration 32, loss = 0.61304144\n",
            "Iteration 33, loss = 0.61071983\n",
            "Iteration 34, loss = 0.61154590\n",
            "Iteration 35, loss = 0.61148320\n",
            "Iteration 36, loss = 0.60960807\n",
            "Iteration 37, loss = 0.60903467\n",
            "Iteration 38, loss = 0.60887731\n",
            "Iteration 39, loss = 0.60786565\n",
            "Iteration 40, loss = 0.60944617\n",
            "Iteration 41, loss = 0.61063797\n",
            "Iteration 42, loss = 0.60871947\n",
            "Iteration 43, loss = 0.60690306\n",
            "Iteration 44, loss = 0.60816689\n",
            "Iteration 45, loss = 0.60760302\n",
            "Iteration 46, loss = 0.60897889\n",
            "Iteration 47, loss = 0.60607330\n",
            "Iteration 48, loss = 0.60521359\n",
            "Iteration 49, loss = 0.60750160\n",
            "Iteration 50, loss = 0.60623043\n",
            "Iteration 51, loss = 0.60710541\n",
            "Iteration 52, loss = 0.60488865\n",
            "Iteration 53, loss = 0.60508298\n",
            "Iteration 54, loss = 0.60442144\n",
            "Iteration 55, loss = 0.60393488\n",
            "Iteration 56, loss = 0.60592192\n",
            "Iteration 57, loss = 0.60306019\n",
            "Iteration 58, loss = 0.60384842\n",
            "Iteration 59, loss = 0.60490235\n",
            "Iteration 60, loss = 0.60311387\n",
            "Iteration 61, loss = 0.60297554\n",
            "Iteration 62, loss = 0.60353474\n",
            "Iteration 63, loss = 0.60166670\n",
            "Iteration 64, loss = 0.60439926\n",
            "Iteration 65, loss = 0.60224119\n",
            "Iteration 66, loss = 0.60149031\n",
            "Iteration 67, loss = 0.60184425\n",
            "Iteration 68, loss = 0.60212604\n",
            "Iteration 69, loss = 0.60068421\n",
            "Iteration 70, loss = 0.60093335\n",
            "Iteration 71, loss = 0.60038032\n",
            "Iteration 72, loss = 0.60060014\n",
            "Iteration 73, loss = 0.59956485\n",
            "Iteration 74, loss = 0.60136655\n",
            "Iteration 75, loss = 0.59883629\n",
            "Iteration 76, loss = 0.59992798\n",
            "Iteration 77, loss = 0.59859970\n",
            "Iteration 78, loss = 0.59911551\n",
            "Iteration 79, loss = 0.59929486\n",
            "Iteration 80, loss = 0.59804912\n",
            "Iteration 81, loss = 0.60082125\n",
            "Iteration 82, loss = 0.59947631\n",
            "Iteration 83, loss = 0.59771274\n",
            "Iteration 84, loss = 0.59747955\n",
            "Iteration 85, loss = 0.59724643\n",
            "Iteration 86, loss = 0.59687777\n",
            "Iteration 87, loss = 0.59593604\n",
            "Iteration 88, loss = 0.59592339\n",
            "Iteration 89, loss = 0.59677490\n",
            "Iteration 90, loss = 0.59676279\n",
            "Iteration 91, loss = 0.59619129\n",
            "Iteration 92, loss = 0.59641807\n",
            "Iteration 93, loss = 0.59648193\n",
            "Iteration 94, loss = 0.59783863\n",
            "Iteration 95, loss = 0.59539090\n",
            "Iteration 96, loss = 0.59728802\n",
            "Iteration 97, loss = 0.59628933\n",
            "Iteration 98, loss = 0.59505754\n",
            "Iteration 99, loss = 0.59318885\n",
            "Iteration 100, loss = 0.59480900\n",
            "Iteration 101, loss = 0.59413216\n",
            "Iteration 102, loss = 0.59442056\n",
            "Iteration 103, loss = 0.59545826\n",
            "Iteration 104, loss = 0.59377867\n",
            "Iteration 105, loss = 0.59462586\n",
            "Iteration 106, loss = 0.59307195\n",
            "Iteration 107, loss = 0.59416966\n",
            "Iteration 108, loss = 0.59431963\n",
            "Iteration 109, loss = 0.59314557\n",
            "Iteration 110, loss = 0.59428631\n",
            "Iteration 111, loss = 0.59528645\n",
            "Iteration 112, loss = 0.59175039\n",
            "Iteration 113, loss = 0.59320793\n",
            "Iteration 114, loss = 0.59060225\n",
            "Iteration 115, loss = 0.59277472\n",
            "Iteration 116, loss = 0.59048232\n",
            "Iteration 117, loss = 0.59111116\n",
            "Iteration 118, loss = 0.59063231\n",
            "Iteration 119, loss = 0.59232950\n",
            "Iteration 120, loss = 0.59238675\n",
            "Iteration 121, loss = 0.59088037\n",
            "Iteration 122, loss = 0.59019863\n",
            "Iteration 123, loss = 0.58930587\n",
            "Iteration 124, loss = 0.58901184\n",
            "Iteration 125, loss = 0.59227526\n",
            "Iteration 126, loss = 0.59205071\n",
            "Iteration 127, loss = 0.59026235\n",
            "Iteration 128, loss = 0.58926057\n",
            "Iteration 129, loss = 0.58848673\n",
            "Iteration 130, loss = 0.58837223\n",
            "Iteration 131, loss = 0.58754074\n",
            "Iteration 132, loss = 0.58695789\n",
            "Iteration 133, loss = 0.58900137\n",
            "Iteration 134, loss = 0.58938146\n",
            "Iteration 135, loss = 0.58713298\n",
            "Iteration 136, loss = 0.58546115\n",
            "Iteration 137, loss = 0.58938198\n",
            "Iteration 138, loss = 0.58684605\n",
            "Iteration 139, loss = 0.58660291\n",
            "Iteration 140, loss = 0.58755901\n",
            "Iteration 141, loss = 0.58503274\n",
            "Iteration 142, loss = 0.58767036\n",
            "Iteration 143, loss = 0.58536645\n",
            "Iteration 144, loss = 0.58615270\n",
            "Iteration 145, loss = 0.58366518\n",
            "Iteration 146, loss = 0.58389489\n",
            "Iteration 147, loss = 0.58374000\n",
            "Iteration 148, loss = 0.58617722\n",
            "Iteration 149, loss = 0.58480972\n",
            "Iteration 150, loss = 0.58525309\n",
            "Iteration 151, loss = 0.58613550\n",
            "Iteration 152, loss = 0.58491648\n",
            "Iteration 153, loss = 0.58453737\n",
            "Iteration 154, loss = 0.58205316\n",
            "Iteration 155, loss = 0.58402999\n",
            "Iteration 156, loss = 0.58227863\n",
            "Iteration 157, loss = 0.58398935\n",
            "Iteration 158, loss = 0.58345104\n",
            "Iteration 159, loss = 0.58174143\n",
            "Iteration 160, loss = 0.58036404\n",
            "Iteration 161, loss = 0.58293691\n",
            "Iteration 162, loss = 0.58071619\n",
            "Iteration 163, loss = 0.57970329\n",
            "Iteration 164, loss = 0.58252206\n",
            "Iteration 165, loss = 0.57989535\n",
            "Iteration 166, loss = 0.58517787\n",
            "Iteration 167, loss = 0.58190496\n",
            "Iteration 168, loss = 0.57862898\n",
            "Iteration 169, loss = 0.57899181\n",
            "Iteration 170, loss = 0.57836769\n",
            "Iteration 171, loss = 0.57912270\n",
            "Iteration 172, loss = 0.57656632\n",
            "Iteration 173, loss = 0.57909473\n",
            "Iteration 174, loss = 0.57740755\n",
            "Iteration 175, loss = 0.57652647\n",
            "Iteration 176, loss = 0.57883849\n",
            "Iteration 177, loss = 0.57744685\n",
            "Iteration 178, loss = 0.57899291\n",
            "Iteration 179, loss = 0.57512150\n",
            "Iteration 180, loss = 0.57677869\n",
            "Iteration 181, loss = 0.57431567\n",
            "Iteration 182, loss = 0.57528637\n",
            "Iteration 183, loss = 0.57597967\n",
            "Iteration 184, loss = 0.57582841\n",
            "Iteration 185, loss = 0.57320704\n",
            "Iteration 186, loss = 0.57205561\n",
            "Iteration 187, loss = 0.57286372\n",
            "Iteration 188, loss = 0.57406479\n",
            "Iteration 189, loss = 0.57410845\n",
            "Iteration 190, loss = 0.57344826\n",
            "Iteration 191, loss = 0.57104197\n",
            "Iteration 192, loss = 0.57348092\n",
            "Iteration 193, loss = 0.57281733\n",
            "Iteration 194, loss = 0.57190282\n",
            "Iteration 195, loss = 0.57402753\n",
            "Iteration 196, loss = 0.57154947\n",
            "Iteration 197, loss = 0.57162073\n",
            "Iteration 198, loss = 0.56889534\n",
            "Iteration 199, loss = 0.56848221\n",
            "Iteration 200, loss = 0.57043035\n",
            "Iteration 201, loss = 0.56676325\n",
            "Iteration 202, loss = 0.56925808\n",
            "Iteration 203, loss = 0.57339824\n",
            "Iteration 204, loss = 0.56743117\n",
            "Iteration 205, loss = 0.57342549\n",
            "Iteration 206, loss = 0.57030504\n",
            "Iteration 207, loss = 0.56562867\n",
            "Iteration 208, loss = 0.56540392\n",
            "Iteration 209, loss = 0.56636751\n",
            "Iteration 210, loss = 0.56515608\n",
            "Iteration 211, loss = 0.56341073\n",
            "Iteration 212, loss = 0.56749565\n",
            "Iteration 213, loss = 0.56201225\n",
            "Iteration 214, loss = 0.56785831\n",
            "Iteration 215, loss = 0.56489952\n",
            "Iteration 216, loss = 0.56527453\n",
            "Iteration 217, loss = 0.56257595\n",
            "Iteration 218, loss = 0.56166357\n",
            "Iteration 219, loss = 0.56214022\n",
            "Iteration 220, loss = 0.56164210\n",
            "Iteration 221, loss = 0.55865340\n",
            "Iteration 222, loss = 0.56489512\n",
            "Iteration 223, loss = 0.55868739\n",
            "Iteration 224, loss = 0.56006310\n",
            "Iteration 225, loss = 0.55999343\n",
            "Iteration 226, loss = 0.56168337\n",
            "Iteration 227, loss = 0.55677015\n",
            "Iteration 228, loss = 0.55796193\n",
            "Iteration 229, loss = 0.55676998\n",
            "Iteration 230, loss = 0.56045053\n",
            "Iteration 231, loss = 0.55667186\n",
            "Iteration 232, loss = 0.55749460\n",
            "Iteration 233, loss = 0.55495405\n",
            "Iteration 234, loss = 0.55511267\n",
            "Iteration 235, loss = 0.56023840\n",
            "Iteration 236, loss = 0.55201197\n",
            "Iteration 237, loss = 0.55996399\n",
            "Iteration 238, loss = 0.55388016\n",
            "Iteration 239, loss = 0.55266601\n",
            "Iteration 240, loss = 0.55327785\n",
            "Iteration 241, loss = 0.55251982\n",
            "Iteration 242, loss = 0.55806574\n",
            "Iteration 243, loss = 0.55232942\n",
            "Iteration 244, loss = 0.55376828\n",
            "Iteration 245, loss = 0.56078535\n",
            "Iteration 246, loss = 0.55528059\n",
            "Iteration 247, loss = 0.54621299\n",
            "Iteration 248, loss = 0.54941764\n",
            "Iteration 249, loss = 0.55079174\n",
            "Iteration 250, loss = 0.54760312\n",
            "Iteration 251, loss = 0.54645983\n",
            "Iteration 252, loss = 0.54359143\n",
            "Iteration 253, loss = 0.55159329\n",
            "Iteration 254, loss = 0.54868208\n",
            "Iteration 255, loss = 0.54530267\n",
            "Iteration 256, loss = 0.54662427\n",
            "Iteration 257, loss = 0.54238426\n",
            "Iteration 258, loss = 0.54336923\n",
            "Iteration 259, loss = 0.54726311\n",
            "Iteration 260, loss = 0.54585403\n",
            "Iteration 261, loss = 0.54504656\n",
            "Iteration 262, loss = 0.54031873\n",
            "Iteration 263, loss = 0.54073259\n",
            "Iteration 264, loss = 0.54768445\n",
            "Iteration 265, loss = 0.53865764\n",
            "Iteration 266, loss = 0.54230397\n",
            "Iteration 267, loss = 0.54290702\n",
            "Iteration 268, loss = 0.54271801\n",
            "Iteration 269, loss = 0.54163733\n",
            "Iteration 270, loss = 0.54989904\n",
            "Iteration 271, loss = 0.53970378\n",
            "Iteration 272, loss = 0.53618638\n",
            "Iteration 273, loss = 0.53917828\n",
            "Iteration 274, loss = 0.54078593\n",
            "Iteration 275, loss = 0.53747693\n",
            "Iteration 276, loss = 0.52884988\n",
            "Iteration 277, loss = 0.53308472\n",
            "Iteration 278, loss = 0.54523140\n",
            "Iteration 279, loss = 0.53459278\n",
            "Iteration 280, loss = 0.54395328\n",
            "Iteration 281, loss = 0.53260757\n",
            "Iteration 282, loss = 0.53319879\n",
            "Iteration 283, loss = 0.53033379\n",
            "Iteration 284, loss = 0.53703626\n",
            "Iteration 285, loss = 0.53146883\n",
            "Iteration 286, loss = 0.53098678\n",
            "Iteration 287, loss = 0.53152283\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67739286\n",
            "Iteration 2, loss = 0.67374733\n",
            "Iteration 3, loss = 0.67145075\n",
            "Iteration 4, loss = 0.67106210\n",
            "Iteration 5, loss = 0.66718663\n",
            "Iteration 6, loss = 0.66317081\n",
            "Iteration 7, loss = 0.66126614\n",
            "Iteration 8, loss = 0.65950200\n",
            "Iteration 9, loss = 0.65701448\n",
            "Iteration 10, loss = 0.65463585\n",
            "Iteration 11, loss = 0.65316077\n",
            "Iteration 12, loss = 0.65062073\n",
            "Iteration 13, loss = 0.64772515\n",
            "Iteration 14, loss = 0.64623586\n",
            "Iteration 15, loss = 0.64523625\n",
            "Iteration 16, loss = 0.64252043\n",
            "Iteration 17, loss = 0.64167073\n",
            "Iteration 18, loss = 0.63887134\n",
            "Iteration 19, loss = 0.63745739\n",
            "Iteration 20, loss = 0.63648967\n",
            "Iteration 21, loss = 0.63400986\n",
            "Iteration 22, loss = 0.63593394\n",
            "Iteration 23, loss = 0.63507174\n",
            "Iteration 24, loss = 0.63121273\n",
            "Iteration 25, loss = 0.63079725\n",
            "Iteration 26, loss = 0.62927491\n",
            "Iteration 27, loss = 0.62971543\n",
            "Iteration 28, loss = 0.62691446\n",
            "Iteration 29, loss = 0.62743672\n",
            "Iteration 30, loss = 0.62623186\n",
            "Iteration 31, loss = 0.62667915\n",
            "Iteration 32, loss = 0.62536305\n",
            "Iteration 33, loss = 0.62522957\n",
            "Iteration 34, loss = 0.62409937\n",
            "Iteration 35, loss = 0.62438639\n",
            "Iteration 36, loss = 0.62337133\n",
            "Iteration 37, loss = 0.62403241\n",
            "Iteration 38, loss = 0.62304003\n",
            "Iteration 39, loss = 0.62463053\n",
            "Iteration 40, loss = 0.62341105\n",
            "Iteration 41, loss = 0.62475361\n",
            "Iteration 42, loss = 0.62219728\n",
            "Iteration 43, loss = 0.62346889\n",
            "Iteration 44, loss = 0.62350434\n",
            "Iteration 45, loss = 0.62243401\n",
            "Iteration 46, loss = 0.62406714\n",
            "Iteration 47, loss = 0.62199350\n",
            "Iteration 48, loss = 0.62174392\n",
            "Iteration 49, loss = 0.62194670\n",
            "Iteration 50, loss = 0.62267421\n",
            "Iteration 51, loss = 0.62022915\n",
            "Iteration 52, loss = 0.62060763\n",
            "Iteration 53, loss = 0.62229102\n",
            "Iteration 54, loss = 0.62177809\n",
            "Iteration 55, loss = 0.62006443\n",
            "Iteration 56, loss = 0.62022178\n",
            "Iteration 57, loss = 0.62125606\n",
            "Iteration 58, loss = 0.62096606\n",
            "Iteration 59, loss = 0.61938966\n",
            "Iteration 60, loss = 0.61972610\n",
            "Iteration 61, loss = 0.62126336\n",
            "Iteration 62, loss = 0.61918006\n",
            "Iteration 63, loss = 0.61892598\n",
            "Iteration 64, loss = 0.61967494\n",
            "Iteration 65, loss = 0.61869241\n",
            "Iteration 66, loss = 0.61828469\n",
            "Iteration 67, loss = 0.61857000\n",
            "Iteration 68, loss = 0.62072985\n",
            "Iteration 69, loss = 0.61999598\n",
            "Iteration 70, loss = 0.61942620\n",
            "Iteration 71, loss = 0.61817958\n",
            "Iteration 72, loss = 0.61805849\n",
            "Iteration 73, loss = 0.61783039\n",
            "Iteration 74, loss = 0.62139640\n",
            "Iteration 75, loss = 0.61846854\n",
            "Iteration 76, loss = 0.61695201\n",
            "Iteration 77, loss = 0.61774027\n",
            "Iteration 78, loss = 0.61850207\n",
            "Iteration 79, loss = 0.61898262\n",
            "Iteration 80, loss = 0.61728257\n",
            "Iteration 81, loss = 0.61679529\n",
            "Iteration 82, loss = 0.61727165\n",
            "Iteration 83, loss = 0.61687580\n",
            "Iteration 84, loss = 0.61674613\n",
            "Iteration 85, loss = 0.61778892\n",
            "Iteration 86, loss = 0.61744220\n",
            "Iteration 87, loss = 0.61665614\n",
            "Iteration 88, loss = 0.61531678\n",
            "Iteration 89, loss = 0.61832751\n",
            "Iteration 90, loss = 0.61614434\n",
            "Iteration 91, loss = 0.61557526\n",
            "Iteration 92, loss = 0.61606840\n",
            "Iteration 93, loss = 0.61489016\n",
            "Iteration 94, loss = 0.61584218\n",
            "Iteration 95, loss = 0.61573161\n",
            "Iteration 96, loss = 0.61746791\n",
            "Iteration 97, loss = 0.61642181\n",
            "Iteration 98, loss = 0.61621781\n",
            "Iteration 99, loss = 0.61673516\n",
            "Iteration 100, loss = 0.61750287\n",
            "Iteration 101, loss = 0.61601826\n",
            "Iteration 102, loss = 0.61637673\n",
            "Iteration 103, loss = 0.61505735\n",
            "Iteration 104, loss = 0.61674516\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68103485\n",
            "Iteration 2, loss = 0.67644109\n",
            "Iteration 3, loss = 0.67046292\n",
            "Iteration 4, loss = 0.66694220\n",
            "Iteration 5, loss = 0.66443693\n",
            "Iteration 6, loss = 0.66087774\n",
            "Iteration 7, loss = 0.65738681\n",
            "Iteration 8, loss = 0.65710827\n",
            "Iteration 9, loss = 0.65355435\n",
            "Iteration 10, loss = 0.65261787\n",
            "Iteration 11, loss = 0.64776667\n",
            "Iteration 12, loss = 0.64517025\n",
            "Iteration 13, loss = 0.64377325\n",
            "Iteration 14, loss = 0.64003857\n",
            "Iteration 15, loss = 0.63803293\n",
            "Iteration 16, loss = 0.63948355\n",
            "Iteration 17, loss = 0.63585014\n",
            "Iteration 18, loss = 0.63405685\n",
            "Iteration 19, loss = 0.63132813\n",
            "Iteration 20, loss = 0.62911051\n",
            "Iteration 21, loss = 0.62844959\n",
            "Iteration 22, loss = 0.62841262\n",
            "Iteration 23, loss = 0.62582534\n",
            "Iteration 24, loss = 0.62534223\n",
            "Iteration 25, loss = 0.62536050\n",
            "Iteration 26, loss = 0.62333679\n",
            "Iteration 27, loss = 0.62308424\n",
            "Iteration 28, loss = 0.62219718\n",
            "Iteration 29, loss = 0.62199200\n",
            "Iteration 30, loss = 0.62068857\n",
            "Iteration 31, loss = 0.62329500\n",
            "Iteration 32, loss = 0.61968077\n",
            "Iteration 33, loss = 0.62280524\n",
            "Iteration 34, loss = 0.62044008\n",
            "Iteration 35, loss = 0.61998671\n",
            "Iteration 36, loss = 0.61830452\n",
            "Iteration 37, loss = 0.62163747\n",
            "Iteration 38, loss = 0.61948287\n",
            "Iteration 39, loss = 0.61987818\n",
            "Iteration 40, loss = 0.61980885\n",
            "Iteration 41, loss = 0.62109935\n",
            "Iteration 42, loss = 0.62169886\n",
            "Iteration 43, loss = 0.61960226\n",
            "Iteration 44, loss = 0.61934576\n",
            "Iteration 45, loss = 0.61955284\n",
            "Iteration 46, loss = 0.61827073\n",
            "Iteration 47, loss = 0.62045440\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66242347\n",
            "Iteration 2, loss = 0.62541687\n",
            "Iteration 3, loss = 0.60581822\n",
            "Iteration 4, loss = 0.60520348\n",
            "Iteration 5, loss = 0.60245691\n",
            "Iteration 6, loss = 0.60612958\n",
            "Iteration 7, loss = 0.59357278\n",
            "Iteration 8, loss = 0.59218655\n",
            "Iteration 9, loss = 0.59396190\n",
            "Iteration 10, loss = 0.59142412\n",
            "Iteration 11, loss = 0.58487388\n",
            "Iteration 12, loss = 0.58455111\n",
            "Iteration 13, loss = 0.57979766\n",
            "Iteration 14, loss = 0.57317747\n",
            "Iteration 15, loss = 0.57286544\n",
            "Iteration 16, loss = 0.58002225\n",
            "Iteration 17, loss = 0.56967832\n",
            "Iteration 18, loss = 0.55959397\n",
            "Iteration 19, loss = 0.55276910\n",
            "Iteration 20, loss = 0.54957832\n",
            "Iteration 21, loss = 0.54201524\n",
            "Iteration 22, loss = 0.53623070\n",
            "Iteration 23, loss = 0.53161003\n",
            "Iteration 24, loss = 0.53001450\n",
            "Iteration 25, loss = 0.52760826\n",
            "Iteration 26, loss = 0.53161965\n",
            "Iteration 27, loss = 0.51782198\n",
            "Iteration 28, loss = 0.50998280\n",
            "Iteration 29, loss = 0.50468664\n",
            "Iteration 30, loss = 0.49725672\n",
            "Iteration 31, loss = 0.48337099\n",
            "Iteration 32, loss = 0.48500944\n",
            "Iteration 33, loss = 0.49254215\n",
            "Iteration 34, loss = 0.48874434\n",
            "Iteration 35, loss = 0.46945473\n",
            "Iteration 36, loss = 0.47784565\n",
            "Iteration 37, loss = 0.46825297\n",
            "Iteration 38, loss = 0.46382674\n",
            "Iteration 39, loss = 0.45747071\n",
            "Iteration 40, loss = 0.46613817\n",
            "Iteration 41, loss = 0.45640568\n",
            "Iteration 42, loss = 0.45711487\n",
            "Iteration 43, loss = 0.45759118\n",
            "Iteration 44, loss = 0.44756016\n",
            "Iteration 45, loss = 0.44829703\n",
            "Iteration 46, loss = 0.44833163\n",
            "Iteration 47, loss = 0.44480302\n",
            "Iteration 48, loss = 0.44357926\n",
            "Iteration 49, loss = 0.44306654\n",
            "Iteration 50, loss = 0.44850418\n",
            "Iteration 51, loss = 0.43714190\n",
            "Iteration 52, loss = 0.44678313\n",
            "Iteration 53, loss = 0.44278220\n",
            "Iteration 54, loss = 0.43607125\n",
            "Iteration 55, loss = 0.43729262\n",
            "Iteration 56, loss = 0.43374430\n",
            "Iteration 57, loss = 0.43729104\n",
            "Iteration 58, loss = 0.43536524\n",
            "Iteration 59, loss = 0.43542562\n",
            "Iteration 60, loss = 0.43398574\n",
            "Iteration 61, loss = 0.43528687\n",
            "Iteration 62, loss = 0.45183639\n",
            "Iteration 63, loss = 0.44344995\n",
            "Iteration 64, loss = 0.43412595\n",
            "Iteration 65, loss = 0.42955958\n",
            "Iteration 66, loss = 0.43052785\n",
            "Iteration 67, loss = 0.42803542\n",
            "Iteration 68, loss = 0.43208995\n",
            "Iteration 69, loss = 0.42765970\n",
            "Iteration 70, loss = 0.42682641\n",
            "Iteration 71, loss = 0.43962572\n",
            "Iteration 72, loss = 0.43664157\n",
            "Iteration 73, loss = 0.43957143\n",
            "Iteration 74, loss = 0.42796768\n",
            "Iteration 75, loss = 0.42556416\n",
            "Iteration 76, loss = 0.42299377\n",
            "Iteration 77, loss = 0.42380037\n",
            "Iteration 78, loss = 0.42859221\n",
            "Iteration 79, loss = 0.42850189\n",
            "Iteration 80, loss = 0.45198241\n",
            "Iteration 81, loss = 0.44060591\n",
            "Iteration 82, loss = 0.43107109\n",
            "Iteration 83, loss = 0.43630735\n",
            "Iteration 84, loss = 0.42363517\n",
            "Iteration 85, loss = 0.42295601\n",
            "Iteration 86, loss = 0.42265170\n",
            "Iteration 87, loss = 0.42500569\n",
            "Iteration 88, loss = 0.41882629\n",
            "Iteration 89, loss = 0.41780640\n",
            "Iteration 90, loss = 0.42544423\n",
            "Iteration 91, loss = 0.41890701\n",
            "Iteration 92, loss = 0.42032761\n",
            "Iteration 93, loss = 0.41642580\n",
            "Iteration 94, loss = 0.41907727\n",
            "Iteration 95, loss = 0.41756894\n",
            "Iteration 96, loss = 0.41407389\n",
            "Iteration 97, loss = 0.41702264\n",
            "Iteration 98, loss = 0.41339511\n",
            "Iteration 99, loss = 0.42155575\n",
            "Iteration 100, loss = 0.41180155\n",
            "Iteration 101, loss = 0.41187343\n",
            "Iteration 102, loss = 0.41218758\n",
            "Iteration 103, loss = 0.41515013\n",
            "Iteration 104, loss = 0.41861165\n",
            "Iteration 105, loss = 0.41663904\n",
            "Iteration 106, loss = 0.41480222\n",
            "Iteration 107, loss = 0.41455592\n",
            "Iteration 108, loss = 0.40835473\n",
            "Iteration 109, loss = 0.40924738\n",
            "Iteration 110, loss = 0.40820822\n",
            "Iteration 111, loss = 0.41240808\n",
            "Iteration 112, loss = 0.42113640\n",
            "Iteration 113, loss = 0.40984055\n",
            "Iteration 114, loss = 0.41193494\n",
            "Iteration 115, loss = 0.40525422\n",
            "Iteration 116, loss = 0.40825713\n",
            "Iteration 117, loss = 0.40886873\n",
            "Iteration 118, loss = 0.40573304\n",
            "Iteration 119, loss = 0.42086153\n",
            "Iteration 120, loss = 0.41781532\n",
            "Iteration 121, loss = 0.40943143\n",
            "Iteration 122, loss = 0.40513400\n",
            "Iteration 123, loss = 0.40518835\n",
            "Iteration 124, loss = 0.40424064\n",
            "Iteration 125, loss = 0.40682814\n",
            "Iteration 126, loss = 0.40451250\n",
            "Iteration 127, loss = 0.41678937\n",
            "Iteration 128, loss = 0.40256191\n",
            "Iteration 129, loss = 0.40985005\n",
            "Iteration 130, loss = 0.40975098\n",
            "Iteration 131, loss = 0.40199703\n",
            "Iteration 132, loss = 0.40241637\n",
            "Iteration 133, loss = 0.40280537\n",
            "Iteration 134, loss = 0.40440121\n",
            "Iteration 135, loss = 0.40715185\n",
            "Iteration 136, loss = 0.41361781\n",
            "Iteration 137, loss = 0.42770158\n",
            "Iteration 138, loss = 0.40145747\n",
            "Iteration 139, loss = 0.39971529\n",
            "Iteration 140, loss = 0.40415383\n",
            "Iteration 141, loss = 0.40208020\n",
            "Iteration 142, loss = 0.39976792\n",
            "Iteration 143, loss = 0.41200389\n",
            "Iteration 144, loss = 0.39444294\n",
            "Iteration 145, loss = 0.41738764\n",
            "Iteration 146, loss = 0.41030024\n",
            "Iteration 147, loss = 0.40524341\n",
            "Iteration 148, loss = 0.40673805\n",
            "Iteration 149, loss = 0.40657089\n",
            "Iteration 150, loss = 0.40362375\n",
            "Iteration 151, loss = 0.41021097\n",
            "Iteration 152, loss = 0.40749862\n",
            "Iteration 153, loss = 0.39757848\n",
            "Iteration 154, loss = 0.39586135\n",
            "Iteration 155, loss = 0.39610706\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67040083\n",
            "Iteration 2, loss = 0.63344755\n",
            "Iteration 3, loss = 0.60987289\n",
            "Iteration 4, loss = 0.60442883\n",
            "Iteration 5, loss = 0.60517709\n",
            "Iteration 6, loss = 0.59981626\n",
            "Iteration 7, loss = 0.59657722\n",
            "Iteration 8, loss = 0.59552867\n",
            "Iteration 9, loss = 0.59366813\n",
            "Iteration 10, loss = 0.59238423\n",
            "Iteration 11, loss = 0.58886157\n",
            "Iteration 12, loss = 0.58740313\n",
            "Iteration 13, loss = 0.58315316\n",
            "Iteration 14, loss = 0.58100451\n",
            "Iteration 15, loss = 0.57708467\n",
            "Iteration 16, loss = 0.57959952\n",
            "Iteration 17, loss = 0.57937075\n",
            "Iteration 18, loss = 0.58401970\n",
            "Iteration 19, loss = 0.56314620\n",
            "Iteration 20, loss = 0.56003320\n",
            "Iteration 21, loss = 0.55568308\n",
            "Iteration 22, loss = 0.55373025\n",
            "Iteration 23, loss = 0.55550723\n",
            "Iteration 24, loss = 0.55099913\n",
            "Iteration 25, loss = 0.54483347\n",
            "Iteration 26, loss = 0.53699714\n",
            "Iteration 27, loss = 0.52673600\n",
            "Iteration 28, loss = 0.51428444\n",
            "Iteration 29, loss = 0.50973102\n",
            "Iteration 30, loss = 0.50609703\n",
            "Iteration 31, loss = 0.49946251\n",
            "Iteration 32, loss = 0.49687734\n",
            "Iteration 33, loss = 0.50609895\n",
            "Iteration 34, loss = 0.48188888\n",
            "Iteration 35, loss = 0.47373075\n",
            "Iteration 36, loss = 0.47407963\n",
            "Iteration 37, loss = 0.47033389\n",
            "Iteration 38, loss = 0.46308053\n",
            "Iteration 39, loss = 0.45799150\n",
            "Iteration 40, loss = 0.45608460\n",
            "Iteration 41, loss = 0.45205871\n",
            "Iteration 42, loss = 0.44548325\n",
            "Iteration 43, loss = 0.44325926\n",
            "Iteration 44, loss = 0.44326663\n",
            "Iteration 45, loss = 0.45063327\n",
            "Iteration 46, loss = 0.43939907\n",
            "Iteration 47, loss = 0.44826816\n",
            "Iteration 48, loss = 0.44258630\n",
            "Iteration 49, loss = 0.43655782\n",
            "Iteration 50, loss = 0.43232891\n",
            "Iteration 51, loss = 0.42546485\n",
            "Iteration 52, loss = 0.44750851\n",
            "Iteration 53, loss = 0.44634302\n",
            "Iteration 54, loss = 0.43828914\n",
            "Iteration 55, loss = 0.42485995\n",
            "Iteration 56, loss = 0.42257522\n",
            "Iteration 57, loss = 0.42589200\n",
            "Iteration 58, loss = 0.43415509\n",
            "Iteration 59, loss = 0.43109510\n",
            "Iteration 60, loss = 0.42027131\n",
            "Iteration 61, loss = 0.42331230\n",
            "Iteration 62, loss = 0.41991292\n",
            "Iteration 63, loss = 0.42692080\n",
            "Iteration 64, loss = 0.44600289\n",
            "Iteration 65, loss = 0.46954819\n",
            "Iteration 66, loss = 0.43952904\n",
            "Iteration 67, loss = 0.42732625\n",
            "Iteration 68, loss = 0.42441884\n",
            "Iteration 69, loss = 0.42201649\n",
            "Iteration 70, loss = 0.41932267\n",
            "Iteration 71, loss = 0.41776423\n",
            "Iteration 72, loss = 0.41749154\n",
            "Iteration 73, loss = 0.42139458\n",
            "Iteration 74, loss = 0.41911091\n",
            "Iteration 75, loss = 0.42376606\n",
            "Iteration 76, loss = 0.42007387\n",
            "Iteration 77, loss = 0.43956365\n",
            "Iteration 78, loss = 0.41307289\n",
            "Iteration 79, loss = 0.41756735\n",
            "Iteration 80, loss = 0.41186543\n",
            "Iteration 81, loss = 0.41834137\n",
            "Iteration 82, loss = 0.41553137\n",
            "Iteration 83, loss = 0.41372992\n",
            "Iteration 84, loss = 0.41205207\n",
            "Iteration 85, loss = 0.41002426\n",
            "Iteration 86, loss = 0.41100686\n",
            "Iteration 87, loss = 0.40786210\n",
            "Iteration 88, loss = 0.42495305\n",
            "Iteration 89, loss = 0.40926326\n",
            "Iteration 90, loss = 0.41371933\n",
            "Iteration 91, loss = 0.41009926\n",
            "Iteration 92, loss = 0.41004782\n",
            "Iteration 93, loss = 0.41285800\n",
            "Iteration 94, loss = 0.41583992\n",
            "Iteration 95, loss = 0.42882337\n",
            "Iteration 96, loss = 0.41767880\n",
            "Iteration 97, loss = 0.40542092\n",
            "Iteration 98, loss = 0.40363000\n",
            "Iteration 99, loss = 0.40496995\n",
            "Iteration 100, loss = 0.40652792\n",
            "Iteration 101, loss = 0.41160392\n",
            "Iteration 102, loss = 0.40756415\n",
            "Iteration 103, loss = 0.40793500\n",
            "Iteration 104, loss = 0.40504513\n",
            "Iteration 105, loss = 0.41340081\n",
            "Iteration 106, loss = 0.40615231\n",
            "Iteration 107, loss = 0.40555456\n",
            "Iteration 108, loss = 0.40131863\n",
            "Iteration 109, loss = 0.40837861\n",
            "Iteration 110, loss = 0.40383304\n",
            "Iteration 111, loss = 0.40556826\n",
            "Iteration 112, loss = 0.40933279\n",
            "Iteration 113, loss = 0.40371977\n",
            "Iteration 114, loss = 0.40707848\n",
            "Iteration 115, loss = 0.39866631\n",
            "Iteration 116, loss = 0.39990138\n",
            "Iteration 117, loss = 0.39978302\n",
            "Iteration 118, loss = 0.39898223\n",
            "Iteration 119, loss = 0.41198502\n",
            "Iteration 120, loss = 0.41164937\n",
            "Iteration 121, loss = 0.41040324\n",
            "Iteration 122, loss = 0.40332624\n",
            "Iteration 123, loss = 0.40108755\n",
            "Iteration 124, loss = 0.39968882\n",
            "Iteration 125, loss = 0.39864309\n",
            "Iteration 126, loss = 0.40245164\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66072367\n",
            "Iteration 2, loss = 0.62014693\n",
            "Iteration 3, loss = 0.60447298\n",
            "Iteration 4, loss = 0.60095744\n",
            "Iteration 5, loss = 0.59815182\n",
            "Iteration 6, loss = 0.60034505\n",
            "Iteration 7, loss = 0.60287291\n",
            "Iteration 8, loss = 0.59188099\n",
            "Iteration 9, loss = 0.59109201\n",
            "Iteration 10, loss = 0.58590128\n",
            "Iteration 11, loss = 0.58500099\n",
            "Iteration 12, loss = 0.58086165\n",
            "Iteration 13, loss = 0.57719174\n",
            "Iteration 14, loss = 0.57227147\n",
            "Iteration 15, loss = 0.56697238\n",
            "Iteration 16, loss = 0.56508534\n",
            "Iteration 17, loss = 0.55631658\n",
            "Iteration 18, loss = 0.54946682\n",
            "Iteration 19, loss = 0.54174523\n",
            "Iteration 20, loss = 0.54373085\n",
            "Iteration 21, loss = 0.54217889\n",
            "Iteration 22, loss = 0.53878251\n",
            "Iteration 23, loss = 0.53336940\n",
            "Iteration 24, loss = 0.50644390\n",
            "Iteration 25, loss = 0.49786149\n",
            "Iteration 26, loss = 0.49247355\n",
            "Iteration 27, loss = 0.48411839\n",
            "Iteration 28, loss = 0.47689142\n",
            "Iteration 29, loss = 0.48247209\n",
            "Iteration 30, loss = 0.47289162\n",
            "Iteration 31, loss = 0.46358646\n",
            "Iteration 32, loss = 0.45151183\n",
            "Iteration 33, loss = 0.45088785\n",
            "Iteration 34, loss = 0.44488480\n",
            "Iteration 35, loss = 0.44987960\n",
            "Iteration 36, loss = 0.44454945\n",
            "Iteration 37, loss = 0.43473364\n",
            "Iteration 38, loss = 0.43937042\n",
            "Iteration 39, loss = 0.43255039\n",
            "Iteration 40, loss = 0.42863969\n",
            "Iteration 41, loss = 0.43373129\n",
            "Iteration 42, loss = 0.42704784\n",
            "Iteration 43, loss = 0.42401573\n",
            "Iteration 44, loss = 0.42224767\n",
            "Iteration 45, loss = 0.42292651\n",
            "Iteration 46, loss = 0.42314494\n",
            "Iteration 47, loss = 0.42125988\n",
            "Iteration 48, loss = 0.42294258\n",
            "Iteration 49, loss = 0.41715854\n",
            "Iteration 50, loss = 0.41806454\n",
            "Iteration 51, loss = 0.41295205\n",
            "Iteration 52, loss = 0.42604878\n",
            "Iteration 53, loss = 0.41982051\n",
            "Iteration 54, loss = 0.43006152\n",
            "Iteration 55, loss = 0.41133927\n",
            "Iteration 56, loss = 0.41759406\n",
            "Iteration 57, loss = 0.41549398\n",
            "Iteration 58, loss = 0.41449938\n",
            "Iteration 59, loss = 0.41382633\n",
            "Iteration 60, loss = 0.41924536\n",
            "Iteration 61, loss = 0.41140981\n",
            "Iteration 62, loss = 0.41155669\n",
            "Iteration 63, loss = 0.41309135\n",
            "Iteration 64, loss = 0.40487967\n",
            "Iteration 65, loss = 0.41990367\n",
            "Iteration 66, loss = 0.41044603\n",
            "Iteration 67, loss = 0.40634053\n",
            "Iteration 68, loss = 0.40716854\n",
            "Iteration 69, loss = 0.40713885\n",
            "Iteration 70, loss = 0.40737851\n",
            "Iteration 71, loss = 0.40128505\n",
            "Iteration 72, loss = 0.41180443\n",
            "Iteration 73, loss = 0.41295657\n",
            "Iteration 74, loss = 0.43506868\n",
            "Iteration 75, loss = 0.42307678\n",
            "Iteration 76, loss = 0.42054180\n",
            "Iteration 77, loss = 0.42611230\n",
            "Iteration 78, loss = 0.41128618\n",
            "Iteration 79, loss = 0.40819430\n",
            "Iteration 80, loss = 0.41595844\n",
            "Iteration 81, loss = 0.39939192\n",
            "Iteration 82, loss = 0.39888246\n",
            "Iteration 83, loss = 0.40075941\n",
            "Iteration 84, loss = 0.40585425\n",
            "Iteration 85, loss = 0.40601813\n",
            "Iteration 86, loss = 0.39942670\n",
            "Iteration 87, loss = 0.39863863\n",
            "Iteration 88, loss = 0.40021068\n",
            "Iteration 89, loss = 0.39777953\n",
            "Iteration 90, loss = 0.39491925\n",
            "Iteration 91, loss = 0.40161816\n",
            "Iteration 92, loss = 0.40628500\n",
            "Iteration 93, loss = 0.40414557\n",
            "Iteration 94, loss = 0.39545680\n",
            "Iteration 95, loss = 0.39662581\n",
            "Iteration 96, loss = 0.39637164\n",
            "Iteration 97, loss = 0.39645855\n",
            "Iteration 98, loss = 0.39447809\n",
            "Iteration 99, loss = 0.39288443\n",
            "Iteration 100, loss = 0.39641804\n",
            "Iteration 101, loss = 0.40434079\n",
            "Iteration 102, loss = 0.39565059\n",
            "Iteration 103, loss = 0.39416181\n",
            "Iteration 104, loss = 0.39002844\n",
            "Iteration 105, loss = 0.39775526\n",
            "Iteration 106, loss = 0.40072058\n",
            "Iteration 107, loss = 0.39300828\n",
            "Iteration 108, loss = 0.38888002\n",
            "Iteration 109, loss = 0.39812332\n",
            "Iteration 110, loss = 0.39251649\n",
            "Iteration 111, loss = 0.40188318\n",
            "Iteration 112, loss = 0.39064580\n",
            "Iteration 113, loss = 0.41201764\n",
            "Iteration 114, loss = 0.39506620\n",
            "Iteration 115, loss = 0.39161502\n",
            "Iteration 116, loss = 0.38545605\n",
            "Iteration 117, loss = 0.38805384\n",
            "Iteration 118, loss = 0.38951150\n",
            "Iteration 119, loss = 0.38814439\n",
            "Iteration 120, loss = 0.38508865\n",
            "Iteration 121, loss = 0.39267490\n",
            "Iteration 122, loss = 0.39835146\n",
            "Iteration 123, loss = 0.40470088\n",
            "Iteration 124, loss = 0.40748066\n",
            "Iteration 125, loss = 0.38678668\n",
            "Iteration 126, loss = 0.38637358\n",
            "Iteration 127, loss = 0.39155574\n",
            "Iteration 128, loss = 0.38325154\n",
            "Iteration 129, loss = 0.38237723\n",
            "Iteration 130, loss = 0.38968559\n",
            "Iteration 131, loss = 0.39450685\n",
            "Iteration 132, loss = 0.38816289\n",
            "Iteration 133, loss = 0.38093159\n",
            "Iteration 134, loss = 0.37972078\n",
            "Iteration 135, loss = 0.38208587\n",
            "Iteration 136, loss = 0.38546314\n",
            "Iteration 137, loss = 0.39225733\n",
            "Iteration 138, loss = 0.38321003\n",
            "Iteration 139, loss = 0.38570180\n",
            "Iteration 140, loss = 0.38362490\n",
            "Iteration 141, loss = 0.38521854\n",
            "Iteration 142, loss = 0.38920416\n",
            "Iteration 143, loss = 0.39114577\n",
            "Iteration 144, loss = 0.39524854\n",
            "Iteration 145, loss = 0.39486197\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66084781\n",
            "Iteration 2, loss = 0.62753188\n",
            "Iteration 3, loss = 0.62615511\n",
            "Iteration 4, loss = 0.61976001\n",
            "Iteration 5, loss = 0.61723097\n",
            "Iteration 6, loss = 0.61525564\n",
            "Iteration 7, loss = 0.61620088\n",
            "Iteration 8, loss = 0.62347799\n",
            "Iteration 9, loss = 0.63744089\n",
            "Iteration 10, loss = 0.62150683\n",
            "Iteration 11, loss = 0.60599635\n",
            "Iteration 12, loss = 0.60476283\n",
            "Iteration 13, loss = 0.60105093\n",
            "Iteration 14, loss = 0.60979073\n",
            "Iteration 15, loss = 0.60001453\n",
            "Iteration 16, loss = 0.58902386\n",
            "Iteration 17, loss = 0.58566018\n",
            "Iteration 18, loss = 0.58045831\n",
            "Iteration 19, loss = 0.57441131\n",
            "Iteration 20, loss = 0.56846495\n",
            "Iteration 21, loss = 0.56220926\n",
            "Iteration 22, loss = 0.55707612\n",
            "Iteration 23, loss = 0.54841577\n",
            "Iteration 24, loss = 0.54004203\n",
            "Iteration 25, loss = 0.53399918\n",
            "Iteration 26, loss = 0.52154749\n",
            "Iteration 27, loss = 0.51624748\n",
            "Iteration 28, loss = 0.50886742\n",
            "Iteration 29, loss = 0.50447962\n",
            "Iteration 30, loss = 0.49638392\n",
            "Iteration 31, loss = 0.48344076\n",
            "Iteration 32, loss = 0.48178222\n",
            "Iteration 33, loss = 0.48353378\n",
            "Iteration 34, loss = 0.47194314\n",
            "Iteration 35, loss = 0.48181487\n",
            "Iteration 36, loss = 0.47706552\n",
            "Iteration 37, loss = 0.45521742\n",
            "Iteration 38, loss = 0.45565390\n",
            "Iteration 39, loss = 0.45731170\n",
            "Iteration 40, loss = 0.45329601\n",
            "Iteration 41, loss = 0.44908728\n",
            "Iteration 42, loss = 0.44580015\n",
            "Iteration 43, loss = 0.44668432\n",
            "Iteration 44, loss = 0.44128557\n",
            "Iteration 45, loss = 0.47224829\n",
            "Iteration 46, loss = 0.46073956\n",
            "Iteration 47, loss = 0.45663981\n",
            "Iteration 48, loss = 0.43391397\n",
            "Iteration 49, loss = 0.43325307\n",
            "Iteration 50, loss = 0.43585697\n",
            "Iteration 51, loss = 0.43838413\n",
            "Iteration 52, loss = 0.44449445\n",
            "Iteration 53, loss = 0.45026732\n",
            "Iteration 54, loss = 0.43864873\n",
            "Iteration 55, loss = 0.43541611\n",
            "Iteration 56, loss = 0.42899160\n",
            "Iteration 57, loss = 0.43999974\n",
            "Iteration 58, loss = 0.43120017\n",
            "Iteration 59, loss = 0.44328366\n",
            "Iteration 60, loss = 0.43741273\n",
            "Iteration 61, loss = 0.43957521\n",
            "Iteration 62, loss = 0.43608929\n",
            "Iteration 63, loss = 0.43917260\n",
            "Iteration 64, loss = 0.43777979\n",
            "Iteration 65, loss = 0.43568852\n",
            "Iteration 66, loss = 0.42495726\n",
            "Iteration 67, loss = 0.43086768\n",
            "Iteration 68, loss = 0.42775233\n",
            "Iteration 69, loss = 0.42364233\n",
            "Iteration 70, loss = 0.42569499\n",
            "Iteration 71, loss = 0.43125688\n",
            "Iteration 72, loss = 0.42628354\n",
            "Iteration 73, loss = 0.43373245\n",
            "Iteration 74, loss = 0.44053009\n",
            "Iteration 75, loss = 0.41975456\n",
            "Iteration 76, loss = 0.42239796\n",
            "Iteration 77, loss = 0.42578612\n",
            "Iteration 78, loss = 0.42088486\n",
            "Iteration 79, loss = 0.42193269\n",
            "Iteration 80, loss = 0.42067327\n",
            "Iteration 81, loss = 0.42226736\n",
            "Iteration 82, loss = 0.42145496\n",
            "Iteration 83, loss = 0.41968818\n",
            "Iteration 84, loss = 0.41962647\n",
            "Iteration 85, loss = 0.42290518\n",
            "Iteration 86, loss = 0.42212232\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67421801\n",
            "Iteration 2, loss = 0.65049348\n",
            "Iteration 3, loss = 0.62156756\n",
            "Iteration 4, loss = 0.61543397\n",
            "Iteration 5, loss = 0.61967569\n",
            "Iteration 6, loss = 0.61211303\n",
            "Iteration 7, loss = 0.61011170\n",
            "Iteration 8, loss = 0.61038870\n",
            "Iteration 9, loss = 0.60689056\n",
            "Iteration 10, loss = 0.60487830\n",
            "Iteration 11, loss = 0.60462502\n",
            "Iteration 12, loss = 0.60062925\n",
            "Iteration 13, loss = 0.60259861\n",
            "Iteration 14, loss = 0.59408108\n",
            "Iteration 15, loss = 0.59101921\n",
            "Iteration 16, loss = 0.58770389\n",
            "Iteration 17, loss = 0.58535434\n",
            "Iteration 18, loss = 0.58575286\n",
            "Iteration 19, loss = 0.58445404\n",
            "Iteration 20, loss = 0.58825970\n",
            "Iteration 21, loss = 0.58065001\n",
            "Iteration 22, loss = 0.56989319\n",
            "Iteration 23, loss = 0.56064091\n",
            "Iteration 24, loss = 0.56023758\n",
            "Iteration 25, loss = 0.55376686\n",
            "Iteration 26, loss = 0.54333051\n",
            "Iteration 27, loss = 0.53528736\n",
            "Iteration 28, loss = 0.53068120\n",
            "Iteration 29, loss = 0.53028393\n",
            "Iteration 30, loss = 0.51906042\n",
            "Iteration 31, loss = 0.51203891\n",
            "Iteration 32, loss = 0.51516998\n",
            "Iteration 33, loss = 0.50202706\n",
            "Iteration 34, loss = 0.49449840\n",
            "Iteration 35, loss = 0.49030171\n",
            "Iteration 36, loss = 0.50570663\n",
            "Iteration 37, loss = 0.49899262\n",
            "Iteration 38, loss = 0.50125844\n",
            "Iteration 39, loss = 0.47702854\n",
            "Iteration 40, loss = 0.47269955\n",
            "Iteration 41, loss = 0.47887509\n",
            "Iteration 42, loss = 0.46903649\n",
            "Iteration 43, loss = 0.47482911\n",
            "Iteration 44, loss = 0.46635725\n",
            "Iteration 45, loss = 0.46329339\n",
            "Iteration 46, loss = 0.46135730\n",
            "Iteration 47, loss = 0.46405718\n",
            "Iteration 48, loss = 0.46204565\n",
            "Iteration 49, loss = 0.46247730\n",
            "Iteration 50, loss = 0.45672785\n",
            "Iteration 51, loss = 0.46192732\n",
            "Iteration 52, loss = 0.45962599\n",
            "Iteration 53, loss = 0.45418738\n",
            "Iteration 54, loss = 0.45632551\n",
            "Iteration 55, loss = 0.45377350\n",
            "Iteration 56, loss = 0.47427023\n",
            "Iteration 57, loss = 0.45873063\n",
            "Iteration 58, loss = 0.45717206\n",
            "Iteration 59, loss = 0.45199308\n",
            "Iteration 60, loss = 0.45431931\n",
            "Iteration 61, loss = 0.45612768\n",
            "Iteration 62, loss = 0.45055469\n",
            "Iteration 63, loss = 0.45101696\n",
            "Iteration 64, loss = 0.44871515\n",
            "Iteration 65, loss = 0.45239124\n",
            "Iteration 66, loss = 0.45395361\n",
            "Iteration 67, loss = 0.44966915\n",
            "Iteration 68, loss = 0.46169499\n",
            "Iteration 69, loss = 0.45092468\n",
            "Iteration 70, loss = 0.45565297\n",
            "Iteration 71, loss = 0.45889918\n",
            "Iteration 72, loss = 0.46563238\n",
            "Iteration 73, loss = 0.44902803\n",
            "Iteration 74, loss = 0.44407626\n",
            "Iteration 75, loss = 0.44162482\n",
            "Iteration 76, loss = 0.44877898\n",
            "Iteration 77, loss = 0.45052767\n",
            "Iteration 78, loss = 0.45858661\n",
            "Iteration 79, loss = 0.44275632\n",
            "Iteration 80, loss = 0.44618674\n",
            "Iteration 81, loss = 0.43862961\n",
            "Iteration 82, loss = 0.45092939\n",
            "Iteration 83, loss = 0.44664469\n",
            "Iteration 84, loss = 0.43993078\n",
            "Iteration 85, loss = 0.43725890\n",
            "Iteration 86, loss = 0.44048216\n",
            "Iteration 87, loss = 0.43991130\n",
            "Iteration 88, loss = 0.44275886\n",
            "Iteration 89, loss = 0.44816500\n",
            "Iteration 90, loss = 0.43790334\n",
            "Iteration 91, loss = 0.43388200\n",
            "Iteration 92, loss = 0.43538446\n",
            "Iteration 93, loss = 0.43660558\n",
            "Iteration 94, loss = 0.43165811\n",
            "Iteration 95, loss = 0.43401826\n",
            "Iteration 96, loss = 0.43196352\n",
            "Iteration 97, loss = 0.43768791\n",
            "Iteration 98, loss = 0.44482491\n",
            "Iteration 99, loss = 0.43633015\n",
            "Iteration 100, loss = 0.43434669\n",
            "Iteration 101, loss = 0.44528194\n",
            "Iteration 102, loss = 0.45060476\n",
            "Iteration 103, loss = 0.43961105\n",
            "Iteration 104, loss = 0.44042989\n",
            "Iteration 105, loss = 0.43631188\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.24614185\n",
            "Iteration 2, loss = 0.66931747\n",
            "Iteration 3, loss = 0.79473731\n",
            "Iteration 4, loss = 0.61778949\n",
            "Iteration 5, loss = 0.66263687\n",
            "Iteration 6, loss = 0.58873800\n",
            "Iteration 7, loss = 0.59725243\n",
            "Iteration 8, loss = 0.59368593\n",
            "Iteration 9, loss = 0.59013844\n",
            "Iteration 10, loss = 0.58420363\n",
            "Iteration 11, loss = 0.56959643\n",
            "Iteration 12, loss = 0.58510536\n",
            "Iteration 13, loss = 0.57091488\n",
            "Iteration 14, loss = 0.57257212\n",
            "Iteration 15, loss = 0.56834743\n",
            "Iteration 16, loss = 0.56978890\n",
            "Iteration 17, loss = 0.56151414\n",
            "Iteration 18, loss = 0.57738882\n",
            "Iteration 19, loss = 0.56844893\n",
            "Iteration 20, loss = 0.55876242\n",
            "Iteration 21, loss = 0.58023657\n",
            "Iteration 22, loss = 0.56703722\n",
            "Iteration 23, loss = 0.56225874\n",
            "Iteration 24, loss = 0.56790456\n",
            "Iteration 25, loss = 0.56504809\n",
            "Iteration 26, loss = 0.56608234\n",
            "Iteration 27, loss = 0.55076015\n",
            "Iteration 28, loss = 0.56146550\n",
            "Iteration 29, loss = 0.54056417\n",
            "Iteration 30, loss = 0.56063380\n",
            "Iteration 31, loss = 0.54874078\n",
            "Iteration 32, loss = 0.56759942\n",
            "Iteration 33, loss = 0.56212604\n",
            "Iteration 34, loss = 0.53687736\n",
            "Iteration 35, loss = 0.55578221\n",
            "Iteration 36, loss = 0.55695914\n",
            "Iteration 37, loss = 0.55348309\n",
            "Iteration 38, loss = 0.54590513\n",
            "Iteration 39, loss = 0.54729378\n",
            "Iteration 40, loss = 0.52979338\n",
            "Iteration 41, loss = 0.54150233\n",
            "Iteration 42, loss = 0.53745448\n",
            "Iteration 43, loss = 0.54682738\n",
            "Iteration 44, loss = 0.54765215\n",
            "Iteration 45, loss = 0.53830479\n",
            "Iteration 46, loss = 0.53275922\n",
            "Iteration 47, loss = 0.55074786\n",
            "Iteration 48, loss = 0.54091876\n",
            "Iteration 49, loss = 0.53640286\n",
            "Iteration 50, loss = 0.51484577\n",
            "Iteration 51, loss = 0.55725719\n",
            "Iteration 52, loss = 0.52884448\n",
            "Iteration 53, loss = 0.51884175\n",
            "Iteration 54, loss = 0.52843685\n",
            "Iteration 55, loss = 0.53476841\n",
            "Iteration 56, loss = 0.51255435\n",
            "Iteration 57, loss = 0.50829506\n",
            "Iteration 58, loss = 0.50169282\n",
            "Iteration 59, loss = 0.51473841\n",
            "Iteration 60, loss = 0.50794881\n",
            "Iteration 61, loss = 0.52244921\n",
            "Iteration 62, loss = 0.51743873\n",
            "Iteration 63, loss = 0.50996845\n",
            "Iteration 64, loss = 0.50485864\n",
            "Iteration 65, loss = 0.50883933\n",
            "Iteration 66, loss = 0.50732994\n",
            "Iteration 67, loss = 0.52160279\n",
            "Iteration 68, loss = 0.51166866\n",
            "Iteration 69, loss = 0.53862114\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.85293531\n",
            "Iteration 2, loss = 0.65321544\n",
            "Iteration 3, loss = 0.63973351\n",
            "Iteration 4, loss = 0.62458730\n",
            "Iteration 5, loss = 0.62516865\n",
            "Iteration 6, loss = 0.63272690\n",
            "Iteration 7, loss = 0.61220698\n",
            "Iteration 8, loss = 0.60734369\n",
            "Iteration 9, loss = 0.59931093\n",
            "Iteration 10, loss = 0.59801586\n",
            "Iteration 11, loss = 0.60335213\n",
            "Iteration 12, loss = 0.60440400\n",
            "Iteration 13, loss = 0.59710978\n",
            "Iteration 14, loss = 0.59851023\n",
            "Iteration 15, loss = 0.59542890\n",
            "Iteration 16, loss = 0.59420388\n",
            "Iteration 17, loss = 0.59529616\n",
            "Iteration 18, loss = 0.59528432\n",
            "Iteration 19, loss = 0.59990282\n",
            "Iteration 20, loss = 0.58634628\n",
            "Iteration 21, loss = 0.58768270\n",
            "Iteration 22, loss = 0.58692357\n",
            "Iteration 23, loss = 0.59283934\n",
            "Iteration 24, loss = 0.58582761\n",
            "Iteration 25, loss = 0.58013301\n",
            "Iteration 26, loss = 0.60489088\n",
            "Iteration 27, loss = 0.57946205\n",
            "Iteration 28, loss = 0.59535583\n",
            "Iteration 29, loss = 0.57507857\n",
            "Iteration 30, loss = 0.58178518\n",
            "Iteration 31, loss = 0.57993183\n",
            "Iteration 32, loss = 0.57868762\n",
            "Iteration 33, loss = 0.56790143\n",
            "Iteration 34, loss = 0.58220265\n",
            "Iteration 35, loss = 0.57941806\n",
            "Iteration 36, loss = 0.56570052\n",
            "Iteration 37, loss = 0.56781798\n",
            "Iteration 38, loss = 0.57335453\n",
            "Iteration 39, loss = 0.56296616\n",
            "Iteration 40, loss = 0.55890789\n",
            "Iteration 41, loss = 0.57055167\n",
            "Iteration 42, loss = 0.57028226\n",
            "Iteration 43, loss = 0.56469023\n",
            "Iteration 44, loss = 0.56500936\n",
            "Iteration 45, loss = 0.55649624\n",
            "Iteration 46, loss = 0.55011908\n",
            "Iteration 47, loss = 0.56088771\n",
            "Iteration 48, loss = 0.55621228\n",
            "Iteration 49, loss = 0.54728543\n",
            "Iteration 50, loss = 0.55341461\n",
            "Iteration 51, loss = 0.54058063\n",
            "Iteration 52, loss = 0.55098438\n",
            "Iteration 53, loss = 0.56230557\n",
            "Iteration 54, loss = 0.56337275\n",
            "Iteration 55, loss = 0.53453418\n",
            "Iteration 56, loss = 0.53774654\n",
            "Iteration 57, loss = 0.53741416\n",
            "Iteration 58, loss = 0.54623318\n",
            "Iteration 59, loss = 0.53789473\n",
            "Iteration 60, loss = 0.53517580\n",
            "Iteration 61, loss = 0.54210835\n",
            "Iteration 62, loss = 0.53127735\n",
            "Iteration 63, loss = 0.53304104\n",
            "Iteration 64, loss = 0.54785640\n",
            "Iteration 65, loss = 0.52743057\n",
            "Iteration 66, loss = 0.54145737\n",
            "Iteration 67, loss = 0.53987843\n",
            "Iteration 68, loss = 0.52228032\n",
            "Iteration 69, loss = 0.52024401\n",
            "Iteration 70, loss = 0.54298168\n",
            "Iteration 71, loss = 0.56408973\n",
            "Iteration 72, loss = 0.55271765\n",
            "Iteration 73, loss = 0.53876662\n",
            "Iteration 74, loss = 0.53911832\n",
            "Iteration 75, loss = 0.53081002\n",
            "Iteration 76, loss = 0.53419298\n",
            "Iteration 77, loss = 0.52649600\n",
            "Iteration 78, loss = 0.52548945\n",
            "Iteration 79, loss = 0.53581118\n",
            "Iteration 80, loss = 0.54635353\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.99948016\n",
            "Iteration 2, loss = 0.63014040\n",
            "Iteration 3, loss = 0.62630808\n",
            "Iteration 4, loss = 0.63011407\n",
            "Iteration 5, loss = 0.61758293\n",
            "Iteration 6, loss = 0.60299243\n",
            "Iteration 7, loss = 0.59594994\n",
            "Iteration 8, loss = 0.59198070\n",
            "Iteration 9, loss = 0.58295658\n",
            "Iteration 10, loss = 0.59001021\n",
            "Iteration 11, loss = 0.58383547\n",
            "Iteration 12, loss = 0.59280436\n",
            "Iteration 13, loss = 0.58168464\n",
            "Iteration 14, loss = 0.57952153\n",
            "Iteration 15, loss = 0.57951402\n",
            "Iteration 16, loss = 0.58319965\n",
            "Iteration 17, loss = 0.56534149\n",
            "Iteration 18, loss = 0.57685630\n",
            "Iteration 19, loss = 0.57501695\n",
            "Iteration 20, loss = 0.58381952\n",
            "Iteration 21, loss = 0.56112065\n",
            "Iteration 22, loss = 0.55671596\n",
            "Iteration 23, loss = 0.56619426\n",
            "Iteration 24, loss = 0.55989485\n",
            "Iteration 25, loss = 0.56037977\n",
            "Iteration 26, loss = 0.55508117\n",
            "Iteration 27, loss = 0.56461008\n",
            "Iteration 28, loss = 0.55150989\n",
            "Iteration 29, loss = 0.54289426\n",
            "Iteration 30, loss = 0.55232771\n",
            "Iteration 31, loss = 0.55072574\n",
            "Iteration 32, loss = 0.54544018\n",
            "Iteration 33, loss = 0.54001047\n",
            "Iteration 34, loss = 0.53435974\n",
            "Iteration 35, loss = 0.52880006\n",
            "Iteration 36, loss = 0.53288706\n",
            "Iteration 37, loss = 0.54510702\n",
            "Iteration 38, loss = 0.52699718\n",
            "Iteration 39, loss = 0.52825744\n",
            "Iteration 40, loss = 0.54061580\n",
            "Iteration 41, loss = 0.52854172\n",
            "Iteration 42, loss = 0.52726037\n",
            "Iteration 43, loss = 0.52619585\n",
            "Iteration 44, loss = 0.53501741\n",
            "Iteration 45, loss = 0.52202781\n",
            "Iteration 46, loss = 0.52197119\n",
            "Iteration 47, loss = 0.53456678\n",
            "Iteration 48, loss = 0.50855557\n",
            "Iteration 49, loss = 0.51943141\n",
            "Iteration 50, loss = 0.52540764\n",
            "Iteration 51, loss = 0.51219818\n",
            "Iteration 52, loss = 0.51979934\n",
            "Iteration 53, loss = 0.50740595\n",
            "Iteration 54, loss = 0.51774254\n",
            "Iteration 55, loss = 0.50245149\n",
            "Iteration 56, loss = 0.50023175\n",
            "Iteration 57, loss = 0.50216217\n",
            "Iteration 58, loss = 0.50769662\n",
            "Iteration 59, loss = 0.49925542\n",
            "Iteration 60, loss = 0.51047761\n",
            "Iteration 61, loss = 0.49479915\n",
            "Iteration 62, loss = 0.49790341\n",
            "Iteration 63, loss = 0.49744607\n",
            "Iteration 64, loss = 0.51284085\n",
            "Iteration 65, loss = 0.52234312\n",
            "Iteration 66, loss = 0.49878658\n",
            "Iteration 67, loss = 0.50090268\n",
            "Iteration 68, loss = 0.48878873\n",
            "Iteration 69, loss = 0.49365133\n",
            "Iteration 70, loss = 0.49668343\n",
            "Iteration 71, loss = 0.50045670\n",
            "Iteration 72, loss = 0.52292405\n",
            "Iteration 73, loss = 0.49042265\n",
            "Iteration 74, loss = 0.49282399\n",
            "Iteration 75, loss = 0.48071877\n",
            "Iteration 76, loss = 0.50250874\n",
            "Iteration 77, loss = 0.49980147\n",
            "Iteration 78, loss = 0.47085282\n",
            "Iteration 79, loss = 0.51170957\n",
            "Iteration 80, loss = 0.50547469\n",
            "Iteration 81, loss = 0.48462160\n",
            "Iteration 82, loss = 0.51024522\n",
            "Iteration 83, loss = 0.48660114\n",
            "Iteration 84, loss = 0.47803648\n",
            "Iteration 85, loss = 0.49407175\n",
            "Iteration 86, loss = 0.47736462\n",
            "Iteration 87, loss = 0.46806059\n",
            "Iteration 88, loss = 0.48050786\n",
            "Iteration 89, loss = 0.49137428\n",
            "Iteration 90, loss = 0.46322325\n",
            "Iteration 91, loss = 0.47675635\n",
            "Iteration 92, loss = 0.48200298\n",
            "Iteration 93, loss = 0.50087424\n",
            "Iteration 94, loss = 0.49111744\n",
            "Iteration 95, loss = 0.46211512\n",
            "Iteration 96, loss = 0.46736758\n",
            "Iteration 97, loss = 0.48488564\n",
            "Iteration 98, loss = 0.46564197\n",
            "Iteration 99, loss = 0.45885448\n",
            "Iteration 100, loss = 0.49025150\n",
            "Iteration 101, loss = 0.49199735\n",
            "Iteration 102, loss = 0.47465205\n",
            "Iteration 103, loss = 0.47804281\n",
            "Iteration 104, loss = 0.48304016\n",
            "Iteration 105, loss = 0.46861197\n",
            "Iteration 106, loss = 0.48024023\n",
            "Iteration 107, loss = 0.47455779\n",
            "Iteration 108, loss = 0.46663628\n",
            "Iteration 109, loss = 0.47882870\n",
            "Iteration 110, loss = 0.51503133\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.78289348\n",
            "Iteration 2, loss = 0.67991333\n",
            "Iteration 3, loss = 0.68502200\n",
            "Iteration 4, loss = 0.65924634\n",
            "Iteration 5, loss = 0.66113969\n",
            "Iteration 6, loss = 0.62783976\n",
            "Iteration 7, loss = 0.62416650\n",
            "Iteration 8, loss = 0.62242928\n",
            "Iteration 9, loss = 0.62026942\n",
            "Iteration 10, loss = 0.62215183\n",
            "Iteration 11, loss = 0.61462432\n",
            "Iteration 12, loss = 0.61665970\n",
            "Iteration 13, loss = 0.60750918\n",
            "Iteration 14, loss = 0.62242741\n",
            "Iteration 15, loss = 0.60829883\n",
            "Iteration 16, loss = 0.60598386\n",
            "Iteration 17, loss = 0.60475148\n",
            "Iteration 18, loss = 0.60772377\n",
            "Iteration 19, loss = 0.60405425\n",
            "Iteration 20, loss = 0.59273864\n",
            "Iteration 21, loss = 0.60285005\n",
            "Iteration 22, loss = 0.59243068\n",
            "Iteration 23, loss = 0.59214121\n",
            "Iteration 24, loss = 0.58857087\n",
            "Iteration 25, loss = 0.59279169\n",
            "Iteration 26, loss = 0.58081165\n",
            "Iteration 27, loss = 0.58662130\n",
            "Iteration 28, loss = 0.59307215\n",
            "Iteration 29, loss = 0.58931163\n",
            "Iteration 30, loss = 0.58612668\n",
            "Iteration 31, loss = 0.58071225\n",
            "Iteration 32, loss = 0.57270430\n",
            "Iteration 33, loss = 0.56609004\n",
            "Iteration 34, loss = 0.60386784\n",
            "Iteration 35, loss = 0.57874726\n",
            "Iteration 36, loss = 0.57153584\n",
            "Iteration 37, loss = 0.57174881\n",
            "Iteration 38, loss = 0.57132731\n",
            "Iteration 39, loss = 0.55908415\n",
            "Iteration 40, loss = 0.56529461\n",
            "Iteration 41, loss = 0.55844471\n",
            "Iteration 42, loss = 0.55158516\n",
            "Iteration 43, loss = 0.55942633\n",
            "Iteration 44, loss = 0.56563281\n",
            "Iteration 45, loss = 0.55383240\n",
            "Iteration 46, loss = 0.56692115\n",
            "Iteration 47, loss = 0.55997005\n",
            "Iteration 48, loss = 0.54061397\n",
            "Iteration 49, loss = 0.55017121\n",
            "Iteration 50, loss = 0.56194955\n",
            "Iteration 51, loss = 0.55972110\n",
            "Iteration 52, loss = 0.53854603\n",
            "Iteration 53, loss = 0.53702691\n",
            "Iteration 54, loss = 0.53781523\n",
            "Iteration 55, loss = 0.53345008\n",
            "Iteration 56, loss = 0.53904978\n",
            "Iteration 57, loss = 0.54689537\n",
            "Iteration 58, loss = 0.54967645\n",
            "Iteration 59, loss = 0.54080007\n",
            "Iteration 60, loss = 0.53359968\n",
            "Iteration 61, loss = 0.53623856\n",
            "Iteration 62, loss = 0.52487512\n",
            "Iteration 63, loss = 0.53485428\n",
            "Iteration 64, loss = 0.53129962\n",
            "Iteration 65, loss = 0.55865851\n",
            "Iteration 66, loss = 0.53223197\n",
            "Iteration 67, loss = 0.51219228\n",
            "Iteration 68, loss = 0.52456830\n",
            "Iteration 69, loss = 0.52443805\n",
            "Iteration 70, loss = 0.52931326\n",
            "Iteration 71, loss = 0.51775661\n",
            "Iteration 72, loss = 0.53844312\n",
            "Iteration 73, loss = 0.52505141\n",
            "Iteration 74, loss = 0.52101398\n",
            "Iteration 75, loss = 0.52325678\n",
            "Iteration 76, loss = 0.51931925\n",
            "Iteration 77, loss = 0.51318509\n",
            "Iteration 78, loss = 0.52754597\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.08686159\n",
            "Iteration 2, loss = 0.77388041\n",
            "Iteration 3, loss = 0.65801785\n",
            "Iteration 4, loss = 0.63715938\n",
            "Iteration 5, loss = 0.61631257\n",
            "Iteration 6, loss = 0.65530486\n",
            "Iteration 7, loss = 0.62093640\n",
            "Iteration 8, loss = 0.61887806\n",
            "Iteration 9, loss = 0.62114681\n",
            "Iteration 10, loss = 0.61847152\n",
            "Iteration 11, loss = 0.61422100\n",
            "Iteration 12, loss = 0.62077464\n",
            "Iteration 13, loss = 0.61562386\n",
            "Iteration 14, loss = 0.61299268\n",
            "Iteration 15, loss = 0.61021656\n",
            "Iteration 16, loss = 0.61468347\n",
            "Iteration 17, loss = 0.61328749\n",
            "Iteration 18, loss = 0.62647323\n",
            "Iteration 19, loss = 0.61447826\n",
            "Iteration 20, loss = 0.60869967\n",
            "Iteration 21, loss = 0.61112157\n",
            "Iteration 22, loss = 0.61495527\n",
            "Iteration 23, loss = 0.61433889\n",
            "Iteration 24, loss = 0.60939580\n",
            "Iteration 25, loss = 0.61362816\n",
            "Iteration 26, loss = 0.60970008\n",
            "Iteration 27, loss = 0.60360565\n",
            "Iteration 28, loss = 0.62195345\n",
            "Iteration 29, loss = 0.61134112\n",
            "Iteration 30, loss = 0.60765900\n",
            "Iteration 31, loss = 0.60774639\n",
            "Iteration 32, loss = 0.60810926\n",
            "Iteration 33, loss = 0.60073464\n",
            "Iteration 34, loss = 0.60026082\n",
            "Iteration 35, loss = 0.60241183\n",
            "Iteration 36, loss = 0.60719252\n",
            "Iteration 37, loss = 0.60246644\n",
            "Iteration 38, loss = 0.60062651\n",
            "Iteration 39, loss = 0.59659258\n",
            "Iteration 40, loss = 0.59889072\n",
            "Iteration 41, loss = 0.59335563\n",
            "Iteration 42, loss = 0.59782779\n",
            "Iteration 43, loss = 0.60048003\n",
            "Iteration 44, loss = 0.59453067\n",
            "Iteration 45, loss = 0.58313957\n",
            "Iteration 46, loss = 0.59841098\n",
            "Iteration 47, loss = 0.59747408\n",
            "Iteration 48, loss = 0.58152603\n",
            "Iteration 49, loss = 0.59410134\n",
            "Iteration 50, loss = 0.58158208\n",
            "Iteration 51, loss = 0.58568504\n",
            "Iteration 52, loss = 0.58905904\n",
            "Iteration 53, loss = 0.58641782\n",
            "Iteration 54, loss = 0.59178672\n",
            "Iteration 55, loss = 0.58282477\n",
            "Iteration 56, loss = 0.58107121\n",
            "Iteration 57, loss = 0.58212078\n",
            "Iteration 58, loss = 0.57630534\n",
            "Iteration 59, loss = 0.57518051\n",
            "Iteration 60, loss = 0.59497697\n",
            "Iteration 61, loss = 0.57674976\n",
            "Iteration 62, loss = 0.57162096\n",
            "Iteration 63, loss = 0.57723685\n",
            "Iteration 64, loss = 0.56500665\n",
            "Iteration 65, loss = 0.56946283\n",
            "Iteration 66, loss = 0.56686598\n",
            "Iteration 67, loss = 0.57618757\n",
            "Iteration 68, loss = 0.55926390\n",
            "Iteration 69, loss = 0.55880888\n",
            "Iteration 70, loss = 0.56781334\n",
            "Iteration 71, loss = 0.55928512\n",
            "Iteration 72, loss = 0.55768420\n",
            "Iteration 73, loss = 0.55627443\n",
            "Iteration 74, loss = 0.56430619\n",
            "Iteration 75, loss = 0.56457724\n",
            "Iteration 76, loss = 0.56244390\n",
            "Iteration 77, loss = 0.55350229\n",
            "Iteration 78, loss = 0.56565918\n",
            "Iteration 79, loss = 0.54909750\n",
            "Iteration 80, loss = 0.54681326\n",
            "Iteration 81, loss = 0.54480667\n",
            "Iteration 82, loss = 0.56241652\n",
            "Iteration 83, loss = 0.53806933\n",
            "Iteration 84, loss = 0.54169682\n",
            "Iteration 85, loss = 0.53286737\n",
            "Iteration 86, loss = 0.54804633\n",
            "Iteration 87, loss = 0.54687251\n",
            "Iteration 88, loss = 0.54417759\n",
            "Iteration 89, loss = 0.53868586\n",
            "Iteration 90, loss = 0.53019446\n",
            "Iteration 91, loss = 0.54005162\n",
            "Iteration 92, loss = 0.54145110\n",
            "Iteration 93, loss = 0.54779815\n",
            "Iteration 94, loss = 0.54631571\n",
            "Iteration 95, loss = 0.54422978\n",
            "Iteration 96, loss = 0.53469870\n",
            "Iteration 97, loss = 0.53053261\n",
            "Iteration 98, loss = 0.52362734\n",
            "Iteration 99, loss = 0.53434363\n",
            "Iteration 100, loss = 0.51532104\n",
            "Iteration 101, loss = 0.54578221\n",
            "Iteration 102, loss = 0.55387697\n",
            "Iteration 103, loss = 0.53529321\n",
            "Iteration 104, loss = 0.51761100\n",
            "Iteration 105, loss = 0.53203706\n",
            "Iteration 106, loss = 0.54036475\n",
            "Iteration 107, loss = 0.53650417\n",
            "Iteration 108, loss = 0.51888422\n",
            "Iteration 109, loss = 0.53503677\n",
            "Iteration 110, loss = 0.51530777\n",
            "Iteration 111, loss = 0.54746764\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70918656\n",
            "Iteration 2, loss = 0.62182897\n",
            "Iteration 3, loss = 0.60539468\n",
            "Iteration 4, loss = 0.60270102\n",
            "Iteration 5, loss = 0.59734580\n",
            "Iteration 6, loss = 0.59548967\n",
            "Iteration 7, loss = 0.59312053\n",
            "Iteration 8, loss = 0.59052615\n",
            "Iteration 9, loss = 0.58614951\n",
            "Iteration 10, loss = 0.58552373\n",
            "Iteration 11, loss = 0.58282244\n",
            "Iteration 12, loss = 0.58243742\n",
            "Iteration 13, loss = 0.57872535\n",
            "Iteration 14, loss = 0.57611817\n",
            "Iteration 15, loss = 0.57349052\n",
            "Iteration 16, loss = 0.57098186\n",
            "Iteration 17, loss = 0.56492430\n",
            "Iteration 18, loss = 0.56324179\n",
            "Iteration 19, loss = 0.55710475\n",
            "Iteration 20, loss = 0.55973729\n",
            "Iteration 21, loss = 0.55219034\n",
            "Iteration 22, loss = 0.54808744\n",
            "Iteration 23, loss = 0.54293763\n",
            "Iteration 24, loss = 0.54298716\n",
            "Iteration 25, loss = 0.53304373\n",
            "Iteration 26, loss = 0.53100482\n",
            "Iteration 27, loss = 0.52856504\n",
            "Iteration 28, loss = 0.52863495\n",
            "Iteration 29, loss = 0.51552067\n",
            "Iteration 30, loss = 0.51444691\n",
            "Iteration 31, loss = 0.51530071\n",
            "Iteration 32, loss = 0.50266794\n",
            "Iteration 33, loss = 0.49779907\n",
            "Iteration 34, loss = 0.49451096\n",
            "Iteration 35, loss = 0.48606886\n",
            "Iteration 36, loss = 0.48187722\n",
            "Iteration 37, loss = 0.47964331\n",
            "Iteration 38, loss = 0.47683126\n",
            "Iteration 39, loss = 0.46883897\n",
            "Iteration 40, loss = 0.46715766\n",
            "Iteration 41, loss = 0.46311159\n",
            "Iteration 42, loss = 0.45993383\n",
            "Iteration 43, loss = 0.46039173\n",
            "Iteration 44, loss = 0.45677304\n",
            "Iteration 45, loss = 0.44846446\n",
            "Iteration 46, loss = 0.44511101\n",
            "Iteration 47, loss = 0.44405166\n",
            "Iteration 48, loss = 0.44111034\n",
            "Iteration 49, loss = 0.43814329\n",
            "Iteration 50, loss = 0.43446364\n",
            "Iteration 51, loss = 0.43396995\n",
            "Iteration 52, loss = 0.42996024\n",
            "Iteration 53, loss = 0.43014715\n",
            "Iteration 54, loss = 0.42977746\n",
            "Iteration 55, loss = 0.42683242\n",
            "Iteration 56, loss = 0.42578780\n",
            "Iteration 57, loss = 0.42102036\n",
            "Iteration 58, loss = 0.42065645\n",
            "Iteration 59, loss = 0.42498531\n",
            "Iteration 60, loss = 0.41534448\n",
            "Iteration 61, loss = 0.42129502\n",
            "Iteration 62, loss = 0.41933185\n",
            "Iteration 63, loss = 0.42466526\n",
            "Iteration 64, loss = 0.42982035\n",
            "Iteration 65, loss = 0.42041186\n",
            "Iteration 66, loss = 0.41721366\n",
            "Iteration 67, loss = 0.43126949\n",
            "Iteration 68, loss = 0.41819081\n",
            "Iteration 69, loss = 0.42663610\n",
            "Iteration 70, loss = 0.40973014\n",
            "Iteration 71, loss = 0.40594489\n",
            "Iteration 72, loss = 0.40601274\n",
            "Iteration 73, loss = 0.40203608\n",
            "Iteration 74, loss = 0.40660041\n",
            "Iteration 75, loss = 0.40665497\n",
            "Iteration 76, loss = 0.40965978\n",
            "Iteration 77, loss = 0.40740278\n",
            "Iteration 78, loss = 0.39694613\n",
            "Iteration 79, loss = 0.39403524\n",
            "Iteration 80, loss = 0.39213125\n",
            "Iteration 81, loss = 0.39425074\n",
            "Iteration 82, loss = 0.39606790\n",
            "Iteration 83, loss = 0.38994228\n",
            "Iteration 84, loss = 0.39650048\n",
            "Iteration 85, loss = 0.38974864\n",
            "Iteration 86, loss = 0.39011818\n",
            "Iteration 87, loss = 0.38935939\n",
            "Iteration 88, loss = 0.38679049\n",
            "Iteration 89, loss = 0.39057154\n",
            "Iteration 90, loss = 0.38418777\n",
            "Iteration 91, loss = 0.38827660\n",
            "Iteration 92, loss = 0.39315783\n",
            "Iteration 93, loss = 0.38956482\n",
            "Iteration 94, loss = 0.38094444\n",
            "Iteration 95, loss = 0.38681186\n",
            "Iteration 96, loss = 0.38727676\n",
            "Iteration 97, loss = 0.38683785\n",
            "Iteration 98, loss = 0.39573362\n",
            "Iteration 99, loss = 0.37768352\n",
            "Iteration 100, loss = 0.37743661\n",
            "Iteration 101, loss = 0.37375072\n",
            "Iteration 102, loss = 0.37607765\n",
            "Iteration 103, loss = 0.37151932\n",
            "Iteration 104, loss = 0.38234335\n",
            "Iteration 105, loss = 0.38185466\n",
            "Iteration 106, loss = 0.37299969\n",
            "Iteration 107, loss = 0.38495254\n",
            "Iteration 108, loss = 0.37244825\n",
            "Iteration 109, loss = 0.36930566\n",
            "Iteration 110, loss = 0.37419112\n",
            "Iteration 111, loss = 0.36629004\n",
            "Iteration 112, loss = 0.36233591\n",
            "Iteration 113, loss = 0.37206919\n",
            "Iteration 114, loss = 0.37167878\n",
            "Iteration 115, loss = 0.36867662\n",
            "Iteration 116, loss = 0.36708503\n",
            "Iteration 117, loss = 0.37044228\n",
            "Iteration 118, loss = 0.37372604\n",
            "Iteration 119, loss = 0.37540749\n",
            "Iteration 120, loss = 0.36740767\n",
            "Iteration 121, loss = 0.36971651\n",
            "Iteration 122, loss = 0.36227886\n",
            "Iteration 123, loss = 0.36204577\n",
            "Iteration 124, loss = 0.35711208\n",
            "Iteration 125, loss = 0.35962940\n",
            "Iteration 126, loss = 0.35741907\n",
            "Iteration 127, loss = 0.35377911\n",
            "Iteration 128, loss = 0.36932635\n",
            "Iteration 129, loss = 0.36768292\n",
            "Iteration 130, loss = 0.36980991\n",
            "Iteration 131, loss = 0.35530831\n",
            "Iteration 132, loss = 0.35281765\n",
            "Iteration 133, loss = 0.35319641\n",
            "Iteration 134, loss = 0.35091162\n",
            "Iteration 135, loss = 0.35632002\n",
            "Iteration 136, loss = 0.35865528\n",
            "Iteration 137, loss = 0.36866548\n",
            "Iteration 138, loss = 0.34584479\n",
            "Iteration 139, loss = 0.36178735\n",
            "Iteration 140, loss = 0.35409731\n",
            "Iteration 141, loss = 0.35824341\n",
            "Iteration 142, loss = 0.35079184\n",
            "Iteration 143, loss = 0.35693223\n",
            "Iteration 144, loss = 0.35311027\n",
            "Iteration 145, loss = 0.35123209\n",
            "Iteration 146, loss = 0.34502113\n",
            "Iteration 147, loss = 0.34770804\n",
            "Iteration 148, loss = 0.37086002\n",
            "Iteration 149, loss = 0.34901760\n",
            "Iteration 150, loss = 0.34800641\n",
            "Iteration 151, loss = 0.34459997\n",
            "Iteration 152, loss = 0.34991223\n",
            "Iteration 153, loss = 0.34191581\n",
            "Iteration 154, loss = 0.34488550\n",
            "Iteration 155, loss = 0.35728382\n",
            "Iteration 156, loss = 0.34623709\n",
            "Iteration 157, loss = 0.34359779\n",
            "Iteration 158, loss = 0.34890503\n",
            "Iteration 159, loss = 0.34461160\n",
            "Iteration 160, loss = 0.34320918\n",
            "Iteration 161, loss = 0.34464763\n",
            "Iteration 162, loss = 0.34003762\n",
            "Iteration 163, loss = 0.33608941\n",
            "Iteration 164, loss = 0.33706384\n",
            "Iteration 165, loss = 0.33939898\n",
            "Iteration 166, loss = 0.34428521\n",
            "Iteration 167, loss = 0.35464246\n",
            "Iteration 168, loss = 0.34187554\n",
            "Iteration 169, loss = 0.35969784\n",
            "Iteration 170, loss = 0.34086369\n",
            "Iteration 171, loss = 0.33419712\n",
            "Iteration 172, loss = 0.33430192\n",
            "Iteration 173, loss = 0.34112821\n",
            "Iteration 174, loss = 0.33412933\n",
            "Iteration 175, loss = 0.34008815\n",
            "Iteration 176, loss = 0.34634146\n",
            "Iteration 177, loss = 0.34520227\n",
            "Iteration 178, loss = 0.33692167\n",
            "Iteration 179, loss = 0.33496412\n",
            "Iteration 180, loss = 0.33883498\n",
            "Iteration 181, loss = 0.34019024\n",
            "Iteration 182, loss = 0.34342768\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67956644\n",
            "Iteration 2, loss = 0.61491613\n",
            "Iteration 3, loss = 0.59609474\n",
            "Iteration 4, loss = 0.58423484\n",
            "Iteration 5, loss = 0.57854969\n",
            "Iteration 6, loss = 0.57210077\n",
            "Iteration 7, loss = 0.56776025\n",
            "Iteration 8, loss = 0.56130210\n",
            "Iteration 9, loss = 0.55732538\n",
            "Iteration 10, loss = 0.55342205\n",
            "Iteration 11, loss = 0.54787875\n",
            "Iteration 12, loss = 0.54116942\n",
            "Iteration 13, loss = 0.53683854\n",
            "Iteration 14, loss = 0.53487658\n",
            "Iteration 15, loss = 0.52931023\n",
            "Iteration 16, loss = 0.52347865\n",
            "Iteration 17, loss = 0.52138098\n",
            "Iteration 18, loss = 0.51625946\n",
            "Iteration 19, loss = 0.51252994\n",
            "Iteration 20, loss = 0.50783846\n",
            "Iteration 21, loss = 0.50642482\n",
            "Iteration 22, loss = 0.50356949\n",
            "Iteration 23, loss = 0.49793175\n",
            "Iteration 24, loss = 0.48847342\n",
            "Iteration 25, loss = 0.48506880\n",
            "Iteration 26, loss = 0.47995429\n",
            "Iteration 27, loss = 0.47051280\n",
            "Iteration 28, loss = 0.47518875\n",
            "Iteration 29, loss = 0.46392931\n",
            "Iteration 30, loss = 0.45914528\n",
            "Iteration 31, loss = 0.46495352\n",
            "Iteration 32, loss = 0.47294762\n",
            "Iteration 33, loss = 0.44670961\n",
            "Iteration 34, loss = 0.45488950\n",
            "Iteration 35, loss = 0.44308886\n",
            "Iteration 36, loss = 0.43614556\n",
            "Iteration 37, loss = 0.42972276\n",
            "Iteration 38, loss = 0.43372187\n",
            "Iteration 39, loss = 0.42521584\n",
            "Iteration 40, loss = 0.42684464\n",
            "Iteration 41, loss = 0.43521954\n",
            "Iteration 42, loss = 0.43066307\n",
            "Iteration 43, loss = 0.41965194\n",
            "Iteration 44, loss = 0.41490454\n",
            "Iteration 45, loss = 0.41444407\n",
            "Iteration 46, loss = 0.41085025\n",
            "Iteration 47, loss = 0.40989455\n",
            "Iteration 48, loss = 0.41364228\n",
            "Iteration 49, loss = 0.41510854\n",
            "Iteration 50, loss = 0.40878711\n",
            "Iteration 51, loss = 0.41055083\n",
            "Iteration 52, loss = 0.41963887\n",
            "Iteration 53, loss = 0.41080956\n",
            "Iteration 54, loss = 0.39963916\n",
            "Iteration 55, loss = 0.39796110\n",
            "Iteration 56, loss = 0.39149087\n",
            "Iteration 57, loss = 0.39464955\n",
            "Iteration 58, loss = 0.39141169\n",
            "Iteration 59, loss = 0.38974534\n",
            "Iteration 60, loss = 0.39716920\n",
            "Iteration 61, loss = 0.38395866\n",
            "Iteration 62, loss = 0.38904169\n",
            "Iteration 63, loss = 0.38809021\n",
            "Iteration 64, loss = 0.38395009\n",
            "Iteration 65, loss = 0.38140035\n",
            "Iteration 66, loss = 0.38512720\n",
            "Iteration 67, loss = 0.39315283\n",
            "Iteration 68, loss = 0.38703395\n",
            "Iteration 69, loss = 0.37840011\n",
            "Iteration 70, loss = 0.37411650\n",
            "Iteration 71, loss = 0.37288484\n",
            "Iteration 72, loss = 0.37125321\n",
            "Iteration 73, loss = 0.37349579\n",
            "Iteration 74, loss = 0.37831709\n",
            "Iteration 75, loss = 0.37036307\n",
            "Iteration 76, loss = 0.37182375\n",
            "Iteration 77, loss = 0.37230358\n",
            "Iteration 78, loss = 0.38911361\n",
            "Iteration 79, loss = 0.38052873\n",
            "Iteration 80, loss = 0.38870079\n",
            "Iteration 81, loss = 0.37328426\n",
            "Iteration 82, loss = 0.36980365\n",
            "Iteration 83, loss = 0.36876281\n",
            "Iteration 84, loss = 0.36409786\n",
            "Iteration 85, loss = 0.36808070\n",
            "Iteration 86, loss = 0.37538086\n",
            "Iteration 87, loss = 0.36832815\n",
            "Iteration 88, loss = 0.36076769\n",
            "Iteration 89, loss = 0.36594535\n",
            "Iteration 90, loss = 0.36237135\n",
            "Iteration 91, loss = 0.36009841\n",
            "Iteration 92, loss = 0.36039308\n",
            "Iteration 93, loss = 0.35455843\n",
            "Iteration 94, loss = 0.36510622\n",
            "Iteration 95, loss = 0.35683140\n",
            "Iteration 96, loss = 0.35685600\n",
            "Iteration 97, loss = 0.35613955\n",
            "Iteration 98, loss = 0.35793287\n",
            "Iteration 99, loss = 0.35564683\n",
            "Iteration 100, loss = 0.35480687\n",
            "Iteration 101, loss = 0.37237963\n",
            "Iteration 102, loss = 0.37189114\n",
            "Iteration 103, loss = 0.36192710\n",
            "Iteration 104, loss = 0.36125766\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64309715\n",
            "Iteration 2, loss = 0.61271524\n",
            "Iteration 3, loss = 0.59918674\n",
            "Iteration 4, loss = 0.59637037\n",
            "Iteration 5, loss = 0.59179795\n",
            "Iteration 6, loss = 0.59073686\n",
            "Iteration 7, loss = 0.58436799\n",
            "Iteration 8, loss = 0.57927768\n",
            "Iteration 9, loss = 0.57272448\n",
            "Iteration 10, loss = 0.56940529\n",
            "Iteration 11, loss = 0.56572538\n",
            "Iteration 12, loss = 0.56595757\n",
            "Iteration 13, loss = 0.55959303\n",
            "Iteration 14, loss = 0.55474402\n",
            "Iteration 15, loss = 0.55269069\n",
            "Iteration 16, loss = 0.54556361\n",
            "Iteration 17, loss = 0.54292187\n",
            "Iteration 18, loss = 0.53706452\n",
            "Iteration 19, loss = 0.53733900\n",
            "Iteration 20, loss = 0.52921017\n",
            "Iteration 21, loss = 0.52449923\n",
            "Iteration 22, loss = 0.52302916\n",
            "Iteration 23, loss = 0.51627509\n",
            "Iteration 24, loss = 0.51767685\n",
            "Iteration 25, loss = 0.51235148\n",
            "Iteration 26, loss = 0.50206977\n",
            "Iteration 27, loss = 0.49776390\n",
            "Iteration 28, loss = 0.49300076\n",
            "Iteration 29, loss = 0.48679924\n",
            "Iteration 30, loss = 0.48266796\n",
            "Iteration 31, loss = 0.47972974\n",
            "Iteration 32, loss = 0.48150379\n",
            "Iteration 33, loss = 0.48489407\n",
            "Iteration 34, loss = 0.46759154\n",
            "Iteration 35, loss = 0.46149346\n",
            "Iteration 36, loss = 0.45699037\n",
            "Iteration 37, loss = 0.45056328\n",
            "Iteration 38, loss = 0.44807715\n",
            "Iteration 39, loss = 0.44801141\n",
            "Iteration 40, loss = 0.43841020\n",
            "Iteration 41, loss = 0.43821941\n",
            "Iteration 42, loss = 0.43964800\n",
            "Iteration 43, loss = 0.42975057\n",
            "Iteration 44, loss = 0.43028406\n",
            "Iteration 45, loss = 0.42614616\n",
            "Iteration 46, loss = 0.42267689\n",
            "Iteration 47, loss = 0.41642194\n",
            "Iteration 48, loss = 0.41705009\n",
            "Iteration 49, loss = 0.41599147\n",
            "Iteration 50, loss = 0.41384808\n",
            "Iteration 51, loss = 0.41375944\n",
            "Iteration 52, loss = 0.41731644\n",
            "Iteration 53, loss = 0.41386203\n",
            "Iteration 54, loss = 0.42495821\n",
            "Iteration 55, loss = 0.40667253\n",
            "Iteration 56, loss = 0.41800185\n",
            "Iteration 57, loss = 0.41262572\n",
            "Iteration 58, loss = 0.40437083\n",
            "Iteration 59, loss = 0.40448030\n",
            "Iteration 60, loss = 0.39513716\n",
            "Iteration 61, loss = 0.39474804\n",
            "Iteration 62, loss = 0.39316014\n",
            "Iteration 63, loss = 0.39059299\n",
            "Iteration 64, loss = 0.39281015\n",
            "Iteration 65, loss = 0.39759576\n",
            "Iteration 66, loss = 0.38908491\n",
            "Iteration 67, loss = 0.40138184\n",
            "Iteration 68, loss = 0.40135442\n",
            "Iteration 69, loss = 0.39020225\n",
            "Iteration 70, loss = 0.39223185\n",
            "Iteration 71, loss = 0.39298649\n",
            "Iteration 72, loss = 0.38380825\n",
            "Iteration 73, loss = 0.38831509\n",
            "Iteration 74, loss = 0.38667126\n",
            "Iteration 75, loss = 0.38526413\n",
            "Iteration 76, loss = 0.38548026\n",
            "Iteration 77, loss = 0.38034903\n",
            "Iteration 78, loss = 0.38009124\n",
            "Iteration 79, loss = 0.37915951\n",
            "Iteration 80, loss = 0.38082925\n",
            "Iteration 81, loss = 0.37833265\n",
            "Iteration 82, loss = 0.37440760\n",
            "Iteration 83, loss = 0.37322920\n",
            "Iteration 84, loss = 0.37063466\n",
            "Iteration 85, loss = 0.37407873\n",
            "Iteration 86, loss = 0.37438909\n",
            "Iteration 87, loss = 0.37513733\n",
            "Iteration 88, loss = 0.37597568\n",
            "Iteration 89, loss = 0.37014701\n",
            "Iteration 90, loss = 0.36906034\n",
            "Iteration 91, loss = 0.36917245\n",
            "Iteration 92, loss = 0.36409735\n",
            "Iteration 93, loss = 0.37422203\n",
            "Iteration 94, loss = 0.37376088\n",
            "Iteration 95, loss = 0.36290261\n",
            "Iteration 96, loss = 0.36346207\n",
            "Iteration 97, loss = 0.36857196\n",
            "Iteration 98, loss = 0.37346726\n",
            "Iteration 99, loss = 0.36341422\n",
            "Iteration 100, loss = 0.36328682\n",
            "Iteration 101, loss = 0.36176377\n",
            "Iteration 102, loss = 0.36491795\n",
            "Iteration 103, loss = 0.36863446\n",
            "Iteration 104, loss = 0.36109189\n",
            "Iteration 105, loss = 0.35984542\n",
            "Iteration 106, loss = 0.35858203\n",
            "Iteration 107, loss = 0.35667582\n",
            "Iteration 108, loss = 0.35589340\n",
            "Iteration 109, loss = 0.35513716\n",
            "Iteration 110, loss = 0.35519365\n",
            "Iteration 111, loss = 0.36394695\n",
            "Iteration 112, loss = 0.36101208\n",
            "Iteration 113, loss = 0.35983986\n",
            "Iteration 114, loss = 0.35811382\n",
            "Iteration 115, loss = 0.35685085\n",
            "Iteration 116, loss = 0.35892595\n",
            "Iteration 117, loss = 0.35599091\n",
            "Iteration 118, loss = 0.35358226\n",
            "Iteration 119, loss = 0.35721701\n",
            "Iteration 120, loss = 0.35346214\n",
            "Iteration 121, loss = 0.35918472\n",
            "Iteration 122, loss = 0.34492661\n",
            "Iteration 123, loss = 0.35127230\n",
            "Iteration 124, loss = 0.35610888\n",
            "Iteration 125, loss = 0.35162443\n",
            "Iteration 126, loss = 0.34950631\n",
            "Iteration 127, loss = 0.34465401\n",
            "Iteration 128, loss = 0.34357697\n",
            "Iteration 129, loss = 0.34529723\n",
            "Iteration 130, loss = 0.34075209\n",
            "Iteration 131, loss = 0.34777264\n",
            "Iteration 132, loss = 0.35850563\n",
            "Iteration 133, loss = 0.36303779\n",
            "Iteration 134, loss = 0.34575038\n",
            "Iteration 135, loss = 0.34198129\n",
            "Iteration 136, loss = 0.33987037\n",
            "Iteration 137, loss = 0.34518058\n",
            "Iteration 138, loss = 0.36674196\n",
            "Iteration 139, loss = 0.36774222\n",
            "Iteration 140, loss = 0.34469404\n",
            "Iteration 141, loss = 0.34753791\n",
            "Iteration 142, loss = 0.33621899\n",
            "Iteration 143, loss = 0.33816445\n",
            "Iteration 144, loss = 0.34199421\n",
            "Iteration 145, loss = 0.35149443\n",
            "Iteration 146, loss = 0.34738241\n",
            "Iteration 147, loss = 0.34606838\n",
            "Iteration 148, loss = 0.33580672\n",
            "Iteration 149, loss = 0.33768499\n",
            "Iteration 150, loss = 0.33638213\n",
            "Iteration 151, loss = 0.34651654\n",
            "Iteration 152, loss = 0.33467347\n",
            "Iteration 153, loss = 0.33874076\n",
            "Iteration 154, loss = 0.33855877\n",
            "Iteration 155, loss = 0.35991189\n",
            "Iteration 156, loss = 0.34003158\n",
            "Iteration 157, loss = 0.33942378\n",
            "Iteration 158, loss = 0.33104576\n",
            "Iteration 159, loss = 0.33487840\n",
            "Iteration 160, loss = 0.33236467\n",
            "Iteration 161, loss = 0.35230285\n",
            "Iteration 162, loss = 0.34528755\n",
            "Iteration 163, loss = 0.33232158\n",
            "Iteration 164, loss = 0.32962882\n",
            "Iteration 165, loss = 0.33313927\n",
            "Iteration 166, loss = 0.33327606\n",
            "Iteration 167, loss = 0.33098496\n",
            "Iteration 168, loss = 0.33670068\n",
            "Iteration 169, loss = 0.33129243\n",
            "Iteration 170, loss = 0.34387031\n",
            "Iteration 171, loss = 0.34986783\n",
            "Iteration 172, loss = 0.33098765\n",
            "Iteration 173, loss = 0.35868832\n",
            "Iteration 174, loss = 0.33970560\n",
            "Iteration 175, loss = 0.34198772\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67266308\n",
            "Iteration 2, loss = 0.62639570\n",
            "Iteration 3, loss = 0.62313023\n",
            "Iteration 4, loss = 0.61534219\n",
            "Iteration 5, loss = 0.61144545\n",
            "Iteration 6, loss = 0.61013507\n",
            "Iteration 7, loss = 0.60622874\n",
            "Iteration 8, loss = 0.60183760\n",
            "Iteration 9, loss = 0.60083349\n",
            "Iteration 10, loss = 0.59717504\n",
            "Iteration 11, loss = 0.59478869\n",
            "Iteration 12, loss = 0.59372533\n",
            "Iteration 13, loss = 0.58874813\n",
            "Iteration 14, loss = 0.58541674\n",
            "Iteration 15, loss = 0.58028831\n",
            "Iteration 16, loss = 0.57584325\n",
            "Iteration 17, loss = 0.56996046\n",
            "Iteration 18, loss = 0.56906864\n",
            "Iteration 19, loss = 0.56277122\n",
            "Iteration 20, loss = 0.56204145\n",
            "Iteration 21, loss = 0.55587339\n",
            "Iteration 22, loss = 0.54473711\n",
            "Iteration 23, loss = 0.54077567\n",
            "Iteration 24, loss = 0.53115054\n",
            "Iteration 25, loss = 0.52873376\n",
            "Iteration 26, loss = 0.52195701\n",
            "Iteration 27, loss = 0.51498835\n",
            "Iteration 28, loss = 0.50729499\n",
            "Iteration 29, loss = 0.49962655\n",
            "Iteration 30, loss = 0.49632186\n",
            "Iteration 31, loss = 0.48651511\n",
            "Iteration 32, loss = 0.48167413\n",
            "Iteration 33, loss = 0.48011365\n",
            "Iteration 34, loss = 0.47613787\n",
            "Iteration 35, loss = 0.46982021\n",
            "Iteration 36, loss = 0.46590576\n",
            "Iteration 37, loss = 0.46369759\n",
            "Iteration 38, loss = 0.46458796\n",
            "Iteration 39, loss = 0.46200706\n",
            "Iteration 40, loss = 0.45206323\n",
            "Iteration 41, loss = 0.45195879\n",
            "Iteration 42, loss = 0.45168797\n",
            "Iteration 43, loss = 0.43786158\n",
            "Iteration 44, loss = 0.44081991\n",
            "Iteration 45, loss = 0.44635277\n",
            "Iteration 46, loss = 0.44001112\n",
            "Iteration 47, loss = 0.44315007\n",
            "Iteration 48, loss = 0.43540730\n",
            "Iteration 49, loss = 0.43811118\n",
            "Iteration 50, loss = 0.42085136\n",
            "Iteration 51, loss = 0.42321555\n",
            "Iteration 52, loss = 0.43236721\n",
            "Iteration 53, loss = 0.43031203\n",
            "Iteration 54, loss = 0.42371483\n",
            "Iteration 55, loss = 0.42670064\n",
            "Iteration 56, loss = 0.42713074\n",
            "Iteration 57, loss = 0.42072128\n",
            "Iteration 58, loss = 0.41410809\n",
            "Iteration 59, loss = 0.41154949\n",
            "Iteration 60, loss = 0.40805703\n",
            "Iteration 61, loss = 0.40893654\n",
            "Iteration 62, loss = 0.41150296\n",
            "Iteration 63, loss = 0.40876382\n",
            "Iteration 64, loss = 0.40904142\n",
            "Iteration 65, loss = 0.40573018\n",
            "Iteration 66, loss = 0.40591313\n",
            "Iteration 67, loss = 0.39699589\n",
            "Iteration 68, loss = 0.39832931\n",
            "Iteration 69, loss = 0.39413462\n",
            "Iteration 70, loss = 0.40187213\n",
            "Iteration 71, loss = 0.41183812\n",
            "Iteration 72, loss = 0.40352931\n",
            "Iteration 73, loss = 0.39456787\n",
            "Iteration 74, loss = 0.39210941\n",
            "Iteration 75, loss = 0.38964093\n",
            "Iteration 76, loss = 0.38782874\n",
            "Iteration 77, loss = 0.38548973\n",
            "Iteration 78, loss = 0.38587851\n",
            "Iteration 79, loss = 0.38752185\n",
            "Iteration 80, loss = 0.38795384\n",
            "Iteration 81, loss = 0.38081074\n",
            "Iteration 82, loss = 0.38649162\n",
            "Iteration 83, loss = 0.39566778\n",
            "Iteration 84, loss = 0.38086379\n",
            "Iteration 85, loss = 0.38880256\n",
            "Iteration 86, loss = 0.38329450\n",
            "Iteration 87, loss = 0.38560563\n",
            "Iteration 88, loss = 0.40549515\n",
            "Iteration 89, loss = 0.39208953\n",
            "Iteration 90, loss = 0.38072812\n",
            "Iteration 91, loss = 0.37964078\n",
            "Iteration 92, loss = 0.37930583\n",
            "Iteration 93, loss = 0.38620299\n",
            "Iteration 94, loss = 0.38029133\n",
            "Iteration 95, loss = 0.37667592\n",
            "Iteration 96, loss = 0.36986034\n",
            "Iteration 97, loss = 0.36935974\n",
            "Iteration 98, loss = 0.37793346\n",
            "Iteration 99, loss = 0.36637262\n",
            "Iteration 100, loss = 0.36717120\n",
            "Iteration 101, loss = 0.36858031\n",
            "Iteration 102, loss = 0.36762674\n",
            "Iteration 103, loss = 0.36757931\n",
            "Iteration 104, loss = 0.36906486\n",
            "Iteration 105, loss = 0.36171280\n",
            "Iteration 106, loss = 0.36592817\n",
            "Iteration 107, loss = 0.36434004\n",
            "Iteration 108, loss = 0.36758741\n",
            "Iteration 109, loss = 0.36577884\n",
            "Iteration 110, loss = 0.35979562\n",
            "Iteration 111, loss = 0.35973673\n",
            "Iteration 112, loss = 0.36312768\n",
            "Iteration 113, loss = 0.37303971\n",
            "Iteration 114, loss = 0.35352993\n",
            "Iteration 115, loss = 0.37321885\n",
            "Iteration 116, loss = 0.39008634\n",
            "Iteration 117, loss = 0.37163234\n",
            "Iteration 118, loss = 0.37021503\n",
            "Iteration 119, loss = 0.36146803\n",
            "Iteration 120, loss = 0.35835547\n",
            "Iteration 121, loss = 0.35394311\n",
            "Iteration 122, loss = 0.35638757\n",
            "Iteration 123, loss = 0.36264828\n",
            "Iteration 124, loss = 0.35186805\n",
            "Iteration 125, loss = 0.35423965\n",
            "Iteration 126, loss = 0.35118670\n",
            "Iteration 127, loss = 0.35014712\n",
            "Iteration 128, loss = 0.35347624\n",
            "Iteration 129, loss = 0.35092811\n",
            "Iteration 130, loss = 0.35622552\n",
            "Iteration 131, loss = 0.35602759\n",
            "Iteration 132, loss = 0.36893868\n",
            "Iteration 133, loss = 0.35568328\n",
            "Iteration 134, loss = 0.35963410\n",
            "Iteration 135, loss = 0.35678717\n",
            "Iteration 136, loss = 0.35752369\n",
            "Iteration 137, loss = 0.35075319\n",
            "Iteration 138, loss = 0.36339098\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.73975031\n",
            "Iteration 2, loss = 0.64038269\n",
            "Iteration 3, loss = 0.62802477\n",
            "Iteration 4, loss = 0.62403402\n",
            "Iteration 5, loss = 0.61937567\n",
            "Iteration 6, loss = 0.61697977\n",
            "Iteration 7, loss = 0.61277859\n",
            "Iteration 8, loss = 0.61097038\n",
            "Iteration 9, loss = 0.60806388\n",
            "Iteration 10, loss = 0.60479120\n",
            "Iteration 11, loss = 0.60202678\n",
            "Iteration 12, loss = 0.59639731\n",
            "Iteration 13, loss = 0.59272334\n",
            "Iteration 14, loss = 0.58732261\n",
            "Iteration 15, loss = 0.58604269\n",
            "Iteration 16, loss = 0.57936739\n",
            "Iteration 17, loss = 0.57295940\n",
            "Iteration 18, loss = 0.56277945\n",
            "Iteration 19, loss = 0.55399783\n",
            "Iteration 20, loss = 0.55240996\n",
            "Iteration 21, loss = 0.54242057\n",
            "Iteration 22, loss = 0.53334281\n",
            "Iteration 23, loss = 0.52904747\n",
            "Iteration 24, loss = 0.53063987\n",
            "Iteration 25, loss = 0.51933218\n",
            "Iteration 26, loss = 0.51471934\n",
            "Iteration 27, loss = 0.50544339\n",
            "Iteration 28, loss = 0.50259740\n",
            "Iteration 29, loss = 0.49354518\n",
            "Iteration 30, loss = 0.49076458\n",
            "Iteration 31, loss = 0.47875920\n",
            "Iteration 32, loss = 0.47612000\n",
            "Iteration 33, loss = 0.47268047\n",
            "Iteration 34, loss = 0.47135228\n",
            "Iteration 35, loss = 0.46885577\n",
            "Iteration 36, loss = 0.46133244\n",
            "Iteration 37, loss = 0.46474381\n",
            "Iteration 38, loss = 0.45632914\n",
            "Iteration 39, loss = 0.45220634\n",
            "Iteration 40, loss = 0.45390616\n",
            "Iteration 41, loss = 0.44848293\n",
            "Iteration 42, loss = 0.44813264\n",
            "Iteration 43, loss = 0.44437222\n",
            "Iteration 44, loss = 0.44549516\n",
            "Iteration 45, loss = 0.44932280\n",
            "Iteration 46, loss = 0.44227464\n",
            "Iteration 47, loss = 0.43538081\n",
            "Iteration 48, loss = 0.43446336\n",
            "Iteration 49, loss = 0.43104244\n",
            "Iteration 50, loss = 0.43924877\n",
            "Iteration 51, loss = 0.42965101\n",
            "Iteration 52, loss = 0.42984111\n",
            "Iteration 53, loss = 0.42776391\n",
            "Iteration 54, loss = 0.43233574\n",
            "Iteration 55, loss = 0.42028866\n",
            "Iteration 56, loss = 0.43168933\n",
            "Iteration 57, loss = 0.43660015\n",
            "Iteration 58, loss = 0.42203751\n",
            "Iteration 59, loss = 0.41975843\n",
            "Iteration 60, loss = 0.42013970\n",
            "Iteration 61, loss = 0.41813509\n",
            "Iteration 62, loss = 0.41215707\n",
            "Iteration 63, loss = 0.41012179\n",
            "Iteration 64, loss = 0.41225194\n",
            "Iteration 65, loss = 0.40736313\n",
            "Iteration 66, loss = 0.40943962\n",
            "Iteration 67, loss = 0.40982483\n",
            "Iteration 68, loss = 0.40234071\n",
            "Iteration 69, loss = 0.41374911\n",
            "Iteration 70, loss = 0.40275250\n",
            "Iteration 71, loss = 0.40541921\n",
            "Iteration 72, loss = 0.40354595\n",
            "Iteration 73, loss = 0.39720732\n",
            "Iteration 74, loss = 0.40356384\n",
            "Iteration 75, loss = 0.40116074\n",
            "Iteration 76, loss = 0.39671394\n",
            "Iteration 77, loss = 0.39425786\n",
            "Iteration 78, loss = 0.39226598\n",
            "Iteration 79, loss = 0.39656062\n",
            "Iteration 80, loss = 0.39412199\n",
            "Iteration 81, loss = 0.39625284\n",
            "Iteration 82, loss = 0.39535402\n",
            "Iteration 83, loss = 0.38893250\n",
            "Iteration 84, loss = 0.39056742\n",
            "Iteration 85, loss = 0.39682146\n",
            "Iteration 86, loss = 0.38412919\n",
            "Iteration 87, loss = 0.40336977\n",
            "Iteration 88, loss = 0.39555514\n",
            "Iteration 89, loss = 0.39598737\n",
            "Iteration 90, loss = 0.38622344\n",
            "Iteration 91, loss = 0.38599546\n",
            "Iteration 92, loss = 0.38255821\n",
            "Iteration 93, loss = 0.38032402\n",
            "Iteration 94, loss = 0.38422716\n",
            "Iteration 95, loss = 0.39234724\n",
            "Iteration 96, loss = 0.38487289\n",
            "Iteration 97, loss = 0.39393089\n",
            "Iteration 98, loss = 0.38259557\n",
            "Iteration 99, loss = 0.37861980\n",
            "Iteration 100, loss = 0.37773201\n",
            "Iteration 101, loss = 0.37757282\n",
            "Iteration 102, loss = 0.37481015\n",
            "Iteration 103, loss = 0.39619274\n",
            "Iteration 104, loss = 0.37835257\n",
            "Iteration 105, loss = 0.37481963\n",
            "Iteration 106, loss = 0.38253366\n",
            "Iteration 107, loss = 0.37892857\n",
            "Iteration 108, loss = 0.37171450\n",
            "Iteration 109, loss = 0.38174712\n",
            "Iteration 110, loss = 0.37589515\n",
            "Iteration 111, loss = 0.36982955\n",
            "Iteration 112, loss = 0.36765416\n",
            "Iteration 113, loss = 0.36822545\n",
            "Iteration 114, loss = 0.36792174\n",
            "Iteration 115, loss = 0.36706569\n",
            "Iteration 116, loss = 0.36904027\n",
            "Iteration 117, loss = 0.37347202\n",
            "Iteration 118, loss = 0.36533992\n",
            "Iteration 119, loss = 0.37039169\n",
            "Iteration 120, loss = 0.36320570\n",
            "Iteration 121, loss = 0.37791879\n",
            "Iteration 122, loss = 0.37043819\n",
            "Iteration 123, loss = 0.36772401\n",
            "Iteration 124, loss = 0.37559313\n",
            "Iteration 125, loss = 0.36974895\n",
            "Iteration 126, loss = 0.37246575\n",
            "Iteration 127, loss = 0.36159778\n",
            "Iteration 128, loss = 0.36399745\n",
            "Iteration 129, loss = 0.36538067\n",
            "Iteration 130, loss = 0.37013864\n",
            "Iteration 131, loss = 0.37120356\n",
            "Iteration 132, loss = 0.36036517\n",
            "Iteration 133, loss = 0.36142048\n",
            "Iteration 134, loss = 0.36090884\n",
            "Iteration 135, loss = 0.35945655\n",
            "Iteration 136, loss = 0.35725533\n",
            "Iteration 137, loss = 0.36097720\n",
            "Iteration 138, loss = 0.36330396\n",
            "Iteration 139, loss = 0.35859011\n",
            "Iteration 140, loss = 0.37249857\n",
            "Iteration 141, loss = 0.36167500\n",
            "Iteration 142, loss = 0.36349959\n",
            "Iteration 143, loss = 0.35509926\n",
            "Iteration 144, loss = 0.35452060\n",
            "Iteration 145, loss = 0.36273264\n",
            "Iteration 146, loss = 0.35595671\n",
            "Iteration 147, loss = 0.35223452\n",
            "Iteration 148, loss = 0.35154677\n",
            "Iteration 149, loss = 0.35717739\n",
            "Iteration 150, loss = 0.34841192\n",
            "Iteration 151, loss = 0.35126976\n",
            "Iteration 152, loss = 0.34851462\n",
            "Iteration 153, loss = 0.35713572\n",
            "Iteration 154, loss = 0.37437484\n",
            "Iteration 155, loss = 0.35652937\n",
            "Iteration 156, loss = 0.35257812\n",
            "Iteration 157, loss = 0.35795507\n",
            "Iteration 158, loss = 0.35589746\n",
            "Iteration 159, loss = 0.34634427\n",
            "Iteration 160, loss = 0.35650207\n",
            "Iteration 161, loss = 0.36327215\n",
            "Iteration 162, loss = 0.35239947\n",
            "Iteration 163, loss = 0.35324806\n",
            "Iteration 164, loss = 0.35654887\n",
            "Iteration 165, loss = 0.34502142\n",
            "Iteration 166, loss = 0.34528719\n",
            "Iteration 167, loss = 0.34540134\n",
            "Iteration 168, loss = 0.34345501\n",
            "Iteration 169, loss = 0.34592265\n",
            "Iteration 170, loss = 0.34828811\n",
            "Iteration 171, loss = 0.35406731\n",
            "Iteration 172, loss = 0.34589970\n",
            "Iteration 173, loss = 0.34136482\n",
            "Iteration 174, loss = 0.34534242\n",
            "Iteration 175, loss = 0.33900895\n",
            "Iteration 176, loss = 0.34041514\n",
            "Iteration 177, loss = 0.34301700\n",
            "Iteration 178, loss = 0.35446887\n",
            "Iteration 179, loss = 0.34097230\n",
            "Iteration 180, loss = 0.34023026\n",
            "Iteration 181, loss = 0.34592109\n",
            "Iteration 182, loss = 0.34048790\n",
            "Iteration 183, loss = 0.34219825\n",
            "Iteration 184, loss = 0.34046184\n",
            "Iteration 185, loss = 0.34770665\n",
            "Iteration 186, loss = 0.34635430\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69362601\n",
            "Iteration 2, loss = 0.67431189\n",
            "Iteration 3, loss = 0.66715000\n",
            "Iteration 4, loss = 0.66345755\n",
            "Iteration 5, loss = 0.65402900\n",
            "Iteration 6, loss = 0.65948741\n",
            "Iteration 7, loss = 0.65027787\n",
            "Iteration 8, loss = 0.64739245\n",
            "Iteration 9, loss = 0.63906541\n",
            "Iteration 10, loss = 0.63436303\n",
            "Iteration 11, loss = 0.63015702\n",
            "Iteration 12, loss = 0.62943886\n",
            "Iteration 13, loss = 0.62268910\n",
            "Iteration 14, loss = 0.62202261\n",
            "Iteration 15, loss = 0.61579275\n",
            "Iteration 16, loss = 0.61520387\n",
            "Iteration 17, loss = 0.61043038\n",
            "Iteration 18, loss = 0.61158596\n",
            "Iteration 19, loss = 0.61044535\n",
            "Iteration 20, loss = 0.60799721\n",
            "Iteration 21, loss = 0.60687024\n",
            "Iteration 22, loss = 0.60584530\n",
            "Iteration 23, loss = 0.60838011\n",
            "Iteration 24, loss = 0.60469108\n",
            "Iteration 25, loss = 0.60374379\n",
            "Iteration 26, loss = 0.60550332\n",
            "Iteration 27, loss = 0.60599628\n",
            "Iteration 28, loss = 0.60292778\n",
            "Iteration 29, loss = 0.60320336\n",
            "Iteration 30, loss = 0.60502978\n",
            "Iteration 31, loss = 0.60290202\n",
            "Iteration 32, loss = 0.60391703\n",
            "Iteration 33, loss = 0.60372238\n",
            "Iteration 34, loss = 0.60242961\n",
            "Iteration 35, loss = 0.60084229\n",
            "Iteration 36, loss = 0.60358271\n",
            "Iteration 37, loss = 0.60345695\n",
            "Iteration 38, loss = 0.60024933\n",
            "Iteration 39, loss = 0.60208799\n",
            "Iteration 40, loss = 0.60121113\n",
            "Iteration 41, loss = 0.60084057\n",
            "Iteration 42, loss = 0.60351282\n",
            "Iteration 43, loss = 0.60383951\n",
            "Iteration 44, loss = 0.60238035\n",
            "Iteration 45, loss = 0.60042443\n",
            "Iteration 46, loss = 0.60017585\n",
            "Iteration 47, loss = 0.59848017\n",
            "Iteration 48, loss = 0.60396557\n",
            "Iteration 49, loss = 0.59934847\n",
            "Iteration 50, loss = 0.60120526\n",
            "Iteration 51, loss = 0.59795733\n",
            "Iteration 52, loss = 0.59898280\n",
            "Iteration 53, loss = 0.60191863\n",
            "Iteration 54, loss = 0.60156993\n",
            "Iteration 55, loss = 0.59831707\n",
            "Iteration 56, loss = 0.59888284\n",
            "Iteration 57, loss = 0.60241596\n",
            "Iteration 58, loss = 0.59968163\n",
            "Iteration 59, loss = 0.59743206\n",
            "Iteration 60, loss = 0.59702172\n",
            "Iteration 61, loss = 0.59689144\n",
            "Iteration 62, loss = 0.59736030\n",
            "Iteration 63, loss = 0.59710622\n",
            "Iteration 64, loss = 0.59855689\n",
            "Iteration 65, loss = 0.59809711\n",
            "Iteration 66, loss = 0.59714902\n",
            "Iteration 67, loss = 0.59946528\n",
            "Iteration 68, loss = 0.59726017\n",
            "Iteration 69, loss = 0.59963741\n",
            "Iteration 70, loss = 0.60145555\n",
            "Iteration 71, loss = 0.59687678\n",
            "Iteration 72, loss = 0.59648310\n",
            "Iteration 73, loss = 0.59559975\n",
            "Iteration 74, loss = 0.59579217\n",
            "Iteration 75, loss = 0.59525761\n",
            "Iteration 76, loss = 0.59409777\n",
            "Iteration 77, loss = 0.59967833\n",
            "Iteration 78, loss = 0.59457312\n",
            "Iteration 79, loss = 0.59503603\n",
            "Iteration 80, loss = 0.59816223\n",
            "Iteration 81, loss = 0.59457622\n",
            "Iteration 82, loss = 0.59508678\n",
            "Iteration 83, loss = 0.59834195\n",
            "Iteration 84, loss = 0.59599962\n",
            "Iteration 85, loss = 0.59763738\n",
            "Iteration 86, loss = 0.59739905\n",
            "Iteration 87, loss = 0.59901721\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68198100\n",
            "Iteration 2, loss = 0.66822662\n",
            "Iteration 3, loss = 0.66007657\n",
            "Iteration 4, loss = 0.65393866\n",
            "Iteration 5, loss = 0.64986381\n",
            "Iteration 6, loss = 0.64686826\n",
            "Iteration 7, loss = 0.63731349\n",
            "Iteration 8, loss = 0.63512376\n",
            "Iteration 9, loss = 0.63125288\n",
            "Iteration 10, loss = 0.62788173\n",
            "Iteration 11, loss = 0.62650521\n",
            "Iteration 12, loss = 0.62381271\n",
            "Iteration 13, loss = 0.62002415\n",
            "Iteration 14, loss = 0.61887442\n",
            "Iteration 15, loss = 0.61269613\n",
            "Iteration 16, loss = 0.61443349\n",
            "Iteration 17, loss = 0.61247494\n",
            "Iteration 18, loss = 0.61128437\n",
            "Iteration 19, loss = 0.60868878\n",
            "Iteration 20, loss = 0.60891134\n",
            "Iteration 21, loss = 0.60824239\n",
            "Iteration 22, loss = 0.60707425\n",
            "Iteration 23, loss = 0.60815103\n",
            "Iteration 24, loss = 0.60851347\n",
            "Iteration 25, loss = 0.60938763\n",
            "Iteration 26, loss = 0.61172544\n",
            "Iteration 27, loss = 0.61001387\n",
            "Iteration 28, loss = 0.60554467\n",
            "Iteration 29, loss = 0.60732828\n",
            "Iteration 30, loss = 0.60935067\n",
            "Iteration 31, loss = 0.60967271\n",
            "Iteration 32, loss = 0.60536346\n",
            "Iteration 33, loss = 0.60608024\n",
            "Iteration 34, loss = 0.60779729\n",
            "Iteration 35, loss = 0.60900996\n",
            "Iteration 36, loss = 0.60404578\n",
            "Iteration 37, loss = 0.60419493\n",
            "Iteration 38, loss = 0.60424560\n",
            "Iteration 39, loss = 0.60272829\n",
            "Iteration 40, loss = 0.60712945\n",
            "Iteration 41, loss = 0.60221361\n",
            "Iteration 42, loss = 0.60344596\n",
            "Iteration 43, loss = 0.60263441\n",
            "Iteration 44, loss = 0.60415971\n",
            "Iteration 45, loss = 0.60457479\n",
            "Iteration 46, loss = 0.60137135\n",
            "Iteration 47, loss = 0.60482257\n",
            "Iteration 48, loss = 0.60522022\n",
            "Iteration 49, loss = 0.60501790\n",
            "Iteration 50, loss = 0.60495022\n",
            "Iteration 51, loss = 0.60369389\n",
            "Iteration 52, loss = 0.60231095\n",
            "Iteration 53, loss = 0.60618950\n",
            "Iteration 54, loss = 0.60381172\n",
            "Iteration 55, loss = 0.60173003\n",
            "Iteration 56, loss = 0.60136552\n",
            "Iteration 57, loss = 0.60169297\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68070568\n",
            "Iteration 2, loss = 0.67158147\n",
            "Iteration 3, loss = 0.66308103\n",
            "Iteration 4, loss = 0.66046590\n",
            "Iteration 5, loss = 0.65505261\n",
            "Iteration 6, loss = 0.64955234\n",
            "Iteration 7, loss = 0.64570302\n",
            "Iteration 8, loss = 0.64074646\n",
            "Iteration 9, loss = 0.63364721\n",
            "Iteration 10, loss = 0.63369065\n",
            "Iteration 11, loss = 0.62920162\n",
            "Iteration 12, loss = 0.62115935\n",
            "Iteration 13, loss = 0.62442171\n",
            "Iteration 14, loss = 0.62015934\n",
            "Iteration 15, loss = 0.61744939\n",
            "Iteration 16, loss = 0.61790706\n",
            "Iteration 17, loss = 0.61499342\n",
            "Iteration 18, loss = 0.61311809\n",
            "Iteration 19, loss = 0.61502595\n",
            "Iteration 20, loss = 0.61189455\n",
            "Iteration 21, loss = 0.61132535\n",
            "Iteration 22, loss = 0.60984743\n",
            "Iteration 23, loss = 0.61198173\n",
            "Iteration 24, loss = 0.60904647\n",
            "Iteration 25, loss = 0.60883510\n",
            "Iteration 26, loss = 0.60786840\n",
            "Iteration 27, loss = 0.60770564\n",
            "Iteration 28, loss = 0.60776205\n",
            "Iteration 29, loss = 0.60694579\n",
            "Iteration 30, loss = 0.60999054\n",
            "Iteration 31, loss = 0.60669595\n",
            "Iteration 32, loss = 0.60630087\n",
            "Iteration 33, loss = 0.60613972\n",
            "Iteration 34, loss = 0.60766381\n",
            "Iteration 35, loss = 0.60610750\n",
            "Iteration 36, loss = 0.60648835\n",
            "Iteration 37, loss = 0.60771859\n",
            "Iteration 38, loss = 0.60747883\n",
            "Iteration 39, loss = 0.60650794\n",
            "Iteration 40, loss = 0.60615613\n",
            "Iteration 41, loss = 0.60830891\n",
            "Iteration 42, loss = 0.60826279\n",
            "Iteration 43, loss = 0.60618950\n",
            "Iteration 44, loss = 0.60496844\n",
            "Iteration 45, loss = 0.60396695\n",
            "Iteration 46, loss = 0.60364922\n",
            "Iteration 47, loss = 0.60306296\n",
            "Iteration 48, loss = 0.60826094\n",
            "Iteration 49, loss = 0.60464862\n",
            "Iteration 50, loss = 0.60416718\n",
            "Iteration 51, loss = 0.60467173\n",
            "Iteration 52, loss = 0.60374554\n",
            "Iteration 53, loss = 0.60673668\n",
            "Iteration 54, loss = 0.60242325\n",
            "Iteration 55, loss = 0.60351963\n",
            "Iteration 56, loss = 0.59997306\n",
            "Iteration 57, loss = 0.60698677\n",
            "Iteration 58, loss = 0.60252118\n",
            "Iteration 59, loss = 0.60427823\n",
            "Iteration 60, loss = 0.60174558\n",
            "Iteration 61, loss = 0.60209335\n",
            "Iteration 62, loss = 0.60469409\n",
            "Iteration 63, loss = 0.60146601\n",
            "Iteration 64, loss = 0.60198535\n",
            "Iteration 65, loss = 0.59865206\n",
            "Iteration 66, loss = 0.60362596\n",
            "Iteration 67, loss = 0.59878797\n",
            "Iteration 68, loss = 0.60224997\n",
            "Iteration 69, loss = 0.60225581\n",
            "Iteration 70, loss = 0.59935808\n",
            "Iteration 71, loss = 0.60272310\n",
            "Iteration 72, loss = 0.59940567\n",
            "Iteration 73, loss = 0.60123578\n",
            "Iteration 74, loss = 0.60083335\n",
            "Iteration 75, loss = 0.60308672\n",
            "Iteration 76, loss = 0.59915495\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67459447\n",
            "Iteration 2, loss = 0.66829609\n",
            "Iteration 3, loss = 0.66674134\n",
            "Iteration 4, loss = 0.65899176\n",
            "Iteration 5, loss = 0.66027194\n",
            "Iteration 6, loss = 0.65472917\n",
            "Iteration 7, loss = 0.65060009\n",
            "Iteration 8, loss = 0.64894938\n",
            "Iteration 9, loss = 0.64670764\n",
            "Iteration 10, loss = 0.64443369\n",
            "Iteration 11, loss = 0.64084915\n",
            "Iteration 12, loss = 0.63884682\n",
            "Iteration 13, loss = 0.63536942\n",
            "Iteration 14, loss = 0.62905599\n",
            "Iteration 15, loss = 0.63376336\n",
            "Iteration 16, loss = 0.63103387\n",
            "Iteration 17, loss = 0.63153148\n",
            "Iteration 18, loss = 0.63253438\n",
            "Iteration 19, loss = 0.62562073\n",
            "Iteration 20, loss = 0.62825655\n",
            "Iteration 21, loss = 0.62862424\n",
            "Iteration 22, loss = 0.62520418\n",
            "Iteration 23, loss = 0.62552365\n",
            "Iteration 24, loss = 0.62668096\n",
            "Iteration 25, loss = 0.62307464\n",
            "Iteration 26, loss = 0.62291253\n",
            "Iteration 27, loss = 0.62227200\n",
            "Iteration 28, loss = 0.62234112\n",
            "Iteration 29, loss = 0.62608692\n",
            "Iteration 30, loss = 0.62615473\n",
            "Iteration 31, loss = 0.62173975\n",
            "Iteration 32, loss = 0.62259567\n",
            "Iteration 33, loss = 0.62205213\n",
            "Iteration 34, loss = 0.62062826\n",
            "Iteration 35, loss = 0.62155817\n",
            "Iteration 36, loss = 0.62033953\n",
            "Iteration 37, loss = 0.62215924\n",
            "Iteration 38, loss = 0.62201614\n",
            "Iteration 39, loss = 0.61988781\n",
            "Iteration 40, loss = 0.61975413\n",
            "Iteration 41, loss = 0.62572979\n",
            "Iteration 42, loss = 0.62119722\n",
            "Iteration 43, loss = 0.61812284\n",
            "Iteration 44, loss = 0.61942853\n",
            "Iteration 45, loss = 0.62162619\n",
            "Iteration 46, loss = 0.61977586\n",
            "Iteration 47, loss = 0.61776309\n",
            "Iteration 48, loss = 0.61765661\n",
            "Iteration 49, loss = 0.62331666\n",
            "Iteration 50, loss = 0.61936897\n",
            "Iteration 51, loss = 0.61886303\n",
            "Iteration 52, loss = 0.62025891\n",
            "Iteration 53, loss = 0.61880137\n",
            "Iteration 54, loss = 0.62067484\n",
            "Iteration 55, loss = 0.61939377\n",
            "Iteration 56, loss = 0.61978659\n",
            "Iteration 57, loss = 0.61912992\n",
            "Iteration 58, loss = 0.61914216\n",
            "Iteration 59, loss = 0.61514981\n",
            "Iteration 60, loss = 0.62420839\n",
            "Iteration 61, loss = 0.61784656\n",
            "Iteration 62, loss = 0.62151618\n",
            "Iteration 63, loss = 0.61806352\n",
            "Iteration 64, loss = 0.61618139\n",
            "Iteration 65, loss = 0.61415578\n",
            "Iteration 66, loss = 0.62179410\n",
            "Iteration 67, loss = 0.62177180\n",
            "Iteration 68, loss = 0.62216370\n",
            "Iteration 69, loss = 0.61815190\n",
            "Iteration 70, loss = 0.61694008\n",
            "Iteration 71, loss = 0.61576263\n",
            "Iteration 72, loss = 0.62100076\n",
            "Iteration 73, loss = 0.61642279\n",
            "Iteration 74, loss = 0.61663556\n",
            "Iteration 75, loss = 0.61570382\n",
            "Iteration 76, loss = 0.61864786\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66857874\n",
            "Iteration 2, loss = 0.66335305\n",
            "Iteration 3, loss = 0.65341396\n",
            "Iteration 4, loss = 0.65701978\n",
            "Iteration 5, loss = 0.65033043\n",
            "Iteration 6, loss = 0.64561092\n",
            "Iteration 7, loss = 0.64130480\n",
            "Iteration 8, loss = 0.63999062\n",
            "Iteration 9, loss = 0.63617382\n",
            "Iteration 10, loss = 0.63320624\n",
            "Iteration 11, loss = 0.63033739\n",
            "Iteration 12, loss = 0.62979013\n",
            "Iteration 13, loss = 0.62897150\n",
            "Iteration 14, loss = 0.62515821\n",
            "Iteration 15, loss = 0.62546709\n",
            "Iteration 16, loss = 0.62450853\n",
            "Iteration 17, loss = 0.62418754\n",
            "Iteration 18, loss = 0.62129556\n",
            "Iteration 19, loss = 0.62275039\n",
            "Iteration 20, loss = 0.62137975\n",
            "Iteration 21, loss = 0.62173930\n",
            "Iteration 22, loss = 0.62201950\n",
            "Iteration 23, loss = 0.62319960\n",
            "Iteration 24, loss = 0.62054795\n",
            "Iteration 25, loss = 0.61810849\n",
            "Iteration 26, loss = 0.62344141\n",
            "Iteration 27, loss = 0.61973021\n",
            "Iteration 28, loss = 0.61856898\n",
            "Iteration 29, loss = 0.61956260\n",
            "Iteration 30, loss = 0.61859589\n",
            "Iteration 31, loss = 0.62217180\n",
            "Iteration 32, loss = 0.61997096\n",
            "Iteration 33, loss = 0.61992482\n",
            "Iteration 34, loss = 0.61824008\n",
            "Iteration 35, loss = 0.61667936\n",
            "Iteration 36, loss = 0.61778160\n",
            "Iteration 37, loss = 0.61939027\n",
            "Iteration 38, loss = 0.61877851\n",
            "Iteration 39, loss = 0.61975712\n",
            "Iteration 40, loss = 0.61816363\n",
            "Iteration 41, loss = 0.61666140\n",
            "Iteration 42, loss = 0.62254285\n",
            "Iteration 43, loss = 0.62139725\n",
            "Iteration 44, loss = 0.61935204\n",
            "Iteration 45, loss = 0.61810385\n",
            "Iteration 46, loss = 0.61663993\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70110869\n",
            "Iteration 2, loss = 0.68293148\n",
            "Iteration 3, loss = 0.67210919\n",
            "Iteration 4, loss = 0.66522220\n",
            "Iteration 5, loss = 0.66095772\n",
            "Iteration 6, loss = 0.65612263\n",
            "Iteration 7, loss = 0.65091340\n",
            "Iteration 8, loss = 0.64510332\n",
            "Iteration 9, loss = 0.63930088\n",
            "Iteration 10, loss = 0.63258222\n",
            "Iteration 11, loss = 0.62714069\n",
            "Iteration 12, loss = 0.62070018\n",
            "Iteration 13, loss = 0.61539730\n",
            "Iteration 14, loss = 0.61094126\n",
            "Iteration 15, loss = 0.60593494\n",
            "Iteration 16, loss = 0.60299273\n",
            "Iteration 17, loss = 0.59985390\n",
            "Iteration 18, loss = 0.59786117\n",
            "Iteration 19, loss = 0.59476476\n",
            "Iteration 20, loss = 0.59270302\n",
            "Iteration 21, loss = 0.59003869\n",
            "Iteration 22, loss = 0.58838686\n",
            "Iteration 23, loss = 0.58601097\n",
            "Iteration 24, loss = 0.58417875\n",
            "Iteration 25, loss = 0.58292827\n",
            "Iteration 26, loss = 0.58012502\n",
            "Iteration 27, loss = 0.57927330\n",
            "Iteration 28, loss = 0.57680789\n",
            "Iteration 29, loss = 0.57455204\n",
            "Iteration 30, loss = 0.57253251\n",
            "Iteration 31, loss = 0.57096317\n",
            "Iteration 32, loss = 0.56862130\n",
            "Iteration 33, loss = 0.56688413\n",
            "Iteration 34, loss = 0.56487146\n",
            "Iteration 35, loss = 0.56238160\n",
            "Iteration 36, loss = 0.56057481\n",
            "Iteration 37, loss = 0.55862062\n",
            "Iteration 38, loss = 0.55699935\n",
            "Iteration 39, loss = 0.55466975\n",
            "Iteration 40, loss = 0.55261315\n",
            "Iteration 41, loss = 0.55091298\n",
            "Iteration 42, loss = 0.54836119\n",
            "Iteration 43, loss = 0.54595588\n",
            "Iteration 44, loss = 0.54537110\n",
            "Iteration 45, loss = 0.54213940\n",
            "Iteration 46, loss = 0.53996023\n",
            "Iteration 47, loss = 0.53740592\n",
            "Iteration 48, loss = 0.53499040\n",
            "Iteration 49, loss = 0.53291245\n",
            "Iteration 50, loss = 0.53118551\n",
            "Iteration 51, loss = 0.52873625\n",
            "Iteration 52, loss = 0.52570150\n",
            "Iteration 53, loss = 0.52403644\n",
            "Iteration 54, loss = 0.52134602\n",
            "Iteration 55, loss = 0.51961847\n",
            "Iteration 56, loss = 0.51655500\n",
            "Iteration 57, loss = 0.51465009\n",
            "Iteration 58, loss = 0.51214668\n",
            "Iteration 59, loss = 0.51018111\n",
            "Iteration 60, loss = 0.50701950\n",
            "Iteration 61, loss = 0.50484412\n",
            "Iteration 62, loss = 0.50306894\n",
            "Iteration 63, loss = 0.50066474\n",
            "Iteration 64, loss = 0.49782434\n",
            "Iteration 65, loss = 0.49630936\n",
            "Iteration 66, loss = 0.49362299\n",
            "Iteration 67, loss = 0.49068001\n",
            "Iteration 68, loss = 0.48754796\n",
            "Iteration 69, loss = 0.48684065\n",
            "Iteration 70, loss = 0.48508320\n",
            "Iteration 71, loss = 0.48248587\n",
            "Iteration 72, loss = 0.48046185\n",
            "Iteration 73, loss = 0.48042565\n",
            "Iteration 74, loss = 0.47674096\n",
            "Iteration 75, loss = 0.47699517\n",
            "Iteration 76, loss = 0.47490478\n",
            "Iteration 77, loss = 0.47299772\n",
            "Iteration 78, loss = 0.47126280\n",
            "Iteration 79, loss = 0.46838317\n",
            "Iteration 80, loss = 0.47169607\n",
            "Iteration 81, loss = 0.46950065\n",
            "Iteration 82, loss = 0.46704333\n",
            "Iteration 83, loss = 0.46964451\n",
            "Iteration 84, loss = 0.46819364\n",
            "Iteration 85, loss = 0.46195639\n",
            "Iteration 86, loss = 0.46057753\n",
            "Iteration 87, loss = 0.46023774\n",
            "Iteration 88, loss = 0.45891774\n",
            "Iteration 89, loss = 0.45666102\n",
            "Iteration 90, loss = 0.45658679\n",
            "Iteration 91, loss = 0.45482784\n",
            "Iteration 92, loss = 0.45396115\n",
            "Iteration 93, loss = 0.45330029\n",
            "Iteration 94, loss = 0.45251110\n",
            "Iteration 95, loss = 0.45362559\n",
            "Iteration 96, loss = 0.45001164\n",
            "Iteration 97, loss = 0.45017461\n",
            "Iteration 98, loss = 0.44862502\n",
            "Iteration 99, loss = 0.44825750\n",
            "Iteration 100, loss = 0.44901273\n",
            "Iteration 101, loss = 0.44687448\n",
            "Iteration 102, loss = 0.44714593\n",
            "Iteration 103, loss = 0.44558644\n",
            "Iteration 104, loss = 0.44447227\n",
            "Iteration 105, loss = 0.44358686\n",
            "Iteration 106, loss = 0.44515094\n",
            "Iteration 107, loss = 0.44559590\n",
            "Iteration 108, loss = 0.44399405\n",
            "Iteration 109, loss = 0.44568563\n",
            "Iteration 110, loss = 0.44342959\n",
            "Iteration 111, loss = 0.43967952\n",
            "Iteration 112, loss = 0.44134828\n",
            "Iteration 113, loss = 0.43970036\n",
            "Iteration 114, loss = 0.43932800\n",
            "Iteration 115, loss = 0.43997142\n",
            "Iteration 116, loss = 0.43933560\n",
            "Iteration 117, loss = 0.44222080\n",
            "Iteration 118, loss = 0.43608119\n",
            "Iteration 119, loss = 0.43924104\n",
            "Iteration 120, loss = 0.43821477\n",
            "Iteration 121, loss = 0.43596086\n",
            "Iteration 122, loss = 0.43745596\n",
            "Iteration 123, loss = 0.43539609\n",
            "Iteration 124, loss = 0.43687679\n",
            "Iteration 125, loss = 0.43504716\n",
            "Iteration 126, loss = 0.43678788\n",
            "Iteration 127, loss = 0.43393696\n",
            "Iteration 128, loss = 0.43506220\n",
            "Iteration 129, loss = 0.43233351\n",
            "Iteration 130, loss = 0.43397636\n",
            "Iteration 131, loss = 0.43233850\n",
            "Iteration 132, loss = 0.43561205\n",
            "Iteration 133, loss = 0.43108964\n",
            "Iteration 134, loss = 0.43126903\n",
            "Iteration 135, loss = 0.43153395\n",
            "Iteration 136, loss = 0.43070350\n",
            "Iteration 137, loss = 0.43066741\n",
            "Iteration 138, loss = 0.43095341\n",
            "Iteration 139, loss = 0.43009622\n",
            "Iteration 140, loss = 0.42847142\n",
            "Iteration 141, loss = 0.43051746\n",
            "Iteration 142, loss = 0.42796622\n",
            "Iteration 143, loss = 0.42951114\n",
            "Iteration 144, loss = 0.42949168\n",
            "Iteration 145, loss = 0.42848885\n",
            "Iteration 146, loss = 0.42900350\n",
            "Iteration 147, loss = 0.42690033\n",
            "Iteration 148, loss = 0.42609476\n",
            "Iteration 149, loss = 0.42632645\n",
            "Iteration 150, loss = 0.42585359\n",
            "Iteration 151, loss = 0.42623860\n",
            "Iteration 152, loss = 0.42634221\n",
            "Iteration 153, loss = 0.42540427\n",
            "Iteration 154, loss = 0.42394298\n",
            "Iteration 155, loss = 0.42501197\n",
            "Iteration 156, loss = 0.42648824\n",
            "Iteration 157, loss = 0.42621846\n",
            "Iteration 158, loss = 0.42206057\n",
            "Iteration 159, loss = 0.42479247\n",
            "Iteration 160, loss = 0.42125345\n",
            "Iteration 161, loss = 0.42255580\n",
            "Iteration 162, loss = 0.42452864\n",
            "Iteration 163, loss = 0.42887499\n",
            "Iteration 164, loss = 0.42145498\n",
            "Iteration 165, loss = 0.42394660\n",
            "Iteration 166, loss = 0.42250134\n",
            "Iteration 167, loss = 0.42023329\n",
            "Iteration 168, loss = 0.42262810\n",
            "Iteration 169, loss = 0.42731811\n",
            "Iteration 170, loss = 0.42486241\n",
            "Iteration 171, loss = 0.42116107\n",
            "Iteration 172, loss = 0.42136929\n",
            "Iteration 173, loss = 0.41951010\n",
            "Iteration 174, loss = 0.41803799\n",
            "Iteration 175, loss = 0.42031676\n",
            "Iteration 176, loss = 0.42822709\n",
            "Iteration 177, loss = 0.41785188\n",
            "Iteration 178, loss = 0.41865415\n",
            "Iteration 179, loss = 0.41925529\n",
            "Iteration 180, loss = 0.41636464\n",
            "Iteration 181, loss = 0.41838181\n",
            "Iteration 182, loss = 0.41646563\n",
            "Iteration 183, loss = 0.41604121\n",
            "Iteration 184, loss = 0.41851378\n",
            "Iteration 185, loss = 0.41610557\n",
            "Iteration 186, loss = 0.41573230\n",
            "Iteration 187, loss = 0.41501002\n",
            "Iteration 188, loss = 0.41515814\n",
            "Iteration 189, loss = 0.41431027\n",
            "Iteration 190, loss = 0.41518810\n",
            "Iteration 191, loss = 0.41930187\n",
            "Iteration 192, loss = 0.41319951\n",
            "Iteration 193, loss = 0.41668733\n",
            "Iteration 194, loss = 0.41279947\n",
            "Iteration 195, loss = 0.41647093\n",
            "Iteration 196, loss = 0.41443493\n",
            "Iteration 197, loss = 0.41640763\n",
            "Iteration 198, loss = 0.41265517\n",
            "Iteration 199, loss = 0.41434411\n",
            "Iteration 200, loss = 0.41437629\n",
            "Iteration 1, loss = 0.79038001\n",
            "Iteration 2, loss = 0.74283358\n",
            "Iteration 3, loss = 0.70884737\n",
            "Iteration 4, loss = 0.68527172\n",
            "Iteration 5, loss = 0.67152729\n",
            "Iteration 6, loss = 0.66196869\n",
            "Iteration 7, loss = 0.65600193\n",
            "Iteration 8, loss = 0.65060964\n",
            "Iteration 9, loss = 0.64611350\n",
            "Iteration 10, loss = 0.64180305\n",
            "Iteration 11, loss = 0.63781803\n",
            "Iteration 12, loss = 0.63402806\n",
            "Iteration 13, loss = 0.63014145\n",
            "Iteration 14, loss = 0.62603446\n",
            "Iteration 15, loss = 0.62221487\n",
            "Iteration 16, loss = 0.61827879\n",
            "Iteration 17, loss = 0.61423174\n",
            "Iteration 18, loss = 0.61102298\n",
            "Iteration 19, loss = 0.60866596\n",
            "Iteration 20, loss = 0.60565298\n",
            "Iteration 21, loss = 0.60301448\n",
            "Iteration 22, loss = 0.60033508\n",
            "Iteration 23, loss = 0.59737770\n",
            "Iteration 24, loss = 0.59454524\n",
            "Iteration 25, loss = 0.59244048\n",
            "Iteration 26, loss = 0.59008397\n",
            "Iteration 27, loss = 0.58785298\n",
            "Iteration 28, loss = 0.58591215\n",
            "Iteration 29, loss = 0.58405544\n",
            "Iteration 30, loss = 0.58184075\n",
            "Iteration 31, loss = 0.58023919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 32, loss = 0.57782526\n",
            "Iteration 33, loss = 0.57598556\n",
            "Iteration 34, loss = 0.57476935\n",
            "Iteration 35, loss = 0.57252720\n",
            "Iteration 36, loss = 0.57129085\n",
            "Iteration 37, loss = 0.56911108\n",
            "Iteration 38, loss = 0.56792701\n",
            "Iteration 39, loss = 0.56614185\n",
            "Iteration 40, loss = 0.56479925\n",
            "Iteration 41, loss = 0.56516758\n",
            "Iteration 42, loss = 0.56164893\n",
            "Iteration 43, loss = 0.55994885\n",
            "Iteration 44, loss = 0.55888935\n",
            "Iteration 45, loss = 0.55732351\n",
            "Iteration 46, loss = 0.55634637\n",
            "Iteration 47, loss = 0.55473069\n",
            "Iteration 48, loss = 0.55309676\n",
            "Iteration 49, loss = 0.55220755\n",
            "Iteration 50, loss = 0.55121697\n",
            "Iteration 51, loss = 0.54874828\n",
            "Iteration 52, loss = 0.54766809\n",
            "Iteration 53, loss = 0.54620425\n",
            "Iteration 54, loss = 0.54482750\n",
            "Iteration 55, loss = 0.54345440\n",
            "Iteration 56, loss = 0.54172455\n",
            "Iteration 57, loss = 0.54010244\n",
            "Iteration 58, loss = 0.53920659\n",
            "Iteration 59, loss = 0.53686925\n",
            "Iteration 60, loss = 0.53444939\n",
            "Iteration 61, loss = 0.53351573\n",
            "Iteration 62, loss = 0.53153221\n",
            "Iteration 63, loss = 0.52952697\n",
            "Iteration 64, loss = 0.52675740\n",
            "Iteration 65, loss = 0.52388805\n",
            "Iteration 66, loss = 0.52289915\n",
            "Iteration 67, loss = 0.52000241\n",
            "Iteration 68, loss = 0.51695257\n",
            "Iteration 69, loss = 0.51508306\n",
            "Iteration 70, loss = 0.51321504\n",
            "Iteration 71, loss = 0.51153451\n",
            "Iteration 72, loss = 0.51048441\n",
            "Iteration 73, loss = 0.50964002\n",
            "Iteration 74, loss = 0.50456084\n",
            "Iteration 75, loss = 0.50257124\n",
            "Iteration 76, loss = 0.50030207\n",
            "Iteration 77, loss = 0.49743515\n",
            "Iteration 78, loss = 0.49600067\n",
            "Iteration 79, loss = 0.49213734\n",
            "Iteration 80, loss = 0.48895896\n",
            "Iteration 81, loss = 0.48641458\n",
            "Iteration 82, loss = 0.48356435\n",
            "Iteration 83, loss = 0.48178271\n",
            "Iteration 84, loss = 0.48002079\n",
            "Iteration 85, loss = 0.47969261\n",
            "Iteration 86, loss = 0.47477961\n",
            "Iteration 87, loss = 0.47394247\n",
            "Iteration 88, loss = 0.47149441\n",
            "Iteration 89, loss = 0.47088661\n",
            "Iteration 90, loss = 0.46994265\n",
            "Iteration 91, loss = 0.46385547\n",
            "Iteration 92, loss = 0.46315376\n",
            "Iteration 93, loss = 0.46092929\n",
            "Iteration 94, loss = 0.46018615\n",
            "Iteration 95, loss = 0.45986066\n",
            "Iteration 96, loss = 0.45609628\n",
            "Iteration 97, loss = 0.45466838\n",
            "Iteration 98, loss = 0.45385698\n",
            "Iteration 99, loss = 0.45162232\n",
            "Iteration 100, loss = 0.45153454\n",
            "Iteration 101, loss = 0.45006132\n",
            "Iteration 102, loss = 0.44790189\n",
            "Iteration 103, loss = 0.44672704\n",
            "Iteration 104, loss = 0.44633997\n",
            "Iteration 105, loss = 0.44532702\n",
            "Iteration 106, loss = 0.44526893\n",
            "Iteration 107, loss = 0.44275702\n",
            "Iteration 108, loss = 0.44137007\n",
            "Iteration 109, loss = 0.44004221\n",
            "Iteration 110, loss = 0.43991745\n",
            "Iteration 111, loss = 0.44148260\n",
            "Iteration 112, loss = 0.44569854\n",
            "Iteration 113, loss = 0.44446236\n",
            "Iteration 114, loss = 0.43779160\n",
            "Iteration 115, loss = 0.43756692\n",
            "Iteration 116, loss = 0.43281180\n",
            "Iteration 117, loss = 0.43440943\n",
            "Iteration 118, loss = 0.43172068\n",
            "Iteration 119, loss = 0.43246372\n",
            "Iteration 120, loss = 0.43150890\n",
            "Iteration 121, loss = 0.42842565\n",
            "Iteration 122, loss = 0.42951839\n",
            "Iteration 123, loss = 0.43097785\n",
            "Iteration 124, loss = 0.43465953\n",
            "Iteration 125, loss = 0.43173318\n",
            "Iteration 126, loss = 0.42651115\n",
            "Iteration 127, loss = 0.42704726\n",
            "Iteration 128, loss = 0.42542824\n",
            "Iteration 129, loss = 0.42424305\n",
            "Iteration 130, loss = 0.42293308\n",
            "Iteration 131, loss = 0.42592889\n",
            "Iteration 132, loss = 0.42031820\n",
            "Iteration 133, loss = 0.42247934\n",
            "Iteration 134, loss = 0.42613047\n",
            "Iteration 135, loss = 0.42812057\n",
            "Iteration 136, loss = 0.42071355\n",
            "Iteration 137, loss = 0.41947443\n",
            "Iteration 138, loss = 0.42053890\n",
            "Iteration 139, loss = 0.41829157\n",
            "Iteration 140, loss = 0.41760746\n",
            "Iteration 141, loss = 0.41813110\n",
            "Iteration 142, loss = 0.41788298\n",
            "Iteration 143, loss = 0.41613562\n",
            "Iteration 144, loss = 0.41610292\n",
            "Iteration 145, loss = 0.41820101\n",
            "Iteration 146, loss = 0.41534006\n",
            "Iteration 147, loss = 0.41569370\n",
            "Iteration 148, loss = 0.42808124\n",
            "Iteration 149, loss = 0.41729919\n",
            "Iteration 150, loss = 0.41506621\n",
            "Iteration 151, loss = 0.41487360\n",
            "Iteration 152, loss = 0.41409309\n",
            "Iteration 153, loss = 0.41321291\n",
            "Iteration 154, loss = 0.41177064\n",
            "Iteration 155, loss = 0.41193915\n",
            "Iteration 156, loss = 0.41176393\n",
            "Iteration 157, loss = 0.41039059\n",
            "Iteration 158, loss = 0.41127029\n",
            "Iteration 159, loss = 0.41017835\n",
            "Iteration 160, loss = 0.41006383\n",
            "Iteration 161, loss = 0.40999669\n",
            "Iteration 162, loss = 0.41085928\n",
            "Iteration 163, loss = 0.40908365\n",
            "Iteration 164, loss = 0.40947986\n",
            "Iteration 165, loss = 0.40885323\n",
            "Iteration 166, loss = 0.40917323\n",
            "Iteration 167, loss = 0.40917368\n",
            "Iteration 168, loss = 0.40769961\n",
            "Iteration 169, loss = 0.41059015\n",
            "Iteration 170, loss = 0.40899820\n",
            "Iteration 171, loss = 0.40801024\n",
            "Iteration 172, loss = 0.40701412\n",
            "Iteration 173, loss = 0.40709952\n",
            "Iteration 174, loss = 0.40573471\n",
            "Iteration 175, loss = 0.40570697\n",
            "Iteration 176, loss = 0.40513646\n",
            "Iteration 177, loss = 0.40456979\n",
            "Iteration 178, loss = 0.40476640\n",
            "Iteration 179, loss = 0.40482034\n",
            "Iteration 180, loss = 0.40484487\n",
            "Iteration 181, loss = 0.40363478\n",
            "Iteration 182, loss = 0.40419121\n",
            "Iteration 183, loss = 0.40370225\n",
            "Iteration 184, loss = 0.40356274\n",
            "Iteration 185, loss = 0.40289940\n",
            "Iteration 186, loss = 0.40249970\n",
            "Iteration 187, loss = 0.40282409\n",
            "Iteration 188, loss = 0.40477331\n",
            "Iteration 189, loss = 0.40951882\n",
            "Iteration 190, loss = 0.40270465\n",
            "Iteration 191, loss = 0.40272964\n",
            "Iteration 192, loss = 0.40229679\n",
            "Iteration 193, loss = 0.40255238\n",
            "Iteration 194, loss = 0.40236281\n",
            "Iteration 195, loss = 0.40359539\n",
            "Iteration 196, loss = 0.40194192\n",
            "Iteration 197, loss = 0.40030037\n",
            "Iteration 198, loss = 0.40061936\n",
            "Iteration 199, loss = 0.40339970\n",
            "Iteration 200, loss = 0.40522882\n",
            "Iteration 1, loss = 0.67391416\n",
            "Iteration 2, loss = 0.66791993\n",
            "Iteration 3, loss = 0.66186191\n",
            "Iteration 4, loss = 0.65647021\n",
            "Iteration 5, loss = 0.65064564\n",
            "Iteration 6, loss = 0.64585061\n",
            "Iteration 7, loss = 0.63853290\n",
            "Iteration 8, loss = 0.63219511\n",
            "Iteration 9, loss = 0.62614180\n",
            "Iteration 10, loss = 0.62084633\n",
            "Iteration 11, loss = 0.61567803\n",
            "Iteration 12, loss = 0.61055083\n",
            "Iteration 13, loss = 0.60739023\n",
            "Iteration 14, loss = 0.60369485\n",
            "Iteration 15, loss = 0.60076009\n",
            "Iteration 16, loss = 0.59805616\n",
            "Iteration 17, loss = 0.59587786\n",
            "Iteration 18, loss = 0.59321945\n",
            "Iteration 19, loss = 0.59102434\n",
            "Iteration 20, loss = 0.58933559\n",
            "Iteration 21, loss = 0.58759091\n",
            "Iteration 22, loss = 0.58524868\n",
            "Iteration 23, loss = 0.58274218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 24, loss = 0.58117406\n",
            "Iteration 25, loss = 0.57894209\n",
            "Iteration 26, loss = 0.57647090\n",
            "Iteration 27, loss = 0.57434996\n",
            "Iteration 28, loss = 0.57200117\n",
            "Iteration 29, loss = 0.57034143\n",
            "Iteration 30, loss = 0.56790335\n",
            "Iteration 31, loss = 0.56591957\n",
            "Iteration 32, loss = 0.56277954\n",
            "Iteration 33, loss = 0.56174634\n",
            "Iteration 34, loss = 0.55905764\n",
            "Iteration 35, loss = 0.55695681\n",
            "Iteration 36, loss = 0.55483182\n",
            "Iteration 37, loss = 0.55204275\n",
            "Iteration 38, loss = 0.55019920\n",
            "Iteration 39, loss = 0.54795904\n",
            "Iteration 40, loss = 0.54494989\n",
            "Iteration 41, loss = 0.54253763\n",
            "Iteration 42, loss = 0.54116582\n",
            "Iteration 43, loss = 0.53772730\n",
            "Iteration 44, loss = 0.53562551\n",
            "Iteration 45, loss = 0.53299652\n",
            "Iteration 46, loss = 0.53156700\n",
            "Iteration 47, loss = 0.52736412\n",
            "Iteration 48, loss = 0.52439494\n",
            "Iteration 49, loss = 0.52247624\n",
            "Iteration 50, loss = 0.51989083\n",
            "Iteration 51, loss = 0.51692305\n",
            "Iteration 52, loss = 0.51859255\n",
            "Iteration 53, loss = 0.51305114\n",
            "Iteration 54, loss = 0.51013555\n",
            "Iteration 55, loss = 0.50679216\n",
            "Iteration 56, loss = 0.50435200\n",
            "Iteration 57, loss = 0.49954037\n",
            "Iteration 58, loss = 0.49936752\n",
            "Iteration 59, loss = 0.49420047\n",
            "Iteration 60, loss = 0.49323485\n",
            "Iteration 61, loss = 0.48892167\n",
            "Iteration 62, loss = 0.48754298\n",
            "Iteration 63, loss = 0.48721423\n",
            "Iteration 64, loss = 0.48180021\n",
            "Iteration 65, loss = 0.47993761\n",
            "Iteration 66, loss = 0.47725054\n",
            "Iteration 67, loss = 0.47541296\n",
            "Iteration 68, loss = 0.47624171\n",
            "Iteration 69, loss = 0.47652838\n",
            "Iteration 70, loss = 0.46769930\n",
            "Iteration 71, loss = 0.46836105\n",
            "Iteration 72, loss = 0.46391532\n",
            "Iteration 73, loss = 0.46380492\n",
            "Iteration 74, loss = 0.46280709\n",
            "Iteration 75, loss = 0.45795466\n",
            "Iteration 76, loss = 0.45728928\n",
            "Iteration 77, loss = 0.45648844\n",
            "Iteration 78, loss = 0.45337129\n",
            "Iteration 79, loss = 0.45347821\n",
            "Iteration 80, loss = 0.45186797\n",
            "Iteration 81, loss = 0.44909234\n",
            "Iteration 82, loss = 0.44777327\n",
            "Iteration 83, loss = 0.44748114\n",
            "Iteration 84, loss = 0.44617719\n",
            "Iteration 85, loss = 0.44670893\n",
            "Iteration 86, loss = 0.44306269\n",
            "Iteration 87, loss = 0.44155684\n",
            "Iteration 88, loss = 0.44094450\n",
            "Iteration 89, loss = 0.44151374\n",
            "Iteration 90, loss = 0.44443326\n",
            "Iteration 91, loss = 0.44025092\n",
            "Iteration 92, loss = 0.44266297\n",
            "Iteration 93, loss = 0.43644155\n",
            "Iteration 94, loss = 0.43796152\n",
            "Iteration 95, loss = 0.43536206\n",
            "Iteration 96, loss = 0.43389644\n",
            "Iteration 97, loss = 0.43861855\n",
            "Iteration 98, loss = 0.43445244\n",
            "Iteration 99, loss = 0.43151146\n",
            "Iteration 100, loss = 0.43244681\n",
            "Iteration 101, loss = 0.43360732\n",
            "Iteration 102, loss = 0.42970004\n",
            "Iteration 103, loss = 0.42765998\n",
            "Iteration 104, loss = 0.42765091\n",
            "Iteration 105, loss = 0.42710146\n",
            "Iteration 106, loss = 0.42821196\n",
            "Iteration 107, loss = 0.42913057\n",
            "Iteration 108, loss = 0.42567557\n",
            "Iteration 109, loss = 0.42864561\n",
            "Iteration 110, loss = 0.42919475\n",
            "Iteration 111, loss = 0.42509404\n",
            "Iteration 112, loss = 0.42565259\n",
            "Iteration 113, loss = 0.42483725\n",
            "Iteration 114, loss = 0.42262524\n",
            "Iteration 115, loss = 0.42193209\n",
            "Iteration 116, loss = 0.42189344\n",
            "Iteration 117, loss = 0.42105581\n",
            "Iteration 118, loss = 0.42184766\n",
            "Iteration 119, loss = 0.42164205\n",
            "Iteration 120, loss = 0.42055288\n",
            "Iteration 121, loss = 0.41942236\n",
            "Iteration 122, loss = 0.41953568\n",
            "Iteration 123, loss = 0.42017111\n",
            "Iteration 124, loss = 0.42165614\n",
            "Iteration 125, loss = 0.41939729\n",
            "Iteration 126, loss = 0.41700366\n",
            "Iteration 127, loss = 0.41848926\n",
            "Iteration 128, loss = 0.41772965\n",
            "Iteration 129, loss = 0.41734110\n",
            "Iteration 130, loss = 0.41854330\n",
            "Iteration 131, loss = 0.41773260\n",
            "Iteration 132, loss = 0.41917570\n",
            "Iteration 133, loss = 0.41551335\n",
            "Iteration 134, loss = 0.41732033\n",
            "Iteration 135, loss = 0.41385202\n",
            "Iteration 136, loss = 0.41306049\n",
            "Iteration 137, loss = 0.41397899\n",
            "Iteration 138, loss = 0.41219066\n",
            "Iteration 139, loss = 0.41721291\n",
            "Iteration 140, loss = 0.41293825\n",
            "Iteration 141, loss = 0.41284759\n",
            "Iteration 142, loss = 0.41164956\n",
            "Iteration 143, loss = 0.41152773\n",
            "Iteration 144, loss = 0.41157342\n",
            "Iteration 145, loss = 0.41186983\n",
            "Iteration 146, loss = 0.41103944\n",
            "Iteration 147, loss = 0.40995782\n",
            "Iteration 148, loss = 0.40999332\n",
            "Iteration 149, loss = 0.41154048\n",
            "Iteration 150, loss = 0.41602637\n",
            "Iteration 151, loss = 0.40748801\n",
            "Iteration 152, loss = 0.40877192\n",
            "Iteration 153, loss = 0.40756209\n",
            "Iteration 154, loss = 0.40726257\n",
            "Iteration 155, loss = 0.40786797\n",
            "Iteration 156, loss = 0.40804854\n",
            "Iteration 157, loss = 0.40688500\n",
            "Iteration 158, loss = 0.40652882\n",
            "Iteration 159, loss = 0.41070099\n",
            "Iteration 160, loss = 0.40712917\n",
            "Iteration 161, loss = 0.40829710\n",
            "Iteration 162, loss = 0.40528995\n",
            "Iteration 163, loss = 0.40671025\n",
            "Iteration 164, loss = 0.40573730\n",
            "Iteration 165, loss = 0.40441972\n",
            "Iteration 166, loss = 0.40389899\n",
            "Iteration 167, loss = 0.40549314\n",
            "Iteration 168, loss = 0.40394685\n",
            "Iteration 169, loss = 0.40542886\n",
            "Iteration 170, loss = 0.40240763\n",
            "Iteration 171, loss = 0.40395239\n",
            "Iteration 172, loss = 0.40314255\n",
            "Iteration 173, loss = 0.40253096\n",
            "Iteration 174, loss = 0.40140040\n",
            "Iteration 175, loss = 0.40210287\n",
            "Iteration 176, loss = 0.40212706\n",
            "Iteration 177, loss = 0.40235940\n",
            "Iteration 178, loss = 0.40123330\n",
            "Iteration 179, loss = 0.40068700\n",
            "Iteration 180, loss = 0.40132865\n",
            "Iteration 181, loss = 0.40067129\n",
            "Iteration 182, loss = 0.39943123\n",
            "Iteration 183, loss = 0.40114165\n",
            "Iteration 184, loss = 0.39980749\n",
            "Iteration 185, loss = 0.39978635\n",
            "Iteration 186, loss = 0.39900698\n",
            "Iteration 187, loss = 0.39971776\n",
            "Iteration 188, loss = 0.39793898\n",
            "Iteration 189, loss = 0.40255224\n",
            "Iteration 190, loss = 0.40337902\n",
            "Iteration 191, loss = 0.39712316\n",
            "Iteration 192, loss = 0.40018088\n",
            "Iteration 193, loss = 0.39699734\n",
            "Iteration 194, loss = 0.39729181\n",
            "Iteration 195, loss = 0.39664443\n",
            "Iteration 196, loss = 0.39831284\n",
            "Iteration 197, loss = 0.39607148\n",
            "Iteration 198, loss = 0.39686279\n",
            "Iteration 199, loss = 0.39629069\n",
            "Iteration 200, loss = 0.39571435\n",
            "Iteration 1, loss = 0.67836538\n",
            "Iteration 2, loss = 0.67138936\n",
            "Iteration 3, loss = 0.66814624\n",
            "Iteration 4, loss = 0.66422567\n",
            "Iteration 5, loss = 0.66107009\n",
            "Iteration 6, loss = 0.65596564\n",
            "Iteration 7, loss = 0.65140540\n",
            "Iteration 8, loss = 0.64638321\n",
            "Iteration 9, loss = 0.64048131\n",
            "Iteration 10, loss = 0.63509570\n",
            "Iteration 11, loss = 0.63003969\n",
            "Iteration 12, loss = 0.62519747\n",
            "Iteration 13, loss = 0.62251158\n",
            "Iteration 14, loss = 0.61882752\n",
            "Iteration 15, loss = 0.61667247\n",
            "Iteration 16, loss = 0.61389486\n",
            "Iteration 17, loss = 0.61194728\n",
            "Iteration 18, loss = 0.61038411\n",
            "Iteration 19, loss = 0.60830328\n",
            "Iteration 20, loss = 0.60633597\n",
            "Iteration 21, loss = 0.60428985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 22, loss = 0.60254612\n",
            "Iteration 23, loss = 0.60085879\n",
            "Iteration 24, loss = 0.59816470\n",
            "Iteration 25, loss = 0.59604079\n",
            "Iteration 26, loss = 0.59383708\n",
            "Iteration 27, loss = 0.59187333\n",
            "Iteration 28, loss = 0.59013265\n",
            "Iteration 29, loss = 0.58806943\n",
            "Iteration 30, loss = 0.58576056\n",
            "Iteration 31, loss = 0.58415526\n",
            "Iteration 32, loss = 0.58158430\n",
            "Iteration 33, loss = 0.57966862\n",
            "Iteration 34, loss = 0.57954312\n",
            "Iteration 35, loss = 0.57456386\n",
            "Iteration 36, loss = 0.57307194\n",
            "Iteration 37, loss = 0.57076519\n",
            "Iteration 38, loss = 0.56759616\n",
            "Iteration 39, loss = 0.56612020\n",
            "Iteration 40, loss = 0.56312143\n",
            "Iteration 41, loss = 0.56045763\n",
            "Iteration 42, loss = 0.55790584\n",
            "Iteration 43, loss = 0.55528414\n",
            "Iteration 44, loss = 0.55262819\n",
            "Iteration 45, loss = 0.54985695\n",
            "Iteration 46, loss = 0.54676127\n",
            "Iteration 47, loss = 0.54441566\n",
            "Iteration 48, loss = 0.54055577\n",
            "Iteration 49, loss = 0.53951893\n",
            "Iteration 50, loss = 0.53400570\n",
            "Iteration 51, loss = 0.53094454\n",
            "Iteration 52, loss = 0.52773305\n",
            "Iteration 53, loss = 0.52631016\n",
            "Iteration 54, loss = 0.52244689\n",
            "Iteration 55, loss = 0.51861264\n",
            "Iteration 56, loss = 0.51592550\n",
            "Iteration 57, loss = 0.51112486\n",
            "Iteration 58, loss = 0.50930797\n",
            "Iteration 59, loss = 0.50795733\n",
            "Iteration 60, loss = 0.50195807\n",
            "Iteration 61, loss = 0.50085359\n",
            "Iteration 62, loss = 0.49797053\n",
            "Iteration 63, loss = 0.49462753\n",
            "Iteration 64, loss = 0.49159176\n",
            "Iteration 65, loss = 0.48735021\n",
            "Iteration 66, loss = 0.48650560\n",
            "Iteration 67, loss = 0.48378972\n",
            "Iteration 68, loss = 0.47987970\n",
            "Iteration 69, loss = 0.48437445\n",
            "Iteration 70, loss = 0.47508401\n",
            "Iteration 71, loss = 0.47421108\n",
            "Iteration 72, loss = 0.47359899\n",
            "Iteration 73, loss = 0.46948733\n",
            "Iteration 74, loss = 0.46676926\n",
            "Iteration 75, loss = 0.46454481\n",
            "Iteration 76, loss = 0.46465042\n",
            "Iteration 77, loss = 0.46197450\n",
            "Iteration 78, loss = 0.46034704\n",
            "Iteration 79, loss = 0.45842020\n",
            "Iteration 80, loss = 0.45620576\n",
            "Iteration 81, loss = 0.45458254\n",
            "Iteration 82, loss = 0.45451809\n",
            "Iteration 83, loss = 0.45223829\n",
            "Iteration 84, loss = 0.45062320\n",
            "Iteration 85, loss = 0.45051576\n",
            "Iteration 86, loss = 0.44949249\n",
            "Iteration 87, loss = 0.44891054\n",
            "Iteration 88, loss = 0.44630443\n",
            "Iteration 89, loss = 0.44589813\n",
            "Iteration 90, loss = 0.44554161\n",
            "Iteration 91, loss = 0.44461723\n",
            "Iteration 92, loss = 0.44308502\n",
            "Iteration 93, loss = 0.44133225\n",
            "Iteration 94, loss = 0.44253831\n",
            "Iteration 95, loss = 0.44115142\n",
            "Iteration 96, loss = 0.44012414\n",
            "Iteration 97, loss = 0.44048620\n",
            "Iteration 98, loss = 0.43687626\n",
            "Iteration 99, loss = 0.44175150\n",
            "Iteration 100, loss = 0.43793773\n",
            "Iteration 101, loss = 0.43626898\n",
            "Iteration 102, loss = 0.43719956\n",
            "Iteration 103, loss = 0.43597959\n",
            "Iteration 104, loss = 0.43459505\n",
            "Iteration 105, loss = 0.43467399\n",
            "Iteration 106, loss = 0.43459123\n",
            "Iteration 107, loss = 0.43330008\n",
            "Iteration 108, loss = 0.43288059\n",
            "Iteration 109, loss = 0.43229397\n",
            "Iteration 110, loss = 0.43303009\n",
            "Iteration 111, loss = 0.43292172\n",
            "Iteration 112, loss = 0.43163347\n",
            "Iteration 113, loss = 0.42968687\n",
            "Iteration 114, loss = 0.43013260\n",
            "Iteration 115, loss = 0.42944672\n",
            "Iteration 116, loss = 0.42958253\n",
            "Iteration 117, loss = 0.42823746\n",
            "Iteration 118, loss = 0.42794928\n",
            "Iteration 119, loss = 0.42804016\n",
            "Iteration 120, loss = 0.43038012\n",
            "Iteration 121, loss = 0.42687621\n",
            "Iteration 122, loss = 0.42632570\n",
            "Iteration 123, loss = 0.42742743\n",
            "Iteration 124, loss = 0.42512341\n",
            "Iteration 125, loss = 0.42501951\n",
            "Iteration 126, loss = 0.42578444\n",
            "Iteration 127, loss = 0.42420124\n",
            "Iteration 128, loss = 0.42397991\n",
            "Iteration 129, loss = 0.42328909\n",
            "Iteration 130, loss = 0.42347512\n",
            "Iteration 131, loss = 0.42231639\n",
            "Iteration 132, loss = 0.42324074\n",
            "Iteration 133, loss = 0.42777138\n",
            "Iteration 134, loss = 0.42823494\n",
            "Iteration 135, loss = 0.42759009\n",
            "Iteration 136, loss = 0.42274103\n",
            "Iteration 137, loss = 0.42218039\n",
            "Iteration 138, loss = 0.42295804\n",
            "Iteration 139, loss = 0.42544348\n",
            "Iteration 140, loss = 0.42411471\n",
            "Iteration 141, loss = 0.42242231\n",
            "Iteration 142, loss = 0.42568681\n",
            "Iteration 143, loss = 0.41882276\n",
            "Iteration 144, loss = 0.41933174\n",
            "Iteration 145, loss = 0.41932841\n",
            "Iteration 146, loss = 0.41795398\n",
            "Iteration 147, loss = 0.41979467\n",
            "Iteration 148, loss = 0.41938920\n",
            "Iteration 149, loss = 0.41678449\n",
            "Iteration 150, loss = 0.41688248\n",
            "Iteration 151, loss = 0.41719061\n",
            "Iteration 152, loss = 0.41636207\n",
            "Iteration 153, loss = 0.41747357\n",
            "Iteration 154, loss = 0.41495187\n",
            "Iteration 155, loss = 0.41988622\n",
            "Iteration 156, loss = 0.41539276\n",
            "Iteration 157, loss = 0.41544956\n",
            "Iteration 158, loss = 0.41590936\n",
            "Iteration 159, loss = 0.41426516\n",
            "Iteration 160, loss = 0.41399831\n",
            "Iteration 161, loss = 0.41525581\n",
            "Iteration 162, loss = 0.41322090\n",
            "Iteration 163, loss = 0.41604877\n",
            "Iteration 164, loss = 0.41923679\n",
            "Iteration 165, loss = 0.41512593\n",
            "Iteration 166, loss = 0.41556733\n",
            "Iteration 167, loss = 0.41390281\n",
            "Iteration 168, loss = 0.41490219\n",
            "Iteration 169, loss = 0.41680776\n",
            "Iteration 170, loss = 0.41287104\n",
            "Iteration 171, loss = 0.41204189\n",
            "Iteration 172, loss = 0.41554269\n",
            "Iteration 173, loss = 0.41407522\n",
            "Iteration 174, loss = 0.40894115\n",
            "Iteration 175, loss = 0.41084798\n",
            "Iteration 176, loss = 0.41235867\n",
            "Iteration 177, loss = 0.41106550\n",
            "Iteration 178, loss = 0.40957555\n",
            "Iteration 179, loss = 0.40966332\n",
            "Iteration 180, loss = 0.40943060\n",
            "Iteration 181, loss = 0.40950967\n",
            "Iteration 182, loss = 0.40938858\n",
            "Iteration 183, loss = 0.41130506\n",
            "Iteration 184, loss = 0.40672507\n",
            "Iteration 185, loss = 0.40762099\n",
            "Iteration 186, loss = 0.40841101\n",
            "Iteration 187, loss = 0.40672252\n",
            "Iteration 188, loss = 0.40700145\n",
            "Iteration 189, loss = 0.40597692\n",
            "Iteration 190, loss = 0.40687722\n",
            "Iteration 191, loss = 0.40702703\n",
            "Iteration 192, loss = 0.40884466\n",
            "Iteration 193, loss = 0.40593410\n",
            "Iteration 194, loss = 0.40544555\n",
            "Iteration 195, loss = 0.40852099\n",
            "Iteration 196, loss = 0.40498246\n",
            "Iteration 197, loss = 0.40738257\n",
            "Iteration 198, loss = 0.40357981\n",
            "Iteration 199, loss = 0.40410311\n",
            "Iteration 200, loss = 0.40518073\n",
            "Iteration 1, loss = 0.68327502\n",
            "Iteration 2, loss = 0.67281134\n",
            "Iteration 3, loss = 0.66717868\n",
            "Iteration 4, loss = 0.66452739\n",
            "Iteration 5, loss = 0.66148105\n",
            "Iteration 6, loss = 0.65839277\n",
            "Iteration 7, loss = 0.65484965\n",
            "Iteration 8, loss = 0.65133757\n",
            "Iteration 9, loss = 0.64755852\n",
            "Iteration 10, loss = 0.64388786\n",
            "Iteration 11, loss = 0.63982869\n",
            "Iteration 12, loss = 0.63485653\n",
            "Iteration 13, loss = 0.63064825\n",
            "Iteration 14, loss = 0.62623822\n",
            "Iteration 15, loss = 0.62324187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 16, loss = 0.62000515\n",
            "Iteration 17, loss = 0.61736147\n",
            "Iteration 18, loss = 0.61452420\n",
            "Iteration 19, loss = 0.61287275\n",
            "Iteration 20, loss = 0.61061697\n",
            "Iteration 21, loss = 0.60893094\n",
            "Iteration 22, loss = 0.60679851\n",
            "Iteration 23, loss = 0.60590538\n",
            "Iteration 24, loss = 0.60436055\n",
            "Iteration 25, loss = 0.60249205\n",
            "Iteration 26, loss = 0.60016953\n",
            "Iteration 27, loss = 0.59855762\n",
            "Iteration 28, loss = 0.59737993\n",
            "Iteration 29, loss = 0.59575459\n",
            "Iteration 30, loss = 0.59446559\n",
            "Iteration 31, loss = 0.59259776\n",
            "Iteration 32, loss = 0.59113807\n",
            "Iteration 33, loss = 0.58965204\n",
            "Iteration 34, loss = 0.58814483\n",
            "Iteration 35, loss = 0.58645452\n",
            "Iteration 36, loss = 0.58511375\n",
            "Iteration 37, loss = 0.58471887\n",
            "Iteration 38, loss = 0.58279837\n",
            "Iteration 39, loss = 0.58069404\n",
            "Iteration 40, loss = 0.57916282\n",
            "Iteration 41, loss = 0.57802329\n",
            "Iteration 42, loss = 0.57611421\n",
            "Iteration 43, loss = 0.57408943\n",
            "Iteration 44, loss = 0.57316577\n",
            "Iteration 45, loss = 0.57159590\n",
            "Iteration 46, loss = 0.56918526\n",
            "Iteration 47, loss = 0.56720969\n",
            "Iteration 48, loss = 0.56532007\n",
            "Iteration 49, loss = 0.56386913\n",
            "Iteration 50, loss = 0.56141803\n",
            "Iteration 51, loss = 0.56040925\n",
            "Iteration 52, loss = 0.55809091\n",
            "Iteration 53, loss = 0.55574090\n",
            "Iteration 54, loss = 0.55407645\n",
            "Iteration 55, loss = 0.55145837\n",
            "Iteration 56, loss = 0.54972578\n",
            "Iteration 57, loss = 0.54763051\n",
            "Iteration 58, loss = 0.54838068\n",
            "Iteration 59, loss = 0.54241510\n",
            "Iteration 60, loss = 0.54072775\n",
            "Iteration 61, loss = 0.53827592\n",
            "Iteration 62, loss = 0.53565540\n",
            "Iteration 63, loss = 0.53667636\n",
            "Iteration 64, loss = 0.53191975\n",
            "Iteration 65, loss = 0.52983922\n",
            "Iteration 66, loss = 0.52759446\n",
            "Iteration 67, loss = 0.52452185\n",
            "Iteration 68, loss = 0.51998251\n",
            "Iteration 69, loss = 0.51748810\n",
            "Iteration 70, loss = 0.51468135\n",
            "Iteration 71, loss = 0.51224680\n",
            "Iteration 72, loss = 0.50859457\n",
            "Iteration 73, loss = 0.50827110\n",
            "Iteration 74, loss = 0.50523531\n",
            "Iteration 75, loss = 0.50211612\n",
            "Iteration 76, loss = 0.49979210\n",
            "Iteration 77, loss = 0.49908392\n",
            "Iteration 78, loss = 0.49397292\n",
            "Iteration 79, loss = 0.49072179\n",
            "Iteration 80, loss = 0.49093829\n",
            "Iteration 81, loss = 0.48976676\n",
            "Iteration 82, loss = 0.48483782\n",
            "Iteration 83, loss = 0.48061201\n",
            "Iteration 84, loss = 0.48172953\n",
            "Iteration 85, loss = 0.47976583\n",
            "Iteration 86, loss = 0.47516283\n",
            "Iteration 87, loss = 0.47348870\n",
            "Iteration 88, loss = 0.47143798\n",
            "Iteration 89, loss = 0.47267106\n",
            "Iteration 90, loss = 0.47073475\n",
            "Iteration 91, loss = 0.46526887\n",
            "Iteration 92, loss = 0.46496700\n",
            "Iteration 93, loss = 0.46928190\n",
            "Iteration 94, loss = 0.46480018\n",
            "Iteration 95, loss = 0.45946983\n",
            "Iteration 96, loss = 0.46012714\n",
            "Iteration 97, loss = 0.45938064\n",
            "Iteration 98, loss = 0.45637244\n",
            "Iteration 99, loss = 0.45427453\n",
            "Iteration 100, loss = 0.45472530\n",
            "Iteration 101, loss = 0.45316236\n",
            "Iteration 102, loss = 0.45720545\n",
            "Iteration 103, loss = 0.45054351\n",
            "Iteration 104, loss = 0.44839906\n",
            "Iteration 105, loss = 0.44847707\n",
            "Iteration 106, loss = 0.44603606\n",
            "Iteration 107, loss = 0.44613501\n",
            "Iteration 108, loss = 0.44542631\n",
            "Iteration 109, loss = 0.44489540\n",
            "Iteration 110, loss = 0.44371409\n",
            "Iteration 111, loss = 0.44266212\n",
            "Iteration 112, loss = 0.44197694\n",
            "Iteration 113, loss = 0.44300788\n",
            "Iteration 114, loss = 0.44128950\n",
            "Iteration 115, loss = 0.44127613\n",
            "Iteration 116, loss = 0.44708103\n",
            "Iteration 117, loss = 0.44133190\n",
            "Iteration 118, loss = 0.43942292\n",
            "Iteration 119, loss = 0.43979304\n",
            "Iteration 120, loss = 0.43891170\n",
            "Iteration 121, loss = 0.43960653\n",
            "Iteration 122, loss = 0.43694045\n",
            "Iteration 123, loss = 0.43889650\n",
            "Iteration 124, loss = 0.43454125\n",
            "Iteration 125, loss = 0.43861521\n",
            "Iteration 126, loss = 0.43830084\n",
            "Iteration 127, loss = 0.43613505\n",
            "Iteration 128, loss = 0.43321112\n",
            "Iteration 129, loss = 0.43539795\n",
            "Iteration 130, loss = 0.44397209\n",
            "Iteration 131, loss = 0.44062272\n",
            "Iteration 132, loss = 0.43195807\n",
            "Iteration 133, loss = 0.43091520\n",
            "Iteration 134, loss = 0.43959173\n",
            "Iteration 135, loss = 0.43668567\n",
            "Iteration 136, loss = 0.43168497\n",
            "Iteration 137, loss = 0.43231520\n",
            "Iteration 138, loss = 0.43031772\n",
            "Iteration 139, loss = 0.42888656\n",
            "Iteration 140, loss = 0.42841903\n",
            "Iteration 141, loss = 0.42845926\n",
            "Iteration 142, loss = 0.42725982\n",
            "Iteration 143, loss = 0.42694518\n",
            "Iteration 144, loss = 0.42830990\n",
            "Iteration 145, loss = 0.42598928\n",
            "Iteration 146, loss = 0.42606400\n",
            "Iteration 147, loss = 0.42526070\n",
            "Iteration 148, loss = 0.42837360\n",
            "Iteration 149, loss = 0.42524625\n",
            "Iteration 150, loss = 0.42592335\n",
            "Iteration 151, loss = 0.42539587\n",
            "Iteration 152, loss = 0.42339397\n",
            "Iteration 153, loss = 0.42541809\n",
            "Iteration 154, loss = 0.43325371\n",
            "Iteration 155, loss = 0.42889631\n",
            "Iteration 156, loss = 0.42062899\n",
            "Iteration 157, loss = 0.42622508\n",
            "Iteration 158, loss = 0.42184693\n",
            "Iteration 159, loss = 0.42221885\n",
            "Iteration 160, loss = 0.42111162\n",
            "Iteration 161, loss = 0.42276413\n",
            "Iteration 162, loss = 0.42030440\n",
            "Iteration 163, loss = 0.41955896\n",
            "Iteration 164, loss = 0.42153527\n",
            "Iteration 165, loss = 0.41985659\n",
            "Iteration 166, loss = 0.41917156\n",
            "Iteration 167, loss = 0.41964709\n",
            "Iteration 168, loss = 0.41844396\n",
            "Iteration 169, loss = 0.41849854\n",
            "Iteration 170, loss = 0.41911341\n",
            "Iteration 171, loss = 0.41972857\n",
            "Iteration 172, loss = 0.42439986\n",
            "Iteration 173, loss = 0.41717209\n",
            "Iteration 174, loss = 0.41689636\n",
            "Iteration 175, loss = 0.41727243\n",
            "Iteration 176, loss = 0.41713074\n",
            "Iteration 177, loss = 0.41798556\n",
            "Iteration 178, loss = 0.41623165\n",
            "Iteration 179, loss = 0.41517951\n",
            "Iteration 180, loss = 0.41632077\n",
            "Iteration 181, loss = 0.41499050\n",
            "Iteration 182, loss = 0.41670831\n",
            "Iteration 183, loss = 0.41466631\n",
            "Iteration 184, loss = 0.41492926\n",
            "Iteration 185, loss = 0.41345704\n",
            "Iteration 186, loss = 0.41388509\n",
            "Iteration 187, loss = 0.41347579\n",
            "Iteration 188, loss = 0.41369771\n",
            "Iteration 189, loss = 0.41303380\n",
            "Iteration 190, loss = 0.41192130\n",
            "Iteration 191, loss = 0.41273782\n",
            "Iteration 192, loss = 0.41229428\n",
            "Iteration 193, loss = 0.41296596\n",
            "Iteration 194, loss = 0.41164555\n",
            "Iteration 195, loss = 0.41251446\n",
            "Iteration 196, loss = 0.40995515\n",
            "Iteration 197, loss = 0.41079664\n",
            "Iteration 198, loss = 0.40990261\n",
            "Iteration 199, loss = 0.41003481\n",
            "Iteration 200, loss = 0.40986854\n",
            "Iteration 1, loss = 0.68296569\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss = 0.65015920\n",
            "Iteration 3, loss = 0.62147266\n",
            "Iteration 4, loss = 0.64048436\n",
            "Iteration 5, loss = 0.70457191\n",
            "Iteration 6, loss = 0.60780178\n",
            "Iteration 7, loss = 0.57074819\n",
            "Iteration 8, loss = 0.57484922\n",
            "Iteration 9, loss = 0.56487212\n",
            "Iteration 10, loss = 0.54169947\n",
            "Iteration 11, loss = 0.53540293\n",
            "Iteration 12, loss = 0.53082360\n",
            "Iteration 13, loss = 0.52185987\n",
            "Iteration 14, loss = 0.50642571\n",
            "Iteration 15, loss = 0.54967728\n",
            "Iteration 16, loss = 0.52734963\n",
            "Iteration 17, loss = 0.51270597\n",
            "Iteration 18, loss = 0.50905967\n",
            "Iteration 19, loss = 0.49678243\n",
            "Iteration 20, loss = 0.46828888\n",
            "Iteration 21, loss = 0.47091625\n",
            "Iteration 22, loss = 0.47440292\n",
            "Iteration 23, loss = 0.47588669\n",
            "Iteration 24, loss = 0.45332359\n",
            "Iteration 25, loss = 0.48031753\n",
            "Iteration 26, loss = 0.46475605\n",
            "Iteration 27, loss = 0.51446175\n",
            "Iteration 28, loss = 0.45719778\n",
            "Iteration 29, loss = 0.45160977\n",
            "Iteration 30, loss = 0.47396939\n",
            "Iteration 31, loss = 0.54568671\n",
            "Iteration 32, loss = 0.43407028\n",
            "Iteration 33, loss = 0.45281028\n",
            "Iteration 34, loss = 0.43817242\n",
            "Iteration 35, loss = 0.47505703\n",
            "Iteration 36, loss = 0.54611346\n",
            "Iteration 37, loss = 0.47077119\n",
            "Iteration 38, loss = 0.45333337\n",
            "Iteration 39, loss = 0.47505339\n",
            "Iteration 40, loss = 0.45537409\n",
            "Iteration 41, loss = 0.45204573\n",
            "Iteration 42, loss = 0.50416094\n",
            "Iteration 43, loss = 0.46158244\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.76268280\n",
            "Iteration 2, loss = 0.63096640\n",
            "Iteration 3, loss = 0.64613797\n",
            "Iteration 4, loss = 0.63878432\n",
            "Iteration 5, loss = 0.61893349\n",
            "Iteration 6, loss = 0.67115869\n",
            "Iteration 7, loss = 0.56638361\n",
            "Iteration 8, loss = 0.58465288\n",
            "Iteration 9, loss = 0.57177801\n",
            "Iteration 10, loss = 0.61124436\n",
            "Iteration 11, loss = 0.55275247\n",
            "Iteration 12, loss = 0.53221532\n",
            "Iteration 13, loss = 0.54675191\n",
            "Iteration 14, loss = 0.58806105\n",
            "Iteration 15, loss = 0.50885917\n",
            "Iteration 16, loss = 0.54415925\n",
            "Iteration 17, loss = 0.51682801\n",
            "Iteration 18, loss = 0.53935933\n",
            "Iteration 19, loss = 0.51049249\n",
            "Iteration 20, loss = 0.47292226\n",
            "Iteration 21, loss = 0.61820748\n",
            "Iteration 22, loss = 0.51176965\n",
            "Iteration 23, loss = 0.47408142\n",
            "Iteration 24, loss = 0.48744054\n",
            "Iteration 25, loss = 0.45609397\n",
            "Iteration 26, loss = 0.45266852\n",
            "Iteration 27, loss = 0.45813569\n",
            "Iteration 28, loss = 0.49696672\n",
            "Iteration 29, loss = 0.54370818\n",
            "Iteration 30, loss = 0.46125734\n",
            "Iteration 31, loss = 0.43816352\n",
            "Iteration 32, loss = 0.42134334\n",
            "Iteration 33, loss = 0.44654430\n",
            "Iteration 34, loss = 0.45138835\n",
            "Iteration 35, loss = 0.44568144\n",
            "Iteration 36, loss = 0.44707411\n",
            "Iteration 37, loss = 0.46642554\n",
            "Iteration 38, loss = 0.43440756\n",
            "Iteration 39, loss = 0.46113110\n",
            "Iteration 40, loss = 0.44989426\n",
            "Iteration 41, loss = 0.46041642\n",
            "Iteration 42, loss = 0.41386349\n",
            "Iteration 43, loss = 0.41924166\n",
            "Iteration 44, loss = 0.46041275\n",
            "Iteration 45, loss = 0.49934169\n",
            "Iteration 46, loss = 0.48600963\n",
            "Iteration 47, loss = 0.41445050\n",
            "Iteration 48, loss = 0.41744167\n",
            "Iteration 49, loss = 0.45001567\n",
            "Iteration 50, loss = 0.44269697\n",
            "Iteration 51, loss = 0.43066249\n",
            "Iteration 52, loss = 0.41243536\n",
            "Iteration 53, loss = 0.43905296\n",
            "Iteration 54, loss = 0.42487455\n",
            "Iteration 55, loss = 0.42745971\n",
            "Iteration 56, loss = 0.42022763\n",
            "Iteration 57, loss = 0.44503258\n",
            "Iteration 58, loss = 0.40593551\n",
            "Iteration 59, loss = 0.40349299\n",
            "Iteration 60, loss = 0.41047647\n",
            "Iteration 61, loss = 0.42636816\n",
            "Iteration 62, loss = 0.40525884\n",
            "Iteration 63, loss = 0.43643116\n",
            "Iteration 64, loss = 0.43432120\n",
            "Iteration 65, loss = 0.40874806\n",
            "Iteration 66, loss = 0.42731610\n",
            "Iteration 67, loss = 0.42833511\n",
            "Iteration 68, loss = 0.41445192\n",
            "Iteration 69, loss = 0.40388755\n",
            "Iteration 70, loss = 0.39292981\n",
            "Iteration 71, loss = 0.39150848\n",
            "Iteration 72, loss = 0.40892147\n",
            "Iteration 73, loss = 0.47960151\n",
            "Iteration 74, loss = 0.39251092\n",
            "Iteration 75, loss = 0.40740639\n",
            "Iteration 76, loss = 0.41152517\n",
            "Iteration 77, loss = 0.41148537\n",
            "Iteration 78, loss = 0.46881844\n",
            "Iteration 79, loss = 0.41765394\n",
            "Iteration 80, loss = 0.39752377\n",
            "Iteration 81, loss = 0.39888081\n",
            "Iteration 82, loss = 0.39091580\n",
            "Iteration 83, loss = 0.39523897\n",
            "Iteration 84, loss = 0.41334116\n",
            "Iteration 85, loss = 0.40362561\n",
            "Iteration 86, loss = 0.39240169\n",
            "Iteration 87, loss = 0.38054460\n",
            "Iteration 88, loss = 0.38558115\n",
            "Iteration 89, loss = 0.41082536\n",
            "Iteration 90, loss = 0.39466808\n",
            "Iteration 91, loss = 0.41352247\n",
            "Iteration 92, loss = 0.42427412\n",
            "Iteration 93, loss = 0.39436553\n",
            "Iteration 94, loss = 0.44566721\n",
            "Iteration 95, loss = 0.41585512\n",
            "Iteration 96, loss = 0.40927840\n",
            "Iteration 97, loss = 0.42381339\n",
            "Iteration 98, loss = 0.39910404\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.72748175\n",
            "Iteration 2, loss = 0.64675904\n",
            "Iteration 3, loss = 0.60991635\n",
            "Iteration 4, loss = 0.58885480\n",
            "Iteration 5, loss = 0.56813355\n",
            "Iteration 6, loss = 0.55443234\n",
            "Iteration 7, loss = 0.59293346\n",
            "Iteration 8, loss = 0.55462757\n",
            "Iteration 9, loss = 0.54794568\n",
            "Iteration 10, loss = 0.59613540\n",
            "Iteration 11, loss = 0.53655810\n",
            "Iteration 12, loss = 0.52968792\n",
            "Iteration 13, loss = 0.51527496\n",
            "Iteration 14, loss = 0.50068282\n",
            "Iteration 15, loss = 0.51038947\n",
            "Iteration 16, loss = 0.47963973\n",
            "Iteration 17, loss = 0.55065898\n",
            "Iteration 18, loss = 0.53689071\n",
            "Iteration 19, loss = 0.52158007\n",
            "Iteration 20, loss = 0.48810004\n",
            "Iteration 21, loss = 0.48669515\n",
            "Iteration 22, loss = 0.46268308\n",
            "Iteration 23, loss = 0.48517629\n",
            "Iteration 24, loss = 0.45733523\n",
            "Iteration 25, loss = 0.45568947\n",
            "Iteration 26, loss = 0.46577508\n",
            "Iteration 27, loss = 0.44475490\n",
            "Iteration 28, loss = 0.45234315\n",
            "Iteration 29, loss = 0.43104073\n",
            "Iteration 30, loss = 0.51183916\n",
            "Iteration 31, loss = 0.48229875\n",
            "Iteration 32, loss = 0.45582093\n",
            "Iteration 33, loss = 0.46172602\n",
            "Iteration 34, loss = 0.43868246\n",
            "Iteration 35, loss = 0.44722956\n",
            "Iteration 36, loss = 0.42662091\n",
            "Iteration 37, loss = 0.41202522\n",
            "Iteration 38, loss = 0.43443363\n",
            "Iteration 39, loss = 0.43071215\n",
            "Iteration 40, loss = 0.43315543\n",
            "Iteration 41, loss = 0.49460800\n",
            "Iteration 42, loss = 0.45983312\n",
            "Iteration 43, loss = 0.43298969\n",
            "Iteration 44, loss = 0.41786262\n",
            "Iteration 45, loss = 0.41631993\n",
            "Iteration 46, loss = 0.41745328\n",
            "Iteration 47, loss = 0.41451273\n",
            "Iteration 48, loss = 0.43090507\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71571320\n",
            "Iteration 2, loss = 0.67434414\n",
            "Iteration 3, loss = 0.62571278\n",
            "Iteration 4, loss = 0.58690589\n",
            "Iteration 5, loss = 0.61598225\n",
            "Iteration 6, loss = 0.68241754\n",
            "Iteration 7, loss = 0.58438019\n",
            "Iteration 8, loss = 0.57187179\n",
            "Iteration 9, loss = 0.55225413\n",
            "Iteration 10, loss = 0.56758911\n",
            "Iteration 11, loss = 0.57916486\n",
            "Iteration 12, loss = 0.57285397\n",
            "Iteration 13, loss = 0.55462287\n",
            "Iteration 14, loss = 0.53937444\n",
            "Iteration 15, loss = 0.50017603\n",
            "Iteration 16, loss = 0.53631823\n",
            "Iteration 17, loss = 0.48699873\n",
            "Iteration 18, loss = 0.51922693\n",
            "Iteration 19, loss = 0.49781780\n",
            "Iteration 20, loss = 0.51722988\n",
            "Iteration 21, loss = 0.53184402\n",
            "Iteration 22, loss = 0.47935166\n",
            "Iteration 23, loss = 0.51041231\n",
            "Iteration 24, loss = 0.47030298\n",
            "Iteration 25, loss = 0.54466421\n",
            "Iteration 26, loss = 0.53590333\n",
            "Iteration 27, loss = 0.46934020\n",
            "Iteration 28, loss = 0.45689806\n",
            "Iteration 29, loss = 0.49555183\n",
            "Iteration 30, loss = 0.46982203\n",
            "Iteration 31, loss = 0.52734370\n",
            "Iteration 32, loss = 0.48341501\n",
            "Iteration 33, loss = 0.47502804\n",
            "Iteration 34, loss = 0.45519012\n",
            "Iteration 35, loss = 0.46604711\n",
            "Iteration 36, loss = 0.44855564\n",
            "Iteration 37, loss = 0.46124419\n",
            "Iteration 38, loss = 0.49333762\n",
            "Iteration 39, loss = 0.53219989\n",
            "Iteration 40, loss = 0.47897720\n",
            "Iteration 41, loss = 0.44654805\n",
            "Iteration 42, loss = 0.44877194\n",
            "Iteration 43, loss = 0.45559079\n",
            "Iteration 44, loss = 0.53405557\n",
            "Iteration 45, loss = 0.69327867\n",
            "Iteration 46, loss = 0.45092539\n",
            "Iteration 47, loss = 0.43345471\n",
            "Iteration 48, loss = 0.45218057\n",
            "Iteration 49, loss = 0.47845361\n",
            "Iteration 50, loss = 0.44722161\n",
            "Iteration 51, loss = 0.42914931\n",
            "Iteration 52, loss = 0.43352456\n",
            "Iteration 53, loss = 0.44277943\n",
            "Iteration 54, loss = 0.45996205\n",
            "Iteration 55, loss = 0.50449679\n",
            "Iteration 56, loss = 0.44663015\n",
            "Iteration 57, loss = 0.43557164\n",
            "Iteration 58, loss = 0.46616693\n",
            "Iteration 59, loss = 0.48926707\n",
            "Iteration 60, loss = 0.49434068\n",
            "Iteration 61, loss = 0.43417707\n",
            "Iteration 62, loss = 0.45572174\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75155348\n",
            "Iteration 2, loss = 0.63286420\n",
            "Iteration 3, loss = 0.60960807\n",
            "Iteration 4, loss = 0.57275752\n",
            "Iteration 5, loss = 0.58403707\n",
            "Iteration 6, loss = 0.53650589\n",
            "Iteration 7, loss = 0.53027111\n",
            "Iteration 8, loss = 0.52863990\n",
            "Iteration 9, loss = 0.52626993\n",
            "Iteration 10, loss = 0.51749980\n",
            "Iteration 11, loss = 0.55363574\n",
            "Iteration 12, loss = 0.49171702\n",
            "Iteration 13, loss = 0.54883365\n",
            "Iteration 14, loss = 0.50775361\n",
            "Iteration 15, loss = 0.49334195\n",
            "Iteration 16, loss = 0.48114299\n",
            "Iteration 17, loss = 0.48544923\n",
            "Iteration 18, loss = 0.48070892\n",
            "Iteration 19, loss = 0.48656056\n",
            "Iteration 20, loss = 0.46726436\n",
            "Iteration 21, loss = 0.52394516\n",
            "Iteration 22, loss = 0.47572613\n",
            "Iteration 23, loss = 0.49408797\n",
            "Iteration 24, loss = 0.46374611\n",
            "Iteration 25, loss = 0.48113081\n",
            "Iteration 26, loss = 0.46992808\n",
            "Iteration 27, loss = 0.46628113\n",
            "Iteration 28, loss = 0.46226865\n",
            "Iteration 29, loss = 0.47396676\n",
            "Iteration 30, loss = 0.50888822\n",
            "Iteration 31, loss = 0.45039571\n",
            "Iteration 32, loss = 0.47088961\n",
            "Iteration 33, loss = 0.49884753\n",
            "Iteration 34, loss = 0.44776880\n",
            "Iteration 35, loss = 0.45192938\n",
            "Iteration 36, loss = 0.49461201\n",
            "Iteration 37, loss = 0.48300336\n",
            "Iteration 38, loss = 0.43131664\n",
            "Iteration 39, loss = 0.45266559\n",
            "Iteration 40, loss = 0.45103315\n",
            "Iteration 41, loss = 0.46094195\n",
            "Iteration 42, loss = 0.45538998\n",
            "Iteration 43, loss = 0.42339672\n",
            "Iteration 44, loss = 0.43913626\n",
            "Iteration 45, loss = 0.43592771\n",
            "Iteration 46, loss = 0.46526130\n",
            "Iteration 47, loss = 0.44071888\n",
            "Iteration 48, loss = 0.43681959\n",
            "Iteration 49, loss = 0.43364288\n",
            "Iteration 50, loss = 0.42540619\n",
            "Iteration 51, loss = 0.43650016\n",
            "Iteration 52, loss = 0.43409234\n",
            "Iteration 53, loss = 0.47469410\n",
            "Iteration 54, loss = 0.42791013\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75629192\n",
            "Iteration 2, loss = 0.62482444\n",
            "Iteration 3, loss = 0.61911812\n",
            "Iteration 4, loss = 0.61057414\n",
            "Iteration 5, loss = 0.60340904\n",
            "Iteration 6, loss = 0.60112259\n",
            "Iteration 7, loss = 0.59569766\n",
            "Iteration 8, loss = 0.59283806\n",
            "Iteration 9, loss = 0.59161833\n",
            "Iteration 10, loss = 0.58970921\n",
            "Iteration 11, loss = 0.58713963\n",
            "Iteration 12, loss = 0.58584759\n",
            "Iteration 13, loss = 0.58368515\n",
            "Iteration 14, loss = 0.58407747\n",
            "Iteration 15, loss = 0.58168104\n",
            "Iteration 16, loss = 0.58136911\n",
            "Iteration 17, loss = 0.57948403\n",
            "Iteration 18, loss = 0.57917764\n",
            "Iteration 19, loss = 0.57950934\n",
            "Iteration 20, loss = 0.57518730\n",
            "Iteration 21, loss = 0.57819049\n",
            "Iteration 22, loss = 0.57618881\n",
            "Iteration 23, loss = 0.57472142\n",
            "Iteration 24, loss = 0.57174024\n",
            "Iteration 25, loss = 0.57312063\n",
            "Iteration 26, loss = 0.57331098\n",
            "Iteration 27, loss = 0.56951305\n",
            "Iteration 28, loss = 0.57194959\n",
            "Iteration 29, loss = 0.56848299\n",
            "Iteration 30, loss = 0.56748358\n",
            "Iteration 31, loss = 0.56794919\n",
            "Iteration 32, loss = 0.56590352\n",
            "Iteration 33, loss = 0.56303696\n",
            "Iteration 34, loss = 0.56553827\n",
            "Iteration 35, loss = 0.56304965\n",
            "Iteration 36, loss = 0.56255082\n",
            "Iteration 37, loss = 0.56009935\n",
            "Iteration 38, loss = 0.56115422\n",
            "Iteration 39, loss = 0.55861457\n",
            "Iteration 40, loss = 0.55790722\n",
            "Iteration 41, loss = 0.55554514\n",
            "Iteration 42, loss = 0.55517939\n",
            "Iteration 43, loss = 0.55393002\n",
            "Iteration 44, loss = 0.55503101\n",
            "Iteration 45, loss = 0.55177282\n",
            "Iteration 46, loss = 0.56157193\n",
            "Iteration 47, loss = 0.55283552\n",
            "Iteration 48, loss = 0.55151521\n",
            "Iteration 49, loss = 0.55205401\n",
            "Iteration 50, loss = 0.54716354\n",
            "Iteration 51, loss = 0.54957797\n",
            "Iteration 52, loss = 0.54350860\n",
            "Iteration 53, loss = 0.54592946\n",
            "Iteration 54, loss = 0.54220111\n",
            "Iteration 55, loss = 0.54094072\n",
            "Iteration 56, loss = 0.54285096\n",
            "Iteration 57, loss = 0.54120227\n",
            "Iteration 58, loss = 0.54029694\n",
            "Iteration 59, loss = 0.53862370\n",
            "Iteration 60, loss = 0.53942985\n",
            "Iteration 61, loss = 0.54217449\n",
            "Iteration 62, loss = 0.53690620\n",
            "Iteration 63, loss = 0.53539698\n",
            "Iteration 64, loss = 0.53122654\n",
            "Iteration 65, loss = 0.53566086\n",
            "Iteration 66, loss = 0.53258257\n",
            "Iteration 67, loss = 0.53074487\n",
            "Iteration 68, loss = 0.52643524\n",
            "Iteration 69, loss = 0.53134991\n",
            "Iteration 70, loss = 0.53374907\n",
            "Iteration 71, loss = 0.52406626\n",
            "Iteration 72, loss = 0.52784326\n",
            "Iteration 73, loss = 0.52045868\n",
            "Iteration 74, loss = 0.52623897\n",
            "Iteration 75, loss = 0.52002329\n",
            "Iteration 76, loss = 0.52525195\n",
            "Iteration 77, loss = 0.51810768\n",
            "Iteration 78, loss = 0.51878973\n",
            "Iteration 79, loss = 0.51859426\n",
            "Iteration 80, loss = 0.51525401\n",
            "Iteration 81, loss = 0.51238957\n",
            "Iteration 82, loss = 0.51446205\n",
            "Iteration 83, loss = 0.51565835\n",
            "Iteration 84, loss = 0.51527935\n",
            "Iteration 85, loss = 0.51473451\n",
            "Iteration 86, loss = 0.52013223\n",
            "Iteration 87, loss = 0.51277873\n",
            "Iteration 88, loss = 0.50867954\n",
            "Iteration 89, loss = 0.50737478\n",
            "Iteration 90, loss = 0.50649359\n",
            "Iteration 91, loss = 0.50225385\n",
            "Iteration 92, loss = 0.50490991\n",
            "Iteration 93, loss = 0.50551026\n",
            "Iteration 94, loss = 0.49823040\n",
            "Iteration 95, loss = 0.50159697\n",
            "Iteration 96, loss = 0.50252453\n",
            "Iteration 97, loss = 0.49566891\n",
            "Iteration 98, loss = 0.50467821\n",
            "Iteration 99, loss = 0.49673800\n",
            "Iteration 100, loss = 0.49670646\n",
            "Iteration 101, loss = 0.49538407\n",
            "Iteration 102, loss = 0.49625423\n",
            "Iteration 103, loss = 0.49054647\n",
            "Iteration 104, loss = 0.49911289\n",
            "Iteration 105, loss = 0.49299165\n",
            "Iteration 106, loss = 0.48991757\n",
            "Iteration 107, loss = 0.48844344\n",
            "Iteration 108, loss = 0.48742432\n",
            "Iteration 109, loss = 0.48294374\n",
            "Iteration 110, loss = 0.49471209\n",
            "Iteration 111, loss = 0.48837914\n",
            "Iteration 112, loss = 0.48981168\n",
            "Iteration 113, loss = 0.48483025\n",
            "Iteration 114, loss = 0.48275316\n",
            "Iteration 115, loss = 0.47708093\n",
            "Iteration 116, loss = 0.48373319\n",
            "Iteration 117, loss = 0.48236604\n",
            "Iteration 118, loss = 0.48152447\n",
            "Iteration 119, loss = 0.47433483\n",
            "Iteration 120, loss = 0.48085006\n",
            "Iteration 121, loss = 0.47247092\n",
            "Iteration 122, loss = 0.48133513\n",
            "Iteration 123, loss = 0.47795148\n",
            "Iteration 124, loss = 0.47815461\n",
            "Iteration 125, loss = 0.47247472\n",
            "Iteration 126, loss = 0.46836857\n",
            "Iteration 127, loss = 0.47294288\n",
            "Iteration 128, loss = 0.46711434\n",
            "Iteration 129, loss = 0.47543365\n",
            "Iteration 130, loss = 0.46668817\n",
            "Iteration 131, loss = 0.47512405\n",
            "Iteration 132, loss = 0.46858345\n",
            "Iteration 133, loss = 0.46675885\n",
            "Iteration 134, loss = 0.47256664\n",
            "Iteration 135, loss = 0.46918030\n",
            "Iteration 136, loss = 0.47868587\n",
            "Iteration 137, loss = 0.47308619\n",
            "Iteration 138, loss = 0.45718611\n",
            "Iteration 139, loss = 0.47943968\n",
            "Iteration 140, loss = 0.46435200\n",
            "Iteration 141, loss = 0.46685959\n",
            "Iteration 142, loss = 0.46638489\n",
            "Iteration 143, loss = 0.46258570\n",
            "Iteration 144, loss = 0.46734390\n",
            "Iteration 145, loss = 0.45951884\n",
            "Iteration 146, loss = 0.45838648\n",
            "Iteration 147, loss = 0.45846732\n",
            "Iteration 148, loss = 0.47760333\n",
            "Iteration 149, loss = 0.46486072\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.73156602\n",
            "Iteration 2, loss = 0.62061671\n",
            "Iteration 3, loss = 0.60750113\n",
            "Iteration 4, loss = 0.59894775\n",
            "Iteration 5, loss = 0.59326257\n",
            "Iteration 6, loss = 0.59067708\n",
            "Iteration 7, loss = 0.58893633\n",
            "Iteration 8, loss = 0.58682263\n",
            "Iteration 9, loss = 0.58956220\n",
            "Iteration 10, loss = 0.58253109\n",
            "Iteration 11, loss = 0.58149142\n",
            "Iteration 12, loss = 0.57937228\n",
            "Iteration 13, loss = 0.57948815\n",
            "Iteration 14, loss = 0.57609385\n",
            "Iteration 15, loss = 0.57772767\n",
            "Iteration 16, loss = 0.57575674\n",
            "Iteration 17, loss = 0.57304991\n",
            "Iteration 18, loss = 0.57365662\n",
            "Iteration 19, loss = 0.57147552\n",
            "Iteration 20, loss = 0.57056473\n",
            "Iteration 21, loss = 0.56808360\n",
            "Iteration 22, loss = 0.56792076\n",
            "Iteration 23, loss = 0.56510067\n",
            "Iteration 24, loss = 0.56712084\n",
            "Iteration 25, loss = 0.56592758\n",
            "Iteration 26, loss = 0.56335078\n",
            "Iteration 27, loss = 0.56133250\n",
            "Iteration 28, loss = 0.56190018\n",
            "Iteration 29, loss = 0.56014915\n",
            "Iteration 30, loss = 0.55895918\n",
            "Iteration 31, loss = 0.55967447\n",
            "Iteration 32, loss = 0.55680844\n",
            "Iteration 33, loss = 0.55786485\n",
            "Iteration 34, loss = 0.55865151\n",
            "Iteration 35, loss = 0.55757539\n",
            "Iteration 36, loss = 0.55637503\n",
            "Iteration 37, loss = 0.55707413\n",
            "Iteration 38, loss = 0.55183615\n",
            "Iteration 39, loss = 0.55411995\n",
            "Iteration 40, loss = 0.55056264\n",
            "Iteration 41, loss = 0.54854379\n",
            "Iteration 42, loss = 0.54975764\n",
            "Iteration 43, loss = 0.54798160\n",
            "Iteration 44, loss = 0.54832784\n",
            "Iteration 45, loss = 0.54679163\n",
            "Iteration 46, loss = 0.54494630\n",
            "Iteration 47, loss = 0.54804194\n",
            "Iteration 48, loss = 0.55070429\n",
            "Iteration 49, loss = 0.54465459\n",
            "Iteration 50, loss = 0.54306582\n",
            "Iteration 51, loss = 0.54320515\n",
            "Iteration 52, loss = 0.54115645\n",
            "Iteration 53, loss = 0.54022519\n",
            "Iteration 54, loss = 0.53743364\n",
            "Iteration 55, loss = 0.53969615\n",
            "Iteration 56, loss = 0.53691139\n",
            "Iteration 57, loss = 0.53802164\n",
            "Iteration 58, loss = 0.53673313\n",
            "Iteration 59, loss = 0.53614513\n",
            "Iteration 60, loss = 0.53663499\n",
            "Iteration 61, loss = 0.53217270\n",
            "Iteration 62, loss = 0.53572323\n",
            "Iteration 63, loss = 0.53731743\n",
            "Iteration 64, loss = 0.53339269\n",
            "Iteration 65, loss = 0.53184077\n",
            "Iteration 66, loss = 0.52975050\n",
            "Iteration 67, loss = 0.52889993\n",
            "Iteration 68, loss = 0.52827745\n",
            "Iteration 69, loss = 0.53181796\n",
            "Iteration 70, loss = 0.53265202\n",
            "Iteration 71, loss = 0.53690433\n",
            "Iteration 72, loss = 0.52379950\n",
            "Iteration 73, loss = 0.52640527\n",
            "Iteration 74, loss = 0.52564637\n",
            "Iteration 75, loss = 0.51952301\n",
            "Iteration 76, loss = 0.52281898\n",
            "Iteration 77, loss = 0.51779382\n",
            "Iteration 78, loss = 0.51835015\n",
            "Iteration 79, loss = 0.52032589\n",
            "Iteration 80, loss = 0.51754248\n",
            "Iteration 81, loss = 0.51815891\n",
            "Iteration 82, loss = 0.51356390\n",
            "Iteration 83, loss = 0.51792458\n",
            "Iteration 84, loss = 0.51692022\n",
            "Iteration 85, loss = 0.51413190\n",
            "Iteration 86, loss = 0.50796973\n",
            "Iteration 87, loss = 0.50687011\n",
            "Iteration 88, loss = 0.50916018\n",
            "Iteration 89, loss = 0.51261378\n",
            "Iteration 90, loss = 0.50492452\n",
            "Iteration 91, loss = 0.51019014\n",
            "Iteration 92, loss = 0.50343375\n",
            "Iteration 93, loss = 0.50353776\n",
            "Iteration 94, loss = 0.51227470\n",
            "Iteration 95, loss = 0.50314382\n",
            "Iteration 96, loss = 0.50213382\n",
            "Iteration 97, loss = 0.51312273\n",
            "Iteration 98, loss = 0.50131181\n",
            "Iteration 99, loss = 0.50053262\n",
            "Iteration 100, loss = 0.50100480\n",
            "Iteration 101, loss = 0.49891875\n",
            "Iteration 102, loss = 0.50240823\n",
            "Iteration 103, loss = 0.49617583\n",
            "Iteration 104, loss = 0.49647600\n",
            "Iteration 105, loss = 0.50025810\n",
            "Iteration 106, loss = 0.50135610\n",
            "Iteration 107, loss = 0.49739326\n",
            "Iteration 108, loss = 0.49236733\n",
            "Iteration 109, loss = 0.50042453\n",
            "Iteration 110, loss = 0.49237687\n",
            "Iteration 111, loss = 0.48777428\n",
            "Iteration 112, loss = 0.48935833\n",
            "Iteration 113, loss = 0.48629556\n",
            "Iteration 114, loss = 0.48703852\n",
            "Iteration 115, loss = 0.48682214\n",
            "Iteration 116, loss = 0.49486341\n",
            "Iteration 117, loss = 0.48650465\n",
            "Iteration 118, loss = 0.48780045\n",
            "Iteration 119, loss = 0.48235555\n",
            "Iteration 120, loss = 0.48396763\n",
            "Iteration 121, loss = 0.47810546\n",
            "Iteration 122, loss = 0.47911412\n",
            "Iteration 123, loss = 0.47914913\n",
            "Iteration 124, loss = 0.47763063\n",
            "Iteration 125, loss = 0.48996036\n",
            "Iteration 126, loss = 0.47891223\n",
            "Iteration 127, loss = 0.48297267\n",
            "Iteration 128, loss = 0.48375056\n",
            "Iteration 129, loss = 0.48932172\n",
            "Iteration 130, loss = 0.49555350\n",
            "Iteration 131, loss = 0.48119905\n",
            "Iteration 132, loss = 0.47394824\n",
            "Iteration 133, loss = 0.48779431\n",
            "Iteration 134, loss = 0.47143441\n",
            "Iteration 135, loss = 0.47195078\n",
            "Iteration 136, loss = 0.47774311\n",
            "Iteration 137, loss = 0.46980388\n",
            "Iteration 138, loss = 0.47151022\n",
            "Iteration 139, loss = 0.47065686\n",
            "Iteration 140, loss = 0.46597649\n",
            "Iteration 141, loss = 0.47741117\n",
            "Iteration 142, loss = 0.47305260\n",
            "Iteration 143, loss = 0.46192173\n",
            "Iteration 144, loss = 0.46059793\n",
            "Iteration 145, loss = 0.47986410\n",
            "Iteration 146, loss = 0.46788839\n",
            "Iteration 147, loss = 0.46628722\n",
            "Iteration 148, loss = 0.46195626\n",
            "Iteration 149, loss = 0.47151195\n",
            "Iteration 150, loss = 0.46807938\n",
            "Iteration 151, loss = 0.46985133\n",
            "Iteration 152, loss = 0.47160080\n",
            "Iteration 153, loss = 0.47250242\n",
            "Iteration 154, loss = 0.45978056\n",
            "Iteration 155, loss = 0.45468245\n",
            "Iteration 156, loss = 0.45563143\n",
            "Iteration 157, loss = 0.45968104\n",
            "Iteration 158, loss = 0.44831861\n",
            "Iteration 159, loss = 0.45364193\n",
            "Iteration 160, loss = 0.46910191\n",
            "Iteration 161, loss = 0.46183436\n",
            "Iteration 162, loss = 0.46902615\n",
            "Iteration 163, loss = 0.45205614\n",
            "Iteration 164, loss = 0.45150865\n",
            "Iteration 165, loss = 0.46115298\n",
            "Iteration 166, loss = 0.46263712\n",
            "Iteration 167, loss = 0.45284357\n",
            "Iteration 168, loss = 0.45583677\n",
            "Iteration 169, loss = 0.44473580\n",
            "Iteration 170, loss = 0.47205666\n",
            "Iteration 171, loss = 0.46567429\n",
            "Iteration 172, loss = 0.44577279\n",
            "Iteration 173, loss = 0.45547653\n",
            "Iteration 174, loss = 0.45959384\n",
            "Iteration 175, loss = 0.44740022\n",
            "Iteration 176, loss = 0.44895427\n",
            "Iteration 177, loss = 0.45443735\n",
            "Iteration 178, loss = 0.46660496\n",
            "Iteration 179, loss = 0.44807040\n",
            "Iteration 180, loss = 0.44359323\n",
            "Iteration 181, loss = 0.44691818\n",
            "Iteration 182, loss = 0.44442070\n",
            "Iteration 183, loss = 0.45493655\n",
            "Iteration 184, loss = 0.44941300\n",
            "Iteration 185, loss = 0.46904498\n",
            "Iteration 186, loss = 0.43586496\n",
            "Iteration 187, loss = 0.45801737\n",
            "Iteration 188, loss = 0.45060650\n",
            "Iteration 189, loss = 0.43962973\n",
            "Iteration 190, loss = 0.44965688\n",
            "Iteration 191, loss = 0.45739074\n",
            "Iteration 192, loss = 0.43897878\n",
            "Iteration 193, loss = 0.44018583\n",
            "Iteration 194, loss = 0.43353516\n",
            "Iteration 195, loss = 0.43551035\n",
            "Iteration 196, loss = 0.43269125\n",
            "Iteration 197, loss = 0.45217608\n",
            "Iteration 198, loss = 0.44783961\n",
            "Iteration 199, loss = 0.43682872\n",
            "Iteration 200, loss = 0.43480943\n",
            "Iteration 201, loss = 0.45163878\n",
            "Iteration 202, loss = 0.42743555\n",
            "Iteration 203, loss = 0.42382442\n",
            "Iteration 204, loss = 0.44144590\n",
            "Iteration 205, loss = 0.44591775\n",
            "Iteration 206, loss = 0.43099449\n",
            "Iteration 207, loss = 0.44621222\n",
            "Iteration 208, loss = 0.45540184\n",
            "Iteration 209, loss = 0.43856279\n",
            "Iteration 210, loss = 0.44168474\n",
            "Iteration 211, loss = 0.44254838\n",
            "Iteration 212, loss = 0.44430312\n",
            "Iteration 213, loss = 0.43412530\n",
            "Iteration 214, loss = 0.44138079\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.63484997\n",
            "Iteration 2, loss = 0.61454465\n",
            "Iteration 3, loss = 0.60626257\n",
            "Iteration 4, loss = 0.60478846\n",
            "Iteration 5, loss = 0.59595363\n",
            "Iteration 6, loss = 0.59481712\n",
            "Iteration 7, loss = 0.59427747\n",
            "Iteration 8, loss = 0.58971100\n",
            "Iteration 9, loss = 0.58802752\n",
            "Iteration 10, loss = 0.58529060\n",
            "Iteration 11, loss = 0.58348451\n",
            "Iteration 12, loss = 0.58225556\n",
            "Iteration 13, loss = 0.58199977\n",
            "Iteration 14, loss = 0.58088003\n",
            "Iteration 15, loss = 0.57872335\n",
            "Iteration 16, loss = 0.57713420\n",
            "Iteration 17, loss = 0.57644030\n",
            "Iteration 18, loss = 0.57507649\n",
            "Iteration 19, loss = 0.57612651\n",
            "Iteration 20, loss = 0.57370797\n",
            "Iteration 21, loss = 0.57196899\n",
            "Iteration 22, loss = 0.57240577\n",
            "Iteration 23, loss = 0.57180126\n",
            "Iteration 24, loss = 0.56963522\n",
            "Iteration 25, loss = 0.56627878\n",
            "Iteration 26, loss = 0.56662565\n",
            "Iteration 27, loss = 0.56538260\n",
            "Iteration 28, loss = 0.56364346\n",
            "Iteration 29, loss = 0.56286257\n",
            "Iteration 30, loss = 0.56202910\n",
            "Iteration 31, loss = 0.56085495\n",
            "Iteration 32, loss = 0.56101555\n",
            "Iteration 33, loss = 0.55895641\n",
            "Iteration 34, loss = 0.55995358\n",
            "Iteration 35, loss = 0.55782734\n",
            "Iteration 36, loss = 0.55771050\n",
            "Iteration 37, loss = 0.55628762\n",
            "Iteration 38, loss = 0.55485305\n",
            "Iteration 39, loss = 0.55464318\n",
            "Iteration 40, loss = 0.55534146\n",
            "Iteration 41, loss = 0.55213923\n",
            "Iteration 42, loss = 0.55178847\n",
            "Iteration 43, loss = 0.55228992\n",
            "Iteration 44, loss = 0.54894515\n",
            "Iteration 45, loss = 0.55016695\n",
            "Iteration 46, loss = 0.54914209\n",
            "Iteration 47, loss = 0.54664079\n",
            "Iteration 48, loss = 0.54667243\n",
            "Iteration 49, loss = 0.54522924\n",
            "Iteration 50, loss = 0.54601308\n",
            "Iteration 51, loss = 0.54275306\n",
            "Iteration 52, loss = 0.54155711\n",
            "Iteration 53, loss = 0.54387880\n",
            "Iteration 54, loss = 0.54003614\n",
            "Iteration 55, loss = 0.54036866\n",
            "Iteration 56, loss = 0.53946187\n",
            "Iteration 57, loss = 0.53739592\n",
            "Iteration 58, loss = 0.53786426\n",
            "Iteration 59, loss = 0.53618202\n",
            "Iteration 60, loss = 0.53483670\n",
            "Iteration 61, loss = 0.53608746\n",
            "Iteration 62, loss = 0.53474344\n",
            "Iteration 63, loss = 0.53176665\n",
            "Iteration 64, loss = 0.53158706\n",
            "Iteration 65, loss = 0.53426505\n",
            "Iteration 66, loss = 0.53006098\n",
            "Iteration 67, loss = 0.52987610\n",
            "Iteration 68, loss = 0.52986547\n",
            "Iteration 69, loss = 0.52799138\n",
            "Iteration 70, loss = 0.53038613\n",
            "Iteration 71, loss = 0.53100829\n",
            "Iteration 72, loss = 0.52700785\n",
            "Iteration 73, loss = 0.52611092\n",
            "Iteration 74, loss = 0.52756863\n",
            "Iteration 75, loss = 0.52302211\n",
            "Iteration 76, loss = 0.52022264\n",
            "Iteration 77, loss = 0.52612753\n",
            "Iteration 78, loss = 0.52432286\n",
            "Iteration 79, loss = 0.52120794\n",
            "Iteration 80, loss = 0.52021010\n",
            "Iteration 81, loss = 0.51920048\n",
            "Iteration 82, loss = 0.51563080\n",
            "Iteration 83, loss = 0.51459501\n",
            "Iteration 84, loss = 0.51758339\n",
            "Iteration 85, loss = 0.51235772\n",
            "Iteration 86, loss = 0.51423475\n",
            "Iteration 87, loss = 0.51282714\n",
            "Iteration 88, loss = 0.51438026\n",
            "Iteration 89, loss = 0.50877273\n",
            "Iteration 90, loss = 0.50951330\n",
            "Iteration 91, loss = 0.51239323\n",
            "Iteration 92, loss = 0.50753557\n",
            "Iteration 93, loss = 0.50526878\n",
            "Iteration 94, loss = 0.50749943\n",
            "Iteration 95, loss = 0.50358253\n",
            "Iteration 96, loss = 0.50537992\n",
            "Iteration 97, loss = 0.50004442\n",
            "Iteration 98, loss = 0.50010160\n",
            "Iteration 99, loss = 0.50245838\n",
            "Iteration 100, loss = 0.50030152\n",
            "Iteration 101, loss = 0.50186528\n",
            "Iteration 102, loss = 0.49727420\n",
            "Iteration 103, loss = 0.49881518\n",
            "Iteration 104, loss = 0.50490992\n",
            "Iteration 105, loss = 0.49622125\n",
            "Iteration 106, loss = 0.49677077\n",
            "Iteration 107, loss = 0.49119990\n",
            "Iteration 108, loss = 0.49129003\n",
            "Iteration 109, loss = 0.49344311\n",
            "Iteration 110, loss = 0.49098400\n",
            "Iteration 111, loss = 0.48791512\n",
            "Iteration 112, loss = 0.48668570\n",
            "Iteration 113, loss = 0.48861601\n",
            "Iteration 114, loss = 0.48574544\n",
            "Iteration 115, loss = 0.48640168\n",
            "Iteration 116, loss = 0.48860795\n",
            "Iteration 117, loss = 0.48175226\n",
            "Iteration 118, loss = 0.48645471\n",
            "Iteration 119, loss = 0.47910983\n",
            "Iteration 120, loss = 0.47869474\n",
            "Iteration 121, loss = 0.48006932\n",
            "Iteration 122, loss = 0.48606275\n",
            "Iteration 123, loss = 0.47992683\n",
            "Iteration 124, loss = 0.48376181\n",
            "Iteration 125, loss = 0.48829661\n",
            "Iteration 126, loss = 0.47178081\n",
            "Iteration 127, loss = 0.48865592\n",
            "Iteration 128, loss = 0.47430434\n",
            "Iteration 129, loss = 0.47269052\n",
            "Iteration 130, loss = 0.47439787\n",
            "Iteration 131, loss = 0.47901325\n",
            "Iteration 132, loss = 0.47349631\n",
            "Iteration 133, loss = 0.47452895\n",
            "Iteration 134, loss = 0.47189288\n",
            "Iteration 135, loss = 0.46546661\n",
            "Iteration 136, loss = 0.46527580\n",
            "Iteration 137, loss = 0.46738825\n",
            "Iteration 138, loss = 0.46885338\n",
            "Iteration 139, loss = 0.46642918\n",
            "Iteration 140, loss = 0.46286179\n",
            "Iteration 141, loss = 0.47078552\n",
            "Iteration 142, loss = 0.47066423\n",
            "Iteration 143, loss = 0.45787557\n",
            "Iteration 144, loss = 0.45671898\n",
            "Iteration 145, loss = 0.46686228\n",
            "Iteration 146, loss = 0.46900866\n",
            "Iteration 147, loss = 0.45831485\n",
            "Iteration 148, loss = 0.45852031\n",
            "Iteration 149, loss = 0.45927569\n",
            "Iteration 150, loss = 0.46008123\n",
            "Iteration 151, loss = 0.45946627\n",
            "Iteration 152, loss = 0.45776231\n",
            "Iteration 153, loss = 0.45326154\n",
            "Iteration 154, loss = 0.44863445\n",
            "Iteration 155, loss = 0.45831262\n",
            "Iteration 156, loss = 0.45595074\n",
            "Iteration 157, loss = 0.45350706\n",
            "Iteration 158, loss = 0.45433127\n",
            "Iteration 159, loss = 0.44814491\n",
            "Iteration 160, loss = 0.44415520\n",
            "Iteration 161, loss = 0.47802812\n",
            "Iteration 162, loss = 0.45011987\n",
            "Iteration 163, loss = 0.45028197\n",
            "Iteration 164, loss = 0.44585307\n",
            "Iteration 165, loss = 0.44321821\n",
            "Iteration 166, loss = 0.45848375\n",
            "Iteration 167, loss = 0.44638442\n",
            "Iteration 168, loss = 0.43897277\n",
            "Iteration 169, loss = 0.44236732\n",
            "Iteration 170, loss = 0.44097970\n",
            "Iteration 171, loss = 0.44109905\n",
            "Iteration 172, loss = 0.46052037\n",
            "Iteration 173, loss = 0.43845373\n",
            "Iteration 174, loss = 0.47595284\n",
            "Iteration 175, loss = 0.44467376\n",
            "Iteration 176, loss = 0.45318851\n",
            "Iteration 177, loss = 0.45124920\n",
            "Iteration 178, loss = 0.43777222\n",
            "Iteration 179, loss = 0.44768937\n",
            "Iteration 180, loss = 0.43900303\n",
            "Iteration 181, loss = 0.45505088\n",
            "Iteration 182, loss = 0.43516017\n",
            "Iteration 183, loss = 0.44334571\n",
            "Iteration 184, loss = 0.44618819\n",
            "Iteration 185, loss = 0.43208807\n",
            "Iteration 186, loss = 0.44809491\n",
            "Iteration 187, loss = 0.43274987\n",
            "Iteration 188, loss = 0.43294877\n",
            "Iteration 189, loss = 0.43909431\n",
            "Iteration 190, loss = 0.43873182\n",
            "Iteration 191, loss = 0.44713765\n",
            "Iteration 192, loss = 0.44485110\n",
            "Iteration 193, loss = 0.44230121\n",
            "Iteration 194, loss = 0.44820122\n",
            "Iteration 195, loss = 0.44378066\n",
            "Iteration 196, loss = 0.44537531\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70385729\n",
            "Iteration 2, loss = 0.63740351\n",
            "Iteration 3, loss = 0.62218027\n",
            "Iteration 4, loss = 0.61652409\n",
            "Iteration 5, loss = 0.61300448\n",
            "Iteration 6, loss = 0.61243183\n",
            "Iteration 7, loss = 0.60857035\n",
            "Iteration 8, loss = 0.61070856\n",
            "Iteration 9, loss = 0.60798314\n",
            "Iteration 10, loss = 0.60590675\n",
            "Iteration 11, loss = 0.60417188\n",
            "Iteration 12, loss = 0.60290208\n",
            "Iteration 13, loss = 0.60258961\n",
            "Iteration 14, loss = 0.60106134\n",
            "Iteration 15, loss = 0.60158169\n",
            "Iteration 16, loss = 0.59925494\n",
            "Iteration 17, loss = 0.59901772\n",
            "Iteration 18, loss = 0.59787035\n",
            "Iteration 19, loss = 0.59730922\n",
            "Iteration 20, loss = 0.59553019\n",
            "Iteration 21, loss = 0.59517998\n",
            "Iteration 22, loss = 0.59254265\n",
            "Iteration 23, loss = 0.59424644\n",
            "Iteration 24, loss = 0.59049559\n",
            "Iteration 25, loss = 0.59159353\n",
            "Iteration 26, loss = 0.59137142\n",
            "Iteration 27, loss = 0.58718275\n",
            "Iteration 28, loss = 0.58688197\n",
            "Iteration 29, loss = 0.58521182\n",
            "Iteration 30, loss = 0.58756163\n",
            "Iteration 31, loss = 0.58318901\n",
            "Iteration 32, loss = 0.58183924\n",
            "Iteration 33, loss = 0.58591290\n",
            "Iteration 34, loss = 0.58757901\n",
            "Iteration 35, loss = 0.58704447\n",
            "Iteration 36, loss = 0.58179249\n",
            "Iteration 37, loss = 0.58089250\n",
            "Iteration 38, loss = 0.57926226\n",
            "Iteration 39, loss = 0.57877887\n",
            "Iteration 40, loss = 0.57763668\n",
            "Iteration 41, loss = 0.57691831\n",
            "Iteration 42, loss = 0.57619304\n",
            "Iteration 43, loss = 0.57404862\n",
            "Iteration 44, loss = 0.57509658\n",
            "Iteration 45, loss = 0.57414948\n",
            "Iteration 46, loss = 0.57602628\n",
            "Iteration 47, loss = 0.57159110\n",
            "Iteration 48, loss = 0.57008133\n",
            "Iteration 49, loss = 0.56929441\n",
            "Iteration 50, loss = 0.56841943\n",
            "Iteration 51, loss = 0.57095721\n",
            "Iteration 52, loss = 0.56492207\n",
            "Iteration 53, loss = 0.56488131\n",
            "Iteration 54, loss = 0.56620738\n",
            "Iteration 55, loss = 0.56488581\n",
            "Iteration 56, loss = 0.56555062\n",
            "Iteration 57, loss = 0.56419836\n",
            "Iteration 58, loss = 0.56208752\n",
            "Iteration 59, loss = 0.56784923\n",
            "Iteration 60, loss = 0.55875063\n",
            "Iteration 61, loss = 0.56010205\n",
            "Iteration 62, loss = 0.55709943\n",
            "Iteration 63, loss = 0.55782093\n",
            "Iteration 64, loss = 0.55888806\n",
            "Iteration 65, loss = 0.56929801\n",
            "Iteration 66, loss = 0.55480280\n",
            "Iteration 67, loss = 0.55584315\n",
            "Iteration 68, loss = 0.55446871\n",
            "Iteration 69, loss = 0.55072266\n",
            "Iteration 70, loss = 0.54955635\n",
            "Iteration 71, loss = 0.54760836\n",
            "Iteration 72, loss = 0.54841854\n",
            "Iteration 73, loss = 0.54646084\n",
            "Iteration 74, loss = 0.54815018\n",
            "Iteration 75, loss = 0.54874191\n",
            "Iteration 76, loss = 0.54333500\n",
            "Iteration 77, loss = 0.54286979\n",
            "Iteration 78, loss = 0.54111250\n",
            "Iteration 79, loss = 0.54858679\n",
            "Iteration 80, loss = 0.53935699\n",
            "Iteration 81, loss = 0.53906525\n",
            "Iteration 82, loss = 0.54257364\n",
            "Iteration 83, loss = 0.54612374\n",
            "Iteration 84, loss = 0.53175275\n",
            "Iteration 85, loss = 0.53648085\n",
            "Iteration 86, loss = 0.53532062\n",
            "Iteration 87, loss = 0.53555012\n",
            "Iteration 88, loss = 0.53261544\n",
            "Iteration 89, loss = 0.52916423\n",
            "Iteration 90, loss = 0.52921720\n",
            "Iteration 91, loss = 0.52754034\n",
            "Iteration 92, loss = 0.52051173\n",
            "Iteration 93, loss = 0.53189348\n",
            "Iteration 94, loss = 0.52253775\n",
            "Iteration 95, loss = 0.52566527\n",
            "Iteration 96, loss = 0.52184362\n",
            "Iteration 97, loss = 0.52069369\n",
            "Iteration 98, loss = 0.52238948\n",
            "Iteration 99, loss = 0.52032804\n",
            "Iteration 100, loss = 0.52092550\n",
            "Iteration 101, loss = 0.52695180\n",
            "Iteration 102, loss = 0.52265493\n",
            "Iteration 103, loss = 0.52159580\n",
            "Iteration 104, loss = 0.52064975\n",
            "Iteration 105, loss = 0.51253162\n",
            "Iteration 106, loss = 0.51036761\n",
            "Iteration 107, loss = 0.51265386\n",
            "Iteration 108, loss = 0.50850160\n",
            "Iteration 109, loss = 0.51203442\n",
            "Iteration 110, loss = 0.50427389\n",
            "Iteration 111, loss = 0.51015785\n",
            "Iteration 112, loss = 0.50190264\n",
            "Iteration 113, loss = 0.50378068\n",
            "Iteration 114, loss = 0.50382065\n",
            "Iteration 115, loss = 0.50044866\n",
            "Iteration 116, loss = 0.49776226\n",
            "Iteration 117, loss = 0.49659584\n",
            "Iteration 118, loss = 0.50394667\n",
            "Iteration 119, loss = 0.50229601\n",
            "Iteration 120, loss = 0.49722993\n",
            "Iteration 121, loss = 0.49549271\n",
            "Iteration 122, loss = 0.49664975\n",
            "Iteration 123, loss = 0.49446506\n",
            "Iteration 124, loss = 0.50880431\n",
            "Iteration 125, loss = 0.49288446\n",
            "Iteration 126, loss = 0.48750883\n",
            "Iteration 127, loss = 0.49046886\n",
            "Iteration 128, loss = 0.48680373\n",
            "Iteration 129, loss = 0.48463286\n",
            "Iteration 130, loss = 0.48378969\n",
            "Iteration 131, loss = 0.48096608\n",
            "Iteration 132, loss = 0.48114893\n",
            "Iteration 133, loss = 0.49249508\n",
            "Iteration 134, loss = 0.48700843\n",
            "Iteration 135, loss = 0.48588776\n",
            "Iteration 136, loss = 0.48526487\n",
            "Iteration 137, loss = 0.47588011\n",
            "Iteration 138, loss = 0.48449653\n",
            "Iteration 139, loss = 0.47666927\n",
            "Iteration 140, loss = 0.47491692\n",
            "Iteration 141, loss = 0.48474008\n",
            "Iteration 142, loss = 0.48167968\n",
            "Iteration 143, loss = 0.48488396\n",
            "Iteration 144, loss = 0.47875696\n",
            "Iteration 145, loss = 0.46272294\n",
            "Iteration 146, loss = 0.48028476\n",
            "Iteration 147, loss = 0.47539408\n",
            "Iteration 148, loss = 0.47941777\n",
            "Iteration 149, loss = 0.48442595\n",
            "Iteration 150, loss = 0.47080292\n",
            "Iteration 151, loss = 0.48539841\n",
            "Iteration 152, loss = 0.48533762\n",
            "Iteration 153, loss = 0.46944270\n",
            "Iteration 154, loss = 0.47198085\n",
            "Iteration 155, loss = 0.46809119\n",
            "Iteration 156, loss = 0.47725077\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66888896\n",
            "Iteration 2, loss = 0.62164809\n",
            "Iteration 3, loss = 0.61629713\n",
            "Iteration 4, loss = 0.61399532\n",
            "Iteration 5, loss = 0.61110381\n",
            "Iteration 6, loss = 0.60845689\n",
            "Iteration 7, loss = 0.60760606\n",
            "Iteration 8, loss = 0.60535602\n",
            "Iteration 9, loss = 0.60256076\n",
            "Iteration 10, loss = 0.60312466\n",
            "Iteration 11, loss = 0.60076557\n",
            "Iteration 12, loss = 0.59866847\n",
            "Iteration 13, loss = 0.59757545\n",
            "Iteration 14, loss = 0.59742131\n",
            "Iteration 15, loss = 0.59598097\n",
            "Iteration 16, loss = 0.59939044\n",
            "Iteration 17, loss = 0.59448705\n",
            "Iteration 18, loss = 0.59191523\n",
            "Iteration 19, loss = 0.59017779\n",
            "Iteration 20, loss = 0.58902503\n",
            "Iteration 21, loss = 0.58844235\n",
            "Iteration 22, loss = 0.58721732\n",
            "Iteration 23, loss = 0.58629968\n",
            "Iteration 24, loss = 0.58636979\n",
            "Iteration 25, loss = 0.58535674\n",
            "Iteration 26, loss = 0.58350824\n",
            "Iteration 27, loss = 0.58522241\n",
            "Iteration 28, loss = 0.58035336\n",
            "Iteration 29, loss = 0.58039306\n",
            "Iteration 30, loss = 0.57983396\n",
            "Iteration 31, loss = 0.57842642\n",
            "Iteration 32, loss = 0.57747004\n",
            "Iteration 33, loss = 0.57915993\n",
            "Iteration 34, loss = 0.57432480\n",
            "Iteration 35, loss = 0.57646190\n",
            "Iteration 36, loss = 0.57672453\n",
            "Iteration 37, loss = 0.57416258\n",
            "Iteration 38, loss = 0.57199265\n",
            "Iteration 39, loss = 0.57249823\n",
            "Iteration 40, loss = 0.57060465\n",
            "Iteration 41, loss = 0.56967295\n",
            "Iteration 42, loss = 0.56817450\n",
            "Iteration 43, loss = 0.56533987\n",
            "Iteration 44, loss = 0.56635171\n",
            "Iteration 45, loss = 0.56874766\n",
            "Iteration 46, loss = 0.56812212\n",
            "Iteration 47, loss = 0.56857377\n",
            "Iteration 48, loss = 0.56585740\n",
            "Iteration 49, loss = 0.56214778\n",
            "Iteration 50, loss = 0.56275821\n",
            "Iteration 51, loss = 0.56089909\n",
            "Iteration 52, loss = 0.55802790\n",
            "Iteration 53, loss = 0.55799914\n",
            "Iteration 54, loss = 0.55671029\n",
            "Iteration 55, loss = 0.55951469\n",
            "Iteration 56, loss = 0.55523116\n",
            "Iteration 57, loss = 0.55510705\n",
            "Iteration 58, loss = 0.55343359\n",
            "Iteration 59, loss = 0.55186790\n",
            "Iteration 60, loss = 0.55366895\n",
            "Iteration 61, loss = 0.55254222\n",
            "Iteration 62, loss = 0.55212188\n",
            "Iteration 63, loss = 0.54848261\n",
            "Iteration 64, loss = 0.54893522\n",
            "Iteration 65, loss = 0.55636865\n",
            "Iteration 66, loss = 0.54962031\n",
            "Iteration 67, loss = 0.54713568\n",
            "Iteration 68, loss = 0.54467610\n",
            "Iteration 69, loss = 0.54243919\n",
            "Iteration 70, loss = 0.54536182\n",
            "Iteration 71, loss = 0.54684398\n",
            "Iteration 72, loss = 0.54093086\n",
            "Iteration 73, loss = 0.54955818\n",
            "Iteration 74, loss = 0.54172273\n",
            "Iteration 75, loss = 0.54683672\n",
            "Iteration 76, loss = 0.54131561\n",
            "Iteration 77, loss = 0.53720382\n",
            "Iteration 78, loss = 0.53446879\n",
            "Iteration 79, loss = 0.53535640\n",
            "Iteration 80, loss = 0.53497320\n",
            "Iteration 81, loss = 0.53897529\n",
            "Iteration 82, loss = 0.53604774\n",
            "Iteration 83, loss = 0.53136565\n",
            "Iteration 84, loss = 0.52769859\n",
            "Iteration 85, loss = 0.52837217\n",
            "Iteration 86, loss = 0.53482287\n",
            "Iteration 87, loss = 0.53609417\n",
            "Iteration 88, loss = 0.52543291\n",
            "Iteration 89, loss = 0.52533301\n",
            "Iteration 90, loss = 0.52981593\n",
            "Iteration 91, loss = 0.52678533\n",
            "Iteration 92, loss = 0.52384269\n",
            "Iteration 93, loss = 0.52076884\n",
            "Iteration 94, loss = 0.51835566\n",
            "Iteration 95, loss = 0.52571117\n",
            "Iteration 96, loss = 0.51787274\n",
            "Iteration 97, loss = 0.51911248\n",
            "Iteration 98, loss = 0.52174058\n",
            "Iteration 99, loss = 0.51686286\n",
            "Iteration 100, loss = 0.51654551\n",
            "Iteration 101, loss = 0.52380185\n",
            "Iteration 102, loss = 0.51366037\n",
            "Iteration 103, loss = 0.51569523\n",
            "Iteration 104, loss = 0.51765621\n",
            "Iteration 105, loss = 0.52382219\n",
            "Iteration 106, loss = 0.51209570\n",
            "Iteration 107, loss = 0.51609918\n",
            "Iteration 108, loss = 0.50837887\n",
            "Iteration 109, loss = 0.51214207\n",
            "Iteration 110, loss = 0.50603160\n",
            "Iteration 111, loss = 0.50945247\n",
            "Iteration 112, loss = 0.50510130\n",
            "Iteration 113, loss = 0.49888374\n",
            "Iteration 114, loss = 0.50393412\n",
            "Iteration 115, loss = 0.51102996\n",
            "Iteration 116, loss = 0.50448514\n",
            "Iteration 117, loss = 0.50089758\n",
            "Iteration 118, loss = 0.50037197\n",
            "Iteration 119, loss = 0.49696200\n",
            "Iteration 120, loss = 0.49905503\n",
            "Iteration 121, loss = 0.49789266\n",
            "Iteration 122, loss = 0.50076863\n",
            "Iteration 123, loss = 0.49062076\n",
            "Iteration 124, loss = 0.49264363\n",
            "Iteration 125, loss = 0.49436083\n",
            "Iteration 126, loss = 0.49023576\n",
            "Iteration 127, loss = 0.48976635\n",
            "Iteration 128, loss = 0.49504691\n",
            "Iteration 129, loss = 0.48913800\n",
            "Iteration 130, loss = 0.48869305\n",
            "Iteration 131, loss = 0.49108101\n",
            "Iteration 132, loss = 0.48541444\n",
            "Iteration 133, loss = 0.49114089\n",
            "Iteration 134, loss = 0.49695272\n",
            "Iteration 135, loss = 0.48209713\n",
            "Iteration 136, loss = 0.48644287\n",
            "Iteration 137, loss = 0.48120400\n",
            "Iteration 138, loss = 0.49917006\n",
            "Iteration 139, loss = 0.50966917\n",
            "Iteration 140, loss = 0.50314169\n",
            "Iteration 141, loss = 0.49156821\n",
            "Iteration 142, loss = 0.49442828\n",
            "Iteration 143, loss = 0.49059824\n",
            "Iteration 144, loss = 0.48304447\n",
            "Iteration 145, loss = 0.49174444\n",
            "Iteration 146, loss = 0.47655894\n",
            "Iteration 147, loss = 0.47967362\n",
            "Iteration 148, loss = 0.48323024\n",
            "Iteration 149, loss = 0.48548990\n",
            "Iteration 150, loss = 0.48187659\n",
            "Iteration 151, loss = 0.48799418\n",
            "Iteration 152, loss = 0.49194098\n",
            "Iteration 153, loss = 0.47020119\n",
            "Iteration 154, loss = 0.48625346\n",
            "Iteration 155, loss = 0.47832827\n",
            "Iteration 156, loss = 0.47185340\n",
            "Iteration 157, loss = 0.47396173\n",
            "Iteration 158, loss = 0.47096425\n",
            "Iteration 159, loss = 0.47525986\n",
            "Iteration 160, loss = 0.48195355\n",
            "Iteration 161, loss = 0.47783674\n",
            "Iteration 162, loss = 0.45942461\n",
            "Iteration 163, loss = 0.46968790\n",
            "Iteration 164, loss = 0.47028371\n",
            "Iteration 165, loss = 0.47247124\n",
            "Iteration 166, loss = 0.45786931\n",
            "Iteration 167, loss = 0.49835983\n",
            "Iteration 168, loss = 0.47489440\n",
            "Iteration 169, loss = 0.47552414\n",
            "Iteration 170, loss = 0.46937893\n",
            "Iteration 171, loss = 0.46977433\n",
            "Iteration 172, loss = 0.47367477\n",
            "Iteration 173, loss = 0.46382729\n",
            "Iteration 174, loss = 0.46234127\n",
            "Iteration 175, loss = 0.45752027\n",
            "Iteration 176, loss = 0.47733377\n",
            "Iteration 177, loss = 0.45833241\n",
            "Iteration 178, loss = 0.46375730\n",
            "Iteration 179, loss = 0.46791786\n",
            "Iteration 180, loss = 0.46889672\n",
            "Iteration 181, loss = 0.47543178\n",
            "Iteration 182, loss = 0.45914817\n",
            "Iteration 183, loss = 0.47517045\n",
            "Iteration 184, loss = 0.45739917\n",
            "Iteration 185, loss = 0.47386732\n",
            "Iteration 186, loss = 0.46209935\n",
            "Iteration 187, loss = 0.46434552\n",
            "Iteration 188, loss = 0.45897912\n",
            "Iteration 189, loss = 0.44981687\n",
            "Iteration 190, loss = 0.48670931\n",
            "Iteration 191, loss = 0.45647434\n",
            "Iteration 192, loss = 0.45956944\n",
            "Iteration 193, loss = 0.46371115\n",
            "Iteration 194, loss = 0.45476731\n",
            "Iteration 195, loss = 0.44679314\n",
            "Iteration 196, loss = 0.46534449\n",
            "Iteration 197, loss = 0.47467312\n",
            "Iteration 198, loss = 0.45449943\n",
            "Iteration 199, loss = 0.45976925\n",
            "Iteration 200, loss = 0.44972186\n",
            "Iteration 201, loss = 0.45762009\n",
            "Iteration 202, loss = 0.45091214\n",
            "Iteration 203, loss = 0.46700841\n",
            "Iteration 204, loss = 0.45295578\n",
            "Iteration 205, loss = 0.47989155\n",
            "Iteration 206, loss = 0.46237322\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68611759\n",
            "Iteration 2, loss = 0.63465380\n",
            "Iteration 3, loss = 0.62134563\n",
            "Iteration 4, loss = 0.61314076\n",
            "Iteration 5, loss = 0.60654205\n",
            "Iteration 6, loss = 0.60235931\n",
            "Iteration 7, loss = 0.59864737\n",
            "Iteration 8, loss = 0.59588005\n",
            "Iteration 9, loss = 0.59266994\n",
            "Iteration 10, loss = 0.59031550\n",
            "Iteration 11, loss = 0.58832537\n",
            "Iteration 12, loss = 0.58597580\n",
            "Iteration 13, loss = 0.58389845\n",
            "Iteration 14, loss = 0.58150222\n",
            "Iteration 15, loss = 0.57991801\n",
            "Iteration 16, loss = 0.57718050\n",
            "Iteration 17, loss = 0.57553643\n",
            "Iteration 18, loss = 0.57294201\n",
            "Iteration 19, loss = 0.56972560\n",
            "Iteration 20, loss = 0.56733906\n",
            "Iteration 21, loss = 0.56502652\n",
            "Iteration 22, loss = 0.56508633\n",
            "Iteration 23, loss = 0.55722084\n",
            "Iteration 24, loss = 0.55682935\n",
            "Iteration 25, loss = 0.55235243\n",
            "Iteration 26, loss = 0.54963234\n",
            "Iteration 27, loss = 0.54452433\n",
            "Iteration 28, loss = 0.54161708\n",
            "Iteration 29, loss = 0.53969054\n",
            "Iteration 30, loss = 0.53416185\n",
            "Iteration 31, loss = 0.53145869\n",
            "Iteration 32, loss = 0.52728687\n",
            "Iteration 33, loss = 0.52554185\n",
            "Iteration 34, loss = 0.51929453\n",
            "Iteration 35, loss = 0.51507857\n",
            "Iteration 36, loss = 0.51498498\n",
            "Iteration 37, loss = 0.50779415\n",
            "Iteration 38, loss = 0.50401295\n",
            "Iteration 39, loss = 0.50008346\n",
            "Iteration 40, loss = 0.49921029\n",
            "Iteration 41, loss = 0.49150916\n",
            "Iteration 42, loss = 0.48671493\n",
            "Iteration 43, loss = 0.48429373\n",
            "Iteration 44, loss = 0.47863778\n",
            "Iteration 45, loss = 0.47466600\n",
            "Iteration 46, loss = 0.47193404\n",
            "Iteration 47, loss = 0.46684216\n",
            "Iteration 48, loss = 0.46369257\n",
            "Iteration 49, loss = 0.45918389\n",
            "Iteration 50, loss = 0.45923871\n",
            "Iteration 51, loss = 0.45552086\n",
            "Iteration 52, loss = 0.45731355\n",
            "Iteration 53, loss = 0.44933255\n",
            "Iteration 54, loss = 0.44703233\n",
            "Iteration 55, loss = 0.44433822\n",
            "Iteration 56, loss = 0.44703018\n",
            "Iteration 57, loss = 0.43890724\n",
            "Iteration 58, loss = 0.43639081\n",
            "Iteration 59, loss = 0.44323061\n",
            "Iteration 60, loss = 0.43484783\n",
            "Iteration 61, loss = 0.43327439\n",
            "Iteration 62, loss = 0.43108979\n",
            "Iteration 63, loss = 0.42888540\n",
            "Iteration 64, loss = 0.43079880\n",
            "Iteration 65, loss = 0.43040396\n",
            "Iteration 66, loss = 0.43330034\n",
            "Iteration 67, loss = 0.43039919\n",
            "Iteration 68, loss = 0.42359026\n",
            "Iteration 69, loss = 0.42402641\n",
            "Iteration 70, loss = 0.42196045\n",
            "Iteration 71, loss = 0.42638093\n",
            "Iteration 72, loss = 0.42555170\n",
            "Iteration 73, loss = 0.42313047\n",
            "Iteration 74, loss = 0.41995320\n",
            "Iteration 75, loss = 0.41789694\n",
            "Iteration 76, loss = 0.41450719\n",
            "Iteration 77, loss = 0.41434336\n",
            "Iteration 78, loss = 0.41595591\n",
            "Iteration 79, loss = 0.41730335\n",
            "Iteration 80, loss = 0.42446122\n",
            "Iteration 81, loss = 0.42174574\n",
            "Iteration 82, loss = 0.41325284\n",
            "Iteration 83, loss = 0.41085987\n",
            "Iteration 84, loss = 0.41201170\n",
            "Iteration 85, loss = 0.40992899\n",
            "Iteration 86, loss = 0.40503772\n",
            "Iteration 87, loss = 0.40657030\n",
            "Iteration 88, loss = 0.40354144\n",
            "Iteration 89, loss = 0.40235614\n",
            "Iteration 90, loss = 0.40629295\n",
            "Iteration 91, loss = 0.41334003\n",
            "Iteration 92, loss = 0.40447752\n",
            "Iteration 93, loss = 0.40157227\n",
            "Iteration 94, loss = 0.39814127\n",
            "Iteration 95, loss = 0.39896150\n",
            "Iteration 96, loss = 0.39802909\n",
            "Iteration 97, loss = 0.39903013\n",
            "Iteration 98, loss = 0.39214301\n",
            "Iteration 99, loss = 0.39661860\n",
            "Iteration 100, loss = 0.39268681\n",
            "Iteration 101, loss = 0.39233089\n",
            "Iteration 102, loss = 0.39019856\n",
            "Iteration 103, loss = 0.39239601\n",
            "Iteration 104, loss = 0.38753019\n",
            "Iteration 105, loss = 0.38963083\n",
            "Iteration 106, loss = 0.38826275\n",
            "Iteration 107, loss = 0.39506078\n",
            "Iteration 108, loss = 0.38925778\n",
            "Iteration 109, loss = 0.39004070\n",
            "Iteration 110, loss = 0.38508600\n",
            "Iteration 111, loss = 0.38098845\n",
            "Iteration 112, loss = 0.38172893\n",
            "Iteration 113, loss = 0.38248178\n",
            "Iteration 114, loss = 0.39525992\n",
            "Iteration 115, loss = 0.37920632\n",
            "Iteration 116, loss = 0.38001110\n",
            "Iteration 117, loss = 0.37821078\n",
            "Iteration 118, loss = 0.37869060\n",
            "Iteration 119, loss = 0.37962922\n",
            "Iteration 120, loss = 0.38705498\n",
            "Iteration 121, loss = 0.37784808\n",
            "Iteration 122, loss = 0.37791457\n",
            "Iteration 123, loss = 0.37385990\n",
            "Iteration 124, loss = 0.38001093\n",
            "Iteration 125, loss = 0.38043040\n",
            "Iteration 126, loss = 0.37618725\n",
            "Iteration 127, loss = 0.37859732\n",
            "Iteration 128, loss = 0.37883438\n",
            "Iteration 129, loss = 0.37090790\n",
            "Iteration 130, loss = 0.37038404\n",
            "Iteration 131, loss = 0.36992899\n",
            "Iteration 132, loss = 0.37143908\n",
            "Iteration 133, loss = 0.37241627\n",
            "Iteration 134, loss = 0.37327911\n",
            "Iteration 135, loss = 0.37233460\n",
            "Iteration 136, loss = 0.37640677\n",
            "Iteration 137, loss = 0.37931921\n",
            "Iteration 138, loss = 0.36913811\n",
            "Iteration 139, loss = 0.36905762\n",
            "Iteration 140, loss = 0.36618241\n",
            "Iteration 141, loss = 0.36611414\n",
            "Iteration 142, loss = 0.36459795\n",
            "Iteration 143, loss = 0.36840889\n",
            "Iteration 144, loss = 0.37337509\n",
            "Iteration 145, loss = 0.36227105\n",
            "Iteration 146, loss = 0.38364277\n",
            "Iteration 147, loss = 0.36146111\n",
            "Iteration 148, loss = 0.36652323\n",
            "Iteration 149, loss = 0.36371915\n",
            "Iteration 150, loss = 0.36551772\n",
            "Iteration 151, loss = 0.37342042\n",
            "Iteration 152, loss = 0.37620673\n",
            "Iteration 153, loss = 0.36009859\n",
            "Iteration 154, loss = 0.36463475\n",
            "Iteration 155, loss = 0.36919627\n",
            "Iteration 156, loss = 0.36106313\n",
            "Iteration 157, loss = 0.35977679\n",
            "Iteration 158, loss = 0.35543019\n",
            "Iteration 159, loss = 0.37110670\n",
            "Iteration 160, loss = 0.35950951\n",
            "Iteration 161, loss = 0.35635632\n",
            "Iteration 162, loss = 0.35710038\n",
            "Iteration 163, loss = 0.35995762\n",
            "Iteration 164, loss = 0.35840398\n",
            "Iteration 165, loss = 0.35744212\n",
            "Iteration 166, loss = 0.35550659\n",
            "Iteration 167, loss = 0.35925991\n",
            "Iteration 168, loss = 0.37455935\n",
            "Iteration 169, loss = 0.35510705\n",
            "Iteration 170, loss = 0.35753990\n",
            "Iteration 171, loss = 0.36310811\n",
            "Iteration 172, loss = 0.35654448\n",
            "Iteration 173, loss = 0.35285547\n",
            "Iteration 174, loss = 0.35828121\n",
            "Iteration 175, loss = 0.35023427\n",
            "Iteration 176, loss = 0.35118707\n",
            "Iteration 177, loss = 0.35304242\n",
            "Iteration 178, loss = 0.36076103\n",
            "Iteration 179, loss = 0.35366923\n",
            "Iteration 180, loss = 0.35029856\n",
            "Iteration 181, loss = 0.35172474\n",
            "Iteration 182, loss = 0.36267413\n",
            "Iteration 183, loss = 0.35768080\n",
            "Iteration 184, loss = 0.35204181\n",
            "Iteration 185, loss = 0.35586793\n",
            "Iteration 186, loss = 0.34778098\n",
            "Iteration 187, loss = 0.34939455\n",
            "Iteration 188, loss = 0.34758537\n",
            "Iteration 189, loss = 0.34622615\n",
            "Iteration 190, loss = 0.34847166\n",
            "Iteration 191, loss = 0.35442236\n",
            "Iteration 192, loss = 0.35887829\n",
            "Iteration 193, loss = 0.35733737\n",
            "Iteration 194, loss = 0.34810893\n",
            "Iteration 195, loss = 0.34759853\n",
            "Iteration 196, loss = 0.34890377\n",
            "Iteration 197, loss = 0.34730605\n",
            "Iteration 198, loss = 0.34711471\n",
            "Iteration 199, loss = 0.35018600\n",
            "Iteration 200, loss = 0.34768770\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.77333398\n",
            "Iteration 2, loss = 0.67832541\n",
            "Iteration 3, loss = 0.63645160\n",
            "Iteration 4, loss = 0.61553057\n",
            "Iteration 5, loss = 0.60797952\n",
            "Iteration 6, loss = 0.60326996\n",
            "Iteration 7, loss = 0.59834500\n",
            "Iteration 8, loss = 0.59581506\n",
            "Iteration 9, loss = 0.59402091\n",
            "Iteration 10, loss = 0.59062151\n",
            "Iteration 11, loss = 0.58781198\n",
            "Iteration 12, loss = 0.58512894\n",
            "Iteration 13, loss = 0.58175165\n",
            "Iteration 14, loss = 0.57988998\n",
            "Iteration 15, loss = 0.57656351\n",
            "Iteration 16, loss = 0.57357293\n",
            "Iteration 17, loss = 0.56885147\n",
            "Iteration 18, loss = 0.56580258\n",
            "Iteration 19, loss = 0.56138534\n",
            "Iteration 20, loss = 0.55628892\n",
            "Iteration 21, loss = 0.55105488\n",
            "Iteration 22, loss = 0.54658178\n",
            "Iteration 23, loss = 0.54116726\n",
            "Iteration 24, loss = 0.53533186\n",
            "Iteration 25, loss = 0.53083134\n",
            "Iteration 26, loss = 0.52519613\n",
            "Iteration 27, loss = 0.51837494\n",
            "Iteration 28, loss = 0.51718125\n",
            "Iteration 29, loss = 0.51474671\n",
            "Iteration 30, loss = 0.50443967\n",
            "Iteration 31, loss = 0.49922572\n",
            "Iteration 32, loss = 0.49410716\n",
            "Iteration 33, loss = 0.48828250\n",
            "Iteration 34, loss = 0.48444195\n",
            "Iteration 35, loss = 0.47853162\n",
            "Iteration 36, loss = 0.47318147\n",
            "Iteration 37, loss = 0.46961057\n",
            "Iteration 38, loss = 0.46648712\n",
            "Iteration 39, loss = 0.46006020\n",
            "Iteration 40, loss = 0.45760907\n",
            "Iteration 41, loss = 0.45442052\n",
            "Iteration 42, loss = 0.45168166\n",
            "Iteration 43, loss = 0.44826700\n",
            "Iteration 44, loss = 0.44748844\n",
            "Iteration 45, loss = 0.44187849\n",
            "Iteration 46, loss = 0.43820330\n",
            "Iteration 47, loss = 0.43803317\n",
            "Iteration 48, loss = 0.43591932\n",
            "Iteration 49, loss = 0.42939139\n",
            "Iteration 50, loss = 0.42473846\n",
            "Iteration 51, loss = 0.42470865\n",
            "Iteration 52, loss = 0.42112256\n",
            "Iteration 53, loss = 0.41834113\n",
            "Iteration 54, loss = 0.41744910\n",
            "Iteration 55, loss = 0.41727083\n",
            "Iteration 56, loss = 0.41818145\n",
            "Iteration 57, loss = 0.41541770\n",
            "Iteration 58, loss = 0.41156059\n",
            "Iteration 59, loss = 0.41046481\n",
            "Iteration 60, loss = 0.40870328\n",
            "Iteration 61, loss = 0.40490826\n",
            "Iteration 62, loss = 0.40337086\n",
            "Iteration 63, loss = 0.40140503\n",
            "Iteration 64, loss = 0.40047485\n",
            "Iteration 65, loss = 0.40058455\n",
            "Iteration 66, loss = 0.40232543\n",
            "Iteration 67, loss = 0.40351422\n",
            "Iteration 68, loss = 0.40063208\n",
            "Iteration 69, loss = 0.39638307\n",
            "Iteration 70, loss = 0.40433070\n",
            "Iteration 71, loss = 0.39644570\n",
            "Iteration 72, loss = 0.39058056\n",
            "Iteration 73, loss = 0.39153644\n",
            "Iteration 74, loss = 0.40077407\n",
            "Iteration 75, loss = 0.39358452\n",
            "Iteration 76, loss = 0.39410084\n",
            "Iteration 77, loss = 0.39020797\n",
            "Iteration 78, loss = 0.38753993\n",
            "Iteration 79, loss = 0.38690057\n",
            "Iteration 80, loss = 0.38947398\n",
            "Iteration 81, loss = 0.39578620\n",
            "Iteration 82, loss = 0.38495741\n",
            "Iteration 83, loss = 0.38170864\n",
            "Iteration 84, loss = 0.38062272\n",
            "Iteration 85, loss = 0.37994799\n",
            "Iteration 86, loss = 0.37748329\n",
            "Iteration 87, loss = 0.37937815\n",
            "Iteration 88, loss = 0.38117264\n",
            "Iteration 89, loss = 0.37777994\n",
            "Iteration 90, loss = 0.38026479\n",
            "Iteration 91, loss = 0.37945826\n",
            "Iteration 92, loss = 0.37451341\n",
            "Iteration 93, loss = 0.37621630\n",
            "Iteration 94, loss = 0.38640106\n",
            "Iteration 95, loss = 0.37541948\n",
            "Iteration 96, loss = 0.37812624\n",
            "Iteration 97, loss = 0.37026815\n",
            "Iteration 98, loss = 0.37124585\n",
            "Iteration 99, loss = 0.36906836\n",
            "Iteration 100, loss = 0.36692046\n",
            "Iteration 101, loss = 0.36960820\n",
            "Iteration 102, loss = 0.37386983\n",
            "Iteration 103, loss = 0.36943620\n",
            "Iteration 104, loss = 0.37731776\n",
            "Iteration 105, loss = 0.37269161\n",
            "Iteration 106, loss = 0.37266839\n",
            "Iteration 107, loss = 0.37083736\n",
            "Iteration 108, loss = 0.37150323\n",
            "Iteration 109, loss = 0.36562425\n",
            "Iteration 110, loss = 0.36437506\n",
            "Iteration 111, loss = 0.36756454\n",
            "Iteration 112, loss = 0.37233173\n",
            "Iteration 113, loss = 0.36963427\n",
            "Iteration 114, loss = 0.37120294\n",
            "Iteration 115, loss = 0.36286772\n",
            "Iteration 116, loss = 0.36748697\n",
            "Iteration 117, loss = 0.36299090\n",
            "Iteration 118, loss = 0.36173029\n",
            "Iteration 119, loss = 0.35962709\n",
            "Iteration 120, loss = 0.35870771\n",
            "Iteration 121, loss = 0.35712084\n",
            "Iteration 122, loss = 0.35614924\n",
            "Iteration 123, loss = 0.35644378\n",
            "Iteration 124, loss = 0.35715023\n",
            "Iteration 125, loss = 0.35501444\n",
            "Iteration 126, loss = 0.35518121\n",
            "Iteration 127, loss = 0.35806188\n",
            "Iteration 128, loss = 0.36243491\n",
            "Iteration 129, loss = 0.35352809\n",
            "Iteration 130, loss = 0.36141072\n",
            "Iteration 131, loss = 0.35397979\n",
            "Iteration 132, loss = 0.35453438\n",
            "Iteration 133, loss = 0.35312352\n",
            "Iteration 134, loss = 0.35153532\n",
            "Iteration 135, loss = 0.34913859\n",
            "Iteration 136, loss = 0.35108342\n",
            "Iteration 137, loss = 0.35617232\n",
            "Iteration 138, loss = 0.35111783\n",
            "Iteration 139, loss = 0.35094943\n",
            "Iteration 140, loss = 0.35128067\n",
            "Iteration 141, loss = 0.34987079\n",
            "Iteration 142, loss = 0.35058318\n",
            "Iteration 143, loss = 0.34868223\n",
            "Iteration 144, loss = 0.34753873\n",
            "Iteration 145, loss = 0.35083779\n",
            "Iteration 146, loss = 0.35233234\n",
            "Iteration 147, loss = 0.35755284\n",
            "Iteration 148, loss = 0.35030939\n",
            "Iteration 149, loss = 0.34950017\n",
            "Iteration 150, loss = 0.34627417\n",
            "Iteration 151, loss = 0.34326931\n",
            "Iteration 152, loss = 0.34445161\n",
            "Iteration 153, loss = 0.34624976\n",
            "Iteration 154, loss = 0.34684271\n",
            "Iteration 155, loss = 0.35068324\n",
            "Iteration 156, loss = 0.35000718\n",
            "Iteration 157, loss = 0.34506295\n",
            "Iteration 158, loss = 0.34264497\n",
            "Iteration 159, loss = 0.34623535\n",
            "Iteration 160, loss = 0.34998362\n",
            "Iteration 161, loss = 0.34796668\n",
            "Iteration 162, loss = 0.34323838\n",
            "Iteration 163, loss = 0.34127134\n",
            "Iteration 164, loss = 0.33985133\n",
            "Iteration 165, loss = 0.33907168\n",
            "Iteration 166, loss = 0.34543753\n",
            "Iteration 167, loss = 0.34184374\n",
            "Iteration 168, loss = 0.35463598\n",
            "Iteration 169, loss = 0.34093608\n",
            "Iteration 170, loss = 0.33796812\n",
            "Iteration 171, loss = 0.33907344\n",
            "Iteration 172, loss = 0.34062407\n",
            "Iteration 173, loss = 0.34323540\n",
            "Iteration 174, loss = 0.33554554\n",
            "Iteration 175, loss = 0.33910993\n",
            "Iteration 176, loss = 0.33872092\n",
            "Iteration 177, loss = 0.33507828\n",
            "Iteration 178, loss = 0.33500101\n",
            "Iteration 179, loss = 0.33971316\n",
            "Iteration 180, loss = 0.33264006\n",
            "Iteration 181, loss = 0.34190317\n",
            "Iteration 182, loss = 0.33377536\n",
            "Iteration 183, loss = 0.33762181\n",
            "Iteration 184, loss = 0.33604669\n",
            "Iteration 185, loss = 0.34455378\n",
            "Iteration 186, loss = 0.34457851\n",
            "Iteration 187, loss = 0.34052358\n",
            "Iteration 188, loss = 0.33930658\n",
            "Iteration 189, loss = 0.33437661\n",
            "Iteration 190, loss = 0.33335579\n",
            "Iteration 191, loss = 0.34161266\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.76007529\n",
            "Iteration 2, loss = 0.68416107\n",
            "Iteration 3, loss = 0.64624140\n",
            "Iteration 4, loss = 0.62373485\n",
            "Iteration 5, loss = 0.61363615\n",
            "Iteration 6, loss = 0.60693972\n",
            "Iteration 7, loss = 0.60369663\n",
            "Iteration 8, loss = 0.60149478\n",
            "Iteration 9, loss = 0.59975298\n",
            "Iteration 10, loss = 0.59841106\n",
            "Iteration 11, loss = 0.59663617\n",
            "Iteration 12, loss = 0.59476600\n",
            "Iteration 13, loss = 0.59315202\n",
            "Iteration 14, loss = 0.59160630\n",
            "Iteration 15, loss = 0.59009609\n",
            "Iteration 16, loss = 0.58855460\n",
            "Iteration 17, loss = 0.58662385\n",
            "Iteration 18, loss = 0.58538339\n",
            "Iteration 19, loss = 0.58327300\n",
            "Iteration 20, loss = 0.58193675\n",
            "Iteration 21, loss = 0.58075803\n",
            "Iteration 22, loss = 0.57827847\n",
            "Iteration 23, loss = 0.57761175\n",
            "Iteration 24, loss = 0.57592552\n",
            "Iteration 25, loss = 0.57347408\n",
            "Iteration 26, loss = 0.57192416\n",
            "Iteration 27, loss = 0.56965835\n",
            "Iteration 28, loss = 0.56881552\n",
            "Iteration 29, loss = 0.56614751\n",
            "Iteration 30, loss = 0.56377875\n",
            "Iteration 31, loss = 0.56203936\n",
            "Iteration 32, loss = 0.55955990\n",
            "Iteration 33, loss = 0.55812747\n",
            "Iteration 34, loss = 0.55597325\n",
            "Iteration 35, loss = 0.55389342\n",
            "Iteration 36, loss = 0.55200106\n",
            "Iteration 37, loss = 0.55146097\n",
            "Iteration 38, loss = 0.55067359\n",
            "Iteration 39, loss = 0.54885142\n",
            "Iteration 40, loss = 0.54574895\n",
            "Iteration 41, loss = 0.54226145\n",
            "Iteration 42, loss = 0.54040189\n",
            "Iteration 43, loss = 0.54056616\n",
            "Iteration 44, loss = 0.53779566\n",
            "Iteration 45, loss = 0.53796620\n",
            "Iteration 46, loss = 0.53647175\n",
            "Iteration 47, loss = 0.53400526\n",
            "Iteration 48, loss = 0.53125543\n",
            "Iteration 49, loss = 0.52979674\n",
            "Iteration 50, loss = 0.52756697\n",
            "Iteration 51, loss = 0.52527041\n",
            "Iteration 52, loss = 0.52102334\n",
            "Iteration 53, loss = 0.51932351\n",
            "Iteration 54, loss = 0.51868165\n",
            "Iteration 55, loss = 0.51438918\n",
            "Iteration 56, loss = 0.51115060\n",
            "Iteration 57, loss = 0.50840331\n",
            "Iteration 58, loss = 0.50554278\n",
            "Iteration 59, loss = 0.50222568\n",
            "Iteration 60, loss = 0.49856679\n",
            "Iteration 61, loss = 0.49694336\n",
            "Iteration 62, loss = 0.49270537\n",
            "Iteration 63, loss = 0.48960060\n",
            "Iteration 64, loss = 0.48572807\n",
            "Iteration 65, loss = 0.48314618\n",
            "Iteration 66, loss = 0.47961334\n",
            "Iteration 67, loss = 0.48457458\n",
            "Iteration 68, loss = 0.47521169\n",
            "Iteration 69, loss = 0.47258983\n",
            "Iteration 70, loss = 0.47082965\n",
            "Iteration 71, loss = 0.46651964\n",
            "Iteration 72, loss = 0.46633499\n",
            "Iteration 73, loss = 0.45961013\n",
            "Iteration 74, loss = 0.45723555\n",
            "Iteration 75, loss = 0.46002427\n",
            "Iteration 76, loss = 0.45235486\n",
            "Iteration 77, loss = 0.46053675\n",
            "Iteration 78, loss = 0.46239446\n",
            "Iteration 79, loss = 0.45684429\n",
            "Iteration 80, loss = 0.44490560\n",
            "Iteration 81, loss = 0.44382287\n",
            "Iteration 82, loss = 0.44504309\n",
            "Iteration 83, loss = 0.45129992\n",
            "Iteration 84, loss = 0.44420426\n",
            "Iteration 85, loss = 0.43649425\n",
            "Iteration 86, loss = 0.43779248\n",
            "Iteration 87, loss = 0.43471275\n",
            "Iteration 88, loss = 0.43158246\n",
            "Iteration 89, loss = 0.43016520\n",
            "Iteration 90, loss = 0.43185346\n",
            "Iteration 91, loss = 0.43148189\n",
            "Iteration 92, loss = 0.42858905\n",
            "Iteration 93, loss = 0.42886707\n",
            "Iteration 94, loss = 0.42673243\n",
            "Iteration 95, loss = 0.42293733\n",
            "Iteration 96, loss = 0.42590766\n",
            "Iteration 97, loss = 0.43926142\n",
            "Iteration 98, loss = 0.43929569\n",
            "Iteration 99, loss = 0.42525872\n",
            "Iteration 100, loss = 0.41748437\n",
            "Iteration 101, loss = 0.41656899\n",
            "Iteration 102, loss = 0.41834439\n",
            "Iteration 103, loss = 0.41335586\n",
            "Iteration 104, loss = 0.41432860\n",
            "Iteration 105, loss = 0.41604961\n",
            "Iteration 106, loss = 0.41135154\n",
            "Iteration 107, loss = 0.41303593\n",
            "Iteration 108, loss = 0.40670036\n",
            "Iteration 109, loss = 0.40616035\n",
            "Iteration 110, loss = 0.41097392\n",
            "Iteration 111, loss = 0.41023329\n",
            "Iteration 112, loss = 0.40486460\n",
            "Iteration 113, loss = 0.40387899\n",
            "Iteration 114, loss = 0.40540486\n",
            "Iteration 115, loss = 0.40183069\n",
            "Iteration 116, loss = 0.40217498\n",
            "Iteration 117, loss = 0.41293528\n",
            "Iteration 118, loss = 0.40864034\n",
            "Iteration 119, loss = 0.39990661\n",
            "Iteration 120, loss = 0.40196690\n",
            "Iteration 121, loss = 0.40074701\n",
            "Iteration 122, loss = 0.40237881\n",
            "Iteration 123, loss = 0.39805025\n",
            "Iteration 124, loss = 0.39770866\n",
            "Iteration 125, loss = 0.39625710\n",
            "Iteration 126, loss = 0.39627124\n",
            "Iteration 127, loss = 0.39501737\n",
            "Iteration 128, loss = 0.39741276\n",
            "Iteration 129, loss = 0.39396531\n",
            "Iteration 130, loss = 0.39258673\n",
            "Iteration 131, loss = 0.39143076\n",
            "Iteration 132, loss = 0.39843290\n",
            "Iteration 133, loss = 0.39450399\n",
            "Iteration 134, loss = 0.38961788\n",
            "Iteration 135, loss = 0.39323791\n",
            "Iteration 136, loss = 0.38820197\n",
            "Iteration 137, loss = 0.38981223\n",
            "Iteration 138, loss = 0.39007372\n",
            "Iteration 139, loss = 0.38617035\n",
            "Iteration 140, loss = 0.38664463\n",
            "Iteration 141, loss = 0.38559635\n",
            "Iteration 142, loss = 0.38434796\n",
            "Iteration 143, loss = 0.38522560\n",
            "Iteration 144, loss = 0.38660977\n",
            "Iteration 145, loss = 0.38353578\n",
            "Iteration 146, loss = 0.38308152\n",
            "Iteration 147, loss = 0.38503089\n",
            "Iteration 148, loss = 0.38583625\n",
            "Iteration 149, loss = 0.37806861\n",
            "Iteration 150, loss = 0.38152010\n",
            "Iteration 151, loss = 0.38593870\n",
            "Iteration 152, loss = 0.37986222\n",
            "Iteration 153, loss = 0.37912701\n",
            "Iteration 154, loss = 0.37854978\n",
            "Iteration 155, loss = 0.38632963\n",
            "Iteration 156, loss = 0.38855486\n",
            "Iteration 157, loss = 0.38280216\n",
            "Iteration 158, loss = 0.37723150\n",
            "Iteration 159, loss = 0.37874171\n",
            "Iteration 160, loss = 0.37363817\n",
            "Iteration 161, loss = 0.37772975\n",
            "Iteration 162, loss = 0.37407024\n",
            "Iteration 163, loss = 0.37229013\n",
            "Iteration 164, loss = 0.37280730\n",
            "Iteration 165, loss = 0.37823392\n",
            "Iteration 166, loss = 0.38280494\n",
            "Iteration 167, loss = 0.38683009\n",
            "Iteration 168, loss = 0.37792417\n",
            "Iteration 169, loss = 0.37757596\n",
            "Iteration 170, loss = 0.38563318\n",
            "Iteration 171, loss = 0.37766125\n",
            "Iteration 172, loss = 0.37445508\n",
            "Iteration 173, loss = 0.37180864\n",
            "Iteration 174, loss = 0.36865072\n",
            "Iteration 175, loss = 0.37285282\n",
            "Iteration 176, loss = 0.37519041\n",
            "Iteration 177, loss = 0.37135730\n",
            "Iteration 178, loss = 0.37356865\n",
            "Iteration 179, loss = 0.36962949\n",
            "Iteration 180, loss = 0.36703659\n",
            "Iteration 181, loss = 0.36929757\n",
            "Iteration 182, loss = 0.36390877\n",
            "Iteration 183, loss = 0.36444140\n",
            "Iteration 184, loss = 0.36326226\n",
            "Iteration 185, loss = 0.36235440\n",
            "Iteration 186, loss = 0.36542811\n",
            "Iteration 187, loss = 0.36645646\n",
            "Iteration 188, loss = 0.36140594\n",
            "Iteration 189, loss = 0.35967861\n",
            "Iteration 190, loss = 0.36244287\n",
            "Iteration 191, loss = 0.36140901\n",
            "Iteration 192, loss = 0.35861142\n",
            "Iteration 193, loss = 0.35838070\n",
            "Iteration 194, loss = 0.36145856\n",
            "Iteration 195, loss = 0.35967585\n",
            "Iteration 196, loss = 0.36295666\n",
            "Iteration 197, loss = 0.36128394\n",
            "Iteration 198, loss = 0.35654916\n",
            "Iteration 199, loss = 0.35993966\n",
            "Iteration 200, loss = 0.36541283\n",
            "Iteration 1, loss = 0.77249478\n",
            "Iteration 2, loss = 0.63241049\n",
            "Iteration 3, loss = 0.61859698\n",
            "Iteration 4, loss = 0.61734352\n",
            "Iteration 5, loss = 0.61039096\n",
            "Iteration 6, loss = 0.60793437\n",
            "Iteration 7, loss = 0.60591289\n",
            "Iteration 8, loss = 0.60344419\n",
            "Iteration 9, loss = 0.60154086\n",
            "Iteration 10, loss = 0.59871852\n",
            "Iteration 11, loss = 0.59794373\n",
            "Iteration 12, loss = 0.59403075\n",
            "Iteration 13, loss = 0.59204720\n",
            "Iteration 14, loss = 0.59007033\n",
            "Iteration 15, loss = 0.58763711\n",
            "Iteration 16, loss = 0.58358896\n",
            "Iteration 17, loss = 0.58101688\n",
            "Iteration 18, loss = 0.57734645\n",
            "Iteration 19, loss = 0.57226090\n",
            "Iteration 20, loss = 0.56835118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 21, loss = 0.56277676\n",
            "Iteration 22, loss = 0.55873080\n",
            "Iteration 23, loss = 0.55275537\n",
            "Iteration 24, loss = 0.54913934\n",
            "Iteration 25, loss = 0.54490850\n",
            "Iteration 26, loss = 0.53944648\n",
            "Iteration 27, loss = 0.53704950\n",
            "Iteration 28, loss = 0.53431591\n",
            "Iteration 29, loss = 0.52914456\n",
            "Iteration 30, loss = 0.52480574\n",
            "Iteration 31, loss = 0.52268448\n",
            "Iteration 32, loss = 0.51418141\n",
            "Iteration 33, loss = 0.51170469\n",
            "Iteration 34, loss = 0.50920278\n",
            "Iteration 35, loss = 0.50454443\n",
            "Iteration 36, loss = 0.50034970\n",
            "Iteration 37, loss = 0.50558690\n",
            "Iteration 38, loss = 0.49640837\n",
            "Iteration 39, loss = 0.49759874\n",
            "Iteration 40, loss = 0.48841476\n",
            "Iteration 41, loss = 0.48726710\n",
            "Iteration 42, loss = 0.48127196\n",
            "Iteration 43, loss = 0.48007094\n",
            "Iteration 44, loss = 0.47543034\n",
            "Iteration 45, loss = 0.47582772\n",
            "Iteration 46, loss = 0.47102473\n",
            "Iteration 47, loss = 0.47016486\n",
            "Iteration 48, loss = 0.46915560\n",
            "Iteration 49, loss = 0.46266084\n",
            "Iteration 50, loss = 0.46315748\n",
            "Iteration 51, loss = 0.46020435\n",
            "Iteration 52, loss = 0.45741923\n",
            "Iteration 53, loss = 0.45723281\n",
            "Iteration 54, loss = 0.45408786\n",
            "Iteration 55, loss = 0.45206641\n",
            "Iteration 56, loss = 0.44942088\n",
            "Iteration 57, loss = 0.44827136\n",
            "Iteration 58, loss = 0.44536247\n",
            "Iteration 59, loss = 0.44269204\n",
            "Iteration 60, loss = 0.44179040\n",
            "Iteration 61, loss = 0.43957687\n",
            "Iteration 62, loss = 0.44116967\n",
            "Iteration 63, loss = 0.43866334\n",
            "Iteration 64, loss = 0.44034113\n",
            "Iteration 65, loss = 0.43524123\n",
            "Iteration 66, loss = 0.43054591\n",
            "Iteration 67, loss = 0.42973083\n",
            "Iteration 68, loss = 0.42893656\n",
            "Iteration 69, loss = 0.43036652\n",
            "Iteration 70, loss = 0.42436995\n",
            "Iteration 71, loss = 0.42274489\n",
            "Iteration 72, loss = 0.42150555\n",
            "Iteration 73, loss = 0.41959963\n",
            "Iteration 74, loss = 0.42139342\n",
            "Iteration 75, loss = 0.41801279\n",
            "Iteration 76, loss = 0.41612885\n",
            "Iteration 77, loss = 0.42176986\n",
            "Iteration 78, loss = 0.41813626\n",
            "Iteration 79, loss = 0.41481517\n",
            "Iteration 80, loss = 0.41238647\n",
            "Iteration 81, loss = 0.40977732\n",
            "Iteration 82, loss = 0.40845527\n",
            "Iteration 83, loss = 0.40932515\n",
            "Iteration 84, loss = 0.40859873\n",
            "Iteration 85, loss = 0.40617721\n",
            "Iteration 86, loss = 0.40761887\n",
            "Iteration 87, loss = 0.40516921\n",
            "Iteration 88, loss = 0.40279747\n",
            "Iteration 89, loss = 0.40407111\n",
            "Iteration 90, loss = 0.40016892\n",
            "Iteration 91, loss = 0.40103309\n",
            "Iteration 92, loss = 0.39680519\n",
            "Iteration 93, loss = 0.39825468\n",
            "Iteration 94, loss = 0.40008800\n",
            "Iteration 95, loss = 0.39635528\n",
            "Iteration 96, loss = 0.39225008\n",
            "Iteration 97, loss = 0.39504782\n",
            "Iteration 98, loss = 0.39448735\n",
            "Iteration 99, loss = 0.39083969\n",
            "Iteration 100, loss = 0.39320803\n",
            "Iteration 101, loss = 0.38940333\n",
            "Iteration 102, loss = 0.39061971\n",
            "Iteration 103, loss = 0.38921108\n",
            "Iteration 104, loss = 0.38974497\n",
            "Iteration 105, loss = 0.38686927\n",
            "Iteration 106, loss = 0.38660057\n",
            "Iteration 107, loss = 0.38471450\n",
            "Iteration 108, loss = 0.38432439\n",
            "Iteration 109, loss = 0.38408559\n",
            "Iteration 110, loss = 0.38363974\n",
            "Iteration 111, loss = 0.38327268\n",
            "Iteration 112, loss = 0.38245471\n",
            "Iteration 113, loss = 0.38277806\n",
            "Iteration 114, loss = 0.38019596\n",
            "Iteration 115, loss = 0.37977796\n",
            "Iteration 116, loss = 0.38098740\n",
            "Iteration 117, loss = 0.38001334\n",
            "Iteration 118, loss = 0.38242362\n",
            "Iteration 119, loss = 0.37738920\n",
            "Iteration 120, loss = 0.38088223\n",
            "Iteration 121, loss = 0.37667258\n",
            "Iteration 122, loss = 0.38310160\n",
            "Iteration 123, loss = 0.37553757\n",
            "Iteration 124, loss = 0.38414495\n",
            "Iteration 125, loss = 0.37449333\n",
            "Iteration 126, loss = 0.37236398\n",
            "Iteration 127, loss = 0.37162944\n",
            "Iteration 128, loss = 0.37292333\n",
            "Iteration 129, loss = 0.37073539\n",
            "Iteration 130, loss = 0.37273937\n",
            "Iteration 131, loss = 0.37395874\n",
            "Iteration 132, loss = 0.37668562\n",
            "Iteration 133, loss = 0.37776810\n",
            "Iteration 134, loss = 0.37147018\n",
            "Iteration 135, loss = 0.36640371\n",
            "Iteration 136, loss = 0.36745259\n",
            "Iteration 137, loss = 0.36734261\n",
            "Iteration 138, loss = 0.36563557\n",
            "Iteration 139, loss = 0.36968156\n",
            "Iteration 140, loss = 0.37228990\n",
            "Iteration 141, loss = 0.37911990\n",
            "Iteration 142, loss = 0.37117048\n",
            "Iteration 143, loss = 0.36583378\n",
            "Iteration 144, loss = 0.36866712\n",
            "Iteration 145, loss = 0.36392386\n",
            "Iteration 146, loss = 0.36341880\n",
            "Iteration 147, loss = 0.36616726\n",
            "Iteration 148, loss = 0.36205221\n",
            "Iteration 149, loss = 0.36668501\n",
            "Iteration 150, loss = 0.37064830\n",
            "Iteration 151, loss = 0.36429981\n",
            "Iteration 152, loss = 0.36125571\n",
            "Iteration 153, loss = 0.35884099\n",
            "Iteration 154, loss = 0.36579038\n",
            "Iteration 155, loss = 0.36935297\n",
            "Iteration 156, loss = 0.37980291\n",
            "Iteration 157, loss = 0.36627621\n",
            "Iteration 158, loss = 0.35895094\n",
            "Iteration 159, loss = 0.35844602\n",
            "Iteration 160, loss = 0.35924231\n",
            "Iteration 161, loss = 0.35710893\n",
            "Iteration 162, loss = 0.35621955\n",
            "Iteration 163, loss = 0.35720676\n",
            "Iteration 164, loss = 0.35412364\n",
            "Iteration 165, loss = 0.35511766\n",
            "Iteration 166, loss = 0.36435611\n",
            "Iteration 167, loss = 0.35928464\n",
            "Iteration 168, loss = 0.36029391\n",
            "Iteration 169, loss = 0.36397747\n",
            "Iteration 170, loss = 0.35373360\n",
            "Iteration 171, loss = 0.35418135\n",
            "Iteration 172, loss = 0.35338136\n",
            "Iteration 173, loss = 0.35201247\n",
            "Iteration 174, loss = 0.35126313\n",
            "Iteration 175, loss = 0.35047017\n",
            "Iteration 176, loss = 0.35454812\n",
            "Iteration 177, loss = 0.35160210\n",
            "Iteration 178, loss = 0.34992102\n",
            "Iteration 179, loss = 0.35016757\n",
            "Iteration 180, loss = 0.35360058\n",
            "Iteration 181, loss = 0.35761127\n",
            "Iteration 182, loss = 0.36029714\n",
            "Iteration 183, loss = 0.35517365\n",
            "Iteration 184, loss = 0.34751674\n",
            "Iteration 185, loss = 0.35345412\n",
            "Iteration 186, loss = 0.35250930\n",
            "Iteration 187, loss = 0.34926359\n",
            "Iteration 188, loss = 0.35477739\n",
            "Iteration 189, loss = 0.34553285\n",
            "Iteration 190, loss = 0.34771028\n",
            "Iteration 191, loss = 0.34567583\n",
            "Iteration 192, loss = 0.34401583\n",
            "Iteration 193, loss = 0.34580488\n",
            "Iteration 194, loss = 0.34581988\n",
            "Iteration 195, loss = 0.34750394\n",
            "Iteration 196, loss = 0.35460907\n",
            "Iteration 197, loss = 0.35860316\n",
            "Iteration 198, loss = 0.34617880\n",
            "Iteration 199, loss = 0.35302965\n",
            "Iteration 200, loss = 0.34656827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.88203853\n",
            "Iteration 2, loss = 0.73502259\n",
            "Iteration 3, loss = 0.67389383\n",
            "Iteration 4, loss = 0.64216694\n",
            "Iteration 5, loss = 0.63313554\n",
            "Iteration 6, loss = 0.62990000\n",
            "Iteration 7, loss = 0.62739117\n",
            "Iteration 8, loss = 0.62606100\n",
            "Iteration 9, loss = 0.62446967\n",
            "Iteration 10, loss = 0.62270990\n",
            "Iteration 11, loss = 0.62183917\n",
            "Iteration 12, loss = 0.62043291\n",
            "Iteration 13, loss = 0.61880537\n",
            "Iteration 14, loss = 0.61784467\n",
            "Iteration 15, loss = 0.61639146\n",
            "Iteration 16, loss = 0.61552366\n",
            "Iteration 17, loss = 0.61431671\n",
            "Iteration 18, loss = 0.61379216\n",
            "Iteration 19, loss = 0.61120186\n",
            "Iteration 20, loss = 0.60988685\n",
            "Iteration 21, loss = 0.60811213\n",
            "Iteration 22, loss = 0.60818239\n",
            "Iteration 23, loss = 0.60506199\n",
            "Iteration 24, loss = 0.60377101\n",
            "Iteration 25, loss = 0.60221683\n",
            "Iteration 26, loss = 0.60049280\n",
            "Iteration 27, loss = 0.59937147\n",
            "Iteration 28, loss = 0.59648416\n",
            "Iteration 29, loss = 0.59434902\n",
            "Iteration 30, loss = 0.59185949\n",
            "Iteration 31, loss = 0.59019617\n",
            "Iteration 32, loss = 0.58768626\n",
            "Iteration 33, loss = 0.58639636\n",
            "Iteration 34, loss = 0.58274591\n",
            "Iteration 35, loss = 0.58036253\n",
            "Iteration 36, loss = 0.57785610\n",
            "Iteration 37, loss = 0.57464183\n",
            "Iteration 38, loss = 0.57208919\n",
            "Iteration 39, loss = 0.57147396\n",
            "Iteration 40, loss = 0.56706445\n",
            "Iteration 41, loss = 0.56359604\n",
            "Iteration 42, loss = 0.56239296\n",
            "Iteration 43, loss = 0.55940925\n",
            "Iteration 44, loss = 0.55597616\n",
            "Iteration 45, loss = 0.55155927\n",
            "Iteration 46, loss = 0.54956905\n",
            "Iteration 47, loss = 0.54481154\n",
            "Iteration 48, loss = 0.54353323\n",
            "Iteration 49, loss = 0.54408206\n",
            "Iteration 50, loss = 0.54264173\n",
            "Iteration 51, loss = 0.53256966\n",
            "Iteration 52, loss = 0.53060981\n",
            "Iteration 53, loss = 0.52715170\n",
            "Iteration 54, loss = 0.52461626\n",
            "Iteration 55, loss = 0.52153354\n",
            "Iteration 56, loss = 0.51945443\n",
            "Iteration 57, loss = 0.51454597\n",
            "Iteration 58, loss = 0.51484757\n",
            "Iteration 59, loss = 0.51103879\n",
            "Iteration 60, loss = 0.50698869\n",
            "Iteration 61, loss = 0.50253370\n",
            "Iteration 62, loss = 0.50122721\n",
            "Iteration 63, loss = 0.49880368\n",
            "Iteration 64, loss = 0.49399182\n",
            "Iteration 65, loss = 0.49270187\n",
            "Iteration 66, loss = 0.48782825\n",
            "Iteration 67, loss = 0.48967306\n",
            "Iteration 68, loss = 0.48438660\n",
            "Iteration 69, loss = 0.47989443\n",
            "Iteration 70, loss = 0.48229930\n",
            "Iteration 71, loss = 0.47347677\n",
            "Iteration 72, loss = 0.47056734\n",
            "Iteration 73, loss = 0.46581889\n",
            "Iteration 74, loss = 0.46159239\n",
            "Iteration 75, loss = 0.45902405\n",
            "Iteration 76, loss = 0.45528388\n",
            "Iteration 77, loss = 0.45320106\n",
            "Iteration 78, loss = 0.44698832\n",
            "Iteration 79, loss = 0.44924048\n",
            "Iteration 80, loss = 0.44548298\n",
            "Iteration 81, loss = 0.44088614\n",
            "Iteration 82, loss = 0.43624896\n",
            "Iteration 83, loss = 0.43249289\n",
            "Iteration 84, loss = 0.43209215\n",
            "Iteration 85, loss = 0.43251781\n",
            "Iteration 86, loss = 0.42601238\n",
            "Iteration 87, loss = 0.42520453\n",
            "Iteration 88, loss = 0.42341577\n",
            "Iteration 89, loss = 0.43242427\n",
            "Iteration 90, loss = 0.42381992\n",
            "Iteration 91, loss = 0.42730383\n",
            "Iteration 92, loss = 0.42879271\n",
            "Iteration 93, loss = 0.41442030\n",
            "Iteration 94, loss = 0.41387722\n",
            "Iteration 95, loss = 0.41108069\n",
            "Iteration 96, loss = 0.40944659\n",
            "Iteration 97, loss = 0.40868350\n",
            "Iteration 98, loss = 0.40707326\n",
            "Iteration 99, loss = 0.41063303\n",
            "Iteration 100, loss = 0.40426632\n",
            "Iteration 101, loss = 0.40643991\n",
            "Iteration 102, loss = 0.40846282\n",
            "Iteration 103, loss = 0.40753184\n",
            "Iteration 104, loss = 0.40646922\n",
            "Iteration 105, loss = 0.40385785\n",
            "Iteration 106, loss = 0.39997435\n",
            "Iteration 107, loss = 0.39944723\n",
            "Iteration 108, loss = 0.39698665\n",
            "Iteration 109, loss = 0.39662702\n",
            "Iteration 110, loss = 0.39606722\n",
            "Iteration 111, loss = 0.39606130\n",
            "Iteration 112, loss = 0.39655699\n",
            "Iteration 113, loss = 0.39515771\n",
            "Iteration 114, loss = 0.39456206\n",
            "Iteration 115, loss = 0.39277258\n",
            "Iteration 116, loss = 0.39189996\n",
            "Iteration 117, loss = 0.39243585\n",
            "Iteration 118, loss = 0.39064772\n",
            "Iteration 119, loss = 0.38954160\n",
            "Iteration 120, loss = 0.38852920\n",
            "Iteration 121, loss = 0.39063615\n",
            "Iteration 122, loss = 0.38789598\n",
            "Iteration 123, loss = 0.39385647\n",
            "Iteration 124, loss = 0.39494151\n",
            "Iteration 125, loss = 0.38930619\n",
            "Iteration 126, loss = 0.38869607\n",
            "Iteration 127, loss = 0.38679118\n",
            "Iteration 128, loss = 0.38459930\n",
            "Iteration 129, loss = 0.38277003\n",
            "Iteration 130, loss = 0.38049214\n",
            "Iteration 131, loss = 0.38282323\n",
            "Iteration 132, loss = 0.38059076\n",
            "Iteration 133, loss = 0.38046223\n",
            "Iteration 134, loss = 0.38143538\n",
            "Iteration 135, loss = 0.38165010\n",
            "Iteration 136, loss = 0.38501008\n",
            "Iteration 137, loss = 0.38089499\n",
            "Iteration 138, loss = 0.37864211\n",
            "Iteration 139, loss = 0.38104378\n",
            "Iteration 140, loss = 0.37975924\n",
            "Iteration 141, loss = 0.37886015\n",
            "Iteration 142, loss = 0.38361641\n",
            "Iteration 143, loss = 0.38128696\n",
            "Iteration 144, loss = 0.37559161\n",
            "Iteration 145, loss = 0.37225986\n",
            "Iteration 146, loss = 0.37467075\n",
            "Iteration 147, loss = 0.37332829\n",
            "Iteration 148, loss = 0.37645090\n",
            "Iteration 149, loss = 0.37157461\n",
            "Iteration 150, loss = 0.37177319\n",
            "Iteration 151, loss = 0.37542070\n",
            "Iteration 152, loss = 0.37442868\n",
            "Iteration 153, loss = 0.37540411\n",
            "Iteration 154, loss = 0.37753026\n",
            "Iteration 155, loss = 0.37101512\n",
            "Iteration 156, loss = 0.36838294\n",
            "Iteration 157, loss = 0.36798202\n",
            "Iteration 158, loss = 0.36962261\n",
            "Iteration 159, loss = 0.37475234\n",
            "Iteration 160, loss = 0.37263806\n",
            "Iteration 161, loss = 0.37386360\n",
            "Iteration 162, loss = 0.36915680\n",
            "Iteration 163, loss = 0.36588049\n",
            "Iteration 164, loss = 0.37332341\n",
            "Iteration 165, loss = 0.36771066\n",
            "Iteration 166, loss = 0.36568074\n",
            "Iteration 167, loss = 0.36472519\n",
            "Iteration 168, loss = 0.36329379\n",
            "Iteration 169, loss = 0.36287852\n",
            "Iteration 170, loss = 0.36829428\n",
            "Iteration 171, loss = 0.36795809\n",
            "Iteration 172, loss = 0.36538308\n",
            "Iteration 173, loss = 0.36358233\n",
            "Iteration 174, loss = 0.36173083\n",
            "Iteration 175, loss = 0.37003196\n",
            "Iteration 176, loss = 0.36777142\n",
            "Iteration 177, loss = 0.36335740\n",
            "Iteration 178, loss = 0.36665054\n",
            "Iteration 179, loss = 0.36280629\n",
            "Iteration 180, loss = 0.36166555\n",
            "Iteration 181, loss = 0.37267607\n",
            "Iteration 182, loss = 0.36506454\n",
            "Iteration 183, loss = 0.35982834\n",
            "Iteration 184, loss = 0.35937511\n",
            "Iteration 185, loss = 0.36043411\n",
            "Iteration 186, loss = 0.36299323\n",
            "Iteration 187, loss = 0.35963427\n",
            "Iteration 188, loss = 0.36071553\n",
            "Iteration 189, loss = 0.35605129\n",
            "Iteration 190, loss = 0.35661438\n",
            "Iteration 191, loss = 0.36371809\n",
            "Iteration 192, loss = 0.35922328\n",
            "Iteration 193, loss = 0.35894223\n",
            "Iteration 194, loss = 0.35560587\n",
            "Iteration 195, loss = 0.35589928\n",
            "Iteration 196, loss = 0.35653142\n",
            "Iteration 197, loss = 0.36083008\n",
            "Iteration 198, loss = 0.36913897\n",
            "Iteration 199, loss = 0.36051015\n",
            "Iteration 200, loss = 0.35938025\n",
            "Iteration 1, loss = 0.63400149\n",
            "Iteration 2, loss = 0.59985611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3, loss = 0.59602183\n",
            "Iteration 4, loss = 0.58756210\n",
            "Iteration 5, loss = 0.58828267\n",
            "Iteration 6, loss = 0.58411625\n",
            "Iteration 7, loss = 0.58155328\n",
            "Iteration 8, loss = 0.58164146\n",
            "Iteration 9, loss = 0.57845809\n",
            "Iteration 10, loss = 0.58166728\n",
            "Iteration 11, loss = 0.58565442\n",
            "Iteration 12, loss = 0.57428082\n",
            "Iteration 13, loss = 0.57236785\n",
            "Iteration 14, loss = 0.57159654\n",
            "Iteration 15, loss = 0.57112617\n",
            "Iteration 16, loss = 0.58149399\n",
            "Iteration 17, loss = 0.56885408\n",
            "Iteration 18, loss = 0.57041776\n",
            "Iteration 19, loss = 0.56344760\n",
            "Iteration 20, loss = 0.55540244\n",
            "Iteration 21, loss = 0.56029365\n",
            "Iteration 22, loss = 0.55467855\n",
            "Iteration 23, loss = 0.55416148\n",
            "Iteration 24, loss = 0.55094996\n",
            "Iteration 25, loss = 0.56628505\n",
            "Iteration 26, loss = 0.55382135\n",
            "Iteration 27, loss = 0.56099010\n",
            "Iteration 28, loss = 0.55853141\n",
            "Iteration 29, loss = 0.56465285\n",
            "Iteration 30, loss = 0.55883978\n",
            "Iteration 31, loss = 0.55967198\n",
            "Iteration 32, loss = 0.55885920\n",
            "Iteration 33, loss = 0.56090654\n",
            "Iteration 34, loss = 0.55726208\n",
            "Iteration 35, loss = 0.54711357\n",
            "Iteration 36, loss = 0.53871811\n",
            "Iteration 37, loss = 0.54696166\n",
            "Iteration 38, loss = 0.53867736\n",
            "Iteration 39, loss = 0.52998519\n",
            "Iteration 40, loss = 0.55542792\n",
            "Iteration 41, loss = 0.54320758\n",
            "Iteration 42, loss = 0.54454900\n",
            "Iteration 43, loss = 0.52378035\n",
            "Iteration 44, loss = 0.53076889\n",
            "Iteration 45, loss = 0.54245182\n",
            "Iteration 46, loss = 0.53103530\n",
            "Iteration 47, loss = 0.52842705\n",
            "Iteration 48, loss = 0.52885872\n",
            "Iteration 49, loss = 0.52459678\n",
            "Iteration 50, loss = 0.53586686\n",
            "Iteration 51, loss = 0.53103756\n",
            "Iteration 52, loss = 0.53498370\n",
            "Iteration 53, loss = 0.52686474\n",
            "Iteration 54, loss = 0.51350333\n",
            "Iteration 55, loss = 0.59009223\n",
            "Iteration 56, loss = 0.52256818\n",
            "Iteration 57, loss = 0.52415076\n",
            "Iteration 58, loss = 0.52383255\n",
            "Iteration 59, loss = 0.53234095\n",
            "Iteration 60, loss = 0.53262139\n",
            "Iteration 61, loss = 0.52790062\n",
            "Iteration 62, loss = 0.53059582\n",
            "Iteration 63, loss = 0.51474758\n",
            "Iteration 64, loss = 0.51176935\n",
            "Iteration 65, loss = 0.52772629\n",
            "Iteration 66, loss = 0.50847663\n",
            "Iteration 67, loss = 0.53646887\n",
            "Iteration 68, loss = 0.50676255\n",
            "Iteration 69, loss = 0.52654773\n",
            "Iteration 70, loss = 0.51994845\n",
            "Iteration 71, loss = 0.51607122\n",
            "Iteration 72, loss = 0.52003064\n",
            "Iteration 73, loss = 0.52515500\n",
            "Iteration 74, loss = 0.53465304\n",
            "Iteration 75, loss = 0.52838781\n",
            "Iteration 76, loss = 0.52710054\n",
            "Iteration 77, loss = 0.49672074\n",
            "Iteration 78, loss = 0.51212282\n",
            "Iteration 79, loss = 0.52565818\n",
            "Iteration 80, loss = 0.51070374\n",
            "Iteration 81, loss = 0.50330417\n",
            "Iteration 82, loss = 0.50203877\n",
            "Iteration 83, loss = 0.51140421\n",
            "Iteration 84, loss = 0.50563174\n",
            "Iteration 85, loss = 0.50774969\n",
            "Iteration 86, loss = 0.48634891\n",
            "Iteration 87, loss = 0.49311416\n",
            "Iteration 88, loss = 0.49579820\n",
            "Iteration 89, loss = 0.50819569\n",
            "Iteration 90, loss = 0.50083095\n",
            "Iteration 91, loss = 0.52227907\n",
            "Iteration 92, loss = 0.53830976\n",
            "Iteration 93, loss = 0.54488770\n",
            "Iteration 94, loss = 0.51432562\n",
            "Iteration 95, loss = 0.52391307\n",
            "Iteration 96, loss = 0.49941356\n",
            "Iteration 97, loss = 0.50715099\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62734828\n",
            "Iteration 2, loss = 0.60114422\n",
            "Iteration 3, loss = 0.58950456\n",
            "Iteration 4, loss = 0.58525989\n",
            "Iteration 5, loss = 0.59210490\n",
            "Iteration 6, loss = 0.58633563\n",
            "Iteration 7, loss = 0.58643586\n",
            "Iteration 8, loss = 0.58548581\n",
            "Iteration 9, loss = 0.57813445\n",
            "Iteration 10, loss = 0.57601554\n",
            "Iteration 11, loss = 0.57843225\n",
            "Iteration 12, loss = 0.57274298\n",
            "Iteration 13, loss = 0.57286058\n",
            "Iteration 14, loss = 0.57142962\n",
            "Iteration 15, loss = 0.57808277\n",
            "Iteration 16, loss = 0.57078584\n",
            "Iteration 17, loss = 0.56915142\n",
            "Iteration 18, loss = 0.56759243\n",
            "Iteration 19, loss = 0.56256882\n",
            "Iteration 20, loss = 0.56524024\n",
            "Iteration 21, loss = 0.56209971\n",
            "Iteration 22, loss = 0.56045611\n",
            "Iteration 23, loss = 0.56906868\n",
            "Iteration 24, loss = 0.57397505\n",
            "Iteration 25, loss = 0.56365033\n",
            "Iteration 26, loss = 0.55369801\n",
            "Iteration 27, loss = 0.54825564\n",
            "Iteration 28, loss = 0.55781066\n",
            "Iteration 29, loss = 0.55611223\n",
            "Iteration 30, loss = 0.55177731\n",
            "Iteration 31, loss = 0.57036270\n",
            "Iteration 32, loss = 0.56090776\n",
            "Iteration 33, loss = 0.54716706\n",
            "Iteration 34, loss = 0.55174892\n",
            "Iteration 35, loss = 0.54726025\n",
            "Iteration 36, loss = 0.54310773\n",
            "Iteration 37, loss = 0.53764074\n",
            "Iteration 38, loss = 0.53775424\n",
            "Iteration 39, loss = 0.55274343\n",
            "Iteration 40, loss = 0.53035529\n",
            "Iteration 41, loss = 0.53367033\n",
            "Iteration 42, loss = 0.54451962\n",
            "Iteration 43, loss = 0.54947894\n",
            "Iteration 44, loss = 0.54786468\n",
            "Iteration 45, loss = 0.54305806\n",
            "Iteration 46, loss = 0.54070271\n",
            "Iteration 47, loss = 0.52914098\n",
            "Iteration 48, loss = 0.53981662\n",
            "Iteration 49, loss = 0.52431517\n",
            "Iteration 50, loss = 0.52877388\n",
            "Iteration 51, loss = 0.53246877\n",
            "Iteration 52, loss = 0.53747931\n",
            "Iteration 53, loss = 0.54585145\n",
            "Iteration 54, loss = 0.54932798\n",
            "Iteration 55, loss = 0.54184467\n",
            "Iteration 56, loss = 0.52883008\n",
            "Iteration 57, loss = 0.53772183\n",
            "Iteration 58, loss = 0.51719819\n",
            "Iteration 59, loss = 0.52375987\n",
            "Iteration 60, loss = 0.51982280\n",
            "Iteration 61, loss = 0.51677296\n",
            "Iteration 62, loss = 0.52835771\n",
            "Iteration 63, loss = 0.50897913\n",
            "Iteration 64, loss = 0.51199942\n",
            "Iteration 65, loss = 0.52111456\n",
            "Iteration 66, loss = 0.52115985\n",
            "Iteration 67, loss = 0.52720391\n",
            "Iteration 68, loss = 0.51348927\n",
            "Iteration 69, loss = 0.51339101\n",
            "Iteration 70, loss = 0.53152269\n",
            "Iteration 71, loss = 0.51644454\n",
            "Iteration 72, loss = 0.50992186\n",
            "Iteration 73, loss = 0.49513671\n",
            "Iteration 74, loss = 0.51204832\n",
            "Iteration 75, loss = 0.50475965\n",
            "Iteration 76, loss = 0.51718466\n",
            "Iteration 77, loss = 0.54775421\n",
            "Iteration 78, loss = 0.52127798\n",
            "Iteration 79, loss = 0.53301604\n",
            "Iteration 80, loss = 0.54362872\n",
            "Iteration 81, loss = 0.51549538\n",
            "Iteration 82, loss = 0.52349327\n",
            "Iteration 83, loss = 0.53658983\n",
            "Iteration 84, loss = 0.51926714\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.63616488\n",
            "Iteration 2, loss = 0.59744108\n",
            "Iteration 3, loss = 0.60746111\n",
            "Iteration 4, loss = 0.59203105\n",
            "Iteration 5, loss = 0.58268275\n",
            "Iteration 6, loss = 0.58682169\n",
            "Iteration 7, loss = 0.58485514\n",
            "Iteration 8, loss = 0.57058688\n",
            "Iteration 9, loss = 0.56899114\n",
            "Iteration 10, loss = 0.57900299\n",
            "Iteration 11, loss = 0.57212545\n",
            "Iteration 12, loss = 0.56542825\n",
            "Iteration 13, loss = 0.56214097\n",
            "Iteration 14, loss = 0.56698095\n",
            "Iteration 15, loss = 0.55884802\n",
            "Iteration 16, loss = 0.55862687\n",
            "Iteration 17, loss = 0.55687514\n",
            "Iteration 18, loss = 0.55571158\n",
            "Iteration 19, loss = 0.54671545\n",
            "Iteration 20, loss = 0.54722154\n",
            "Iteration 21, loss = 0.55686841\n",
            "Iteration 22, loss = 0.54481750\n",
            "Iteration 23, loss = 0.55732295\n",
            "Iteration 24, loss = 0.54545847\n",
            "Iteration 25, loss = 0.53883624\n",
            "Iteration 26, loss = 0.54036553\n",
            "Iteration 27, loss = 0.54263794\n",
            "Iteration 28, loss = 0.53911197\n",
            "Iteration 29, loss = 0.54439930\n",
            "Iteration 30, loss = 0.55111955\n",
            "Iteration 31, loss = 0.52270826\n",
            "Iteration 32, loss = 0.53779937\n",
            "Iteration 33, loss = 0.53343035\n",
            "Iteration 34, loss = 0.53663341\n",
            "Iteration 35, loss = 0.53121949\n",
            "Iteration 36, loss = 0.53233502\n",
            "Iteration 37, loss = 0.53505491\n",
            "Iteration 38, loss = 0.54370142\n",
            "Iteration 39, loss = 0.53862678\n",
            "Iteration 40, loss = 0.51991663\n",
            "Iteration 41, loss = 0.54843741\n",
            "Iteration 42, loss = 0.54186353\n",
            "Iteration 43, loss = 0.54238127\n",
            "Iteration 44, loss = 0.53070745\n",
            "Iteration 45, loss = 0.52597585\n",
            "Iteration 46, loss = 0.56244193\n",
            "Iteration 47, loss = 0.52726303\n",
            "Iteration 48, loss = 0.51950907\n",
            "Iteration 49, loss = 0.52905182\n",
            "Iteration 50, loss = 0.52031813\n",
            "Iteration 51, loss = 0.51556146\n",
            "Iteration 52, loss = 0.52058793\n",
            "Iteration 53, loss = 0.52214336\n",
            "Iteration 54, loss = 0.51421030\n",
            "Iteration 55, loss = 0.50551904\n",
            "Iteration 56, loss = 0.52934391\n",
            "Iteration 57, loss = 0.52181687\n",
            "Iteration 58, loss = 0.53358906\n",
            "Iteration 59, loss = 0.51140390\n",
            "Iteration 60, loss = 0.51442451\n",
            "Iteration 61, loss = 0.51974322\n",
            "Iteration 62, loss = 0.53743174\n",
            "Iteration 63, loss = 0.51797847\n",
            "Iteration 64, loss = 0.50351998\n",
            "Iteration 65, loss = 0.51338882\n",
            "Iteration 66, loss = 0.51589127\n",
            "Iteration 67, loss = 0.50749279\n",
            "Iteration 68, loss = 0.52255629\n",
            "Iteration 69, loss = 0.51268960\n",
            "Iteration 70, loss = 0.50156655\n",
            "Iteration 71, loss = 0.50999296\n",
            "Iteration 72, loss = 0.52240039\n",
            "Iteration 73, loss = 0.49934723\n",
            "Iteration 74, loss = 0.50867094\n",
            "Iteration 75, loss = 0.50218518\n",
            "Iteration 76, loss = 0.50842772\n",
            "Iteration 77, loss = 0.52069419\n",
            "Iteration 78, loss = 0.51933874\n",
            "Iteration 79, loss = 0.52030427\n",
            "Iteration 80, loss = 0.52499303\n",
            "Iteration 81, loss = 0.51627921\n",
            "Iteration 82, loss = 0.51736289\n",
            "Iteration 83, loss = 0.49971818\n",
            "Iteration 84, loss = 0.50414597\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.65490889\n",
            "Iteration 2, loss = 0.61676519\n",
            "Iteration 3, loss = 0.60943327\n",
            "Iteration 4, loss = 0.61078440\n",
            "Iteration 5, loss = 0.60417839\n",
            "Iteration 6, loss = 0.61467805\n",
            "Iteration 7, loss = 0.60983174\n",
            "Iteration 8, loss = 0.59124771\n",
            "Iteration 9, loss = 0.59550591\n",
            "Iteration 10, loss = 0.59308824\n",
            "Iteration 11, loss = 0.59461434\n",
            "Iteration 12, loss = 0.59649084\n",
            "Iteration 13, loss = 0.59662289\n",
            "Iteration 14, loss = 0.59207718\n",
            "Iteration 15, loss = 0.58181181\n",
            "Iteration 16, loss = 0.58418818\n",
            "Iteration 17, loss = 0.58339197\n",
            "Iteration 18, loss = 0.59118544\n",
            "Iteration 19, loss = 0.58132373\n",
            "Iteration 20, loss = 0.57096631\n",
            "Iteration 21, loss = 0.57931227\n",
            "Iteration 22, loss = 0.59042002\n",
            "Iteration 23, loss = 0.56884972\n",
            "Iteration 24, loss = 0.56939121\n",
            "Iteration 25, loss = 0.56744305\n",
            "Iteration 26, loss = 0.55410207\n",
            "Iteration 27, loss = 0.56730898\n",
            "Iteration 28, loss = 0.54730805\n",
            "Iteration 29, loss = 0.56196049\n",
            "Iteration 30, loss = 0.55646439\n",
            "Iteration 31, loss = 0.55084278\n",
            "Iteration 32, loss = 0.55543858\n",
            "Iteration 33, loss = 0.56233366\n",
            "Iteration 34, loss = 0.56104760\n",
            "Iteration 35, loss = 0.56037327\n",
            "Iteration 36, loss = 0.55509594\n",
            "Iteration 37, loss = 0.57927658\n",
            "Iteration 38, loss = 0.55138166\n",
            "Iteration 39, loss = 0.54378753\n",
            "Iteration 40, loss = 0.56975329\n",
            "Iteration 41, loss = 0.53403504\n",
            "Iteration 42, loss = 0.54969807\n",
            "Iteration 43, loss = 0.54920052\n",
            "Iteration 44, loss = 0.53987499\n",
            "Iteration 45, loss = 0.54809704\n",
            "Iteration 46, loss = 0.53344082\n",
            "Iteration 47, loss = 0.56219890\n",
            "Iteration 48, loss = 0.55302314\n",
            "Iteration 49, loss = 0.55869592\n",
            "Iteration 50, loss = 0.53759658\n",
            "Iteration 51, loss = 0.56450973\n",
            "Iteration 52, loss = 0.53320282\n",
            "Iteration 53, loss = 0.52417906\n",
            "Iteration 54, loss = 0.53401662\n",
            "Iteration 55, loss = 0.52975134\n",
            "Iteration 56, loss = 0.53225654\n",
            "Iteration 57, loss = 0.53147768\n",
            "Iteration 58, loss = 0.52274096\n",
            "Iteration 59, loss = 0.53075390\n",
            "Iteration 60, loss = 0.51876826\n",
            "Iteration 61, loss = 0.52518289\n",
            "Iteration 62, loss = 0.53181158\n",
            "Iteration 63, loss = 0.52771817\n",
            "Iteration 64, loss = 0.52813245\n",
            "Iteration 65, loss = 0.52457781\n",
            "Iteration 66, loss = 0.52605427\n",
            "Iteration 67, loss = 0.52772947\n",
            "Iteration 68, loss = 0.52983054\n",
            "Iteration 69, loss = 0.52828589\n",
            "Iteration 70, loss = 0.53081319\n",
            "Iteration 71, loss = 0.53554750\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62296739\n",
            "Iteration 2, loss = 0.63095679\n",
            "Iteration 3, loss = 0.61490413\n",
            "Iteration 4, loss = 0.60588264\n",
            "Iteration 5, loss = 0.60132297\n",
            "Iteration 6, loss = 0.60088111\n",
            "Iteration 7, loss = 0.59421585\n",
            "Iteration 8, loss = 0.60115601\n",
            "Iteration 9, loss = 0.58937638\n",
            "Iteration 10, loss = 0.59362739\n",
            "Iteration 11, loss = 0.58027591\n",
            "Iteration 12, loss = 0.58826830\n",
            "Iteration 13, loss = 0.59279689\n",
            "Iteration 14, loss = 0.59715796\n",
            "Iteration 15, loss = 0.58374872\n",
            "Iteration 16, loss = 0.59020078\n",
            "Iteration 17, loss = 0.60421578\n",
            "Iteration 18, loss = 0.59188089\n",
            "Iteration 19, loss = 0.58578041\n",
            "Iteration 20, loss = 0.59727463\n",
            "Iteration 21, loss = 0.59087777\n",
            "Iteration 22, loss = 0.57880359\n",
            "Iteration 23, loss = 0.57912533\n",
            "Iteration 24, loss = 0.57778223\n",
            "Iteration 25, loss = 0.56985710\n",
            "Iteration 26, loss = 0.56818242\n",
            "Iteration 27, loss = 0.56663461\n",
            "Iteration 28, loss = 0.57723326\n",
            "Iteration 29, loss = 0.54873395\n",
            "Iteration 30, loss = 0.56345680\n",
            "Iteration 31, loss = 0.56289018\n",
            "Iteration 32, loss = 0.55502099\n",
            "Iteration 33, loss = 0.55372337\n",
            "Iteration 34, loss = 0.55824252\n",
            "Iteration 35, loss = 0.55662665\n",
            "Iteration 36, loss = 0.56194799\n",
            "Iteration 37, loss = 0.57293382\n",
            "Iteration 38, loss = 0.54957308\n",
            "Iteration 39, loss = 0.55029706\n",
            "Iteration 40, loss = 0.54866400\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.65453683\n",
            "Iteration 2, loss = 0.62407280\n",
            "Iteration 3, loss = 0.60384499\n",
            "Iteration 4, loss = 0.59873845\n",
            "Iteration 5, loss = 0.60280798\n",
            "Iteration 6, loss = 0.59671464\n",
            "Iteration 7, loss = 0.59420651\n",
            "Iteration 8, loss = 0.59235935\n",
            "Iteration 9, loss = 0.58885668\n",
            "Iteration 10, loss = 0.59168804\n",
            "Iteration 11, loss = 0.58679648\n",
            "Iteration 12, loss = 0.58013692\n",
            "Iteration 13, loss = 0.58817904\n",
            "Iteration 14, loss = 0.57853399\n",
            "Iteration 15, loss = 0.57870983\n",
            "Iteration 16, loss = 0.57194336\n",
            "Iteration 17, loss = 0.56824840\n",
            "Iteration 18, loss = 0.55975165\n",
            "Iteration 19, loss = 0.55890762\n",
            "Iteration 20, loss = 0.54921845\n",
            "Iteration 21, loss = 0.54242536\n",
            "Iteration 22, loss = 0.53826466\n",
            "Iteration 23, loss = 0.53308949\n",
            "Iteration 24, loss = 0.53038571\n",
            "Iteration 25, loss = 0.51775356\n",
            "Iteration 26, loss = 0.51480287\n",
            "Iteration 27, loss = 0.50452986\n",
            "Iteration 28, loss = 0.50756937\n",
            "Iteration 29, loss = 0.49605925\n",
            "Iteration 30, loss = 0.49114727\n",
            "Iteration 31, loss = 0.48216492\n",
            "Iteration 32, loss = 0.48885119\n",
            "Iteration 33, loss = 0.49063954\n",
            "Iteration 34, loss = 0.49285248\n",
            "Iteration 35, loss = 0.46930938\n",
            "Iteration 36, loss = 0.46739378\n",
            "Iteration 37, loss = 0.46339880\n",
            "Iteration 38, loss = 0.46389054\n",
            "Iteration 39, loss = 0.45402094\n",
            "Iteration 40, loss = 0.46469405\n",
            "Iteration 41, loss = 0.45968584\n",
            "Iteration 42, loss = 0.45071528\n",
            "Iteration 43, loss = 0.44800814\n",
            "Iteration 44, loss = 0.46073706\n",
            "Iteration 45, loss = 0.45007698\n",
            "Iteration 46, loss = 0.44835072\n",
            "Iteration 47, loss = 0.45278839\n",
            "Iteration 48, loss = 0.44157530\n",
            "Iteration 49, loss = 0.44142055\n",
            "Iteration 50, loss = 0.44205913\n",
            "Iteration 51, loss = 0.44201428\n",
            "Iteration 52, loss = 0.45126617\n",
            "Iteration 53, loss = 0.44701946\n",
            "Iteration 54, loss = 0.43660993\n",
            "Iteration 55, loss = 0.43667121\n",
            "Iteration 56, loss = 0.45701084\n",
            "Iteration 57, loss = 0.44669196\n",
            "Iteration 58, loss = 0.43706827\n",
            "Iteration 59, loss = 0.44023294\n",
            "Iteration 60, loss = 0.43595571\n",
            "Iteration 61, loss = 0.43815636\n",
            "Iteration 62, loss = 0.43072290\n",
            "Iteration 63, loss = 0.43692960\n",
            "Iteration 64, loss = 0.44365970\n",
            "Iteration 65, loss = 0.43173691\n",
            "Iteration 66, loss = 0.43754487\n",
            "Iteration 67, loss = 0.43229828\n",
            "Iteration 68, loss = 0.42808617\n",
            "Iteration 69, loss = 0.42605853\n",
            "Iteration 70, loss = 0.43110336\n",
            "Iteration 71, loss = 0.44511183\n",
            "Iteration 72, loss = 0.43275440\n",
            "Iteration 73, loss = 0.42627185\n",
            "Iteration 74, loss = 0.43281669\n",
            "Iteration 75, loss = 0.43440508\n",
            "Iteration 76, loss = 0.42610917\n",
            "Iteration 77, loss = 0.42581274\n",
            "Iteration 78, loss = 0.42847816\n",
            "Iteration 79, loss = 0.42778610\n",
            "Iteration 80, loss = 0.42654614\n",
            "Iteration 81, loss = 0.45147558\n",
            "Iteration 82, loss = 0.42672533\n",
            "Iteration 83, loss = 0.42248294\n",
            "Iteration 84, loss = 0.42030202\n",
            "Iteration 85, loss = 0.42551194\n",
            "Iteration 86, loss = 0.42581972\n",
            "Iteration 87, loss = 0.41860232\n",
            "Iteration 88, loss = 0.41961034\n",
            "Iteration 89, loss = 0.41979341\n",
            "Iteration 90, loss = 0.42825856\n",
            "Iteration 91, loss = 0.43675954\n",
            "Iteration 92, loss = 0.42284682\n",
            "Iteration 93, loss = 0.41969479\n",
            "Iteration 94, loss = 0.42374759\n",
            "Iteration 95, loss = 0.44524399\n",
            "Iteration 96, loss = 0.45880532\n",
            "Iteration 97, loss = 0.46121075\n",
            "Iteration 98, loss = 0.43243955\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.65970379\n",
            "Iteration 2, loss = 0.61640715\n",
            "Iteration 3, loss = 0.61014233\n",
            "Iteration 4, loss = 0.60284435\n",
            "Iteration 5, loss = 0.60531187\n",
            "Iteration 6, loss = 0.60435121\n",
            "Iteration 7, loss = 0.60242783\n",
            "Iteration 8, loss = 0.60184587\n",
            "Iteration 9, loss = 0.59344829\n",
            "Iteration 10, loss = 0.60303980\n",
            "Iteration 11, loss = 0.59070916\n",
            "Iteration 12, loss = 0.58553724\n",
            "Iteration 13, loss = 0.58476363\n",
            "Iteration 14, loss = 0.58562203\n",
            "Iteration 15, loss = 0.58379719\n",
            "Iteration 16, loss = 0.57936278\n",
            "Iteration 17, loss = 0.57793550\n",
            "Iteration 18, loss = 0.57843218\n",
            "Iteration 19, loss = 0.56518392\n",
            "Iteration 20, loss = 0.56457682\n",
            "Iteration 21, loss = 0.55661612\n",
            "Iteration 22, loss = 0.55202257\n",
            "Iteration 23, loss = 0.54565908\n",
            "Iteration 24, loss = 0.53947107\n",
            "Iteration 25, loss = 0.53227100\n",
            "Iteration 26, loss = 0.53019177\n",
            "Iteration 27, loss = 0.51778516\n",
            "Iteration 28, loss = 0.51301348\n",
            "Iteration 29, loss = 0.50652955\n",
            "Iteration 30, loss = 0.50007291\n",
            "Iteration 31, loss = 0.49997271\n",
            "Iteration 32, loss = 0.49408733\n",
            "Iteration 33, loss = 0.49463146\n",
            "Iteration 34, loss = 0.48130681\n",
            "Iteration 35, loss = 0.48426739\n",
            "Iteration 36, loss = 0.46814351\n",
            "Iteration 37, loss = 0.46241082\n",
            "Iteration 38, loss = 0.46496731\n",
            "Iteration 39, loss = 0.45315465\n",
            "Iteration 40, loss = 0.45491482\n",
            "Iteration 41, loss = 0.45037722\n",
            "Iteration 42, loss = 0.44699180\n",
            "Iteration 43, loss = 0.44647963\n",
            "Iteration 44, loss = 0.44640848\n",
            "Iteration 45, loss = 0.49839239\n",
            "Iteration 46, loss = 0.45091465\n",
            "Iteration 47, loss = 0.44800245\n",
            "Iteration 48, loss = 0.46689734\n",
            "Iteration 49, loss = 0.45109825\n",
            "Iteration 50, loss = 0.46095155\n",
            "Iteration 51, loss = 0.43402327\n",
            "Iteration 52, loss = 0.43339425\n",
            "Iteration 53, loss = 0.43116763\n",
            "Iteration 54, loss = 0.43078462\n",
            "Iteration 55, loss = 0.43243322\n",
            "Iteration 56, loss = 0.42744818\n",
            "Iteration 57, loss = 0.42633873\n",
            "Iteration 58, loss = 0.42616854\n",
            "Iteration 59, loss = 0.43334321\n",
            "Iteration 60, loss = 0.43531262\n",
            "Iteration 61, loss = 0.42800591\n",
            "Iteration 62, loss = 0.42920533\n",
            "Iteration 63, loss = 0.42313625\n",
            "Iteration 64, loss = 0.42614419\n",
            "Iteration 65, loss = 0.43302434\n",
            "Iteration 66, loss = 0.42577494\n",
            "Iteration 67, loss = 0.41779724\n",
            "Iteration 68, loss = 0.41904647\n",
            "Iteration 69, loss = 0.41984245\n",
            "Iteration 70, loss = 0.41625740\n",
            "Iteration 71, loss = 0.41482486\n",
            "Iteration 72, loss = 0.42394073\n",
            "Iteration 73, loss = 0.43400412\n",
            "Iteration 74, loss = 0.43138379\n",
            "Iteration 75, loss = 0.42064561\n",
            "Iteration 76, loss = 0.41543810\n",
            "Iteration 77, loss = 0.41623397\n",
            "Iteration 78, loss = 0.41239814\n",
            "Iteration 79, loss = 0.41353236\n",
            "Iteration 80, loss = 0.41504724\n",
            "Iteration 81, loss = 0.41270628\n",
            "Iteration 82, loss = 0.41344312\n",
            "Iteration 83, loss = 0.41014937\n",
            "Iteration 84, loss = 0.40924231\n",
            "Iteration 85, loss = 0.41361986\n",
            "Iteration 86, loss = 0.41477897\n",
            "Iteration 87, loss = 0.40947907\n",
            "Iteration 88, loss = 0.42029922\n",
            "Iteration 89, loss = 0.41759493\n",
            "Iteration 90, loss = 0.41959546\n",
            "Iteration 91, loss = 0.40630601\n",
            "Iteration 92, loss = 0.40773005\n",
            "Iteration 93, loss = 0.42083538\n",
            "Iteration 94, loss = 0.41765071\n",
            "Iteration 95, loss = 0.42215294\n",
            "Iteration 96, loss = 0.42676310\n",
            "Iteration 97, loss = 0.42062072\n",
            "Iteration 98, loss = 0.41448768\n",
            "Iteration 99, loss = 0.41437767\n",
            "Iteration 100, loss = 0.40340423\n",
            "Iteration 101, loss = 0.40355860\n",
            "Iteration 102, loss = 0.40288570\n",
            "Iteration 103, loss = 0.40590719\n",
            "Iteration 104, loss = 0.40481068\n",
            "Iteration 105, loss = 0.40293383\n",
            "Iteration 106, loss = 0.41229030\n",
            "Iteration 107, loss = 0.40963966\n",
            "Iteration 108, loss = 0.40686323\n",
            "Iteration 109, loss = 0.40239080\n",
            "Iteration 110, loss = 0.40506809\n",
            "Iteration 111, loss = 0.40071700\n",
            "Iteration 112, loss = 0.40463813\n",
            "Iteration 113, loss = 0.40148859\n",
            "Iteration 114, loss = 0.40891132\n",
            "Iteration 115, loss = 0.39848168\n",
            "Iteration 116, loss = 0.41175715\n",
            "Iteration 117, loss = 0.40836601\n",
            "Iteration 118, loss = 0.41200376\n",
            "Iteration 119, loss = 0.39885103\n",
            "Iteration 120, loss = 0.40884670\n",
            "Iteration 121, loss = 0.40631279\n",
            "Iteration 122, loss = 0.40001146\n",
            "Iteration 123, loss = 0.40106994\n",
            "Iteration 124, loss = 0.40262210\n",
            "Iteration 125, loss = 0.40293092\n",
            "Iteration 126, loss = 0.40173873\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66920217\n",
            "Iteration 2, loss = 0.62466203\n",
            "Iteration 3, loss = 0.60704088\n",
            "Iteration 4, loss = 0.60398154\n",
            "Iteration 5, loss = 0.60064237\n",
            "Iteration 6, loss = 0.59742716\n",
            "Iteration 7, loss = 0.59519985\n",
            "Iteration 8, loss = 0.59727665\n",
            "Iteration 9, loss = 0.59274367\n",
            "Iteration 10, loss = 0.59102877\n",
            "Iteration 11, loss = 0.58514112\n",
            "Iteration 12, loss = 0.58186458\n",
            "Iteration 13, loss = 0.57771417\n",
            "Iteration 14, loss = 0.57457523\n",
            "Iteration 15, loss = 0.56930572\n",
            "Iteration 16, loss = 0.56895610\n",
            "Iteration 17, loss = 0.56095768\n",
            "Iteration 18, loss = 0.55524315\n",
            "Iteration 19, loss = 0.54523050\n",
            "Iteration 20, loss = 0.54323036\n",
            "Iteration 21, loss = 0.53571345\n",
            "Iteration 22, loss = 0.52523168\n",
            "Iteration 23, loss = 0.51294764\n",
            "Iteration 24, loss = 0.50840315\n",
            "Iteration 25, loss = 0.49781283\n",
            "Iteration 26, loss = 0.48721971\n",
            "Iteration 27, loss = 0.48089283\n",
            "Iteration 28, loss = 0.49179805\n",
            "Iteration 29, loss = 0.46977828\n",
            "Iteration 30, loss = 0.46804885\n",
            "Iteration 31, loss = 0.45445672\n",
            "Iteration 32, loss = 0.46709302\n",
            "Iteration 33, loss = 0.47936693\n",
            "Iteration 34, loss = 0.46274422\n",
            "Iteration 35, loss = 0.45099218\n",
            "Iteration 36, loss = 0.44014558\n",
            "Iteration 37, loss = 0.44000262\n",
            "Iteration 38, loss = 0.43560411\n",
            "Iteration 39, loss = 0.43257469\n",
            "Iteration 40, loss = 0.45765280\n",
            "Iteration 41, loss = 0.44253475\n",
            "Iteration 42, loss = 0.42910176\n",
            "Iteration 43, loss = 0.42943722\n",
            "Iteration 44, loss = 0.43283382\n",
            "Iteration 45, loss = 0.43128491\n",
            "Iteration 46, loss = 0.42855277\n",
            "Iteration 47, loss = 0.42144577\n",
            "Iteration 48, loss = 0.41952545\n",
            "Iteration 49, loss = 0.42449056\n",
            "Iteration 50, loss = 0.42082615\n",
            "Iteration 51, loss = 0.42679544\n",
            "Iteration 52, loss = 0.41776285\n",
            "Iteration 53, loss = 0.41836274\n",
            "Iteration 54, loss = 0.41694098\n",
            "Iteration 55, loss = 0.41813283\n",
            "Iteration 56, loss = 0.42436292\n",
            "Iteration 57, loss = 0.41862574\n",
            "Iteration 58, loss = 0.42466745\n",
            "Iteration 59, loss = 0.42321657\n",
            "Iteration 60, loss = 0.42779632\n",
            "Iteration 61, loss = 0.43477597\n",
            "Iteration 62, loss = 0.41291408\n",
            "Iteration 63, loss = 0.42508122\n",
            "Iteration 64, loss = 0.45727486\n",
            "Iteration 65, loss = 0.41899346\n",
            "Iteration 66, loss = 0.42595935\n",
            "Iteration 67, loss = 0.41626969\n",
            "Iteration 68, loss = 0.42545173\n",
            "Iteration 69, loss = 0.40582521\n",
            "Iteration 70, loss = 0.41336778\n",
            "Iteration 71, loss = 0.40777231\n",
            "Iteration 72, loss = 0.40724274\n",
            "Iteration 73, loss = 0.40918694\n",
            "Iteration 74, loss = 0.41196541\n",
            "Iteration 75, loss = 0.43168413\n",
            "Iteration 76, loss = 0.43899265\n",
            "Iteration 77, loss = 0.42109328\n",
            "Iteration 78, loss = 0.40764205\n",
            "Iteration 79, loss = 0.40888630\n",
            "Iteration 80, loss = 0.40939762\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66524561\n",
            "Iteration 2, loss = 0.62812893\n",
            "Iteration 3, loss = 0.62461942\n",
            "Iteration 4, loss = 0.62069236\n",
            "Iteration 5, loss = 0.61651951\n",
            "Iteration 6, loss = 0.61476780\n",
            "Iteration 7, loss = 0.61166887\n",
            "Iteration 8, loss = 0.60926678\n",
            "Iteration 9, loss = 0.60805774\n",
            "Iteration 10, loss = 0.60699003\n",
            "Iteration 11, loss = 0.60403727\n",
            "Iteration 12, loss = 0.60001517\n",
            "Iteration 13, loss = 0.59701269\n",
            "Iteration 14, loss = 0.59013044\n",
            "Iteration 15, loss = 0.58618997\n",
            "Iteration 16, loss = 0.59074989\n",
            "Iteration 17, loss = 0.58569346\n",
            "Iteration 18, loss = 0.57563257\n",
            "Iteration 19, loss = 0.56482624\n",
            "Iteration 20, loss = 0.55891986\n",
            "Iteration 21, loss = 0.54805120\n",
            "Iteration 22, loss = 0.54160565\n",
            "Iteration 23, loss = 0.53332584\n",
            "Iteration 24, loss = 0.52856992\n",
            "Iteration 25, loss = 0.51681179\n",
            "Iteration 26, loss = 0.51684485\n",
            "Iteration 27, loss = 0.50066020\n",
            "Iteration 28, loss = 0.49500201\n",
            "Iteration 29, loss = 0.48541524\n",
            "Iteration 30, loss = 0.48105398\n",
            "Iteration 31, loss = 0.47557137\n",
            "Iteration 32, loss = 0.46965406\n",
            "Iteration 33, loss = 0.46997244\n",
            "Iteration 34, loss = 0.45962338\n",
            "Iteration 35, loss = 0.45861480\n",
            "Iteration 36, loss = 0.45905901\n",
            "Iteration 37, loss = 0.45197605\n",
            "Iteration 38, loss = 0.45484038\n",
            "Iteration 39, loss = 0.46376057\n",
            "Iteration 40, loss = 0.46373292\n",
            "Iteration 41, loss = 0.45224598\n",
            "Iteration 42, loss = 0.45368272\n",
            "Iteration 43, loss = 0.44657071\n",
            "Iteration 44, loss = 0.44658630\n",
            "Iteration 45, loss = 0.43235236\n",
            "Iteration 46, loss = 0.43858171\n",
            "Iteration 47, loss = 0.43559431\n",
            "Iteration 48, loss = 0.43927828\n",
            "Iteration 49, loss = 0.43401450\n",
            "Iteration 50, loss = 0.43535881\n",
            "Iteration 51, loss = 0.43568353\n",
            "Iteration 52, loss = 0.43276904\n",
            "Iteration 53, loss = 0.43166373\n",
            "Iteration 54, loss = 0.42999693\n",
            "Iteration 55, loss = 0.42968304\n",
            "Iteration 56, loss = 0.43237912\n",
            "Iteration 57, loss = 0.43260656\n",
            "Iteration 58, loss = 0.43125142\n",
            "Iteration 59, loss = 0.42952766\n",
            "Iteration 60, loss = 0.42798699\n",
            "Iteration 61, loss = 0.42895788\n",
            "Iteration 62, loss = 0.43464328\n",
            "Iteration 63, loss = 0.43393620\n",
            "Iteration 64, loss = 0.43129228\n",
            "Iteration 65, loss = 0.42859771\n",
            "Iteration 66, loss = 0.42860701\n",
            "Iteration 67, loss = 0.42679729\n",
            "Iteration 68, loss = 0.42820678\n",
            "Iteration 69, loss = 0.42380014\n",
            "Iteration 70, loss = 0.44043809\n",
            "Iteration 71, loss = 0.43811501\n",
            "Iteration 72, loss = 0.43477723\n",
            "Iteration 73, loss = 0.42332380\n",
            "Iteration 74, loss = 0.42936189\n",
            "Iteration 75, loss = 0.43021299\n",
            "Iteration 76, loss = 0.42676723\n",
            "Iteration 77, loss = 0.42773256\n",
            "Iteration 78, loss = 0.41858221\n",
            "Iteration 79, loss = 0.42811445\n",
            "Iteration 80, loss = 0.41968949\n",
            "Iteration 81, loss = 0.41879281\n",
            "Iteration 82, loss = 0.42142670\n",
            "Iteration 83, loss = 0.42147515\n",
            "Iteration 84, loss = 0.42015409\n",
            "Iteration 85, loss = 0.41781586\n",
            "Iteration 86, loss = 0.42818948\n",
            "Iteration 87, loss = 0.43389056\n",
            "Iteration 88, loss = 0.41630258\n",
            "Iteration 89, loss = 0.42251996\n",
            "Iteration 90, loss = 0.42053927\n",
            "Iteration 91, loss = 0.41891180\n",
            "Iteration 92, loss = 0.41565953\n",
            "Iteration 93, loss = 0.42015947\n",
            "Iteration 94, loss = 0.41690461\n",
            "Iteration 95, loss = 0.41805915\n",
            "Iteration 96, loss = 0.41367250\n",
            "Iteration 97, loss = 0.41776385\n",
            "Iteration 98, loss = 0.42008129\n",
            "Iteration 99, loss = 0.41662852\n",
            "Iteration 100, loss = 0.41276439\n",
            "Iteration 101, loss = 0.43417293\n",
            "Iteration 102, loss = 0.42964747\n",
            "Iteration 103, loss = 0.43472843\n",
            "Iteration 104, loss = 0.41309696\n",
            "Iteration 105, loss = 0.41776546\n",
            "Iteration 106, loss = 0.41580943\n",
            "Iteration 107, loss = 0.41463063\n",
            "Iteration 108, loss = 0.41123132\n",
            "Iteration 109, loss = 0.41007248\n",
            "Iteration 110, loss = 0.41305940\n",
            "Iteration 111, loss = 0.40908129\n",
            "Iteration 112, loss = 0.41870201\n",
            "Iteration 113, loss = 0.41921171\n",
            "Iteration 114, loss = 0.41887314\n",
            "Iteration 115, loss = 0.42175555\n",
            "Iteration 116, loss = 0.43118974\n",
            "Iteration 117, loss = 0.42460430\n",
            "Iteration 118, loss = 0.42289771\n",
            "Iteration 119, loss = 0.42839026\n",
            "Iteration 120, loss = 0.42154498\n",
            "Iteration 121, loss = 0.42231741\n",
            "Iteration 122, loss = 0.41022801\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69859533\n",
            "Iteration 2, loss = 0.65190558\n",
            "Iteration 3, loss = 0.62375587\n",
            "Iteration 4, loss = 0.61856017\n",
            "Iteration 5, loss = 0.61435074\n",
            "Iteration 6, loss = 0.61344360\n",
            "Iteration 7, loss = 0.61092940\n",
            "Iteration 8, loss = 0.60833027\n",
            "Iteration 9, loss = 0.60728796\n",
            "Iteration 10, loss = 0.60730605\n",
            "Iteration 11, loss = 0.60398484\n",
            "Iteration 12, loss = 0.60016434\n",
            "Iteration 13, loss = 0.59722829\n",
            "Iteration 14, loss = 0.59773188\n",
            "Iteration 15, loss = 0.59949679\n",
            "Iteration 16, loss = 0.59247846\n",
            "Iteration 17, loss = 0.58767554\n",
            "Iteration 18, loss = 0.58088635\n",
            "Iteration 19, loss = 0.57999790\n",
            "Iteration 20, loss = 0.57522219\n",
            "Iteration 21, loss = 0.56706868\n",
            "Iteration 22, loss = 0.56534390\n",
            "Iteration 23, loss = 0.55663013\n",
            "Iteration 24, loss = 0.55154171\n",
            "Iteration 25, loss = 0.54435084\n",
            "Iteration 26, loss = 0.55483362\n",
            "Iteration 27, loss = 0.53096259\n",
            "Iteration 28, loss = 0.53098532\n",
            "Iteration 29, loss = 0.52603728\n",
            "Iteration 30, loss = 0.51477708\n",
            "Iteration 31, loss = 0.51235293\n",
            "Iteration 32, loss = 0.50466265\n",
            "Iteration 33, loss = 0.50073513\n",
            "Iteration 34, loss = 0.49087340\n",
            "Iteration 35, loss = 0.48929917\n",
            "Iteration 36, loss = 0.48013346\n",
            "Iteration 37, loss = 0.48810477\n",
            "Iteration 38, loss = 0.48800050\n",
            "Iteration 39, loss = 0.47834694\n",
            "Iteration 40, loss = 0.48052255\n",
            "Iteration 41, loss = 0.46627151\n",
            "Iteration 42, loss = 0.46953095\n",
            "Iteration 43, loss = 0.46847441\n",
            "Iteration 44, loss = 0.46082238\n",
            "Iteration 45, loss = 0.46047708\n",
            "Iteration 46, loss = 0.45765359\n",
            "Iteration 47, loss = 0.45758605\n",
            "Iteration 48, loss = 0.46189266\n",
            "Iteration 49, loss = 0.46548945\n",
            "Iteration 50, loss = 0.45380202\n",
            "Iteration 51, loss = 0.45421767\n",
            "Iteration 52, loss = 0.45752871\n",
            "Iteration 53, loss = 0.46387002\n",
            "Iteration 54, loss = 0.46479245\n",
            "Iteration 55, loss = 0.47148005\n",
            "Iteration 56, loss = 0.46931442\n",
            "Iteration 57, loss = 0.45856148\n",
            "Iteration 58, loss = 0.45647986\n",
            "Iteration 59, loss = 0.45247527\n",
            "Iteration 60, loss = 0.44744438\n",
            "Iteration 61, loss = 0.44608028\n",
            "Iteration 62, loss = 0.44574087\n",
            "Iteration 63, loss = 0.44734393\n",
            "Iteration 64, loss = 0.44330156\n",
            "Iteration 65, loss = 0.44765808\n",
            "Iteration 66, loss = 0.44206137\n",
            "Iteration 67, loss = 0.44679643\n",
            "Iteration 68, loss = 0.46371364\n",
            "Iteration 69, loss = 0.45324440\n",
            "Iteration 70, loss = 0.44182895\n",
            "Iteration 71, loss = 0.44212612\n",
            "Iteration 72, loss = 0.43993876\n",
            "Iteration 73, loss = 0.44208763\n",
            "Iteration 74, loss = 0.45931717\n",
            "Iteration 75, loss = 0.43871299\n",
            "Iteration 76, loss = 0.43563349\n",
            "Iteration 77, loss = 0.44034559\n",
            "Iteration 78, loss = 0.45259975\n",
            "Iteration 79, loss = 0.44130235\n",
            "Iteration 80, loss = 0.44144109\n",
            "Iteration 81, loss = 0.43752398\n",
            "Iteration 82, loss = 0.45031185\n",
            "Iteration 83, loss = 0.44763477\n",
            "Iteration 84, loss = 0.45334993\n",
            "Iteration 85, loss = 0.44067401\n",
            "Iteration 86, loss = 0.44074334\n",
            "Iteration 87, loss = 0.43466192\n",
            "Iteration 88, loss = 0.43398099\n",
            "Iteration 89, loss = 0.43135841\n",
            "Iteration 90, loss = 0.43233796\n",
            "Iteration 91, loss = 0.43505062\n",
            "Iteration 92, loss = 0.43036814\n",
            "Iteration 93, loss = 0.43144884\n",
            "Iteration 94, loss = 0.43023295\n",
            "Iteration 95, loss = 0.44602007\n",
            "Iteration 96, loss = 0.43543748\n",
            "Iteration 97, loss = 0.43449655\n",
            "Iteration 98, loss = 0.43294736\n",
            "Iteration 99, loss = 0.42855824\n",
            "Iteration 100, loss = 0.42495100\n",
            "Iteration 101, loss = 0.43047431\n",
            "Iteration 102, loss = 0.45035709\n",
            "Iteration 103, loss = 0.43164321\n",
            "Iteration 104, loss = 0.42998207\n",
            "Iteration 105, loss = 0.42607372\n",
            "Iteration 106, loss = 0.42542179\n",
            "Iteration 107, loss = 0.42085635\n",
            "Iteration 108, loss = 0.42596349\n",
            "Iteration 109, loss = 0.43036739\n",
            "Iteration 110, loss = 0.42539798\n",
            "Iteration 111, loss = 0.42004876\n",
            "Iteration 112, loss = 0.42111249\n",
            "Iteration 113, loss = 0.41941579\n",
            "Iteration 114, loss = 0.42635200\n",
            "Iteration 115, loss = 0.42671116\n",
            "Iteration 116, loss = 0.41875893\n",
            "Iteration 117, loss = 0.41861571\n",
            "Iteration 118, loss = 0.41601904\n",
            "Iteration 119, loss = 0.43424410\n",
            "Iteration 120, loss = 0.42401751\n",
            "Iteration 121, loss = 0.41878254\n",
            "Iteration 122, loss = 0.41661482\n",
            "Iteration 123, loss = 0.41761171\n",
            "Iteration 124, loss = 0.41515190\n",
            "Iteration 125, loss = 0.41339532\n",
            "Iteration 126, loss = 0.42045559\n",
            "Iteration 127, loss = 0.41372684\n",
            "Iteration 128, loss = 0.41389574\n",
            "Iteration 129, loss = 0.42160573\n",
            "Iteration 130, loss = 0.41014108\n",
            "Iteration 131, loss = 0.42729693\n",
            "Iteration 132, loss = 0.44201250\n",
            "Iteration 133, loss = 0.42371553\n",
            "Iteration 134, loss = 0.42219704\n",
            "Iteration 135, loss = 0.43313848\n",
            "Iteration 136, loss = 0.41507819\n",
            "Iteration 137, loss = 0.41381968\n",
            "Iteration 138, loss = 0.41561891\n",
            "Iteration 139, loss = 0.42535321\n",
            "Iteration 140, loss = 0.41170999\n",
            "Iteration 141, loss = 0.41439094\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.60887830\n",
            "Iteration 2, loss = 0.59808149\n",
            "Iteration 3, loss = 0.60180064\n",
            "Iteration 4, loss = 0.59755558\n",
            "Iteration 5, loss = 0.58887776\n",
            "Iteration 6, loss = 0.58014921\n",
            "Iteration 7, loss = 0.57668187\n",
            "Iteration 8, loss = 0.57143009\n",
            "Iteration 9, loss = 0.57965369\n",
            "Iteration 10, loss = 0.57069874\n",
            "Iteration 11, loss = 0.58549141\n",
            "Iteration 12, loss = 0.57718576\n",
            "Iteration 13, loss = 0.56950825\n",
            "Iteration 14, loss = 0.57126980\n",
            "Iteration 15, loss = 0.54772475\n",
            "Iteration 16, loss = 0.56534766\n",
            "Iteration 17, loss = 0.55819625\n",
            "Iteration 18, loss = 0.56091803\n",
            "Iteration 19, loss = 0.55841857\n",
            "Iteration 20, loss = 0.56814154\n",
            "Iteration 21, loss = 0.55582349\n",
            "Iteration 22, loss = 0.55354227\n",
            "Iteration 23, loss = 0.55247053\n",
            "Iteration 24, loss = 0.54957474\n",
            "Iteration 25, loss = 0.54587973\n",
            "Iteration 26, loss = 0.53648472\n",
            "Iteration 27, loss = 0.54077264\n",
            "Iteration 28, loss = 0.54301841\n",
            "Iteration 29, loss = 0.55776453\n",
            "Iteration 30, loss = 0.53382848\n",
            "Iteration 31, loss = 0.51838085\n",
            "Iteration 32, loss = 0.51939493\n",
            "Iteration 33, loss = 0.53743214\n",
            "Iteration 34, loss = 0.54377243\n",
            "Iteration 35, loss = 0.53651519\n",
            "Iteration 36, loss = 0.53883687\n",
            "Iteration 37, loss = 0.51362518\n",
            "Iteration 38, loss = 0.52894001\n",
            "Iteration 39, loss = 0.51801339\n",
            "Iteration 40, loss = 0.53631287\n",
            "Iteration 41, loss = 0.51919516\n",
            "Iteration 42, loss = 0.51827294\n",
            "Iteration 43, loss = 0.52468768\n",
            "Iteration 44, loss = 0.52361263\n",
            "Iteration 45, loss = 0.51140035\n",
            "Iteration 46, loss = 0.51479396\n",
            "Iteration 47, loss = 0.54013146\n",
            "Iteration 48, loss = 0.52095887\n",
            "Iteration 49, loss = 0.51551008\n",
            "Iteration 50, loss = 0.50522855\n",
            "Iteration 51, loss = 0.50797894\n",
            "Iteration 52, loss = 0.49644333\n",
            "Iteration 53, loss = 0.51419219\n",
            "Iteration 54, loss = 0.50401030\n",
            "Iteration 55, loss = 0.50181546\n",
            "Iteration 56, loss = 0.49941837\n",
            "Iteration 57, loss = 0.50813253\n",
            "Iteration 58, loss = 0.52249904\n",
            "Iteration 59, loss = 0.50050649\n",
            "Iteration 60, loss = 0.51133638\n",
            "Iteration 61, loss = 0.51817010\n",
            "Iteration 62, loss = 0.53471693\n",
            "Iteration 63, loss = 0.52837708\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62710017\n",
            "Iteration 2, loss = 0.60414307\n",
            "Iteration 3, loss = 0.60145353\n",
            "Iteration 4, loss = 0.59286119\n",
            "Iteration 5, loss = 0.58266862\n",
            "Iteration 6, loss = 0.57883132\n",
            "Iteration 7, loss = 0.57815655\n",
            "Iteration 8, loss = 0.57722681\n",
            "Iteration 9, loss = 0.57615470\n",
            "Iteration 10, loss = 0.58186373\n",
            "Iteration 11, loss = 0.58178797\n",
            "Iteration 12, loss = 0.56726622\n",
            "Iteration 13, loss = 0.56127943\n",
            "Iteration 14, loss = 0.56240624\n",
            "Iteration 15, loss = 0.56570367\n",
            "Iteration 16, loss = 0.57393242\n",
            "Iteration 17, loss = 0.55855146\n",
            "Iteration 18, loss = 0.56033982\n",
            "Iteration 19, loss = 0.56550624\n",
            "Iteration 20, loss = 0.57350246\n",
            "Iteration 21, loss = 0.56163907\n",
            "Iteration 22, loss = 0.55667622\n",
            "Iteration 23, loss = 0.55156493\n",
            "Iteration 24, loss = 0.54651878\n",
            "Iteration 25, loss = 0.53994092\n",
            "Iteration 26, loss = 0.54477752\n",
            "Iteration 27, loss = 0.54057890\n",
            "Iteration 28, loss = 0.54886096\n",
            "Iteration 29, loss = 0.54642575\n",
            "Iteration 30, loss = 0.53984542\n",
            "Iteration 31, loss = 0.54885410\n",
            "Iteration 32, loss = 0.53703752\n",
            "Iteration 33, loss = 0.55368837\n",
            "Iteration 34, loss = 0.53945181\n",
            "Iteration 35, loss = 0.53674928\n",
            "Iteration 36, loss = 0.54242011\n",
            "Iteration 37, loss = 0.52517062\n",
            "Iteration 38, loss = 0.53836786\n",
            "Iteration 39, loss = 0.54116036\n",
            "Iteration 40, loss = 0.53615947\n",
            "Iteration 41, loss = 0.53096981\n",
            "Iteration 42, loss = 0.53140588\n",
            "Iteration 43, loss = 0.52554189\n",
            "Iteration 44, loss = 0.52787445\n",
            "Iteration 45, loss = 0.51311311\n",
            "Iteration 46, loss = 0.52266507\n",
            "Iteration 47, loss = 0.51919361\n",
            "Iteration 48, loss = 0.50536074\n",
            "Iteration 49, loss = 0.53220024\n",
            "Iteration 50, loss = 0.52086100\n",
            "Iteration 51, loss = 0.53479696\n",
            "Iteration 52, loss = 0.52566787\n",
            "Iteration 53, loss = 0.51289445\n",
            "Iteration 54, loss = 0.54946111\n",
            "Iteration 55, loss = 0.52161758\n",
            "Iteration 56, loss = 0.51261992\n",
            "Iteration 57, loss = 0.51941019\n",
            "Iteration 58, loss = 0.51037993\n",
            "Iteration 59, loss = 0.52137031\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.63711007\n",
            "Iteration 2, loss = 0.59860919\n",
            "Iteration 3, loss = 0.58261878\n",
            "Iteration 4, loss = 0.58567769\n",
            "Iteration 5, loss = 0.58673080\n",
            "Iteration 6, loss = 0.57092438\n",
            "Iteration 7, loss = 0.57514236\n",
            "Iteration 8, loss = 0.56157137\n",
            "Iteration 9, loss = 0.57316964\n",
            "Iteration 10, loss = 0.54714811\n",
            "Iteration 11, loss = 0.56156185\n",
            "Iteration 12, loss = 0.55845418\n",
            "Iteration 13, loss = 0.54973156\n",
            "Iteration 14, loss = 0.54133649\n",
            "Iteration 15, loss = 0.55294617\n",
            "Iteration 16, loss = 0.55680394\n",
            "Iteration 17, loss = 0.54060221\n",
            "Iteration 18, loss = 0.55089153\n",
            "Iteration 19, loss = 0.53916032\n",
            "Iteration 20, loss = 0.54567873\n",
            "Iteration 21, loss = 0.54964085\n",
            "Iteration 22, loss = 0.54420835\n",
            "Iteration 23, loss = 0.53712091\n",
            "Iteration 24, loss = 0.53859987\n",
            "Iteration 25, loss = 0.51958105\n",
            "Iteration 26, loss = 0.52819244\n",
            "Iteration 27, loss = 0.52959325\n",
            "Iteration 28, loss = 0.54184875\n",
            "Iteration 29, loss = 0.51478879\n",
            "Iteration 30, loss = 0.52960288\n",
            "Iteration 31, loss = 0.52080719\n",
            "Iteration 32, loss = 0.50833492\n",
            "Iteration 33, loss = 0.53657931\n",
            "Iteration 34, loss = 0.52010033\n",
            "Iteration 35, loss = 0.51370155\n",
            "Iteration 36, loss = 0.51485445\n",
            "Iteration 37, loss = 0.51875006\n",
            "Iteration 38, loss = 0.49398034\n",
            "Iteration 39, loss = 0.50714112\n",
            "Iteration 40, loss = 0.50610615\n",
            "Iteration 41, loss = 0.49536267\n",
            "Iteration 42, loss = 0.49900440\n",
            "Iteration 43, loss = 0.51944717\n",
            "Iteration 44, loss = 0.49448452\n",
            "Iteration 45, loss = 0.50403602\n",
            "Iteration 46, loss = 0.51184200\n",
            "Iteration 47, loss = 0.50268828\n",
            "Iteration 48, loss = 0.50384251\n",
            "Iteration 49, loss = 0.51055402\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64359783\n",
            "Iteration 2, loss = 0.62979672\n",
            "Iteration 3, loss = 0.61584581\n",
            "Iteration 4, loss = 0.61800028\n",
            "Iteration 5, loss = 0.60767457\n",
            "Iteration 6, loss = 0.61399090\n",
            "Iteration 7, loss = 0.60450867\n",
            "Iteration 8, loss = 0.59790211\n",
            "Iteration 9, loss = 0.59180052\n",
            "Iteration 10, loss = 0.57875392\n",
            "Iteration 11, loss = 0.58176178\n",
            "Iteration 12, loss = 0.59224138\n",
            "Iteration 13, loss = 0.58453119\n",
            "Iteration 14, loss = 0.58313584\n",
            "Iteration 15, loss = 0.58854430\n",
            "Iteration 16, loss = 0.59738890\n",
            "Iteration 17, loss = 0.57954172\n",
            "Iteration 18, loss = 0.56607870\n",
            "Iteration 19, loss = 0.57372346\n",
            "Iteration 20, loss = 0.58123884\n",
            "Iteration 21, loss = 0.57276921\n",
            "Iteration 22, loss = 0.57709117\n",
            "Iteration 23, loss = 0.55965010\n",
            "Iteration 24, loss = 0.57813567\n",
            "Iteration 25, loss = 0.56109315\n",
            "Iteration 26, loss = 0.55601433\n",
            "Iteration 27, loss = 0.56575089\n",
            "Iteration 28, loss = 0.55885250\n",
            "Iteration 29, loss = 0.56737691\n",
            "Iteration 30, loss = 0.56269966\n",
            "Iteration 31, loss = 0.54152792\n",
            "Iteration 32, loss = 0.56923332\n",
            "Iteration 33, loss = 0.55961842\n",
            "Iteration 34, loss = 0.55152239\n",
            "Iteration 35, loss = 0.54683263\n",
            "Iteration 36, loss = 0.53971354\n",
            "Iteration 37, loss = 0.54319555\n",
            "Iteration 38, loss = 0.57045077\n",
            "Iteration 39, loss = 0.54958810\n",
            "Iteration 40, loss = 0.53724256\n",
            "Iteration 41, loss = 0.53562969\n",
            "Iteration 42, loss = 0.53444168\n",
            "Iteration 43, loss = 0.52393866\n",
            "Iteration 44, loss = 0.54358444\n",
            "Iteration 45, loss = 0.51965114\n",
            "Iteration 46, loss = 0.52585964\n",
            "Iteration 47, loss = 0.54997127\n",
            "Iteration 48, loss = 0.52908559\n",
            "Iteration 49, loss = 0.53194262\n",
            "Iteration 50, loss = 0.55267211\n",
            "Iteration 51, loss = 0.52667522\n",
            "Iteration 52, loss = 0.52816675\n",
            "Iteration 53, loss = 0.52176913\n",
            "Iteration 54, loss = 0.54627307\n",
            "Iteration 55, loss = 0.51964643\n",
            "Iteration 56, loss = 0.53119898\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.63574836\n",
            "Iteration 2, loss = 0.61471018\n",
            "Iteration 3, loss = 0.62125069\n",
            "Iteration 4, loss = 0.60760942\n",
            "Iteration 5, loss = 0.60089449\n",
            "Iteration 6, loss = 0.59611848\n",
            "Iteration 7, loss = 0.60613488\n",
            "Iteration 8, loss = 0.58567924\n",
            "Iteration 9, loss = 0.60358421\n",
            "Iteration 10, loss = 0.59348750\n",
            "Iteration 11, loss = 0.58294216\n",
            "Iteration 12, loss = 0.59113312\n",
            "Iteration 13, loss = 0.57679996\n",
            "Iteration 14, loss = 0.60046870\n",
            "Iteration 15, loss = 0.59175975\n",
            "Iteration 16, loss = 0.57063735\n",
            "Iteration 17, loss = 0.58112797\n",
            "Iteration 18, loss = 0.56959468\n",
            "Iteration 19, loss = 0.55712376\n",
            "Iteration 20, loss = 0.56508426\n",
            "Iteration 21, loss = 0.56086336\n",
            "Iteration 22, loss = 0.55397999\n",
            "Iteration 23, loss = 0.56294293\n",
            "Iteration 24, loss = 0.56525328\n",
            "Iteration 25, loss = 0.56525508\n",
            "Iteration 26, loss = 0.55890564\n",
            "Iteration 27, loss = 0.55628557\n",
            "Iteration 28, loss = 0.55779997\n",
            "Iteration 29, loss = 0.54154046\n",
            "Iteration 30, loss = 0.55330511\n",
            "Iteration 31, loss = 0.55382752\n",
            "Iteration 32, loss = 0.53715004\n",
            "Iteration 33, loss = 0.56595308\n",
            "Iteration 34, loss = 0.53960526\n",
            "Iteration 35, loss = 0.54572000\n",
            "Iteration 36, loss = 0.55036298\n",
            "Iteration 37, loss = 0.53454820\n",
            "Iteration 38, loss = 0.57958186\n",
            "Iteration 39, loss = 0.54200752\n",
            "Iteration 40, loss = 0.53249171\n",
            "Iteration 41, loss = 0.53154523\n",
            "Iteration 42, loss = 0.52295823\n",
            "Iteration 43, loss = 0.55804471\n",
            "Iteration 44, loss = 0.53391574\n",
            "Iteration 45, loss = 0.53175636\n",
            "Iteration 46, loss = 0.53862804\n",
            "Iteration 47, loss = 0.52204928\n",
            "Iteration 48, loss = 0.53714953\n",
            "Iteration 49, loss = 0.53280595\n",
            "Iteration 50, loss = 0.53227635\n",
            "Iteration 51, loss = 0.52950180\n",
            "Iteration 52, loss = 0.53328589\n",
            "Iteration 53, loss = 0.53830012\n",
            "Iteration 54, loss = 0.53809320\n",
            "Iteration 55, loss = 0.52822939\n",
            "Iteration 56, loss = 0.52410864\n",
            "Iteration 57, loss = 0.51555960\n",
            "Iteration 58, loss = 0.56512257\n",
            "Iteration 59, loss = 0.51969116\n",
            "Iteration 60, loss = 0.54989239\n",
            "Iteration 61, loss = 0.52745941\n",
            "Iteration 62, loss = 0.52872452\n",
            "Iteration 63, loss = 0.52812928\n",
            "Iteration 64, loss = 0.52542848\n",
            "Iteration 65, loss = 0.52809174\n",
            "Iteration 66, loss = 0.52633585\n",
            "Iteration 67, loss = 0.51864144\n",
            "Iteration 68, loss = 0.51878387\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66441264\n",
            "Iteration 2, loss = 0.63169033\n",
            "Iteration 3, loss = 0.61170562\n",
            "Iteration 4, loss = 0.60096868\n",
            "Iteration 5, loss = 0.59553997\n",
            "Iteration 6, loss = 0.59146183\n",
            "Iteration 7, loss = 0.58840369\n",
            "Iteration 8, loss = 0.58727195\n",
            "Iteration 9, loss = 0.58600238\n",
            "Iteration 10, loss = 0.58455398\n",
            "Iteration 11, loss = 0.58405707\n",
            "Iteration 12, loss = 0.58195416\n",
            "Iteration 13, loss = 0.58065736\n",
            "Iteration 14, loss = 0.57872994\n",
            "Iteration 15, loss = 0.57717536\n",
            "Iteration 16, loss = 0.57536869\n",
            "Iteration 17, loss = 0.57495649\n",
            "Iteration 18, loss = 0.57352826\n",
            "Iteration 19, loss = 0.57319461\n",
            "Iteration 20, loss = 0.57228280\n",
            "Iteration 21, loss = 0.57137220\n",
            "Iteration 22, loss = 0.57089776\n",
            "Iteration 23, loss = 0.56946613\n",
            "Iteration 24, loss = 0.56957883\n",
            "Iteration 25, loss = 0.56907822\n",
            "Iteration 26, loss = 0.56820159\n",
            "Iteration 27, loss = 0.56719554\n",
            "Iteration 28, loss = 0.56652309\n",
            "Iteration 29, loss = 0.56651958\n",
            "Iteration 30, loss = 0.56598033\n",
            "Iteration 31, loss = 0.56597264\n",
            "Iteration 32, loss = 0.56415070\n",
            "Iteration 33, loss = 0.56408494\n",
            "Iteration 34, loss = 0.56247866\n",
            "Iteration 35, loss = 0.56190844\n",
            "Iteration 36, loss = 0.56164006\n",
            "Iteration 37, loss = 0.56551376\n",
            "Iteration 38, loss = 0.55752114\n",
            "Iteration 39, loss = 0.55825329\n",
            "Iteration 40, loss = 0.55703608\n",
            "Iteration 41, loss = 0.55920678\n",
            "Iteration 42, loss = 0.55797790\n",
            "Iteration 43, loss = 0.55307113\n",
            "Iteration 44, loss = 0.55242755\n",
            "Iteration 45, loss = 0.55191783\n",
            "Iteration 46, loss = 0.55207540\n",
            "Iteration 47, loss = 0.54965683\n",
            "Iteration 48, loss = 0.55061636\n",
            "Iteration 49, loss = 0.54943399\n",
            "Iteration 50, loss = 0.54723745\n",
            "Iteration 51, loss = 0.54923606\n",
            "Iteration 52, loss = 0.54848396\n",
            "Iteration 53, loss = 0.55010830\n",
            "Iteration 54, loss = 0.54597703\n",
            "Iteration 55, loss = 0.54523934\n",
            "Iteration 56, loss = 0.54460344\n",
            "Iteration 57, loss = 0.54575807\n",
            "Iteration 58, loss = 0.54415627\n",
            "Iteration 59, loss = 0.54538726\n",
            "Iteration 60, loss = 0.54260153\n",
            "Iteration 61, loss = 0.54310438\n",
            "Iteration 62, loss = 0.54364473\n",
            "Iteration 63, loss = 0.54308026\n",
            "Iteration 64, loss = 0.54012692\n",
            "Iteration 65, loss = 0.53931655\n",
            "Iteration 66, loss = 0.54090804\n",
            "Iteration 67, loss = 0.53942905\n",
            "Iteration 68, loss = 0.53929266\n",
            "Iteration 69, loss = 0.53607795\n",
            "Iteration 70, loss = 0.53535442\n",
            "Iteration 71, loss = 0.53559157\n",
            "Iteration 72, loss = 0.54106273\n",
            "Iteration 73, loss = 0.53547653\n",
            "Iteration 74, loss = 0.53631145\n",
            "Iteration 75, loss = 0.53614922\n",
            "Iteration 76, loss = 0.53342312\n",
            "Iteration 77, loss = 0.53295079\n",
            "Iteration 78, loss = 0.53617758\n",
            "Iteration 79, loss = 0.53405143\n",
            "Iteration 80, loss = 0.52940184\n",
            "Iteration 81, loss = 0.53037999\n",
            "Iteration 82, loss = 0.52586718\n",
            "Iteration 83, loss = 0.52651118\n",
            "Iteration 84, loss = 0.52854089\n",
            "Iteration 85, loss = 0.52602302\n",
            "Iteration 86, loss = 0.52405869\n",
            "Iteration 87, loss = 0.52268099\n",
            "Iteration 88, loss = 0.52605368\n",
            "Iteration 89, loss = 0.52634962\n",
            "Iteration 90, loss = 0.52744799\n",
            "Iteration 91, loss = 0.52064540\n",
            "Iteration 92, loss = 0.52073646\n",
            "Iteration 93, loss = 0.52056613\n",
            "Iteration 94, loss = 0.51911428\n",
            "Iteration 95, loss = 0.51981391\n",
            "Iteration 96, loss = 0.52359095\n",
            "Iteration 97, loss = 0.51481261\n",
            "Iteration 98, loss = 0.51964391\n",
            "Iteration 99, loss = 0.51347881\n",
            "Iteration 100, loss = 0.51879800\n",
            "Iteration 101, loss = 0.51288514\n",
            "Iteration 102, loss = 0.51174896\n",
            "Iteration 103, loss = 0.51229430\n",
            "Iteration 104, loss = 0.51895133\n",
            "Iteration 105, loss = 0.51405729\n",
            "Iteration 106, loss = 0.51132764\n",
            "Iteration 107, loss = 0.51798616\n",
            "Iteration 108, loss = 0.50873366\n",
            "Iteration 109, loss = 0.50684770\n",
            "Iteration 110, loss = 0.51218443\n",
            "Iteration 111, loss = 0.50376360\n",
            "Iteration 112, loss = 0.51773516\n",
            "Iteration 113, loss = 0.50815792\n",
            "Iteration 114, loss = 0.50741848\n",
            "Iteration 115, loss = 0.51149058\n",
            "Iteration 116, loss = 0.50264271\n",
            "Iteration 117, loss = 0.50738026\n",
            "Iteration 118, loss = 0.50294633\n",
            "Iteration 119, loss = 0.50698249\n",
            "Iteration 120, loss = 0.49995828\n",
            "Iteration 121, loss = 0.50275119\n",
            "Iteration 122, loss = 0.50486410\n",
            "Iteration 123, loss = 0.50646004\n",
            "Iteration 124, loss = 0.51007358\n",
            "Iteration 125, loss = 0.51116052\n",
            "Iteration 126, loss = 0.49995057\n",
            "Iteration 127, loss = 0.49574013\n",
            "Iteration 128, loss = 0.49623909\n",
            "Iteration 129, loss = 0.49922784\n",
            "Iteration 130, loss = 0.49484755\n",
            "Iteration 131, loss = 0.49877870\n",
            "Iteration 132, loss = 0.49673785\n",
            "Iteration 133, loss = 0.52723044\n",
            "Iteration 134, loss = 0.51657382\n",
            "Iteration 135, loss = 0.50974819\n",
            "Iteration 136, loss = 0.50175267\n",
            "Iteration 137, loss = 0.49551331\n",
            "Iteration 138, loss = 0.49193454\n",
            "Iteration 139, loss = 0.48821130\n",
            "Iteration 140, loss = 0.49637782\n",
            "Iteration 141, loss = 0.49360389\n",
            "Iteration 142, loss = 0.48775338\n",
            "Iteration 143, loss = 0.48781644\n",
            "Iteration 144, loss = 0.48787386\n",
            "Iteration 145, loss = 0.48943619\n",
            "Iteration 146, loss = 0.48556446\n",
            "Iteration 147, loss = 0.48482017\n",
            "Iteration 148, loss = 0.48345764\n",
            "Iteration 149, loss = 0.48544614\n",
            "Iteration 150, loss = 0.48130006\n",
            "Iteration 151, loss = 0.50130140\n",
            "Iteration 152, loss = 0.48154925\n",
            "Iteration 153, loss = 0.47871190\n",
            "Iteration 154, loss = 0.48364127\n",
            "Iteration 155, loss = 0.48445475\n",
            "Iteration 156, loss = 0.48067537\n",
            "Iteration 157, loss = 0.48568437\n",
            "Iteration 158, loss = 0.48602085\n",
            "Iteration 159, loss = 0.48097653\n",
            "Iteration 160, loss = 0.48382483\n",
            "Iteration 161, loss = 0.48144588\n",
            "Iteration 162, loss = 0.47347070\n",
            "Iteration 163, loss = 0.48289698\n",
            "Iteration 164, loss = 0.47285956\n",
            "Iteration 165, loss = 0.49023773\n",
            "Iteration 166, loss = 0.48270485\n",
            "Iteration 167, loss = 0.48406259\n",
            "Iteration 168, loss = 0.48197629\n",
            "Iteration 169, loss = 0.47202683\n",
            "Iteration 170, loss = 0.48267177\n",
            "Iteration 171, loss = 0.48347723\n",
            "Iteration 172, loss = 0.48351803\n",
            "Iteration 173, loss = 0.46822954\n",
            "Iteration 174, loss = 0.47521585\n",
            "Iteration 175, loss = 0.47920281\n",
            "Iteration 176, loss = 0.48234838\n",
            "Iteration 177, loss = 0.48018103\n",
            "Iteration 178, loss = 0.47801244\n",
            "Iteration 179, loss = 0.47465465\n",
            "Iteration 180, loss = 0.47661578\n",
            "Iteration 181, loss = 0.47156879\n",
            "Iteration 182, loss = 0.48251236\n",
            "Iteration 183, loss = 0.47215070\n",
            "Iteration 184, loss = 0.47603278\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62754221\n",
            "Iteration 2, loss = 0.61012512\n",
            "Iteration 3, loss = 0.59971113\n",
            "Iteration 4, loss = 0.59772378\n",
            "Iteration 5, loss = 0.59510826\n",
            "Iteration 6, loss = 0.59403660\n",
            "Iteration 7, loss = 0.59232820\n",
            "Iteration 8, loss = 0.59137300\n",
            "Iteration 9, loss = 0.59082485\n",
            "Iteration 10, loss = 0.58962330\n",
            "Iteration 11, loss = 0.58888195\n",
            "Iteration 12, loss = 0.58782944\n",
            "Iteration 13, loss = 0.58690460\n",
            "Iteration 14, loss = 0.58581925\n",
            "Iteration 15, loss = 0.58532185\n",
            "Iteration 16, loss = 0.58418763\n",
            "Iteration 17, loss = 0.58380130\n",
            "Iteration 18, loss = 0.58358524\n",
            "Iteration 19, loss = 0.58231306\n",
            "Iteration 20, loss = 0.58135452\n",
            "Iteration 21, loss = 0.58144119\n",
            "Iteration 22, loss = 0.57999769\n",
            "Iteration 23, loss = 0.57982150\n",
            "Iteration 24, loss = 0.57924827\n",
            "Iteration 25, loss = 0.57813677\n",
            "Iteration 26, loss = 0.57816536\n",
            "Iteration 27, loss = 0.57719535\n",
            "Iteration 28, loss = 0.57661438\n",
            "Iteration 29, loss = 0.57635293\n",
            "Iteration 30, loss = 0.57578437\n",
            "Iteration 31, loss = 0.57509187\n",
            "Iteration 32, loss = 0.57583092\n",
            "Iteration 33, loss = 0.57442584\n",
            "Iteration 34, loss = 0.57388610\n",
            "Iteration 35, loss = 0.57286694\n",
            "Iteration 36, loss = 0.57248296\n",
            "Iteration 37, loss = 0.57163114\n",
            "Iteration 38, loss = 0.57137440\n",
            "Iteration 39, loss = 0.57113366\n",
            "Iteration 40, loss = 0.57058479\n",
            "Iteration 41, loss = 0.57019759\n",
            "Iteration 42, loss = 0.56934944\n",
            "Iteration 43, loss = 0.56843638\n",
            "Iteration 44, loss = 0.56826049\n",
            "Iteration 45, loss = 0.56803773\n",
            "Iteration 46, loss = 0.56710910\n",
            "Iteration 47, loss = 0.56671943\n",
            "Iteration 48, loss = 0.56653929\n",
            "Iteration 49, loss = 0.56603552\n",
            "Iteration 50, loss = 0.56507527\n",
            "Iteration 51, loss = 0.56503555\n",
            "Iteration 52, loss = 0.56463862\n",
            "Iteration 53, loss = 0.56400372\n",
            "Iteration 54, loss = 0.56426848\n",
            "Iteration 55, loss = 0.56337601\n",
            "Iteration 56, loss = 0.56228375\n",
            "Iteration 57, loss = 0.56161415\n",
            "Iteration 58, loss = 0.56104084\n",
            "Iteration 59, loss = 0.56052125\n",
            "Iteration 60, loss = 0.56054011\n",
            "Iteration 61, loss = 0.56004916\n",
            "Iteration 62, loss = 0.55995419\n",
            "Iteration 63, loss = 0.55840780\n",
            "Iteration 64, loss = 0.55777333\n",
            "Iteration 65, loss = 0.55729052\n",
            "Iteration 66, loss = 0.55657720\n",
            "Iteration 67, loss = 0.55672189\n",
            "Iteration 68, loss = 0.55589978\n",
            "Iteration 69, loss = 0.55531019\n",
            "Iteration 70, loss = 0.55659340\n",
            "Iteration 71, loss = 0.55451593\n",
            "Iteration 72, loss = 0.55350257\n",
            "Iteration 73, loss = 0.55362915\n",
            "Iteration 74, loss = 0.55245123\n",
            "Iteration 75, loss = 0.55105983\n",
            "Iteration 76, loss = 0.55152483\n",
            "Iteration 77, loss = 0.55323121\n",
            "Iteration 78, loss = 0.55271746\n",
            "Iteration 79, loss = 0.55280405\n",
            "Iteration 80, loss = 0.55013512\n",
            "Iteration 81, loss = 0.55011253\n",
            "Iteration 82, loss = 0.54813254\n",
            "Iteration 83, loss = 0.54798232\n",
            "Iteration 84, loss = 0.54589697\n",
            "Iteration 85, loss = 0.54575120\n",
            "Iteration 86, loss = 0.54541517\n",
            "Iteration 87, loss = 0.54439804\n",
            "Iteration 88, loss = 0.54564468\n",
            "Iteration 89, loss = 0.54443163\n",
            "Iteration 90, loss = 0.54129242\n",
            "Iteration 91, loss = 0.54118191\n",
            "Iteration 92, loss = 0.54001583\n",
            "Iteration 93, loss = 0.54044302\n",
            "Iteration 94, loss = 0.53921972\n",
            "Iteration 95, loss = 0.53821413\n",
            "Iteration 96, loss = 0.53844028\n",
            "Iteration 97, loss = 0.53502830\n",
            "Iteration 98, loss = 0.53685227\n",
            "Iteration 99, loss = 0.53737539\n",
            "Iteration 100, loss = 0.53470023\n",
            "Iteration 101, loss = 0.53272580\n",
            "Iteration 102, loss = 0.53486651\n",
            "Iteration 103, loss = 0.53356135\n",
            "Iteration 104, loss = 0.53552131\n",
            "Iteration 105, loss = 0.53517309\n",
            "Iteration 106, loss = 0.53125377\n",
            "Iteration 107, loss = 0.53332594\n",
            "Iteration 108, loss = 0.53010539\n",
            "Iteration 109, loss = 0.53677434\n",
            "Iteration 110, loss = 0.53374265\n",
            "Iteration 111, loss = 0.53241090\n",
            "Iteration 112, loss = 0.52604477\n",
            "Iteration 113, loss = 0.52628704\n",
            "Iteration 114, loss = 0.52585654\n",
            "Iteration 115, loss = 0.52223950\n",
            "Iteration 116, loss = 0.52386336\n",
            "Iteration 117, loss = 0.52670016\n",
            "Iteration 118, loss = 0.52521213\n",
            "Iteration 119, loss = 0.52267223\n",
            "Iteration 120, loss = 0.52336955\n",
            "Iteration 121, loss = 0.51767070\n",
            "Iteration 122, loss = 0.52734417\n",
            "Iteration 123, loss = 0.52054534\n",
            "Iteration 124, loss = 0.51962771\n",
            "Iteration 125, loss = 0.51583592\n",
            "Iteration 126, loss = 0.51480464\n",
            "Iteration 127, loss = 0.51402353\n",
            "Iteration 128, loss = 0.51366766\n",
            "Iteration 129, loss = 0.51821744\n",
            "Iteration 130, loss = 0.51196876\n",
            "Iteration 131, loss = 0.51258568\n",
            "Iteration 132, loss = 0.51331814\n",
            "Iteration 133, loss = 0.50926928\n",
            "Iteration 134, loss = 0.50947278\n",
            "Iteration 135, loss = 0.51013852\n",
            "Iteration 136, loss = 0.50584853\n",
            "Iteration 137, loss = 0.50693198\n",
            "Iteration 138, loss = 0.50856090\n",
            "Iteration 139, loss = 0.51321200\n",
            "Iteration 140, loss = 0.50549878\n",
            "Iteration 141, loss = 0.50770914\n",
            "Iteration 142, loss = 0.50457756\n",
            "Iteration 143, loss = 0.50387832\n",
            "Iteration 144, loss = 0.50562550\n",
            "Iteration 145, loss = 0.50547838\n",
            "Iteration 146, loss = 0.50559257\n",
            "Iteration 147, loss = 0.49966093\n",
            "Iteration 148, loss = 0.50011458\n",
            "Iteration 149, loss = 0.49584556\n",
            "Iteration 150, loss = 0.49990125\n",
            "Iteration 151, loss = 0.49881681\n",
            "Iteration 152, loss = 0.49835624\n",
            "Iteration 153, loss = 0.49779293\n",
            "Iteration 154, loss = 0.49502114\n",
            "Iteration 155, loss = 0.49650847\n",
            "Iteration 156, loss = 0.49438349\n",
            "Iteration 157, loss = 0.50991236\n",
            "Iteration 158, loss = 0.50994141\n",
            "Iteration 159, loss = 0.49766492\n",
            "Iteration 160, loss = 0.49226372\n",
            "Iteration 161, loss = 0.49658956\n",
            "Iteration 162, loss = 0.48817584\n",
            "Iteration 163, loss = 0.48558004\n",
            "Iteration 164, loss = 0.49237156\n",
            "Iteration 165, loss = 0.49574521\n",
            "Iteration 166, loss = 0.48796387\n",
            "Iteration 167, loss = 0.48756358\n",
            "Iteration 168, loss = 0.48573563\n",
            "Iteration 169, loss = 0.48613394\n",
            "Iteration 170, loss = 0.49278688\n",
            "Iteration 171, loss = 0.49290314\n",
            "Iteration 172, loss = 0.48370418\n",
            "Iteration 173, loss = 0.50734377\n",
            "Iteration 174, loss = 0.49229316\n",
            "Iteration 175, loss = 0.48728275\n",
            "Iteration 176, loss = 0.48219449\n",
            "Iteration 177, loss = 0.48456363\n",
            "Iteration 178, loss = 0.47973636\n",
            "Iteration 179, loss = 0.47852240\n",
            "Iteration 180, loss = 0.47938489\n",
            "Iteration 181, loss = 0.47452087\n",
            "Iteration 182, loss = 0.48664109\n",
            "Iteration 183, loss = 0.48053389\n",
            "Iteration 184, loss = 0.47825629\n",
            "Iteration 185, loss = 0.47816172\n",
            "Iteration 186, loss = 0.47506849\n",
            "Iteration 187, loss = 0.48626757\n",
            "Iteration 188, loss = 0.48186812\n",
            "Iteration 189, loss = 0.48213816\n",
            "Iteration 190, loss = 0.48521001\n",
            "Iteration 191, loss = 0.48003805\n",
            "Iteration 192, loss = 0.47584466\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.84489817\n",
            "Iteration 2, loss = 0.71289832\n",
            "Iteration 3, loss = 0.64611361\n",
            "Iteration 4, loss = 0.61354302\n",
            "Iteration 5, loss = 0.60551954\n",
            "Iteration 6, loss = 0.60200866\n",
            "Iteration 7, loss = 0.60047362\n",
            "Iteration 8, loss = 0.59825448\n",
            "Iteration 9, loss = 0.59703765\n",
            "Iteration 10, loss = 0.59518073\n",
            "Iteration 11, loss = 0.59320402\n",
            "Iteration 12, loss = 0.59213536\n",
            "Iteration 13, loss = 0.59059087\n",
            "Iteration 14, loss = 0.58975084\n",
            "Iteration 15, loss = 0.58895986\n",
            "Iteration 16, loss = 0.58721701\n",
            "Iteration 17, loss = 0.58658053\n",
            "Iteration 18, loss = 0.58469773\n",
            "Iteration 19, loss = 0.58515149\n",
            "Iteration 20, loss = 0.58444716\n",
            "Iteration 21, loss = 0.58235610\n",
            "Iteration 22, loss = 0.58174436\n",
            "Iteration 23, loss = 0.58132054\n",
            "Iteration 24, loss = 0.58134265\n",
            "Iteration 25, loss = 0.58039394\n",
            "Iteration 26, loss = 0.57961125\n",
            "Iteration 27, loss = 0.57891414\n",
            "Iteration 28, loss = 0.57891901\n",
            "Iteration 29, loss = 0.57855003\n",
            "Iteration 30, loss = 0.57767537\n",
            "Iteration 31, loss = 0.57649576\n",
            "Iteration 32, loss = 0.57591537\n",
            "Iteration 33, loss = 0.57574811\n",
            "Iteration 34, loss = 0.57482363\n",
            "Iteration 35, loss = 0.57356828\n",
            "Iteration 36, loss = 0.57233143\n",
            "Iteration 37, loss = 0.57088779\n",
            "Iteration 38, loss = 0.57042150\n",
            "Iteration 39, loss = 0.56883363\n",
            "Iteration 40, loss = 0.56528843\n",
            "Iteration 41, loss = 0.56392448\n",
            "Iteration 42, loss = 0.56628231\n",
            "Iteration 43, loss = 0.56166962\n",
            "Iteration 44, loss = 0.55966429\n",
            "Iteration 45, loss = 0.56176044\n",
            "Iteration 46, loss = 0.56092490\n",
            "Iteration 47, loss = 0.55839555\n",
            "Iteration 48, loss = 0.55818841\n",
            "Iteration 49, loss = 0.55934935\n",
            "Iteration 50, loss = 0.56485720\n",
            "Iteration 51, loss = 0.55767841\n",
            "Iteration 52, loss = 0.55584273\n",
            "Iteration 53, loss = 0.55440857\n",
            "Iteration 54, loss = 0.55311179\n",
            "Iteration 55, loss = 0.55302919\n",
            "Iteration 56, loss = 0.55386061\n",
            "Iteration 57, loss = 0.55030119\n",
            "Iteration 58, loss = 0.55133928\n",
            "Iteration 59, loss = 0.54820637\n",
            "Iteration 60, loss = 0.54824962\n",
            "Iteration 61, loss = 0.54822051\n",
            "Iteration 62, loss = 0.54801515\n",
            "Iteration 63, loss = 0.54629096\n",
            "Iteration 64, loss = 0.54650506\n",
            "Iteration 65, loss = 0.54449792\n",
            "Iteration 66, loss = 0.54579831\n",
            "Iteration 67, loss = 0.54703872\n",
            "Iteration 68, loss = 0.54427694\n",
            "Iteration 69, loss = 0.54649713\n",
            "Iteration 70, loss = 0.54596853\n",
            "Iteration 71, loss = 0.54054787\n",
            "Iteration 72, loss = 0.53754298\n",
            "Iteration 73, loss = 0.53880713\n",
            "Iteration 74, loss = 0.53678410\n",
            "Iteration 75, loss = 0.53786991\n",
            "Iteration 76, loss = 0.53812769\n",
            "Iteration 77, loss = 0.54107502\n",
            "Iteration 78, loss = 0.54177617\n",
            "Iteration 79, loss = 0.53887892\n",
            "Iteration 80, loss = 0.53412201\n",
            "Iteration 81, loss = 0.53196451\n",
            "Iteration 82, loss = 0.53736724\n",
            "Iteration 83, loss = 0.53563836\n",
            "Iteration 84, loss = 0.53085989\n",
            "Iteration 85, loss = 0.53318822\n",
            "Iteration 86, loss = 0.53051484\n",
            "Iteration 87, loss = 0.54099085\n",
            "Iteration 88, loss = 0.52806978\n",
            "Iteration 89, loss = 0.52437392\n",
            "Iteration 90, loss = 0.52527158\n",
            "Iteration 91, loss = 0.52721660\n",
            "Iteration 92, loss = 0.52553779\n",
            "Iteration 93, loss = 0.52906868\n",
            "Iteration 94, loss = 0.52151250\n",
            "Iteration 95, loss = 0.52211355\n",
            "Iteration 96, loss = 0.51943401\n",
            "Iteration 97, loss = 0.52026564\n",
            "Iteration 98, loss = 0.51736804\n",
            "Iteration 99, loss = 0.51729246\n",
            "Iteration 100, loss = 0.52237100\n",
            "Iteration 101, loss = 0.51596214\n",
            "Iteration 102, loss = 0.52361477\n",
            "Iteration 103, loss = 0.51968958\n",
            "Iteration 104, loss = 0.51292922\n",
            "Iteration 105, loss = 0.51250681\n",
            "Iteration 106, loss = 0.51240869\n",
            "Iteration 107, loss = 0.51710758\n",
            "Iteration 108, loss = 0.51411016\n",
            "Iteration 109, loss = 0.51304311\n",
            "Iteration 110, loss = 0.51498246\n",
            "Iteration 111, loss = 0.51353725\n",
            "Iteration 112, loss = 0.50925593\n",
            "Iteration 113, loss = 0.51365834\n",
            "Iteration 114, loss = 0.50367905\n",
            "Iteration 115, loss = 0.50495308\n",
            "Iteration 116, loss = 0.51201476\n",
            "Iteration 117, loss = 0.50432329\n",
            "Iteration 118, loss = 0.50285737\n",
            "Iteration 119, loss = 0.50450100\n",
            "Iteration 120, loss = 0.50475371\n",
            "Iteration 121, loss = 0.49973070\n",
            "Iteration 122, loss = 0.50312937\n",
            "Iteration 123, loss = 0.50146934\n",
            "Iteration 124, loss = 0.49976620\n",
            "Iteration 125, loss = 0.49617840\n",
            "Iteration 126, loss = 0.50261373\n",
            "Iteration 127, loss = 0.50551024\n",
            "Iteration 128, loss = 0.50192188\n",
            "Iteration 129, loss = 0.50018930\n",
            "Iteration 130, loss = 0.49402204\n",
            "Iteration 131, loss = 0.49869590\n",
            "Iteration 132, loss = 0.49703046\n",
            "Iteration 133, loss = 0.48981528\n",
            "Iteration 134, loss = 0.48343421\n",
            "Iteration 135, loss = 0.50047683\n",
            "Iteration 136, loss = 0.49034823\n",
            "Iteration 137, loss = 0.48543633\n",
            "Iteration 138, loss = 0.48255045\n",
            "Iteration 139, loss = 0.48949278\n",
            "Iteration 140, loss = 0.48901216\n",
            "Iteration 141, loss = 0.48768061\n",
            "Iteration 142, loss = 0.48072747\n",
            "Iteration 143, loss = 0.49252457\n",
            "Iteration 144, loss = 0.48258534\n",
            "Iteration 145, loss = 0.49013126\n",
            "Iteration 146, loss = 0.48523442\n",
            "Iteration 147, loss = 0.48908581\n",
            "Iteration 148, loss = 0.47732522\n",
            "Iteration 149, loss = 0.49191385\n",
            "Iteration 150, loss = 0.48198594\n",
            "Iteration 151, loss = 0.47712139\n",
            "Iteration 152, loss = 0.48421910\n",
            "Iteration 153, loss = 0.48444248\n",
            "Iteration 154, loss = 0.47295446\n",
            "Iteration 155, loss = 0.47990579\n",
            "Iteration 156, loss = 0.48608778\n",
            "Iteration 157, loss = 0.48382484\n",
            "Iteration 158, loss = 0.48235003\n",
            "Iteration 159, loss = 0.47755552\n",
            "Iteration 160, loss = 0.50353841\n",
            "Iteration 161, loss = 0.47961683\n",
            "Iteration 162, loss = 0.47596913\n",
            "Iteration 163, loss = 0.47682884\n",
            "Iteration 164, loss = 0.47336380\n",
            "Iteration 165, loss = 0.48021608\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64910631\n",
            "Iteration 2, loss = 0.63140835\n",
            "Iteration 3, loss = 0.62139045\n",
            "Iteration 4, loss = 0.61413099\n",
            "Iteration 5, loss = 0.60884560\n",
            "Iteration 6, loss = 0.60414518\n",
            "Iteration 7, loss = 0.60338166\n",
            "Iteration 8, loss = 0.59878736\n",
            "Iteration 9, loss = 0.59923486\n",
            "Iteration 10, loss = 0.59706900\n",
            "Iteration 11, loss = 0.59579228\n",
            "Iteration 12, loss = 0.59479925\n",
            "Iteration 13, loss = 0.59461706\n",
            "Iteration 14, loss = 0.59237843\n",
            "Iteration 15, loss = 0.59285426\n",
            "Iteration 16, loss = 0.58980225\n",
            "Iteration 17, loss = 0.59000455\n",
            "Iteration 18, loss = 0.59030687\n",
            "Iteration 19, loss = 0.58755106\n",
            "Iteration 20, loss = 0.58705611\n",
            "Iteration 21, loss = 0.58691472\n",
            "Iteration 22, loss = 0.58600820\n",
            "Iteration 23, loss = 0.58599877\n",
            "Iteration 24, loss = 0.58499471\n",
            "Iteration 25, loss = 0.58443749\n",
            "Iteration 26, loss = 0.58291232\n",
            "Iteration 27, loss = 0.58269925\n",
            "Iteration 28, loss = 0.58227436\n",
            "Iteration 29, loss = 0.58088793\n",
            "Iteration 30, loss = 0.58107382\n",
            "Iteration 31, loss = 0.57982869\n",
            "Iteration 32, loss = 0.57944587\n",
            "Iteration 33, loss = 0.57803007\n",
            "Iteration 34, loss = 0.57826053\n",
            "Iteration 35, loss = 0.57770639\n",
            "Iteration 36, loss = 0.57633184\n",
            "Iteration 37, loss = 0.57702985\n",
            "Iteration 38, loss = 0.57626414\n",
            "Iteration 39, loss = 0.57543637\n",
            "Iteration 40, loss = 0.57543101\n",
            "Iteration 41, loss = 0.57403925\n",
            "Iteration 42, loss = 0.57361423\n",
            "Iteration 43, loss = 0.57366133\n",
            "Iteration 44, loss = 0.57317933\n",
            "Iteration 45, loss = 0.57234864\n",
            "Iteration 46, loss = 0.57287270\n",
            "Iteration 47, loss = 0.57152917\n",
            "Iteration 48, loss = 0.57167980\n",
            "Iteration 49, loss = 0.57096754\n",
            "Iteration 50, loss = 0.57193324\n",
            "Iteration 51, loss = 0.56853074\n",
            "Iteration 52, loss = 0.57231724\n",
            "Iteration 53, loss = 0.56906850\n",
            "Iteration 54, loss = 0.56773469\n",
            "Iteration 55, loss = 0.56869734\n",
            "Iteration 56, loss = 0.56639070\n",
            "Iteration 57, loss = 0.56651956\n",
            "Iteration 58, loss = 0.56572762\n",
            "Iteration 59, loss = 0.56467453\n",
            "Iteration 60, loss = 0.56402797\n",
            "Iteration 61, loss = 0.56276077\n",
            "Iteration 62, loss = 0.56541025\n",
            "Iteration 63, loss = 0.56325419\n",
            "Iteration 64, loss = 0.56469806\n",
            "Iteration 65, loss = 0.56328774\n",
            "Iteration 66, loss = 0.56325419\n",
            "Iteration 67, loss = 0.56079971\n",
            "Iteration 68, loss = 0.56052219\n",
            "Iteration 69, loss = 0.55857228\n",
            "Iteration 70, loss = 0.55899325\n",
            "Iteration 71, loss = 0.55917913\n",
            "Iteration 72, loss = 0.55853819\n",
            "Iteration 73, loss = 0.55763629\n",
            "Iteration 74, loss = 0.55523836\n",
            "Iteration 75, loss = 0.55389985\n",
            "Iteration 76, loss = 0.55344194\n",
            "Iteration 77, loss = 0.55162935\n",
            "Iteration 78, loss = 0.55452722\n",
            "Iteration 79, loss = 0.55366110\n",
            "Iteration 80, loss = 0.55019604\n",
            "Iteration 81, loss = 0.54624877\n",
            "Iteration 82, loss = 0.54610330\n",
            "Iteration 83, loss = 0.54656007\n",
            "Iteration 84, loss = 0.54698958\n",
            "Iteration 85, loss = 0.54761722\n",
            "Iteration 86, loss = 0.54383236\n",
            "Iteration 87, loss = 0.54417767\n",
            "Iteration 88, loss = 0.54207769\n",
            "Iteration 89, loss = 0.54507229\n",
            "Iteration 90, loss = 0.54017265\n",
            "Iteration 91, loss = 0.54085391\n",
            "Iteration 92, loss = 0.54281433\n",
            "Iteration 93, loss = 0.53837990\n",
            "Iteration 94, loss = 0.53793080\n",
            "Iteration 95, loss = 0.54094455\n",
            "Iteration 96, loss = 0.53422852\n",
            "Iteration 97, loss = 0.53603854\n",
            "Iteration 98, loss = 0.53291492\n",
            "Iteration 99, loss = 0.53280835\n",
            "Iteration 100, loss = 0.52960723\n",
            "Iteration 101, loss = 0.53158858\n",
            "Iteration 102, loss = 0.52607987\n",
            "Iteration 103, loss = 0.53050706\n",
            "Iteration 104, loss = 0.53952846\n",
            "Iteration 105, loss = 0.53654091\n",
            "Iteration 106, loss = 0.52639893\n",
            "Iteration 107, loss = 0.52605936\n",
            "Iteration 108, loss = 0.52811589\n",
            "Iteration 109, loss = 0.52645850\n",
            "Iteration 110, loss = 0.52339900\n",
            "Iteration 111, loss = 0.52485877\n",
            "Iteration 112, loss = 0.52720103\n",
            "Iteration 113, loss = 0.52924357\n",
            "Iteration 114, loss = 0.52732542\n",
            "Iteration 115, loss = 0.51913372\n",
            "Iteration 116, loss = 0.52280179\n",
            "Iteration 117, loss = 0.51914827\n",
            "Iteration 118, loss = 0.51544688\n",
            "Iteration 119, loss = 0.51953078\n",
            "Iteration 120, loss = 0.51623744\n",
            "Iteration 121, loss = 0.51241413\n",
            "Iteration 122, loss = 0.52152286\n",
            "Iteration 123, loss = 0.51520719\n",
            "Iteration 124, loss = 0.51501951\n",
            "Iteration 125, loss = 0.52091060\n",
            "Iteration 126, loss = 0.51254742\n",
            "Iteration 127, loss = 0.51103388\n",
            "Iteration 128, loss = 0.50895735\n",
            "Iteration 129, loss = 0.50737972\n",
            "Iteration 130, loss = 0.50973819\n",
            "Iteration 131, loss = 0.51104708\n",
            "Iteration 132, loss = 0.50423425\n",
            "Iteration 133, loss = 0.51866397\n",
            "Iteration 134, loss = 0.51078467\n",
            "Iteration 135, loss = 0.53953117\n",
            "Iteration 136, loss = 0.51225697\n",
            "Iteration 137, loss = 0.50827320\n",
            "Iteration 138, loss = 0.50550793\n",
            "Iteration 139, loss = 0.50214646\n",
            "Iteration 140, loss = 0.50105489\n",
            "Iteration 141, loss = 0.50837478\n",
            "Iteration 142, loss = 0.50396029\n",
            "Iteration 143, loss = 0.50047360\n",
            "Iteration 144, loss = 0.50981850\n",
            "Iteration 145, loss = 0.50694623\n",
            "Iteration 146, loss = 0.49923777\n",
            "Iteration 147, loss = 0.50078425\n",
            "Iteration 148, loss = 0.50024133\n",
            "Iteration 149, loss = 0.49982361\n",
            "Iteration 150, loss = 0.50718116\n",
            "Iteration 151, loss = 0.49630397\n",
            "Iteration 152, loss = 0.49907327\n",
            "Iteration 153, loss = 0.49952968\n",
            "Iteration 154, loss = 0.49564971\n",
            "Iteration 155, loss = 0.49760227\n",
            "Iteration 156, loss = 0.48541375\n",
            "Iteration 157, loss = 0.49132289\n",
            "Iteration 158, loss = 0.49283649\n",
            "Iteration 159, loss = 0.49274650\n",
            "Iteration 160, loss = 0.49123105\n",
            "Iteration 161, loss = 0.49650246\n",
            "Iteration 162, loss = 0.48710718\n",
            "Iteration 163, loss = 0.48868116\n",
            "Iteration 164, loss = 0.48675806\n",
            "Iteration 165, loss = 0.50338591\n",
            "Iteration 166, loss = 0.50105598\n",
            "Iteration 167, loss = 0.48778215\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64630447\n",
            "Iteration 2, loss = 0.63072367\n",
            "Iteration 3, loss = 0.62316226\n",
            "Iteration 4, loss = 0.61826960\n",
            "Iteration 5, loss = 0.61709682\n",
            "Iteration 6, loss = 0.61473995\n",
            "Iteration 7, loss = 0.61374510\n",
            "Iteration 8, loss = 0.61176857\n",
            "Iteration 9, loss = 0.60990632\n",
            "Iteration 10, loss = 0.60741335\n",
            "Iteration 11, loss = 0.60529815\n",
            "Iteration 12, loss = 0.60494132\n",
            "Iteration 13, loss = 0.60397102\n",
            "Iteration 14, loss = 0.60268938\n",
            "Iteration 15, loss = 0.60244972\n",
            "Iteration 16, loss = 0.60156076\n",
            "Iteration 17, loss = 0.60200462\n",
            "Iteration 18, loss = 0.60015906\n",
            "Iteration 19, loss = 0.60019774\n",
            "Iteration 20, loss = 0.59974013\n",
            "Iteration 21, loss = 0.59894598\n",
            "Iteration 22, loss = 0.59830735\n",
            "Iteration 23, loss = 0.59781440\n",
            "Iteration 24, loss = 0.59739690\n",
            "Iteration 25, loss = 0.59685347\n",
            "Iteration 26, loss = 0.59624385\n",
            "Iteration 27, loss = 0.59598201\n",
            "Iteration 28, loss = 0.59512735\n",
            "Iteration 29, loss = 0.59485516\n",
            "Iteration 30, loss = 0.59418218\n",
            "Iteration 31, loss = 0.59406960\n",
            "Iteration 32, loss = 0.59342059\n",
            "Iteration 33, loss = 0.59354345\n",
            "Iteration 34, loss = 0.59460846\n",
            "Iteration 35, loss = 0.59196466\n",
            "Iteration 36, loss = 0.59224598\n",
            "Iteration 37, loss = 0.59120844\n",
            "Iteration 38, loss = 0.59138596\n",
            "Iteration 39, loss = 0.59040036\n",
            "Iteration 40, loss = 0.58965683\n",
            "Iteration 41, loss = 0.58938432\n",
            "Iteration 42, loss = 0.58925648\n",
            "Iteration 43, loss = 0.58936366\n",
            "Iteration 44, loss = 0.58865087\n",
            "Iteration 45, loss = 0.58910371\n",
            "Iteration 46, loss = 0.58927119\n",
            "Iteration 47, loss = 0.58731816\n",
            "Iteration 48, loss = 0.58791390\n",
            "Iteration 49, loss = 0.58672969\n",
            "Iteration 50, loss = 0.58653355\n",
            "Iteration 51, loss = 0.58679487\n",
            "Iteration 52, loss = 0.58565029\n",
            "Iteration 53, loss = 0.58567577\n",
            "Iteration 54, loss = 0.58496200\n",
            "Iteration 55, loss = 0.58501661\n",
            "Iteration 56, loss = 0.58457941\n",
            "Iteration 57, loss = 0.58384618\n",
            "Iteration 58, loss = 0.58376662\n",
            "Iteration 59, loss = 0.58427985\n",
            "Iteration 60, loss = 0.58461514\n",
            "Iteration 61, loss = 0.58325014\n",
            "Iteration 62, loss = 0.58377081\n",
            "Iteration 63, loss = 0.58240083\n",
            "Iteration 64, loss = 0.58233994\n",
            "Iteration 65, loss = 0.58181196\n",
            "Iteration 66, loss = 0.58201541\n",
            "Iteration 67, loss = 0.58135129\n",
            "Iteration 68, loss = 0.58115431\n",
            "Iteration 69, loss = 0.58073353\n",
            "Iteration 70, loss = 0.58024204\n",
            "Iteration 71, loss = 0.58006712\n",
            "Iteration 72, loss = 0.57915693\n",
            "Iteration 73, loss = 0.57930000\n",
            "Iteration 74, loss = 0.57861916\n",
            "Iteration 75, loss = 0.57825746\n",
            "Iteration 76, loss = 0.57934362\n",
            "Iteration 77, loss = 0.57810635\n",
            "Iteration 78, loss = 0.57886227\n",
            "Iteration 79, loss = 0.57797223\n",
            "Iteration 80, loss = 0.58084032\n",
            "Iteration 81, loss = 0.57818425\n",
            "Iteration 82, loss = 0.57726130\n",
            "Iteration 83, loss = 0.57712569\n",
            "Iteration 84, loss = 0.57616866\n",
            "Iteration 85, loss = 0.57652877\n",
            "Iteration 86, loss = 0.57540447\n",
            "Iteration 87, loss = 0.57844061\n",
            "Iteration 88, loss = 0.57416714\n",
            "Iteration 89, loss = 0.57543001\n",
            "Iteration 90, loss = 0.57413398\n",
            "Iteration 91, loss = 0.57433002\n",
            "Iteration 92, loss = 0.57332365\n",
            "Iteration 93, loss = 0.57445172\n",
            "Iteration 94, loss = 0.57185906\n",
            "Iteration 95, loss = 0.57257779\n",
            "Iteration 96, loss = 0.57233400\n",
            "Iteration 97, loss = 0.57126054\n",
            "Iteration 98, loss = 0.57136057\n",
            "Iteration 99, loss = 0.57196119\n",
            "Iteration 100, loss = 0.56904461\n",
            "Iteration 101, loss = 0.56919764\n",
            "Iteration 102, loss = 0.57003804\n",
            "Iteration 103, loss = 0.56971053\n",
            "Iteration 104, loss = 0.56713967\n",
            "Iteration 105, loss = 0.56774962\n",
            "Iteration 106, loss = 0.56820810\n",
            "Iteration 107, loss = 0.56656858\n",
            "Iteration 108, loss = 0.56763767\n",
            "Iteration 109, loss = 0.56619485\n",
            "Iteration 110, loss = 0.56655133\n",
            "Iteration 111, loss = 0.56509710\n",
            "Iteration 112, loss = 0.56480892\n",
            "Iteration 113, loss = 0.56379250\n",
            "Iteration 114, loss = 0.56673896\n",
            "Iteration 115, loss = 0.56322033\n",
            "Iteration 116, loss = 0.56338539\n",
            "Iteration 117, loss = 0.56227209\n",
            "Iteration 118, loss = 0.56489098\n",
            "Iteration 119, loss = 0.56164784\n",
            "Iteration 120, loss = 0.56144108\n",
            "Iteration 121, loss = 0.56009682\n",
            "Iteration 122, loss = 0.56115877\n",
            "Iteration 123, loss = 0.55773156\n",
            "Iteration 124, loss = 0.55618384\n",
            "Iteration 125, loss = 0.56072247\n",
            "Iteration 126, loss = 0.55840231\n",
            "Iteration 127, loss = 0.55667734\n",
            "Iteration 128, loss = 0.55345244\n",
            "Iteration 129, loss = 0.55554295\n",
            "Iteration 130, loss = 0.55283654\n",
            "Iteration 131, loss = 0.55475597\n",
            "Iteration 132, loss = 0.55567640\n",
            "Iteration 133, loss = 0.55526015\n",
            "Iteration 134, loss = 0.55730785\n",
            "Iteration 135, loss = 0.55267925\n",
            "Iteration 136, loss = 0.55092773\n",
            "Iteration 137, loss = 0.55105582\n",
            "Iteration 138, loss = 0.55804506\n",
            "Iteration 139, loss = 0.54933543\n",
            "Iteration 140, loss = 0.55255566\n",
            "Iteration 141, loss = 0.54660485\n",
            "Iteration 142, loss = 0.54744801\n",
            "Iteration 143, loss = 0.55170144\n",
            "Iteration 144, loss = 0.54823216\n",
            "Iteration 145, loss = 0.54648077\n",
            "Iteration 146, loss = 0.54660448\n",
            "Iteration 147, loss = 0.54466392\n",
            "Iteration 148, loss = 0.54273966\n",
            "Iteration 149, loss = 0.54242700\n",
            "Iteration 150, loss = 0.54470057\n",
            "Iteration 151, loss = 0.54225383\n",
            "Iteration 152, loss = 0.54228473\n",
            "Iteration 153, loss = 0.54106529\n",
            "Iteration 154, loss = 0.54113259\n",
            "Iteration 155, loss = 0.53949096\n",
            "Iteration 156, loss = 0.53745950\n",
            "Iteration 157, loss = 0.54325793\n",
            "Iteration 158, loss = 0.53669072\n",
            "Iteration 159, loss = 0.53755053\n",
            "Iteration 160, loss = 0.54151702\n",
            "Iteration 161, loss = 0.53387285\n",
            "Iteration 162, loss = 0.53759277\n",
            "Iteration 163, loss = 0.53847691\n",
            "Iteration 164, loss = 0.53864978\n",
            "Iteration 165, loss = 0.54057120\n",
            "Iteration 166, loss = 0.53193862\n",
            "Iteration 167, loss = 0.53123865\n",
            "Iteration 168, loss = 0.53197547\n",
            "Iteration 169, loss = 0.53319340\n",
            "Iteration 170, loss = 0.53017664\n",
            "Iteration 171, loss = 0.52976561\n",
            "Iteration 172, loss = 0.53047213\n",
            "Iteration 173, loss = 0.53081011\n",
            "Iteration 174, loss = 0.52257774\n",
            "Iteration 175, loss = 0.53482030\n",
            "Iteration 176, loss = 0.52361674\n",
            "Iteration 177, loss = 0.52874866\n",
            "Iteration 178, loss = 0.52717354\n",
            "Iteration 179, loss = 0.52546823\n",
            "Iteration 180, loss = 0.51906506\n",
            "Iteration 181, loss = 0.52068660\n",
            "Iteration 182, loss = 0.52068900\n",
            "Iteration 183, loss = 0.52175495\n",
            "Iteration 184, loss = 0.53276737\n",
            "Iteration 185, loss = 0.52793033\n",
            "Iteration 186, loss = 0.53295360\n",
            "Iteration 187, loss = 0.52493955\n",
            "Iteration 188, loss = 0.51588952\n",
            "Iteration 189, loss = 0.52420646\n",
            "Iteration 190, loss = 0.51685920\n",
            "Iteration 191, loss = 0.51537816\n",
            "Iteration 192, loss = 0.51636451\n",
            "Iteration 193, loss = 0.51161239\n",
            "Iteration 194, loss = 0.51502955\n",
            "Iteration 195, loss = 0.51782477\n",
            "Iteration 196, loss = 0.51818913\n",
            "Iteration 197, loss = 0.51431365\n",
            "Iteration 198, loss = 0.51295076\n",
            "Iteration 199, loss = 0.52130001\n",
            "Iteration 200, loss = 0.52044131\n",
            "Iteration 1, loss = 0.71098192\n",
            "Iteration 2, loss = 0.63752761\n",
            "Iteration 3, loss = 0.59716931\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4, loss = 0.59020926\n",
            "Iteration 5, loss = 0.58867996\n",
            "Iteration 6, loss = 0.58734485\n",
            "Iteration 7, loss = 0.58433776\n",
            "Iteration 8, loss = 0.58394143\n",
            "Iteration 9, loss = 0.58142234\n",
            "Iteration 10, loss = 0.57999947\n",
            "Iteration 11, loss = 0.58054165\n",
            "Iteration 12, loss = 0.57824219\n",
            "Iteration 13, loss = 0.57768618\n",
            "Iteration 14, loss = 0.57699237\n",
            "Iteration 15, loss = 0.57483595\n",
            "Iteration 16, loss = 0.57416256\n",
            "Iteration 17, loss = 0.57347578\n",
            "Iteration 18, loss = 0.57183209\n",
            "Iteration 19, loss = 0.57085966\n",
            "Iteration 20, loss = 0.57083630\n",
            "Iteration 21, loss = 0.57096342\n",
            "Iteration 22, loss = 0.57068787\n",
            "Iteration 23, loss = 0.56846967\n",
            "Iteration 24, loss = 0.56701809\n",
            "Iteration 25, loss = 0.56623227\n",
            "Iteration 26, loss = 0.56433034\n",
            "Iteration 27, loss = 0.56361855\n",
            "Iteration 28, loss = 0.56270298\n",
            "Iteration 29, loss = 0.56180392\n",
            "Iteration 30, loss = 0.56077391\n",
            "Iteration 31, loss = 0.55915044\n",
            "Iteration 32, loss = 0.55941515\n",
            "Iteration 33, loss = 0.55930028\n",
            "Iteration 34, loss = 0.55594073\n",
            "Iteration 35, loss = 0.55550048\n",
            "Iteration 36, loss = 0.55517734\n",
            "Iteration 37, loss = 0.55413062\n",
            "Iteration 38, loss = 0.55185558\n",
            "Iteration 39, loss = 0.55312505\n",
            "Iteration 40, loss = 0.55269015\n",
            "Iteration 41, loss = 0.54963273\n",
            "Iteration 42, loss = 0.54755827\n",
            "Iteration 43, loss = 0.54753512\n",
            "Iteration 44, loss = 0.54828619\n",
            "Iteration 45, loss = 0.55431492\n",
            "Iteration 46, loss = 0.54601910\n",
            "Iteration 47, loss = 0.54411033\n",
            "Iteration 48, loss = 0.54924886\n",
            "Iteration 49, loss = 0.54797651\n",
            "Iteration 50, loss = 0.54442401\n",
            "Iteration 51, loss = 0.53912513\n",
            "Iteration 52, loss = 0.54056508\n",
            "Iteration 53, loss = 0.53761387\n",
            "Iteration 54, loss = 0.53857060\n",
            "Iteration 55, loss = 0.53613235\n",
            "Iteration 56, loss = 0.53663137\n",
            "Iteration 57, loss = 0.53575493\n",
            "Iteration 58, loss = 0.53528598\n",
            "Iteration 59, loss = 0.53223163\n",
            "Iteration 60, loss = 0.53155672\n",
            "Iteration 61, loss = 0.53362661\n",
            "Iteration 62, loss = 0.53848620\n",
            "Iteration 63, loss = 0.52923207\n",
            "Iteration 64, loss = 0.52984357\n",
            "Iteration 65, loss = 0.52720349\n",
            "Iteration 66, loss = 0.53015842\n",
            "Iteration 67, loss = 0.52775071\n",
            "Iteration 68, loss = 0.53326627\n",
            "Iteration 69, loss = 0.52846392\n",
            "Iteration 70, loss = 0.52157877\n",
            "Iteration 71, loss = 0.52388037\n",
            "Iteration 72, loss = 0.52717279\n",
            "Iteration 73, loss = 0.52516295\n",
            "Iteration 74, loss = 0.52087624\n",
            "Iteration 75, loss = 0.51649676\n",
            "Iteration 76, loss = 0.52179135\n",
            "Iteration 77, loss = 0.51745345\n",
            "Iteration 78, loss = 0.51952299\n",
            "Iteration 79, loss = 0.51815264\n",
            "Iteration 80, loss = 0.51848382\n",
            "Iteration 81, loss = 0.52158657\n",
            "Iteration 82, loss = 0.51178091\n",
            "Iteration 83, loss = 0.51806952\n",
            "Iteration 84, loss = 0.51184860\n",
            "Iteration 85, loss = 0.51356319\n",
            "Iteration 86, loss = 0.50896941\n",
            "Iteration 87, loss = 0.51327152\n",
            "Iteration 88, loss = 0.51383825\n",
            "Iteration 89, loss = 0.51191587\n",
            "Iteration 90, loss = 0.51196658\n",
            "Iteration 91, loss = 0.50478913\n",
            "Iteration 92, loss = 0.50830722\n",
            "Iteration 93, loss = 0.50531044\n",
            "Iteration 94, loss = 0.50257930\n",
            "Iteration 95, loss = 0.50314931\n",
            "Iteration 96, loss = 0.50796208\n",
            "Iteration 97, loss = 0.50322649\n",
            "Iteration 98, loss = 0.50456624\n",
            "Iteration 99, loss = 0.50390734\n",
            "Iteration 100, loss = 0.50030687\n",
            "Iteration 101, loss = 0.50078765\n",
            "Iteration 102, loss = 0.49884802\n",
            "Iteration 103, loss = 0.50603685\n",
            "Iteration 104, loss = 0.49914426\n",
            "Iteration 105, loss = 0.49751785\n",
            "Iteration 106, loss = 0.49920704\n",
            "Iteration 107, loss = 0.49690148\n",
            "Iteration 108, loss = 0.50350047\n",
            "Iteration 109, loss = 0.49618300\n",
            "Iteration 110, loss = 0.49841550\n",
            "Iteration 111, loss = 0.48894269\n",
            "Iteration 112, loss = 0.49866559\n",
            "Iteration 113, loss = 0.48951417\n",
            "Iteration 114, loss = 0.49243610\n",
            "Iteration 115, loss = 0.49434776\n",
            "Iteration 116, loss = 0.50461728\n",
            "Iteration 117, loss = 0.48977001\n",
            "Iteration 118, loss = 0.49120306\n",
            "Iteration 119, loss = 0.49261530\n",
            "Iteration 120, loss = 0.49631624\n",
            "Iteration 121, loss = 0.48522305\n",
            "Iteration 122, loss = 0.48469236\n",
            "Iteration 123, loss = 0.50275939\n",
            "Iteration 124, loss = 0.48110699\n",
            "Iteration 125, loss = 0.48694356\n",
            "Iteration 126, loss = 0.50782359\n",
            "Iteration 127, loss = 0.48263314\n",
            "Iteration 128, loss = 0.48431472\n",
            "Iteration 129, loss = 0.47628233\n",
            "Iteration 130, loss = 0.48158614\n",
            "Iteration 131, loss = 0.49111987\n",
            "Iteration 132, loss = 0.48126456\n",
            "Iteration 133, loss = 0.47937755\n",
            "Iteration 134, loss = 0.47528331\n",
            "Iteration 135, loss = 0.48369778\n",
            "Iteration 136, loss = 0.48261888\n",
            "Iteration 137, loss = 0.47115721\n",
            "Iteration 138, loss = 0.47679243\n",
            "Iteration 139, loss = 0.47821530\n",
            "Iteration 140, loss = 0.48549445\n",
            "Iteration 141, loss = 0.48487841\n",
            "Iteration 142, loss = 0.47575223\n",
            "Iteration 143, loss = 0.47921890\n",
            "Iteration 144, loss = 0.46955284\n",
            "Iteration 145, loss = 0.47459377\n",
            "Iteration 146, loss = 0.47632850\n",
            "Iteration 147, loss = 0.47095623\n",
            "Iteration 148, loss = 0.48124742\n",
            "Iteration 149, loss = 0.47440024\n",
            "Iteration 150, loss = 0.46579711\n",
            "Iteration 151, loss = 0.47662564\n",
            "Iteration 152, loss = 0.47034153\n",
            "Iteration 153, loss = 0.47795225\n",
            "Iteration 154, loss = 0.48130959\n",
            "Iteration 155, loss = 0.47589953\n",
            "Iteration 156, loss = 0.47393120\n",
            "Iteration 157, loss = 0.47856359\n",
            "Iteration 158, loss = 0.47232544\n",
            "Iteration 159, loss = 0.46221013\n",
            "Iteration 160, loss = 0.47116463\n",
            "Iteration 161, loss = 0.46740804\n",
            "Iteration 162, loss = 0.47432561\n",
            "Iteration 163, loss = 0.45593707\n",
            "Iteration 164, loss = 0.47998651\n",
            "Iteration 165, loss = 0.45959226\n",
            "Iteration 166, loss = 0.46720004\n",
            "Iteration 167, loss = 0.47285665\n",
            "Iteration 168, loss = 0.46474774\n",
            "Iteration 169, loss = 0.46439018\n",
            "Iteration 170, loss = 0.46630070\n",
            "Iteration 171, loss = 0.47521101\n",
            "Iteration 172, loss = 0.46875767\n",
            "Iteration 173, loss = 0.46857036\n",
            "Iteration 174, loss = 0.46303769\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62378346\n",
            "Iteration 2, loss = 0.60504114\n",
            "Iteration 3, loss = 0.59933101\n",
            "Iteration 4, loss = 0.59566199\n",
            "Iteration 5, loss = 0.59174183\n",
            "Iteration 6, loss = 0.58919962\n",
            "Iteration 7, loss = 0.58703123\n",
            "Iteration 8, loss = 0.58529196\n",
            "Iteration 9, loss = 0.58416814\n",
            "Iteration 10, loss = 0.58213401\n",
            "Iteration 11, loss = 0.58084013\n",
            "Iteration 12, loss = 0.57995031\n",
            "Iteration 13, loss = 0.57969228\n",
            "Iteration 14, loss = 0.58022511\n",
            "Iteration 15, loss = 0.57713285\n",
            "Iteration 16, loss = 0.57650255\n",
            "Iteration 17, loss = 0.57494548\n",
            "Iteration 18, loss = 0.57539219\n",
            "Iteration 19, loss = 0.57518700\n",
            "Iteration 20, loss = 0.57432755\n",
            "Iteration 21, loss = 0.57141224\n",
            "Iteration 22, loss = 0.57207922\n",
            "Iteration 23, loss = 0.56997716\n",
            "Iteration 24, loss = 0.56945330\n",
            "Iteration 25, loss = 0.56725186\n",
            "Iteration 26, loss = 0.56669219\n",
            "Iteration 27, loss = 0.56461492\n",
            "Iteration 28, loss = 0.56463223\n",
            "Iteration 29, loss = 0.56359739\n",
            "Iteration 30, loss = 0.56583282\n",
            "Iteration 31, loss = 0.56270455\n",
            "Iteration 32, loss = 0.56165556\n",
            "Iteration 33, loss = 0.56287370\n",
            "Iteration 34, loss = 0.56045887\n",
            "Iteration 35, loss = 0.55878861\n",
            "Iteration 36, loss = 0.55823862\n",
            "Iteration 37, loss = 0.55810028\n",
            "Iteration 38, loss = 0.55738482\n",
            "Iteration 39, loss = 0.55643209\n",
            "Iteration 40, loss = 0.55397695\n",
            "Iteration 41, loss = 0.55380195\n",
            "Iteration 42, loss = 0.55276037\n",
            "Iteration 43, loss = 0.55212137\n",
            "Iteration 44, loss = 0.55134549\n",
            "Iteration 45, loss = 0.55105861\n",
            "Iteration 46, loss = 0.54857809\n",
            "Iteration 47, loss = 0.55154594\n",
            "Iteration 48, loss = 0.54757803\n",
            "Iteration 49, loss = 0.54696672\n",
            "Iteration 50, loss = 0.54738716\n",
            "Iteration 51, loss = 0.54545082\n",
            "Iteration 52, loss = 0.54468984\n",
            "Iteration 53, loss = 0.54619093\n",
            "Iteration 54, loss = 0.54321897\n",
            "Iteration 55, loss = 0.54640629\n",
            "Iteration 56, loss = 0.54262102\n",
            "Iteration 57, loss = 0.54001124\n",
            "Iteration 58, loss = 0.54199180\n",
            "Iteration 59, loss = 0.54178009\n",
            "Iteration 60, loss = 0.54161637\n",
            "Iteration 61, loss = 0.53907000\n",
            "Iteration 62, loss = 0.53707913\n",
            "Iteration 63, loss = 0.53796219\n",
            "Iteration 64, loss = 0.53699287\n",
            "Iteration 65, loss = 0.53760808\n",
            "Iteration 66, loss = 0.53387983\n",
            "Iteration 67, loss = 0.53332943\n",
            "Iteration 68, loss = 0.53223513\n",
            "Iteration 69, loss = 0.53302590\n",
            "Iteration 70, loss = 0.52950985\n",
            "Iteration 71, loss = 0.53175247\n",
            "Iteration 72, loss = 0.53551108\n",
            "Iteration 73, loss = 0.52780327\n",
            "Iteration 74, loss = 0.53020399\n",
            "Iteration 75, loss = 0.52989868\n",
            "Iteration 76, loss = 0.52759929\n",
            "Iteration 77, loss = 0.52418194\n",
            "Iteration 78, loss = 0.52399192\n",
            "Iteration 79, loss = 0.52203113\n",
            "Iteration 80, loss = 0.52240073\n",
            "Iteration 81, loss = 0.52569822\n",
            "Iteration 82, loss = 0.52358300\n",
            "Iteration 83, loss = 0.52900136\n",
            "Iteration 84, loss = 0.51543722\n",
            "Iteration 85, loss = 0.52100953\n",
            "Iteration 86, loss = 0.51815443\n",
            "Iteration 87, loss = 0.51750903\n",
            "Iteration 88, loss = 0.51828278\n",
            "Iteration 89, loss = 0.51985305\n",
            "Iteration 90, loss = 0.51808130\n",
            "Iteration 91, loss = 0.51329974\n",
            "Iteration 92, loss = 0.51794469\n",
            "Iteration 93, loss = 0.51817620\n",
            "Iteration 94, loss = 0.51195303\n",
            "Iteration 95, loss = 0.51158915\n",
            "Iteration 96, loss = 0.50998747\n",
            "Iteration 97, loss = 0.50720848\n",
            "Iteration 98, loss = 0.50934869\n",
            "Iteration 99, loss = 0.51236939\n",
            "Iteration 100, loss = 0.50808140\n",
            "Iteration 101, loss = 0.50549383\n",
            "Iteration 102, loss = 0.50006058\n",
            "Iteration 103, loss = 0.50166998\n",
            "Iteration 104, loss = 0.50524546\n",
            "Iteration 105, loss = 0.50191000\n",
            "Iteration 106, loss = 0.49802046\n",
            "Iteration 107, loss = 0.50331213\n",
            "Iteration 108, loss = 0.49910423\n",
            "Iteration 109, loss = 0.50250548\n",
            "Iteration 110, loss = 0.50357885\n",
            "Iteration 111, loss = 0.50336814\n",
            "Iteration 112, loss = 0.49864673\n",
            "Iteration 113, loss = 0.49876542\n",
            "Iteration 114, loss = 0.49982696\n",
            "Iteration 115, loss = 0.49430493\n",
            "Iteration 116, loss = 0.48652540\n",
            "Iteration 117, loss = 0.50326882\n",
            "Iteration 118, loss = 0.49313338\n",
            "Iteration 119, loss = 0.49525637\n",
            "Iteration 120, loss = 0.49034638\n",
            "Iteration 121, loss = 0.49254569\n",
            "Iteration 122, loss = 0.49058138\n",
            "Iteration 123, loss = 0.49584444\n",
            "Iteration 124, loss = 0.48806858\n",
            "Iteration 125, loss = 0.49104792\n",
            "Iteration 126, loss = 0.48813235\n",
            "Iteration 127, loss = 0.48337709\n",
            "Iteration 128, loss = 0.48605680\n",
            "Iteration 129, loss = 0.48851684\n",
            "Iteration 130, loss = 0.48894069\n",
            "Iteration 131, loss = 0.49570903\n",
            "Iteration 132, loss = 0.51115315\n",
            "Iteration 133, loss = 0.50435034\n",
            "Iteration 134, loss = 0.50043979\n",
            "Iteration 135, loss = 0.49570679\n",
            "Iteration 136, loss = 0.49732825\n",
            "Iteration 137, loss = 0.49137355\n",
            "Iteration 138, loss = 0.48975654\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.78241308\n",
            "Iteration 2, loss = 0.65863727\n",
            "Iteration 3, loss = 0.62590758\n",
            "Iteration 4, loss = 0.61174079\n",
            "Iteration 5, loss = 0.60664137\n",
            "Iteration 6, loss = 0.60537688\n",
            "Iteration 7, loss = 0.60260220\n",
            "Iteration 8, loss = 0.60153688\n",
            "Iteration 9, loss = 0.59979512\n",
            "Iteration 10, loss = 0.59799067\n",
            "Iteration 11, loss = 0.59761971\n",
            "Iteration 12, loss = 0.59548398\n",
            "Iteration 13, loss = 0.59439077\n",
            "Iteration 14, loss = 0.59372851\n",
            "Iteration 15, loss = 0.59340582\n",
            "Iteration 16, loss = 0.59285354\n",
            "Iteration 17, loss = 0.59219391\n",
            "Iteration 18, loss = 0.59200378\n",
            "Iteration 19, loss = 0.59046221\n",
            "Iteration 20, loss = 0.58976490\n",
            "Iteration 21, loss = 0.58942354\n",
            "Iteration 22, loss = 0.58872833\n",
            "Iteration 23, loss = 0.58809172\n",
            "Iteration 24, loss = 0.58794427\n",
            "Iteration 25, loss = 0.58795878\n",
            "Iteration 26, loss = 0.58682451\n",
            "Iteration 27, loss = 0.58632398\n",
            "Iteration 28, loss = 0.58666749\n",
            "Iteration 29, loss = 0.58490262\n",
            "Iteration 30, loss = 0.58464813\n",
            "Iteration 31, loss = 0.58379521\n",
            "Iteration 32, loss = 0.58286467\n",
            "Iteration 33, loss = 0.58239857\n",
            "Iteration 34, loss = 0.58191387\n",
            "Iteration 35, loss = 0.58123268\n",
            "Iteration 36, loss = 0.57991772\n",
            "Iteration 37, loss = 0.57953473\n",
            "Iteration 38, loss = 0.57848443\n",
            "Iteration 39, loss = 0.57778233\n",
            "Iteration 40, loss = 0.57776192\n",
            "Iteration 41, loss = 0.57676640\n",
            "Iteration 42, loss = 0.57589538\n",
            "Iteration 43, loss = 0.57599293\n",
            "Iteration 44, loss = 0.57509050\n",
            "Iteration 45, loss = 0.57524651\n",
            "Iteration 46, loss = 0.57520511\n",
            "Iteration 47, loss = 0.57325726\n",
            "Iteration 48, loss = 0.57435402\n",
            "Iteration 49, loss = 0.57206849\n",
            "Iteration 50, loss = 0.57202426\n",
            "Iteration 51, loss = 0.57045673\n",
            "Iteration 52, loss = 0.57011791\n",
            "Iteration 53, loss = 0.57049763\n",
            "Iteration 54, loss = 0.56852553\n",
            "Iteration 55, loss = 0.56816905\n",
            "Iteration 56, loss = 0.56751324\n",
            "Iteration 57, loss = 0.56601363\n",
            "Iteration 58, loss = 0.56581853\n",
            "Iteration 59, loss = 0.56421511\n",
            "Iteration 60, loss = 0.56333191\n",
            "Iteration 61, loss = 0.56235284\n",
            "Iteration 62, loss = 0.56266461\n",
            "Iteration 63, loss = 0.56156004\n",
            "Iteration 64, loss = 0.56160446\n",
            "Iteration 65, loss = 0.55942714\n",
            "Iteration 66, loss = 0.56082968\n",
            "Iteration 67, loss = 0.55945687\n",
            "Iteration 68, loss = 0.55725220\n",
            "Iteration 69, loss = 0.55718385\n",
            "Iteration 70, loss = 0.55876816\n",
            "Iteration 71, loss = 0.55574935\n",
            "Iteration 72, loss = 0.55531338\n",
            "Iteration 73, loss = 0.55608780\n",
            "Iteration 74, loss = 0.55449573\n",
            "Iteration 75, loss = 0.55376987\n",
            "Iteration 76, loss = 0.55261351\n",
            "Iteration 77, loss = 0.55413080\n",
            "Iteration 78, loss = 0.55209517\n",
            "Iteration 79, loss = 0.54964683\n",
            "Iteration 80, loss = 0.54961950\n",
            "Iteration 81, loss = 0.54819407\n",
            "Iteration 82, loss = 0.54879501\n",
            "Iteration 83, loss = 0.54530949\n",
            "Iteration 84, loss = 0.54769864\n",
            "Iteration 85, loss = 0.54693004\n",
            "Iteration 86, loss = 0.54596628\n",
            "Iteration 87, loss = 0.54618291\n",
            "Iteration 88, loss = 0.54258358\n",
            "Iteration 89, loss = 0.54402890\n",
            "Iteration 90, loss = 0.54863256\n",
            "Iteration 91, loss = 0.54352726\n",
            "Iteration 92, loss = 0.54140956\n",
            "Iteration 93, loss = 0.54045205\n",
            "Iteration 94, loss = 0.53741061\n",
            "Iteration 95, loss = 0.53886205\n",
            "Iteration 96, loss = 0.53752989\n",
            "Iteration 97, loss = 0.53563942\n",
            "Iteration 98, loss = 0.53583361\n",
            "Iteration 99, loss = 0.53677555\n",
            "Iteration 100, loss = 0.53615629\n",
            "Iteration 101, loss = 0.53345555\n",
            "Iteration 102, loss = 0.53475760\n",
            "Iteration 103, loss = 0.53297490\n",
            "Iteration 104, loss = 0.53181674\n",
            "Iteration 105, loss = 0.53048428\n",
            "Iteration 106, loss = 0.53162108\n",
            "Iteration 107, loss = 0.52887494\n",
            "Iteration 108, loss = 0.53281920\n",
            "Iteration 109, loss = 0.52857227\n",
            "Iteration 110, loss = 0.52728283\n",
            "Iteration 111, loss = 0.52766705\n",
            "Iteration 112, loss = 0.52733693\n",
            "Iteration 113, loss = 0.52717757\n",
            "Iteration 114, loss = 0.52269863\n",
            "Iteration 115, loss = 0.52306426\n",
            "Iteration 116, loss = 0.52237855\n",
            "Iteration 117, loss = 0.51996419\n",
            "Iteration 118, loss = 0.52548885\n",
            "Iteration 119, loss = 0.51779218\n",
            "Iteration 120, loss = 0.52089776\n",
            "Iteration 121, loss = 0.51640694\n",
            "Iteration 122, loss = 0.51656322\n",
            "Iteration 123, loss = 0.51638859\n",
            "Iteration 124, loss = 0.51592630\n",
            "Iteration 125, loss = 0.51403863\n",
            "Iteration 126, loss = 0.51182050\n",
            "Iteration 127, loss = 0.51506253\n",
            "Iteration 128, loss = 0.51408908\n",
            "Iteration 129, loss = 0.50678058\n",
            "Iteration 130, loss = 0.50885990\n",
            "Iteration 131, loss = 0.50459202\n",
            "Iteration 132, loss = 0.50727646\n",
            "Iteration 133, loss = 0.50130783\n",
            "Iteration 134, loss = 0.51204561\n",
            "Iteration 135, loss = 0.50481919\n",
            "Iteration 136, loss = 0.50605089\n",
            "Iteration 137, loss = 0.50147583\n",
            "Iteration 138, loss = 0.49793447\n",
            "Iteration 139, loss = 0.50559846\n",
            "Iteration 140, loss = 0.50316509\n",
            "Iteration 141, loss = 0.49921085\n",
            "Iteration 142, loss = 0.50637427\n",
            "Iteration 143, loss = 0.49814316\n",
            "Iteration 144, loss = 0.49549462\n",
            "Iteration 145, loss = 0.49574102\n",
            "Iteration 146, loss = 0.49118307\n",
            "Iteration 147, loss = 0.49534537\n",
            "Iteration 148, loss = 0.49183614\n",
            "Iteration 149, loss = 0.49592211\n",
            "Iteration 150, loss = 0.49375554\n",
            "Iteration 151, loss = 0.49023471\n",
            "Iteration 152, loss = 0.49403390\n",
            "Iteration 153, loss = 0.49684595\n",
            "Iteration 154, loss = 0.48687412\n",
            "Iteration 155, loss = 0.48503973\n",
            "Iteration 156, loss = 0.48532577\n",
            "Iteration 157, loss = 0.48847447\n",
            "Iteration 158, loss = 0.48564102\n",
            "Iteration 159, loss = 0.48202694\n",
            "Iteration 160, loss = 0.49244961\n",
            "Iteration 161, loss = 0.48508267\n",
            "Iteration 162, loss = 0.48367730\n",
            "Iteration 163, loss = 0.48072099\n",
            "Iteration 164, loss = 0.48214606\n",
            "Iteration 165, loss = 0.48440231\n",
            "Iteration 166, loss = 0.48049895\n",
            "Iteration 167, loss = 0.48062116\n",
            "Iteration 168, loss = 0.48166247\n",
            "Iteration 169, loss = 0.47886869\n",
            "Iteration 170, loss = 0.48256127\n",
            "Iteration 171, loss = 0.48027939\n",
            "Iteration 172, loss = 0.47792661\n",
            "Iteration 173, loss = 0.47875829\n",
            "Iteration 174, loss = 0.47684861\n",
            "Iteration 175, loss = 0.48294363\n",
            "Iteration 176, loss = 0.47441400\n",
            "Iteration 177, loss = 0.47347085\n",
            "Iteration 178, loss = 0.46839270\n",
            "Iteration 179, loss = 0.47770232\n",
            "Iteration 180, loss = 0.46771817\n",
            "Iteration 181, loss = 0.47469595\n",
            "Iteration 182, loss = 0.47145238\n",
            "Iteration 183, loss = 0.47489489\n",
            "Iteration 184, loss = 0.47092114\n",
            "Iteration 185, loss = 0.47437170\n",
            "Iteration 186, loss = 0.46372490\n",
            "Iteration 187, loss = 0.47103742\n",
            "Iteration 188, loss = 0.47023460\n",
            "Iteration 189, loss = 0.46583321\n",
            "Iteration 190, loss = 0.46518163\n",
            "Iteration 191, loss = 0.46229735\n",
            "Iteration 192, loss = 0.47396580\n",
            "Iteration 193, loss = 0.46180661\n",
            "Iteration 194, loss = 0.46664623\n",
            "Iteration 195, loss = 0.48520683\n",
            "Iteration 196, loss = 0.45955771\n",
            "Iteration 197, loss = 0.47222076\n",
            "Iteration 198, loss = 0.46887541\n",
            "Iteration 199, loss = 0.46215847\n",
            "Iteration 200, loss = 0.45119987\n",
            "Iteration 201, loss = 0.45432992\n",
            "Iteration 202, loss = 0.47174825\n",
            "Iteration 203, loss = 0.45388949\n",
            "Iteration 204, loss = 0.46777297\n",
            "Iteration 205, loss = 0.47141548\n",
            "Iteration 206, loss = 0.46581216\n",
            "Iteration 207, loss = 0.45301284\n",
            "Iteration 208, loss = 0.45440088\n",
            "Iteration 209, loss = 0.45516513\n",
            "Iteration 210, loss = 0.45183212\n",
            "Iteration 211, loss = 0.44281529\n",
            "Iteration 212, loss = 0.46168097\n",
            "Iteration 213, loss = 0.48816820\n",
            "Iteration 214, loss = 0.47018614\n",
            "Iteration 215, loss = 0.45246702\n",
            "Iteration 216, loss = 0.47550465\n",
            "Iteration 217, loss = 0.46027650\n",
            "Iteration 218, loss = 0.45250513\n",
            "Iteration 219, loss = 0.45497845\n",
            "Iteration 220, loss = 0.47182844\n",
            "Iteration 221, loss = 0.45859811\n",
            "Iteration 222, loss = 0.45434753\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67035127\n",
            "Iteration 2, loss = 0.64113243\n",
            "Iteration 3, loss = 0.62797514\n",
            "Iteration 4, loss = 0.62195090\n",
            "Iteration 5, loss = 0.61814627\n",
            "Iteration 6, loss = 0.61593703\n",
            "Iteration 7, loss = 0.61356723\n",
            "Iteration 8, loss = 0.61299484\n",
            "Iteration 9, loss = 0.61087470\n",
            "Iteration 10, loss = 0.61051660\n",
            "Iteration 11, loss = 0.60909260\n",
            "Iteration 12, loss = 0.60821890\n",
            "Iteration 13, loss = 0.60651917\n",
            "Iteration 14, loss = 0.60570451\n",
            "Iteration 15, loss = 0.60524333\n",
            "Iteration 16, loss = 0.60486730\n",
            "Iteration 17, loss = 0.60498125\n",
            "Iteration 18, loss = 0.60311841\n",
            "Iteration 19, loss = 0.60267089\n",
            "Iteration 20, loss = 0.60063117\n",
            "Iteration 21, loss = 0.59960174\n",
            "Iteration 22, loss = 0.59883815\n",
            "Iteration 23, loss = 0.59806397\n",
            "Iteration 24, loss = 0.59804885\n",
            "Iteration 25, loss = 0.59787657\n",
            "Iteration 26, loss = 0.59651053\n",
            "Iteration 27, loss = 0.59602389\n",
            "Iteration 28, loss = 0.59606260\n",
            "Iteration 29, loss = 0.59438827\n",
            "Iteration 30, loss = 0.59456712\n",
            "Iteration 31, loss = 0.59313781\n",
            "Iteration 32, loss = 0.59351665\n",
            "Iteration 33, loss = 0.59390618\n",
            "Iteration 34, loss = 0.59139318\n",
            "Iteration 35, loss = 0.59150021\n",
            "Iteration 36, loss = 0.59126017\n",
            "Iteration 37, loss = 0.59276129\n",
            "Iteration 38, loss = 0.59004052\n",
            "Iteration 39, loss = 0.58903768\n",
            "Iteration 40, loss = 0.58898503\n",
            "Iteration 41, loss = 0.58936628\n",
            "Iteration 42, loss = 0.58698996\n",
            "Iteration 43, loss = 0.58912235\n",
            "Iteration 44, loss = 0.58773158\n",
            "Iteration 45, loss = 0.58608610\n",
            "Iteration 46, loss = 0.58523936\n",
            "Iteration 47, loss = 0.58383170\n",
            "Iteration 48, loss = 0.58473754\n",
            "Iteration 49, loss = 0.58398242\n",
            "Iteration 50, loss = 0.58196963\n",
            "Iteration 51, loss = 0.58169295\n",
            "Iteration 52, loss = 0.58340521\n",
            "Iteration 53, loss = 0.58053780\n",
            "Iteration 54, loss = 0.57979712\n",
            "Iteration 55, loss = 0.57838309\n",
            "Iteration 56, loss = 0.57812768\n",
            "Iteration 57, loss = 0.57794681\n",
            "Iteration 58, loss = 0.57690931\n",
            "Iteration 59, loss = 0.57688903\n",
            "Iteration 60, loss = 0.57511123\n",
            "Iteration 61, loss = 0.57550600\n",
            "Iteration 62, loss = 0.57460671\n",
            "Iteration 63, loss = 0.57503622\n",
            "Iteration 64, loss = 0.57628228\n",
            "Iteration 65, loss = 0.57237327\n",
            "Iteration 66, loss = 0.57365957\n",
            "Iteration 67, loss = 0.57078870\n",
            "Iteration 68, loss = 0.57267022\n",
            "Iteration 69, loss = 0.57170657\n",
            "Iteration 70, loss = 0.57073970\n",
            "Iteration 71, loss = 0.56767078\n",
            "Iteration 72, loss = 0.56822044\n",
            "Iteration 73, loss = 0.56533931\n",
            "Iteration 74, loss = 0.56716432\n",
            "Iteration 75, loss = 0.56522696\n",
            "Iteration 76, loss = 0.56246309\n",
            "Iteration 77, loss = 0.56359048\n",
            "Iteration 78, loss = 0.56478309\n",
            "Iteration 79, loss = 0.56223282\n",
            "Iteration 80, loss = 0.56184171\n",
            "Iteration 81, loss = 0.56397438\n",
            "Iteration 82, loss = 0.55892948\n",
            "Iteration 83, loss = 0.55707171\n",
            "Iteration 84, loss = 0.55750599\n",
            "Iteration 85, loss = 0.55857548\n",
            "Iteration 86, loss = 0.55623136\n",
            "Iteration 87, loss = 0.55677935\n",
            "Iteration 88, loss = 0.55668130\n",
            "Iteration 89, loss = 0.55291435\n",
            "Iteration 90, loss = 0.55340652\n",
            "Iteration 91, loss = 0.55000774\n",
            "Iteration 92, loss = 0.54836333\n",
            "Iteration 93, loss = 0.54688233\n",
            "Iteration 94, loss = 0.55308377\n",
            "Iteration 95, loss = 0.55654991\n",
            "Iteration 96, loss = 0.55002864\n",
            "Iteration 97, loss = 0.54662335\n",
            "Iteration 98, loss = 0.54263979\n",
            "Iteration 99, loss = 0.54256827\n",
            "Iteration 100, loss = 0.54653262\n",
            "Iteration 101, loss = 0.53945897\n",
            "Iteration 102, loss = 0.53957052\n",
            "Iteration 103, loss = 0.53634433\n",
            "Iteration 104, loss = 0.54023870\n",
            "Iteration 105, loss = 0.54594499\n",
            "Iteration 106, loss = 0.53303594\n",
            "Iteration 107, loss = 0.54086144\n",
            "Iteration 108, loss = 0.53867846\n",
            "Iteration 109, loss = 0.53247115\n",
            "Iteration 110, loss = 0.53299797\n",
            "Iteration 111, loss = 0.53322246\n",
            "Iteration 112, loss = 0.53735671\n",
            "Iteration 113, loss = 0.53311199\n",
            "Iteration 114, loss = 0.53266046\n",
            "Iteration 115, loss = 0.52881300\n",
            "Iteration 116, loss = 0.52428038\n",
            "Iteration 117, loss = 0.52548340\n",
            "Iteration 118, loss = 0.52803393\n",
            "Iteration 119, loss = 0.52061951\n",
            "Iteration 120, loss = 0.51784752\n",
            "Iteration 121, loss = 0.51798732\n",
            "Iteration 122, loss = 0.52023417\n",
            "Iteration 123, loss = 0.51779055\n",
            "Iteration 124, loss = 0.51950121\n",
            "Iteration 125, loss = 0.52747709\n",
            "Iteration 126, loss = 0.52687659\n",
            "Iteration 127, loss = 0.51144382\n",
            "Iteration 128, loss = 0.51085658\n",
            "Iteration 129, loss = 0.51258057\n",
            "Iteration 130, loss = 0.50763213\n",
            "Iteration 131, loss = 0.51933824\n",
            "Iteration 132, loss = 0.50908299\n",
            "Iteration 133, loss = 0.50829566\n",
            "Iteration 134, loss = 0.50422717\n",
            "Iteration 135, loss = 0.50558933\n",
            "Iteration 136, loss = 0.50398685\n",
            "Iteration 137, loss = 0.50475023\n",
            "Iteration 138, loss = 0.49380274\n",
            "Iteration 139, loss = 0.49288296\n",
            "Iteration 140, loss = 0.50816898\n",
            "Iteration 141, loss = 0.50914484\n",
            "Iteration 142, loss = 0.50238838\n",
            "Iteration 143, loss = 0.50348271\n",
            "Iteration 144, loss = 0.49547126\n",
            "Iteration 145, loss = 0.49256630\n",
            "Iteration 146, loss = 0.49732537\n",
            "Iteration 147, loss = 0.50410670\n",
            "Iteration 148, loss = 0.49706519\n",
            "Iteration 149, loss = 0.49195423\n",
            "Iteration 150, loss = 0.49793694\n",
            "Iteration 151, loss = 0.48771463\n",
            "Iteration 152, loss = 0.48927032\n",
            "Iteration 153, loss = 0.48689318\n",
            "Iteration 154, loss = 0.48653358\n",
            "Iteration 155, loss = 0.48257834\n",
            "Iteration 156, loss = 0.48252944\n",
            "Iteration 157, loss = 0.49036740\n",
            "Iteration 158, loss = 0.48939879\n",
            "Iteration 159, loss = 0.48445846\n",
            "Iteration 160, loss = 0.49130769\n",
            "Iteration 161, loss = 0.48673408\n",
            "Iteration 162, loss = 0.50609031\n",
            "Iteration 163, loss = 0.48322532\n",
            "Iteration 164, loss = 0.47971920\n",
            "Iteration 165, loss = 0.48202180\n",
            "Iteration 166, loss = 0.49148715\n",
            "Iteration 167, loss = 0.48065352\n",
            "Iteration 168, loss = 0.47458005\n",
            "Iteration 169, loss = 0.47627952\n",
            "Iteration 170, loss = 0.47255574\n",
            "Iteration 171, loss = 0.48909444\n",
            "Iteration 172, loss = 0.47895206\n",
            "Iteration 173, loss = 0.47941760\n",
            "Iteration 174, loss = 0.47490701\n",
            "Iteration 175, loss = 0.47500375\n",
            "Iteration 176, loss = 0.46398052\n",
            "Iteration 177, loss = 0.47289411\n",
            "Iteration 178, loss = 0.47700321\n",
            "Iteration 179, loss = 0.48511780\n",
            "Iteration 180, loss = 0.46645981\n",
            "Iteration 181, loss = 0.47275605\n",
            "Iteration 182, loss = 0.47285766\n",
            "Iteration 183, loss = 0.47385663\n",
            "Iteration 184, loss = 0.46277613\n",
            "Iteration 185, loss = 0.46916849\n",
            "Iteration 186, loss = 0.47111787\n",
            "Iteration 187, loss = 0.47476144\n",
            "Iteration 188, loss = 0.46789482\n",
            "Iteration 189, loss = 0.46797861\n",
            "Iteration 190, loss = 0.46647988\n",
            "Iteration 191, loss = 0.48163170\n",
            "Iteration 192, loss = 0.46443128\n",
            "Iteration 193, loss = 0.46904012\n",
            "Iteration 194, loss = 0.46232360\n",
            "Iteration 195, loss = 0.47563708\n",
            "Iteration 196, loss = 0.46512588\n",
            "Iteration 197, loss = 0.46354319\n",
            "Iteration 198, loss = 0.46155122\n",
            "Iteration 199, loss = 0.45589263\n",
            "Iteration 200, loss = 0.46539786\n",
            "Iteration 201, loss = 0.46296025\n",
            "Iteration 202, loss = 0.46687603\n",
            "Iteration 203, loss = 0.45278609\n",
            "Iteration 204, loss = 0.46542618\n",
            "Iteration 205, loss = 0.45371112\n",
            "Iteration 206, loss = 0.46081456\n",
            "Iteration 207, loss = 0.45241104\n",
            "Iteration 208, loss = 0.45796211\n",
            "Iteration 209, loss = 0.45507595\n",
            "Iteration 210, loss = 0.47294480\n",
            "Iteration 211, loss = 0.46662012\n",
            "Iteration 212, loss = 0.45673623\n",
            "Iteration 213, loss = 0.45746248\n",
            "Iteration 214, loss = 0.47313491\n",
            "Iteration 215, loss = 0.45469831\n",
            "Iteration 216, loss = 0.44883657\n",
            "Iteration 217, loss = 0.45315026\n",
            "Iteration 218, loss = 0.46847047\n",
            "Iteration 219, loss = 0.45369746\n",
            "Iteration 220, loss = 0.46672989\n",
            "Iteration 221, loss = 0.46034470\n",
            "Iteration 222, loss = 0.45213157\n",
            "Iteration 223, loss = 0.45070547\n",
            "Iteration 224, loss = 0.44267268\n",
            "Iteration 225, loss = 0.46236036\n",
            "Iteration 226, loss = 0.45391890\n",
            "Iteration 227, loss = 0.45562483\n",
            "Iteration 228, loss = 0.46781062\n",
            "Iteration 229, loss = 0.45391545\n",
            "Iteration 230, loss = 0.46687591\n",
            "Iteration 231, loss = 0.43853787\n",
            "Iteration 232, loss = 0.46916627\n",
            "Iteration 233, loss = 0.44984821\n",
            "Iteration 234, loss = 0.44210987\n",
            "Iteration 235, loss = 0.44668991\n",
            "Iteration 236, loss = 0.44307943\n",
            "Iteration 237, loss = 0.44623732\n",
            "Iteration 238, loss = 0.44832592\n",
            "Iteration 239, loss = 0.44903956\n",
            "Iteration 240, loss = 0.44793691\n",
            "Iteration 241, loss = 0.44304587\n",
            "Iteration 242, loss = 0.47196560\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62960977\n",
            "Iteration 2, loss = 0.61554116\n",
            "Iteration 3, loss = 0.61115636\n",
            "Iteration 4, loss = 0.61156776\n",
            "Iteration 5, loss = 0.60737824\n",
            "Iteration 6, loss = 0.60682973\n",
            "Iteration 7, loss = 0.60632161\n",
            "Iteration 8, loss = 0.60445089\n",
            "Iteration 9, loss = 0.60286666\n",
            "Iteration 10, loss = 0.60216360\n",
            "Iteration 11, loss = 0.60164291\n",
            "Iteration 12, loss = 0.60001815\n",
            "Iteration 13, loss = 0.59876254\n",
            "Iteration 14, loss = 0.59834706\n",
            "Iteration 15, loss = 0.59743268\n",
            "Iteration 16, loss = 0.59598479\n",
            "Iteration 17, loss = 0.59521786\n",
            "Iteration 18, loss = 0.59500077\n",
            "Iteration 19, loss = 0.59365867\n",
            "Iteration 20, loss = 0.59228272\n",
            "Iteration 21, loss = 0.59205168\n",
            "Iteration 22, loss = 0.59279574\n",
            "Iteration 23, loss = 0.59290987\n",
            "Iteration 24, loss = 0.59144626\n",
            "Iteration 25, loss = 0.58666245\n",
            "Iteration 26, loss = 0.58664826\n",
            "Iteration 27, loss = 0.58533812\n",
            "Iteration 28, loss = 0.58451479\n",
            "Iteration 29, loss = 0.58492659\n",
            "Iteration 30, loss = 0.58463580\n",
            "Iteration 31, loss = 0.58208519\n",
            "Iteration 32, loss = 0.58146674\n",
            "Iteration 33, loss = 0.58438808\n",
            "Iteration 34, loss = 0.58414198\n",
            "Iteration 35, loss = 0.58271888\n",
            "Iteration 36, loss = 0.57820655\n",
            "Iteration 37, loss = 0.58045231\n",
            "Iteration 38, loss = 0.57883578\n",
            "Iteration 39, loss = 0.58521395\n",
            "Iteration 40, loss = 0.57572775\n",
            "Iteration 41, loss = 0.57725544\n",
            "Iteration 42, loss = 0.57645983\n",
            "Iteration 43, loss = 0.57538863\n",
            "Iteration 44, loss = 0.57474903\n",
            "Iteration 45, loss = 0.57323295\n",
            "Iteration 46, loss = 0.57393101\n",
            "Iteration 47, loss = 0.57516348\n",
            "Iteration 48, loss = 0.57092583\n",
            "Iteration 49, loss = 0.57154991\n",
            "Iteration 50, loss = 0.57113430\n",
            "Iteration 51, loss = 0.57038577\n",
            "Iteration 52, loss = 0.56901767\n",
            "Iteration 53, loss = 0.56878716\n",
            "Iteration 54, loss = 0.56941915\n",
            "Iteration 55, loss = 0.57031329\n",
            "Iteration 56, loss = 0.57233765\n",
            "Iteration 57, loss = 0.56540568\n",
            "Iteration 58, loss = 0.56578774\n",
            "Iteration 59, loss = 0.56488328\n",
            "Iteration 60, loss = 0.56423652\n",
            "Iteration 61, loss = 0.56191751\n",
            "Iteration 62, loss = 0.56179674\n",
            "Iteration 63, loss = 0.56207987\n",
            "Iteration 64, loss = 0.56000454\n",
            "Iteration 65, loss = 0.55890465\n",
            "Iteration 66, loss = 0.55776492\n",
            "Iteration 67, loss = 0.55708874\n",
            "Iteration 68, loss = 0.55783842\n",
            "Iteration 69, loss = 0.55943238\n",
            "Iteration 70, loss = 0.55409228\n",
            "Iteration 71, loss = 0.55431321\n",
            "Iteration 72, loss = 0.55526443\n",
            "Iteration 73, loss = 0.55379759\n",
            "Iteration 74, loss = 0.56409408\n",
            "Iteration 75, loss = 0.55426990\n",
            "Iteration 76, loss = 0.55413022\n",
            "Iteration 77, loss = 0.55276306\n",
            "Iteration 78, loss = 0.55385715\n",
            "Iteration 79, loss = 0.54851146\n",
            "Iteration 80, loss = 0.54814501\n",
            "Iteration 81, loss = 0.54669363\n",
            "Iteration 82, loss = 0.54726493\n",
            "Iteration 83, loss = 0.54596104\n",
            "Iteration 84, loss = 0.54482728\n",
            "Iteration 85, loss = 0.54480488\n",
            "Iteration 86, loss = 0.55582234\n",
            "Iteration 87, loss = 0.54662793\n",
            "Iteration 88, loss = 0.54751780\n",
            "Iteration 89, loss = 0.54292589\n",
            "Iteration 90, loss = 0.53973617\n",
            "Iteration 91, loss = 0.54265279\n",
            "Iteration 92, loss = 0.54373024\n",
            "Iteration 93, loss = 0.54154925\n",
            "Iteration 94, loss = 0.53828105\n",
            "Iteration 95, loss = 0.53847175\n",
            "Iteration 96, loss = 0.53726871\n",
            "Iteration 97, loss = 0.53401172\n",
            "Iteration 98, loss = 0.53681718\n",
            "Iteration 99, loss = 0.53698262\n",
            "Iteration 100, loss = 0.53700390\n",
            "Iteration 101, loss = 0.53851061\n",
            "Iteration 102, loss = 0.53054383\n",
            "Iteration 103, loss = 0.53045069\n",
            "Iteration 104, loss = 0.53133042\n",
            "Iteration 105, loss = 0.52838147\n",
            "Iteration 106, loss = 0.53799800\n",
            "Iteration 107, loss = 0.53252507\n",
            "Iteration 108, loss = 0.53543663\n",
            "Iteration 109, loss = 0.53474328\n",
            "Iteration 110, loss = 0.52370077\n",
            "Iteration 111, loss = 0.52620853\n",
            "Iteration 112, loss = 0.52721583\n",
            "Iteration 113, loss = 0.52739024\n",
            "Iteration 114, loss = 0.52670334\n",
            "Iteration 115, loss = 0.52761052\n",
            "Iteration 116, loss = 0.52697828\n",
            "Iteration 117, loss = 0.52147669\n",
            "Iteration 118, loss = 0.51879037\n",
            "Iteration 119, loss = 0.51978125\n",
            "Iteration 120, loss = 0.52610285\n",
            "Iteration 121, loss = 0.53377798\n",
            "Iteration 122, loss = 0.52310500\n",
            "Iteration 123, loss = 0.52137168\n",
            "Iteration 124, loss = 0.52167439\n",
            "Iteration 125, loss = 0.51969166\n",
            "Iteration 126, loss = 0.51524688\n",
            "Iteration 127, loss = 0.51478079\n",
            "Iteration 128, loss = 0.51089691\n",
            "Iteration 129, loss = 0.51309973\n",
            "Iteration 130, loss = 0.51043927\n",
            "Iteration 131, loss = 0.51224949\n",
            "Iteration 132, loss = 0.50700937\n",
            "Iteration 133, loss = 0.51494836\n",
            "Iteration 134, loss = 0.50776682\n",
            "Iteration 135, loss = 0.50842050\n",
            "Iteration 136, loss = 0.50951051\n",
            "Iteration 137, loss = 0.51298883\n",
            "Iteration 138, loss = 0.50283750\n",
            "Iteration 139, loss = 0.50729878\n",
            "Iteration 140, loss = 0.50349518\n",
            "Iteration 141, loss = 0.50334597\n",
            "Iteration 142, loss = 0.50152620\n",
            "Iteration 143, loss = 0.50549405\n",
            "Iteration 144, loss = 0.50110449\n",
            "Iteration 145, loss = 0.50320830\n",
            "Iteration 146, loss = 0.49503150\n",
            "Iteration 147, loss = 0.49765985\n",
            "Iteration 148, loss = 0.50475017\n",
            "Iteration 149, loss = 0.50527939\n",
            "Iteration 150, loss = 0.49477591\n",
            "Iteration 151, loss = 0.50220134\n",
            "Iteration 152, loss = 0.51792671\n",
            "Iteration 153, loss = 0.50718340\n",
            "Iteration 154, loss = 0.49759906\n",
            "Iteration 155, loss = 0.49655198\n",
            "Iteration 156, loss = 0.49489681\n",
            "Iteration 157, loss = 0.49446905\n",
            "Iteration 158, loss = 0.49298952\n",
            "Iteration 159, loss = 0.48499490\n",
            "Iteration 160, loss = 0.48985316\n",
            "Iteration 161, loss = 0.48828211\n",
            "Iteration 162, loss = 0.49156900\n",
            "Iteration 163, loss = 0.50848414\n",
            "Iteration 164, loss = 0.49278094\n",
            "Iteration 165, loss = 0.49532489\n",
            "Iteration 166, loss = 0.49319638\n",
            "Iteration 167, loss = 0.49024388\n",
            "Iteration 168, loss = 0.48736969\n",
            "Iteration 169, loss = 0.47861536\n",
            "Iteration 170, loss = 0.49280860\n",
            "Iteration 171, loss = 0.48325616\n",
            "Iteration 172, loss = 0.50494433\n",
            "Iteration 173, loss = 0.49236830\n",
            "Iteration 174, loss = 0.48114514\n",
            "Iteration 175, loss = 0.48459441\n",
            "Iteration 176, loss = 0.47899117\n",
            "Iteration 177, loss = 0.48032263\n",
            "Iteration 178, loss = 0.50352055\n",
            "Iteration 179, loss = 0.49851305\n",
            "Iteration 180, loss = 0.49588121\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69001972\n",
            "Iteration 2, loss = 0.68749767\n",
            "Iteration 3, loss = 0.68445034\n",
            "Iteration 4, loss = 0.68195328\n",
            "Iteration 5, loss = 0.67995111\n",
            "Iteration 6, loss = 0.67877567\n",
            "Iteration 7, loss = 0.67777862\n",
            "Iteration 8, loss = 0.67706907\n",
            "Iteration 9, loss = 0.67655470\n",
            "Iteration 10, loss = 0.67604389\n",
            "Iteration 11, loss = 0.67570033\n",
            "Iteration 12, loss = 0.67532578\n",
            "Iteration 13, loss = 0.67502607\n",
            "Iteration 14, loss = 0.67481385\n",
            "Iteration 15, loss = 0.67453069\n",
            "Iteration 16, loss = 0.67433727\n",
            "Iteration 17, loss = 0.67409971\n",
            "Iteration 18, loss = 0.67391337\n",
            "Iteration 19, loss = 0.67369952\n",
            "Iteration 20, loss = 0.67346212\n",
            "Iteration 21, loss = 0.67325600\n",
            "Iteration 22, loss = 0.67304963\n",
            "Iteration 23, loss = 0.67288794\n",
            "Iteration 24, loss = 0.67273173\n",
            "Iteration 25, loss = 0.67245577\n",
            "Iteration 26, loss = 0.67232624\n",
            "Iteration 27, loss = 0.67209143\n",
            "Iteration 28, loss = 0.67183660\n",
            "Iteration 29, loss = 0.67170497\n",
            "Iteration 30, loss = 0.67145437\n",
            "Iteration 31, loss = 0.67128774\n",
            "Iteration 32, loss = 0.67114076\n",
            "Iteration 33, loss = 0.67086619\n",
            "Iteration 34, loss = 0.67064077\n",
            "Iteration 35, loss = 0.67042560\n",
            "Iteration 36, loss = 0.67022572\n",
            "Iteration 37, loss = 0.67002136\n",
            "Iteration 38, loss = 0.66988530\n",
            "Iteration 39, loss = 0.66959538\n",
            "Iteration 40, loss = 0.66939929\n",
            "Iteration 41, loss = 0.66918474\n",
            "Iteration 42, loss = 0.66901675\n",
            "Iteration 43, loss = 0.66874519\n",
            "Iteration 44, loss = 0.66858158\n",
            "Iteration 45, loss = 0.66829903\n",
            "Iteration 46, loss = 0.66810786\n",
            "Iteration 47, loss = 0.66797626\n",
            "Iteration 48, loss = 0.66762291\n",
            "Iteration 49, loss = 0.66741588\n",
            "Iteration 50, loss = 0.66722150\n",
            "Iteration 51, loss = 0.66701648\n",
            "Iteration 52, loss = 0.66675703\n",
            "Iteration 53, loss = 0.66656569\n",
            "Iteration 54, loss = 0.66635045\n",
            "Iteration 55, loss = 0.66608269\n",
            "Iteration 56, loss = 0.66586073\n",
            "Iteration 57, loss = 0.66563549\n",
            "Iteration 58, loss = 0.66541354\n",
            "Iteration 59, loss = 0.66520005\n",
            "Iteration 60, loss = 0.66501772\n",
            "Iteration 61, loss = 0.66476338\n",
            "Iteration 62, loss = 0.66453481\n",
            "Iteration 63, loss = 0.66434945\n",
            "Iteration 64, loss = 0.66412776\n",
            "Iteration 65, loss = 0.66389944\n",
            "Iteration 66, loss = 0.66364414\n",
            "Iteration 67, loss = 0.66342906\n",
            "Iteration 68, loss = 0.66319982\n",
            "Iteration 69, loss = 0.66296802\n",
            "Iteration 70, loss = 0.66277315\n",
            "Iteration 71, loss = 0.66251084\n",
            "Iteration 72, loss = 0.66230541\n",
            "Iteration 73, loss = 0.66208407\n",
            "Iteration 74, loss = 0.66199610\n",
            "Iteration 75, loss = 0.66167193\n",
            "Iteration 76, loss = 0.66144013\n",
            "Iteration 77, loss = 0.66116407\n",
            "Iteration 78, loss = 0.66099158\n",
            "Iteration 79, loss = 0.66074715\n",
            "Iteration 80, loss = 0.66056077\n",
            "Iteration 81, loss = 0.66029533\n",
            "Iteration 82, loss = 0.66007862\n",
            "Iteration 83, loss = 0.65986083\n",
            "Iteration 84, loss = 0.65965239\n",
            "Iteration 85, loss = 0.65941990\n",
            "Iteration 86, loss = 0.65917671\n",
            "Iteration 87, loss = 0.65896958\n",
            "Iteration 88, loss = 0.65879296\n",
            "Iteration 89, loss = 0.65854331\n",
            "Iteration 90, loss = 0.65831079\n",
            "Iteration 91, loss = 0.65807222\n",
            "Iteration 92, loss = 0.65785263\n",
            "Iteration 93, loss = 0.65765089\n",
            "Iteration 94, loss = 0.65738435\n",
            "Iteration 95, loss = 0.65720340\n",
            "Iteration 96, loss = 0.65702622\n",
            "Iteration 97, loss = 0.65672317\n",
            "Iteration 98, loss = 0.65648486\n",
            "Iteration 99, loss = 0.65630822\n",
            "Iteration 100, loss = 0.65607543\n",
            "Iteration 101, loss = 0.65583334\n",
            "Iteration 102, loss = 0.65573430\n",
            "Iteration 103, loss = 0.65538338\n",
            "Iteration 104, loss = 0.65517285\n",
            "Iteration 105, loss = 0.65489656\n",
            "Iteration 106, loss = 0.65469315\n",
            "Iteration 107, loss = 0.65452829\n",
            "Iteration 108, loss = 0.65418928\n",
            "Iteration 109, loss = 0.65397962\n",
            "Iteration 110, loss = 0.65378734\n",
            "Iteration 111, loss = 0.65352274\n",
            "Iteration 112, loss = 0.65330541\n",
            "Iteration 113, loss = 0.65308564\n",
            "Iteration 114, loss = 0.65284322\n",
            "Iteration 115, loss = 0.65261310\n",
            "Iteration 116, loss = 0.65236745\n",
            "Iteration 117, loss = 0.65217492\n",
            "Iteration 118, loss = 0.65211761\n",
            "Iteration 119, loss = 0.65174257\n",
            "Iteration 120, loss = 0.65152338\n",
            "Iteration 121, loss = 0.65126080\n",
            "Iteration 122, loss = 0.65105050\n",
            "Iteration 123, loss = 0.65079145\n",
            "Iteration 124, loss = 0.65052689\n",
            "Iteration 125, loss = 0.65033788\n",
            "Iteration 126, loss = 0.65015859\n",
            "Iteration 127, loss = 0.64985100\n",
            "Iteration 128, loss = 0.64963307\n",
            "Iteration 129, loss = 0.64948217\n",
            "Iteration 130, loss = 0.64913567\n",
            "Iteration 131, loss = 0.64890046\n",
            "Iteration 132, loss = 0.64866909\n",
            "Iteration 133, loss = 0.64842518\n",
            "Iteration 134, loss = 0.64822506\n",
            "Iteration 135, loss = 0.64795165\n",
            "Iteration 136, loss = 0.64772501\n",
            "Iteration 137, loss = 0.64749828\n",
            "Iteration 138, loss = 0.64729079\n",
            "Iteration 139, loss = 0.64701443\n",
            "Iteration 140, loss = 0.64679448\n",
            "Iteration 141, loss = 0.64650886\n",
            "Iteration 142, loss = 0.64641238\n",
            "Iteration 143, loss = 0.64605161\n",
            "Iteration 144, loss = 0.64583729\n",
            "Iteration 145, loss = 0.64551758\n",
            "Iteration 146, loss = 0.64526066\n",
            "Iteration 147, loss = 0.64506926\n",
            "Iteration 148, loss = 0.64482173\n",
            "Iteration 149, loss = 0.64458890\n",
            "Iteration 150, loss = 0.64430065\n",
            "Iteration 151, loss = 0.64405902\n",
            "Iteration 152, loss = 0.64382383\n",
            "Iteration 153, loss = 0.64355574\n",
            "Iteration 154, loss = 0.64331109\n",
            "Iteration 155, loss = 0.64304054\n",
            "Iteration 156, loss = 0.64281187\n",
            "Iteration 157, loss = 0.64254510\n",
            "Iteration 158, loss = 0.64230622\n",
            "Iteration 159, loss = 0.64205374\n",
            "Iteration 160, loss = 0.64184089\n",
            "Iteration 161, loss = 0.64162863\n",
            "Iteration 162, loss = 0.64135167\n",
            "Iteration 163, loss = 0.64121271\n",
            "Iteration 164, loss = 0.64082968\n",
            "Iteration 165, loss = 0.64059744\n",
            "Iteration 166, loss = 0.64033242\n",
            "Iteration 167, loss = 0.64009733\n",
            "Iteration 168, loss = 0.63987209\n",
            "Iteration 169, loss = 0.63960661\n",
            "Iteration 170, loss = 0.63937713\n",
            "Iteration 171, loss = 0.63916992\n",
            "Iteration 172, loss = 0.63888180\n",
            "Iteration 173, loss = 0.63866215\n",
            "Iteration 174, loss = 0.63842143\n",
            "Iteration 175, loss = 0.63815136\n",
            "Iteration 176, loss = 0.63788735\n",
            "Iteration 177, loss = 0.63763682\n",
            "Iteration 178, loss = 0.63741430\n",
            "Iteration 179, loss = 0.63718259\n",
            "Iteration 180, loss = 0.63693001\n",
            "Iteration 181, loss = 0.63669196\n",
            "Iteration 182, loss = 0.63641761\n",
            "Iteration 183, loss = 0.63618096\n",
            "Iteration 184, loss = 0.63592091\n",
            "Iteration 185, loss = 0.63569551\n",
            "Iteration 186, loss = 0.63545825\n",
            "Iteration 187, loss = 0.63519863\n",
            "Iteration 188, loss = 0.63498510\n",
            "Iteration 189, loss = 0.63480332\n",
            "Iteration 190, loss = 0.63458504\n",
            "Iteration 191, loss = 0.63428062\n",
            "Iteration 192, loss = 0.63401948\n",
            "Iteration 193, loss = 0.63381688\n",
            "Iteration 194, loss = 0.63361264\n",
            "Iteration 195, loss = 0.63335718\n",
            "Iteration 196, loss = 0.63311819\n",
            "Iteration 197, loss = 0.63289822\n",
            "Iteration 198, loss = 0.63261854\n",
            "Iteration 199, loss = 0.63235703\n",
            "Iteration 200, loss = 0.63216831\n",
            "Iteration 201, loss = 0.63191995\n",
            "Iteration 202, loss = 0.63174055\n",
            "Iteration 203, loss = 0.63146513\n",
            "Iteration 204, loss = 0.63126522\n",
            "Iteration 205, loss = 0.63120390\n",
            "Iteration 206, loss = 0.63082164\n",
            "Iteration 207, loss = 0.63062370\n",
            "Iteration 208, loss = 0.63037221\n",
            "Iteration 209, loss = 0.63016224\n",
            "Iteration 210, loss = 0.62993569\n",
            "Iteration 211, loss = 0.62974822\n",
            "Iteration 212, loss = 0.62950058\n",
            "Iteration 213, loss = 0.62925329\n",
            "Iteration 214, loss = 0.62910896\n",
            "Iteration 215, loss = 0.62884925\n",
            "Iteration 216, loss = 0.62861537\n",
            "Iteration 217, loss = 0.62842401\n",
            "Iteration 218, loss = 0.62831707\n",
            "Iteration 219, loss = 0.62803002\n",
            "Iteration 220, loss = 0.62779229\n",
            "Iteration 221, loss = 0.62757149\n",
            "Iteration 222, loss = 0.62740227\n",
            "Iteration 223, loss = 0.62715683\n",
            "Iteration 224, loss = 0.62694895\n",
            "Iteration 225, loss = 0.62674167\n",
            "Iteration 226, loss = 0.62655745\n",
            "Iteration 227, loss = 0.62636164\n",
            "Iteration 228, loss = 0.62618414\n",
            "Iteration 229, loss = 0.62596162\n",
            "Iteration 230, loss = 0.62578733\n",
            "Iteration 231, loss = 0.62561700\n",
            "Iteration 232, loss = 0.62541262\n",
            "Iteration 233, loss = 0.62520242\n",
            "Iteration 234, loss = 0.62503338\n",
            "Iteration 235, loss = 0.62483566\n",
            "Iteration 236, loss = 0.62462181\n",
            "Iteration 237, loss = 0.62445825\n",
            "Iteration 238, loss = 0.62432561\n",
            "Iteration 239, loss = 0.62404723\n",
            "Iteration 240, loss = 0.62388381\n",
            "Iteration 241, loss = 0.62374805\n",
            "Iteration 242, loss = 0.62354925\n",
            "Iteration 243, loss = 0.62334350\n",
            "Iteration 244, loss = 0.62321167\n",
            "Iteration 245, loss = 0.62300920\n",
            "Iteration 246, loss = 0.62283063\n",
            "Iteration 247, loss = 0.62269810\n",
            "Iteration 248, loss = 0.62251065\n",
            "Iteration 249, loss = 0.62240057\n",
            "Iteration 250, loss = 0.62213795\n",
            "Iteration 251, loss = 0.62197409\n",
            "Iteration 252, loss = 0.62190543\n",
            "Iteration 253, loss = 0.62168509\n",
            "Iteration 254, loss = 0.62143993\n",
            "Iteration 255, loss = 0.62130514\n",
            "Iteration 256, loss = 0.62107649\n",
            "Iteration 257, loss = 0.62098946\n",
            "Iteration 258, loss = 0.62073860\n",
            "Iteration 259, loss = 0.62058352\n",
            "Iteration 260, loss = 0.62044977\n",
            "Iteration 261, loss = 0.62029831\n",
            "Iteration 262, loss = 0.62011536\n",
            "Iteration 263, loss = 0.61993291\n",
            "Iteration 264, loss = 0.61981436\n",
            "Iteration 265, loss = 0.61968297\n",
            "Iteration 266, loss = 0.61955300\n",
            "Iteration 267, loss = 0.61939543\n",
            "Iteration 268, loss = 0.61928001\n",
            "Iteration 269, loss = 0.61919343\n",
            "Iteration 270, loss = 0.61891704\n",
            "Iteration 271, loss = 0.61879717\n",
            "Iteration 272, loss = 0.61866517\n",
            "Iteration 273, loss = 0.61848905\n",
            "Iteration 274, loss = 0.61836701\n",
            "Iteration 275, loss = 0.61826471\n",
            "Iteration 276, loss = 0.61810992\n",
            "Iteration 277, loss = 0.61797051\n",
            "Iteration 278, loss = 0.61792055\n",
            "Iteration 279, loss = 0.61771103\n",
            "Iteration 280, loss = 0.61760400\n",
            "Iteration 281, loss = 0.61746777\n",
            "Iteration 282, loss = 0.61730999\n",
            "Iteration 283, loss = 0.61715722\n",
            "Iteration 284, loss = 0.61707311\n",
            "Iteration 285, loss = 0.61693189\n",
            "Iteration 286, loss = 0.61684806\n",
            "Iteration 287, loss = 0.61666611\n",
            "Iteration 288, loss = 0.61665268\n",
            "Iteration 289, loss = 0.61642978\n",
            "Iteration 290, loss = 0.61634302\n",
            "Iteration 291, loss = 0.61620230\n",
            "Iteration 292, loss = 0.61610996\n",
            "Iteration 293, loss = 0.61594966\n",
            "Iteration 294, loss = 0.61592818\n",
            "Iteration 295, loss = 0.61573775\n",
            "Iteration 296, loss = 0.61561702\n",
            "Iteration 297, loss = 0.61558095\n",
            "Iteration 298, loss = 0.61540762\n",
            "Iteration 299, loss = 0.61528561\n",
            "Iteration 300, loss = 0.61517883\n",
            "Iteration 301, loss = 0.61510090\n",
            "Iteration 302, loss = 0.61495511\n",
            "Iteration 303, loss = 0.61492085\n",
            "Iteration 304, loss = 0.61474536\n",
            "Iteration 305, loss = 0.61464744\n",
            "Iteration 306, loss = 0.61454666\n",
            "Iteration 307, loss = 0.61445152\n",
            "Iteration 308, loss = 0.61438354\n",
            "Iteration 309, loss = 0.61425999\n",
            "Iteration 310, loss = 0.61412222\n",
            "Iteration 311, loss = 0.61402888\n",
            "Iteration 312, loss = 0.61395781\n",
            "Iteration 313, loss = 0.61380531\n",
            "Iteration 314, loss = 0.61379190\n",
            "Iteration 315, loss = 0.61367334\n",
            "Iteration 316, loss = 0.61358141\n",
            "Iteration 317, loss = 0.61349771\n",
            "Iteration 318, loss = 0.61334924\n",
            "Iteration 319, loss = 0.61328572\n",
            "Iteration 320, loss = 0.61315819\n",
            "Iteration 321, loss = 0.61309668\n",
            "Iteration 322, loss = 0.61302017\n",
            "Iteration 323, loss = 0.61291566\n",
            "Iteration 324, loss = 0.61285073\n",
            "Iteration 325, loss = 0.61288941\n",
            "Iteration 326, loss = 0.61268107\n",
            "Iteration 327, loss = 0.61254626\n",
            "Iteration 328, loss = 0.61248710\n",
            "Iteration 329, loss = 0.61236719\n",
            "Iteration 330, loss = 0.61234749\n",
            "Iteration 331, loss = 0.61225051\n",
            "Iteration 332, loss = 0.61211299\n",
            "Iteration 333, loss = 0.61211667\n",
            "Iteration 334, loss = 0.61199338\n",
            "Iteration 335, loss = 0.61190829\n",
            "Iteration 336, loss = 0.61185677\n",
            "Iteration 337, loss = 0.61173633\n",
            "Iteration 338, loss = 0.61165966\n",
            "Iteration 339, loss = 0.61160183\n",
            "Iteration 340, loss = 0.61150059\n",
            "Iteration 341, loss = 0.61143634\n",
            "Iteration 342, loss = 0.61142687\n",
            "Iteration 343, loss = 0.61123976\n",
            "Iteration 344, loss = 0.61121940\n",
            "Iteration 345, loss = 0.61114741\n",
            "Iteration 346, loss = 0.61106224\n",
            "Iteration 347, loss = 0.61099182\n",
            "Iteration 348, loss = 0.61087782\n",
            "Iteration 349, loss = 0.61086791\n",
            "Iteration 350, loss = 0.61072037\n",
            "Iteration 351, loss = 0.61066644\n",
            "Iteration 352, loss = 0.61060253\n",
            "Iteration 353, loss = 0.61051995\n",
            "Iteration 354, loss = 0.61046541\n",
            "Iteration 355, loss = 0.61037404\n",
            "Iteration 356, loss = 0.61030892\n",
            "Iteration 357, loss = 0.61026392\n",
            "Iteration 358, loss = 0.61020086\n",
            "Iteration 359, loss = 0.61014786\n",
            "Iteration 360, loss = 0.61007304\n",
            "Iteration 361, loss = 0.60997257\n",
            "Iteration 362, loss = 0.60989645\n",
            "Iteration 363, loss = 0.60985047\n",
            "Iteration 364, loss = 0.60980442\n",
            "Iteration 365, loss = 0.60969052\n",
            "Iteration 366, loss = 0.60963696\n",
            "Iteration 367, loss = 0.60957115\n",
            "Iteration 368, loss = 0.60953564\n",
            "Iteration 369, loss = 0.60944384\n",
            "Iteration 370, loss = 0.60959274\n",
            "Iteration 371, loss = 0.60944266\n",
            "Iteration 372, loss = 0.60926795\n",
            "Iteration 373, loss = 0.60919852\n",
            "Iteration 374, loss = 0.60919911\n",
            "Iteration 375, loss = 0.60921921\n",
            "Iteration 376, loss = 0.60894976\n",
            "Iteration 377, loss = 0.60895060\n",
            "Iteration 378, loss = 0.60888099\n",
            "Iteration 379, loss = 0.60890918\n",
            "Iteration 380, loss = 0.60876235\n",
            "Iteration 381, loss = 0.60872612\n",
            "Iteration 382, loss = 0.60867831\n",
            "Iteration 383, loss = 0.60864071\n",
            "Iteration 384, loss = 0.60860326\n",
            "Iteration 385, loss = 0.60844694\n",
            "Iteration 386, loss = 0.60843461\n",
            "Iteration 387, loss = 0.60841913\n",
            "Iteration 388, loss = 0.60839386\n",
            "Iteration 389, loss = 0.60819711\n",
            "Iteration 390, loss = 0.60819573\n",
            "Iteration 391, loss = 0.60810538\n",
            "Iteration 392, loss = 0.60804562\n",
            "Iteration 393, loss = 0.60803265\n",
            "Iteration 394, loss = 0.60795028\n",
            "Iteration 395, loss = 0.60786848\n",
            "Iteration 396, loss = 0.60794261\n",
            "Iteration 397, loss = 0.60781108\n",
            "Iteration 398, loss = 0.60775628\n",
            "Iteration 399, loss = 0.60769375\n",
            "Iteration 400, loss = 0.60760300\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69120931\n",
            "Iteration 2, loss = 0.68916479\n",
            "Iteration 3, loss = 0.68685838\n",
            "Iteration 4, loss = 0.68505373\n",
            "Iteration 5, loss = 0.68345219\n",
            "Iteration 6, loss = 0.68233830\n",
            "Iteration 7, loss = 0.68136857\n",
            "Iteration 8, loss = 0.68081822\n",
            "Iteration 9, loss = 0.68021874\n",
            "Iteration 10, loss = 0.67981761\n",
            "Iteration 11, loss = 0.67939782\n",
            "Iteration 12, loss = 0.67906581\n",
            "Iteration 13, loss = 0.67871525\n",
            "Iteration 14, loss = 0.67845009\n",
            "Iteration 15, loss = 0.67817134\n",
            "Iteration 16, loss = 0.67788789\n",
            "Iteration 17, loss = 0.67761732\n",
            "Iteration 18, loss = 0.67737353\n",
            "Iteration 19, loss = 0.67712849\n",
            "Iteration 20, loss = 0.67686673\n",
            "Iteration 21, loss = 0.67660713\n",
            "Iteration 22, loss = 0.67639365\n",
            "Iteration 23, loss = 0.67613348\n",
            "Iteration 24, loss = 0.67584686\n",
            "Iteration 25, loss = 0.67563670\n",
            "Iteration 26, loss = 0.67535234\n",
            "Iteration 27, loss = 0.67518747\n",
            "Iteration 28, loss = 0.67484838\n",
            "Iteration 29, loss = 0.67461954\n",
            "Iteration 30, loss = 0.67440082\n",
            "Iteration 31, loss = 0.67422222\n",
            "Iteration 32, loss = 0.67395121\n",
            "Iteration 33, loss = 0.67371604\n",
            "Iteration 34, loss = 0.67341716\n",
            "Iteration 35, loss = 0.67316654\n",
            "Iteration 36, loss = 0.67295823\n",
            "Iteration 37, loss = 0.67272452\n",
            "Iteration 38, loss = 0.67240221\n",
            "Iteration 39, loss = 0.67213958\n",
            "Iteration 40, loss = 0.67177421\n",
            "Iteration 41, loss = 0.67151536\n",
            "Iteration 42, loss = 0.67097436\n",
            "Iteration 43, loss = 0.67056950\n",
            "Iteration 44, loss = 0.67018184\n",
            "Iteration 45, loss = 0.66999446\n",
            "Iteration 46, loss = 0.66953379\n",
            "Iteration 47, loss = 0.66922093\n",
            "Iteration 48, loss = 0.66898401\n",
            "Iteration 49, loss = 0.66869544\n",
            "Iteration 50, loss = 0.66844150\n",
            "Iteration 51, loss = 0.66814367\n",
            "Iteration 52, loss = 0.66786383\n",
            "Iteration 53, loss = 0.66762737\n",
            "Iteration 54, loss = 0.66733909\n",
            "Iteration 55, loss = 0.66723549\n",
            "Iteration 56, loss = 0.66691415\n",
            "Iteration 57, loss = 0.66662197\n",
            "Iteration 58, loss = 0.66635681\n",
            "Iteration 59, loss = 0.66614723\n",
            "Iteration 60, loss = 0.66585991\n",
            "Iteration 61, loss = 0.66565036\n",
            "Iteration 62, loss = 0.66538750\n",
            "Iteration 63, loss = 0.66512091\n",
            "Iteration 64, loss = 0.66485351\n",
            "Iteration 65, loss = 0.66457074\n",
            "Iteration 66, loss = 0.66434556\n",
            "Iteration 67, loss = 0.66398493\n",
            "Iteration 68, loss = 0.66370006\n",
            "Iteration 69, loss = 0.66342329\n",
            "Iteration 70, loss = 0.66315918\n",
            "Iteration 71, loss = 0.66286948\n",
            "Iteration 72, loss = 0.66260280\n",
            "Iteration 73, loss = 0.66233778\n",
            "Iteration 74, loss = 0.66207360\n",
            "Iteration 75, loss = 0.66182541\n",
            "Iteration 76, loss = 0.66158563\n",
            "Iteration 77, loss = 0.66139439\n",
            "Iteration 78, loss = 0.66099999\n",
            "Iteration 79, loss = 0.66071866\n",
            "Iteration 80, loss = 0.66045888\n",
            "Iteration 81, loss = 0.66020865\n",
            "Iteration 82, loss = 0.65990609\n",
            "Iteration 83, loss = 0.65972708\n",
            "Iteration 84, loss = 0.65931393\n",
            "Iteration 85, loss = 0.65904602\n",
            "Iteration 86, loss = 0.65884329\n",
            "Iteration 87, loss = 0.65858116\n",
            "Iteration 88, loss = 0.65819244\n",
            "Iteration 89, loss = 0.65795600\n",
            "Iteration 90, loss = 0.65767005\n",
            "Iteration 91, loss = 0.65739064\n",
            "Iteration 92, loss = 0.65719553\n",
            "Iteration 93, loss = 0.65694163\n",
            "Iteration 94, loss = 0.65662752\n",
            "Iteration 95, loss = 0.65648986\n",
            "Iteration 96, loss = 0.65611985\n",
            "Iteration 97, loss = 0.65600966\n",
            "Iteration 98, loss = 0.65561935\n",
            "Iteration 99, loss = 0.65532499\n",
            "Iteration 100, loss = 0.65511192\n",
            "Iteration 101, loss = 0.65489935\n",
            "Iteration 102, loss = 0.65463054\n",
            "Iteration 103, loss = 0.65448220\n",
            "Iteration 104, loss = 0.65415864\n",
            "Iteration 105, loss = 0.65388636\n",
            "Iteration 106, loss = 0.65363243\n",
            "Iteration 107, loss = 0.65339113\n",
            "Iteration 108, loss = 0.65309425\n",
            "Iteration 109, loss = 0.65287767\n",
            "Iteration 110, loss = 0.65258315\n",
            "Iteration 111, loss = 0.65240307\n",
            "Iteration 112, loss = 0.65205721\n",
            "Iteration 113, loss = 0.65190508\n",
            "Iteration 114, loss = 0.65158269\n",
            "Iteration 115, loss = 0.65131037\n",
            "Iteration 116, loss = 0.65110898\n",
            "Iteration 117, loss = 0.65084235\n",
            "Iteration 118, loss = 0.65053926\n",
            "Iteration 119, loss = 0.65027986\n",
            "Iteration 120, loss = 0.65002618\n",
            "Iteration 121, loss = 0.64981195\n",
            "Iteration 122, loss = 0.64949999\n",
            "Iteration 123, loss = 0.64926808\n",
            "Iteration 124, loss = 0.64902936\n",
            "Iteration 125, loss = 0.64871826\n",
            "Iteration 126, loss = 0.64846802\n",
            "Iteration 127, loss = 0.64817535\n",
            "Iteration 128, loss = 0.64791162\n",
            "Iteration 129, loss = 0.64768392\n",
            "Iteration 130, loss = 0.64737939\n",
            "Iteration 131, loss = 0.64717054\n",
            "Iteration 132, loss = 0.64690320\n",
            "Iteration 133, loss = 0.64664943\n",
            "Iteration 134, loss = 0.64645230\n",
            "Iteration 135, loss = 0.64611939\n",
            "Iteration 136, loss = 0.64581532\n",
            "Iteration 137, loss = 0.64553951\n",
            "Iteration 138, loss = 0.64531430\n",
            "Iteration 139, loss = 0.64505996\n",
            "Iteration 140, loss = 0.64477712\n",
            "Iteration 141, loss = 0.64451254\n",
            "Iteration 142, loss = 0.64425750\n",
            "Iteration 143, loss = 0.64400243\n",
            "Iteration 144, loss = 0.64375158\n",
            "Iteration 145, loss = 0.64347330\n",
            "Iteration 146, loss = 0.64319395\n",
            "Iteration 147, loss = 0.64296837\n",
            "Iteration 148, loss = 0.64266600\n",
            "Iteration 149, loss = 0.64247796\n",
            "Iteration 150, loss = 0.64216605\n",
            "Iteration 151, loss = 0.64193734\n",
            "Iteration 152, loss = 0.64171487\n",
            "Iteration 153, loss = 0.64137005\n",
            "Iteration 154, loss = 0.64118296\n",
            "Iteration 155, loss = 0.64095537\n",
            "Iteration 156, loss = 0.64071026\n",
            "Iteration 157, loss = 0.64036675\n",
            "Iteration 158, loss = 0.64019828\n",
            "Iteration 159, loss = 0.63987365\n",
            "Iteration 160, loss = 0.63960582\n",
            "Iteration 161, loss = 0.63932320\n",
            "Iteration 162, loss = 0.63915910\n",
            "Iteration 163, loss = 0.63884392\n",
            "Iteration 164, loss = 0.63857719\n",
            "Iteration 165, loss = 0.63830553\n",
            "Iteration 166, loss = 0.63808703\n",
            "Iteration 167, loss = 0.63778366\n",
            "Iteration 168, loss = 0.63755277\n",
            "Iteration 169, loss = 0.63731689\n",
            "Iteration 170, loss = 0.63707493\n",
            "Iteration 171, loss = 0.63675345\n",
            "Iteration 172, loss = 0.63653229\n",
            "Iteration 173, loss = 0.63626335\n",
            "Iteration 174, loss = 0.63604163\n",
            "Iteration 175, loss = 0.63576222\n",
            "Iteration 176, loss = 0.63546739\n",
            "Iteration 177, loss = 0.63523174\n",
            "Iteration 178, loss = 0.63498754\n",
            "Iteration 179, loss = 0.63473079\n",
            "Iteration 180, loss = 0.63450214\n",
            "Iteration 181, loss = 0.63421476\n",
            "Iteration 182, loss = 0.63401892\n",
            "Iteration 183, loss = 0.63377645\n",
            "Iteration 184, loss = 0.63352271\n",
            "Iteration 185, loss = 0.63325182\n",
            "Iteration 186, loss = 0.63309386\n",
            "Iteration 187, loss = 0.63277497\n",
            "Iteration 188, loss = 0.63249687\n",
            "Iteration 189, loss = 0.63222979\n",
            "Iteration 190, loss = 0.63211824\n",
            "Iteration 191, loss = 0.63175822\n",
            "Iteration 192, loss = 0.63157397\n",
            "Iteration 193, loss = 0.63127950\n",
            "Iteration 194, loss = 0.63113220\n",
            "Iteration 195, loss = 0.63084283\n",
            "Iteration 196, loss = 0.63061698\n",
            "Iteration 197, loss = 0.63035568\n",
            "Iteration 198, loss = 0.63008687\n",
            "Iteration 199, loss = 0.62984795\n",
            "Iteration 200, loss = 0.62961763\n",
            "Iteration 201, loss = 0.62935477\n",
            "Iteration 202, loss = 0.62907728\n",
            "Iteration 203, loss = 0.62887090\n",
            "Iteration 204, loss = 0.62863853\n",
            "Iteration 205, loss = 0.62850359\n",
            "Iteration 206, loss = 0.62812028\n",
            "Iteration 207, loss = 0.62792084\n",
            "Iteration 208, loss = 0.62770883\n",
            "Iteration 209, loss = 0.62737455\n",
            "Iteration 210, loss = 0.62710521\n",
            "Iteration 211, loss = 0.62685961\n",
            "Iteration 212, loss = 0.62662531\n",
            "Iteration 213, loss = 0.62634435\n",
            "Iteration 214, loss = 0.62622046\n",
            "Iteration 215, loss = 0.62601206\n",
            "Iteration 216, loss = 0.62568233\n",
            "Iteration 217, loss = 0.62546413\n",
            "Iteration 218, loss = 0.62523896\n",
            "Iteration 219, loss = 0.62494044\n",
            "Iteration 220, loss = 0.62467334\n",
            "Iteration 221, loss = 0.62464524\n",
            "Iteration 222, loss = 0.62424527\n",
            "Iteration 223, loss = 0.62401472\n",
            "Iteration 224, loss = 0.62381765\n",
            "Iteration 225, loss = 0.62351900\n",
            "Iteration 226, loss = 0.62330662\n",
            "Iteration 227, loss = 0.62309112\n",
            "Iteration 228, loss = 0.62285365\n",
            "Iteration 229, loss = 0.62266270\n",
            "Iteration 230, loss = 0.62236928\n",
            "Iteration 231, loss = 0.62215441\n",
            "Iteration 232, loss = 0.62193167\n",
            "Iteration 233, loss = 0.62174830\n",
            "Iteration 234, loss = 0.62151834\n",
            "Iteration 235, loss = 0.62127017\n",
            "Iteration 236, loss = 0.62109857\n",
            "Iteration 237, loss = 0.62088990\n",
            "Iteration 238, loss = 0.62068064\n",
            "Iteration 239, loss = 0.62046353\n",
            "Iteration 240, loss = 0.62023857\n",
            "Iteration 241, loss = 0.62013549\n",
            "Iteration 242, loss = 0.61980297\n",
            "Iteration 243, loss = 0.61963185\n",
            "Iteration 244, loss = 0.61944378\n",
            "Iteration 245, loss = 0.61925582\n",
            "Iteration 246, loss = 0.61905547\n",
            "Iteration 247, loss = 0.61884545\n",
            "Iteration 248, loss = 0.61871982\n",
            "Iteration 249, loss = 0.61851851\n",
            "Iteration 250, loss = 0.61837098\n",
            "Iteration 251, loss = 0.61817233\n",
            "Iteration 252, loss = 0.61809679\n",
            "Iteration 253, loss = 0.61781415\n",
            "Iteration 254, loss = 0.61761618\n",
            "Iteration 255, loss = 0.61748391\n",
            "Iteration 256, loss = 0.61730698\n",
            "Iteration 257, loss = 0.61714306\n",
            "Iteration 258, loss = 0.61702336\n",
            "Iteration 259, loss = 0.61678778\n",
            "Iteration 260, loss = 0.61665195\n",
            "Iteration 261, loss = 0.61650135\n",
            "Iteration 262, loss = 0.61633367\n",
            "Iteration 263, loss = 0.61616246\n",
            "Iteration 264, loss = 0.61598046\n",
            "Iteration 265, loss = 0.61586337\n",
            "Iteration 266, loss = 0.61573384\n",
            "Iteration 267, loss = 0.61553710\n",
            "Iteration 268, loss = 0.61545151\n",
            "Iteration 269, loss = 0.61527871\n",
            "Iteration 270, loss = 0.61510413\n",
            "Iteration 271, loss = 0.61491734\n",
            "Iteration 272, loss = 0.61480897\n",
            "Iteration 273, loss = 0.61467058\n",
            "Iteration 274, loss = 0.61462030\n",
            "Iteration 275, loss = 0.61443788\n",
            "Iteration 276, loss = 0.61432654\n",
            "Iteration 277, loss = 0.61415411\n",
            "Iteration 278, loss = 0.61397279\n",
            "Iteration 279, loss = 0.61387214\n",
            "Iteration 280, loss = 0.61372563\n",
            "Iteration 281, loss = 0.61364934\n",
            "Iteration 282, loss = 0.61342923\n",
            "Iteration 283, loss = 0.61339378\n",
            "Iteration 284, loss = 0.61320244\n",
            "Iteration 285, loss = 0.61307480\n",
            "Iteration 286, loss = 0.61306038\n",
            "Iteration 287, loss = 0.61279816\n",
            "Iteration 288, loss = 0.61270113\n",
            "Iteration 289, loss = 0.61263988\n",
            "Iteration 290, loss = 0.61250092\n",
            "Iteration 291, loss = 0.61242834\n",
            "Iteration 292, loss = 0.61228717\n",
            "Iteration 293, loss = 0.61209463\n",
            "Iteration 294, loss = 0.61207806\n",
            "Iteration 295, loss = 0.61193163\n",
            "Iteration 296, loss = 0.61181326\n",
            "Iteration 297, loss = 0.61162834\n",
            "Iteration 298, loss = 0.61155835\n",
            "Iteration 299, loss = 0.61141141\n",
            "Iteration 300, loss = 0.61129032\n",
            "Iteration 301, loss = 0.61124262\n",
            "Iteration 302, loss = 0.61107858\n",
            "Iteration 303, loss = 0.61095667\n",
            "Iteration 304, loss = 0.61087366\n",
            "Iteration 305, loss = 0.61076605\n",
            "Iteration 306, loss = 0.61063001\n",
            "Iteration 307, loss = 0.61058907\n",
            "Iteration 308, loss = 0.61042699\n",
            "Iteration 309, loss = 0.61034762\n",
            "Iteration 310, loss = 0.61028568\n",
            "Iteration 311, loss = 0.61012735\n",
            "Iteration 312, loss = 0.61007747\n",
            "Iteration 313, loss = 0.60994078\n",
            "Iteration 314, loss = 0.60999281\n",
            "Iteration 315, loss = 0.60973731\n",
            "Iteration 316, loss = 0.60963427\n",
            "Iteration 317, loss = 0.60957935\n",
            "Iteration 318, loss = 0.60942235\n",
            "Iteration 319, loss = 0.60935698\n",
            "Iteration 320, loss = 0.60925057\n",
            "Iteration 321, loss = 0.60914254\n",
            "Iteration 322, loss = 0.60920700\n",
            "Iteration 323, loss = 0.60899255\n",
            "Iteration 324, loss = 0.60893169\n",
            "Iteration 325, loss = 0.60882999\n",
            "Iteration 326, loss = 0.60877745\n",
            "Iteration 327, loss = 0.60865744\n",
            "Iteration 328, loss = 0.60854329\n",
            "Iteration 329, loss = 0.60850599\n",
            "Iteration 330, loss = 0.60836543\n",
            "Iteration 331, loss = 0.60836885\n",
            "Iteration 332, loss = 0.60819691\n",
            "Iteration 333, loss = 0.60805198\n",
            "Iteration 334, loss = 0.60798374\n",
            "Iteration 335, loss = 0.60788482\n",
            "Iteration 336, loss = 0.60784720\n",
            "Iteration 337, loss = 0.60777705\n",
            "Iteration 338, loss = 0.60770561\n",
            "Iteration 339, loss = 0.60757955\n",
            "Iteration 340, loss = 0.60752839\n",
            "Iteration 341, loss = 0.60743553\n",
            "Iteration 342, loss = 0.60733621\n",
            "Iteration 343, loss = 0.60728347\n",
            "Iteration 344, loss = 0.60719191\n",
            "Iteration 345, loss = 0.60713606\n",
            "Iteration 346, loss = 0.60705574\n",
            "Iteration 347, loss = 0.60698380\n",
            "Iteration 348, loss = 0.60691111\n",
            "Iteration 349, loss = 0.60679406\n",
            "Iteration 350, loss = 0.60679805\n",
            "Iteration 351, loss = 0.60673589\n",
            "Iteration 352, loss = 0.60661913\n",
            "Iteration 353, loss = 0.60649258\n",
            "Iteration 354, loss = 0.60651195\n",
            "Iteration 355, loss = 0.60638942\n",
            "Iteration 356, loss = 0.60636880\n",
            "Iteration 357, loss = 0.60624969\n",
            "Iteration 358, loss = 0.60617455\n",
            "Iteration 359, loss = 0.60617485\n",
            "Iteration 360, loss = 0.60607600\n",
            "Iteration 361, loss = 0.60596918\n",
            "Iteration 362, loss = 0.60591231\n",
            "Iteration 363, loss = 0.60586502\n",
            "Iteration 364, loss = 0.60579955\n",
            "Iteration 365, loss = 0.60566598\n",
            "Iteration 366, loss = 0.60567397\n",
            "Iteration 367, loss = 0.60559181\n",
            "Iteration 368, loss = 0.60554471\n",
            "Iteration 369, loss = 0.60541478\n",
            "Iteration 370, loss = 0.60534760\n",
            "Iteration 371, loss = 0.60529652\n",
            "Iteration 372, loss = 0.60523968\n",
            "Iteration 373, loss = 0.60520320\n",
            "Iteration 374, loss = 0.60510600\n",
            "Iteration 375, loss = 0.60510051\n",
            "Iteration 376, loss = 0.60504413\n",
            "Iteration 377, loss = 0.60493876\n",
            "Iteration 378, loss = 0.60485708\n",
            "Iteration 379, loss = 0.60479149\n",
            "Iteration 380, loss = 0.60471984\n",
            "Iteration 381, loss = 0.60467542\n",
            "Iteration 382, loss = 0.60460288\n",
            "Iteration 383, loss = 0.60456718\n",
            "Iteration 384, loss = 0.60452456\n",
            "Iteration 385, loss = 0.60442501\n",
            "Iteration 386, loss = 0.60438256\n",
            "Iteration 387, loss = 0.60431607\n",
            "Iteration 388, loss = 0.60427702\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71266708\n",
            "Iteration 2, loss = 0.70451785\n",
            "Iteration 3, loss = 0.69563830\n",
            "Iteration 4, loss = 0.68832145\n",
            "Iteration 5, loss = 0.68292763\n",
            "Iteration 6, loss = 0.67922066\n",
            "Iteration 7, loss = 0.67664424\n",
            "Iteration 8, loss = 0.67483648\n",
            "Iteration 9, loss = 0.67341013\n",
            "Iteration 10, loss = 0.67267849\n",
            "Iteration 11, loss = 0.67182655\n",
            "Iteration 12, loss = 0.67134600\n",
            "Iteration 13, loss = 0.67082354\n",
            "Iteration 14, loss = 0.67070582\n",
            "Iteration 15, loss = 0.67017393\n",
            "Iteration 16, loss = 0.66991191\n",
            "Iteration 17, loss = 0.66973224\n",
            "Iteration 18, loss = 0.66966002\n",
            "Iteration 19, loss = 0.66920496\n",
            "Iteration 20, loss = 0.66904108\n",
            "Iteration 21, loss = 0.66883936\n",
            "Iteration 22, loss = 0.66864818\n",
            "Iteration 23, loss = 0.66841118\n",
            "Iteration 24, loss = 0.66819255\n",
            "Iteration 25, loss = 0.66797418\n",
            "Iteration 26, loss = 0.66779588\n",
            "Iteration 27, loss = 0.66759320\n",
            "Iteration 28, loss = 0.66733719\n",
            "Iteration 29, loss = 0.66714935\n",
            "Iteration 30, loss = 0.66690440\n",
            "Iteration 31, loss = 0.66669584\n",
            "Iteration 32, loss = 0.66650123\n",
            "Iteration 33, loss = 0.66628248\n",
            "Iteration 34, loss = 0.66606313\n",
            "Iteration 35, loss = 0.66588073\n",
            "Iteration 36, loss = 0.66566941\n",
            "Iteration 37, loss = 0.66540099\n",
            "Iteration 38, loss = 0.66523769\n",
            "Iteration 39, loss = 0.66497973\n",
            "Iteration 40, loss = 0.66472760\n",
            "Iteration 41, loss = 0.66450692\n",
            "Iteration 42, loss = 0.66430247\n",
            "Iteration 43, loss = 0.66407724\n",
            "Iteration 44, loss = 0.66386261\n",
            "Iteration 45, loss = 0.66359483\n",
            "Iteration 46, loss = 0.66337274\n",
            "Iteration 47, loss = 0.66312667\n",
            "Iteration 48, loss = 0.66288374\n",
            "Iteration 49, loss = 0.66268544\n",
            "Iteration 50, loss = 0.66240792\n",
            "Iteration 51, loss = 0.66216724\n",
            "Iteration 52, loss = 0.66192712\n",
            "Iteration 53, loss = 0.66167962\n",
            "Iteration 54, loss = 0.66146751\n",
            "Iteration 55, loss = 0.66116930\n",
            "Iteration 56, loss = 0.66097426\n",
            "Iteration 57, loss = 0.66072538\n",
            "Iteration 58, loss = 0.66042849\n",
            "Iteration 59, loss = 0.66016383\n",
            "Iteration 60, loss = 0.65997279\n",
            "Iteration 61, loss = 0.65973211\n",
            "Iteration 62, loss = 0.65937677\n",
            "Iteration 63, loss = 0.65911386\n",
            "Iteration 64, loss = 0.65888744\n",
            "Iteration 65, loss = 0.65863997\n",
            "Iteration 66, loss = 0.65839613\n",
            "Iteration 67, loss = 0.65810193\n",
            "Iteration 68, loss = 0.65781451\n",
            "Iteration 69, loss = 0.65757252\n",
            "Iteration 70, loss = 0.65731537\n",
            "Iteration 71, loss = 0.65705212\n",
            "Iteration 72, loss = 0.65676739\n",
            "Iteration 73, loss = 0.65657538\n",
            "Iteration 74, loss = 0.65628655\n",
            "Iteration 75, loss = 0.65603886\n",
            "Iteration 76, loss = 0.65571616\n",
            "Iteration 77, loss = 0.65543121\n",
            "Iteration 78, loss = 0.65512830\n",
            "Iteration 79, loss = 0.65487039\n",
            "Iteration 80, loss = 0.65475657\n",
            "Iteration 81, loss = 0.65436194\n",
            "Iteration 82, loss = 0.65413260\n",
            "Iteration 83, loss = 0.65379883\n",
            "Iteration 84, loss = 0.65362658\n",
            "Iteration 85, loss = 0.65327397\n",
            "Iteration 86, loss = 0.65299077\n",
            "Iteration 87, loss = 0.65271552\n",
            "Iteration 88, loss = 0.65245620\n",
            "Iteration 89, loss = 0.65218140\n",
            "Iteration 90, loss = 0.65194461\n",
            "Iteration 91, loss = 0.65182701\n",
            "Iteration 92, loss = 0.65138310\n",
            "Iteration 93, loss = 0.65110610\n",
            "Iteration 94, loss = 0.65082413\n",
            "Iteration 95, loss = 0.65055360\n",
            "Iteration 96, loss = 0.65030610\n",
            "Iteration 97, loss = 0.65008123\n",
            "Iteration 98, loss = 0.64984471\n",
            "Iteration 99, loss = 0.64950625\n",
            "Iteration 100, loss = 0.64920431\n",
            "Iteration 101, loss = 0.64895386\n",
            "Iteration 102, loss = 0.64865622\n",
            "Iteration 103, loss = 0.64843800\n",
            "Iteration 104, loss = 0.64807906\n",
            "Iteration 105, loss = 0.64778533\n",
            "Iteration 106, loss = 0.64751940\n",
            "Iteration 107, loss = 0.64723757\n",
            "Iteration 108, loss = 0.64697001\n",
            "Iteration 109, loss = 0.64668409\n",
            "Iteration 110, loss = 0.64644628\n",
            "Iteration 111, loss = 0.64618828\n",
            "Iteration 112, loss = 0.64589859\n",
            "Iteration 113, loss = 0.64565967\n",
            "Iteration 114, loss = 0.64536650\n",
            "Iteration 115, loss = 0.64507316\n",
            "Iteration 116, loss = 0.64487158\n",
            "Iteration 117, loss = 0.64452936\n",
            "Iteration 118, loss = 0.64425167\n",
            "Iteration 119, loss = 0.64401514\n",
            "Iteration 120, loss = 0.64372410\n",
            "Iteration 121, loss = 0.64358071\n",
            "Iteration 122, loss = 0.64322876\n",
            "Iteration 123, loss = 0.64292050\n",
            "Iteration 124, loss = 0.64268865\n",
            "Iteration 125, loss = 0.64238053\n",
            "Iteration 126, loss = 0.64209472\n",
            "Iteration 127, loss = 0.64185196\n",
            "Iteration 128, loss = 0.64157149\n",
            "Iteration 129, loss = 0.64129173\n",
            "Iteration 130, loss = 0.64107338\n",
            "Iteration 131, loss = 0.64078753\n",
            "Iteration 132, loss = 0.64051957\n",
            "Iteration 133, loss = 0.64024489\n",
            "Iteration 134, loss = 0.63998132\n",
            "Iteration 135, loss = 0.63970847\n",
            "Iteration 136, loss = 0.63937311\n",
            "Iteration 137, loss = 0.63909144\n",
            "Iteration 138, loss = 0.63881009\n",
            "Iteration 139, loss = 0.63854178\n",
            "Iteration 140, loss = 0.63827822\n",
            "Iteration 141, loss = 0.63804189\n",
            "Iteration 142, loss = 0.63780234\n",
            "Iteration 143, loss = 0.63743486\n",
            "Iteration 144, loss = 0.63722927\n",
            "Iteration 145, loss = 0.63692438\n",
            "Iteration 146, loss = 0.63665619\n",
            "Iteration 147, loss = 0.63637202\n",
            "Iteration 148, loss = 0.63619121\n",
            "Iteration 149, loss = 0.63584124\n",
            "Iteration 150, loss = 0.63561201\n",
            "Iteration 151, loss = 0.63529755\n",
            "Iteration 152, loss = 0.63503144\n",
            "Iteration 153, loss = 0.63475292\n",
            "Iteration 154, loss = 0.63447124\n",
            "Iteration 155, loss = 0.63420571\n",
            "Iteration 156, loss = 0.63394599\n",
            "Iteration 157, loss = 0.63363859\n",
            "Iteration 158, loss = 0.63343126\n",
            "Iteration 159, loss = 0.63316497\n",
            "Iteration 160, loss = 0.63282418\n",
            "Iteration 161, loss = 0.63255121\n",
            "Iteration 162, loss = 0.63230275\n",
            "Iteration 163, loss = 0.63203075\n",
            "Iteration 164, loss = 0.63181553\n",
            "Iteration 165, loss = 0.63149775\n",
            "Iteration 166, loss = 0.63121792\n",
            "Iteration 167, loss = 0.63096773\n",
            "Iteration 168, loss = 0.63070290\n",
            "Iteration 169, loss = 0.63045982\n",
            "Iteration 170, loss = 0.63017465\n",
            "Iteration 171, loss = 0.62991683\n",
            "Iteration 172, loss = 0.62959186\n",
            "Iteration 173, loss = 0.62935754\n",
            "Iteration 174, loss = 0.62913194\n",
            "Iteration 175, loss = 0.62881033\n",
            "Iteration 176, loss = 0.62854923\n",
            "Iteration 177, loss = 0.62828613\n",
            "Iteration 178, loss = 0.62801716\n",
            "Iteration 179, loss = 0.62782674\n",
            "Iteration 180, loss = 0.62755990\n",
            "Iteration 181, loss = 0.62726744\n",
            "Iteration 182, loss = 0.62705278\n",
            "Iteration 183, loss = 0.62680210\n",
            "Iteration 184, loss = 0.62665088\n",
            "Iteration 185, loss = 0.62633967\n",
            "Iteration 186, loss = 0.62607601\n",
            "Iteration 187, loss = 0.62583454\n",
            "Iteration 188, loss = 0.62561544\n",
            "Iteration 189, loss = 0.62537700\n",
            "Iteration 190, loss = 0.62519493\n",
            "Iteration 191, loss = 0.62492952\n",
            "Iteration 192, loss = 0.62472588\n",
            "Iteration 193, loss = 0.62447773\n",
            "Iteration 194, loss = 0.62426439\n",
            "Iteration 195, loss = 0.62409821\n",
            "Iteration 196, loss = 0.62391978\n",
            "Iteration 197, loss = 0.62365143\n",
            "Iteration 198, loss = 0.62342987\n",
            "Iteration 199, loss = 0.62326030\n",
            "Iteration 200, loss = 0.62299758\n",
            "Iteration 201, loss = 0.62280721\n",
            "Iteration 202, loss = 0.62257196\n",
            "Iteration 203, loss = 0.62238643\n",
            "Iteration 204, loss = 0.62221841\n",
            "Iteration 205, loss = 0.62196748\n",
            "Iteration 206, loss = 0.62181263\n",
            "Iteration 207, loss = 0.62163480\n",
            "Iteration 208, loss = 0.62140298\n",
            "Iteration 209, loss = 0.62125652\n",
            "Iteration 210, loss = 0.62106227\n",
            "Iteration 211, loss = 0.62085156\n",
            "Iteration 212, loss = 0.62069181\n",
            "Iteration 213, loss = 0.62057170\n",
            "Iteration 214, loss = 0.62052723\n",
            "Iteration 215, loss = 0.62015433\n",
            "Iteration 216, loss = 0.61994909\n",
            "Iteration 217, loss = 0.61985563\n",
            "Iteration 218, loss = 0.61959511\n",
            "Iteration 219, loss = 0.61943589\n",
            "Iteration 220, loss = 0.61931645\n",
            "Iteration 221, loss = 0.61910295\n",
            "Iteration 222, loss = 0.61893203\n",
            "Iteration 223, loss = 0.61880855\n",
            "Iteration 224, loss = 0.61861491\n",
            "Iteration 225, loss = 0.61844020\n",
            "Iteration 226, loss = 0.61828007\n",
            "Iteration 227, loss = 0.61814409\n",
            "Iteration 228, loss = 0.61797014\n",
            "Iteration 229, loss = 0.61778807\n",
            "Iteration 230, loss = 0.61764980\n",
            "Iteration 231, loss = 0.61756966\n",
            "Iteration 232, loss = 0.61738784\n",
            "Iteration 233, loss = 0.61720065\n",
            "Iteration 234, loss = 0.61703576\n",
            "Iteration 235, loss = 0.61694284\n",
            "Iteration 236, loss = 0.61680206\n",
            "Iteration 237, loss = 0.61662390\n",
            "Iteration 238, loss = 0.61644796\n",
            "Iteration 239, loss = 0.61631017\n",
            "Iteration 240, loss = 0.61617589\n",
            "Iteration 241, loss = 0.61604136\n",
            "Iteration 242, loss = 0.61594926\n",
            "Iteration 243, loss = 0.61577751\n",
            "Iteration 244, loss = 0.61565339\n",
            "Iteration 245, loss = 0.61553411\n",
            "Iteration 246, loss = 0.61541976\n",
            "Iteration 247, loss = 0.61525778\n",
            "Iteration 248, loss = 0.61521778\n",
            "Iteration 249, loss = 0.61507843\n",
            "Iteration 250, loss = 0.61490362\n",
            "Iteration 251, loss = 0.61477603\n",
            "Iteration 252, loss = 0.61467151\n",
            "Iteration 253, loss = 0.61454946\n",
            "Iteration 254, loss = 0.61441798\n",
            "Iteration 255, loss = 0.61452477\n",
            "Iteration 256, loss = 0.61422361\n",
            "Iteration 257, loss = 0.61408217\n",
            "Iteration 258, loss = 0.61396072\n",
            "Iteration 259, loss = 0.61385657\n",
            "Iteration 260, loss = 0.61374652\n",
            "Iteration 261, loss = 0.61364295\n",
            "Iteration 262, loss = 0.61363538\n",
            "Iteration 263, loss = 0.61350041\n",
            "Iteration 264, loss = 0.61337834\n",
            "Iteration 265, loss = 0.61318334\n",
            "Iteration 266, loss = 0.61302647\n",
            "Iteration 267, loss = 0.61297929\n",
            "Iteration 268, loss = 0.61282011\n",
            "Iteration 269, loss = 0.61272220\n",
            "Iteration 270, loss = 0.61267620\n",
            "Iteration 271, loss = 0.61259847\n",
            "Iteration 272, loss = 0.61243892\n",
            "Iteration 273, loss = 0.61235225\n",
            "Iteration 274, loss = 0.61232304\n",
            "Iteration 275, loss = 0.61215535\n",
            "Iteration 276, loss = 0.61208340\n",
            "Iteration 277, loss = 0.61196885\n",
            "Iteration 278, loss = 0.61186286\n",
            "Iteration 279, loss = 0.61177088\n",
            "Iteration 280, loss = 0.61170865\n",
            "Iteration 281, loss = 0.61161603\n",
            "Iteration 282, loss = 0.61154862\n",
            "Iteration 283, loss = 0.61142839\n",
            "Iteration 284, loss = 0.61132801\n",
            "Iteration 285, loss = 0.61127677\n",
            "Iteration 286, loss = 0.61118698\n",
            "Iteration 287, loss = 0.61109626\n",
            "Iteration 288, loss = 0.61098907\n",
            "Iteration 289, loss = 0.61095153\n",
            "Iteration 290, loss = 0.61079884\n",
            "Iteration 291, loss = 0.61071897\n",
            "Iteration 292, loss = 0.61064565\n",
            "Iteration 293, loss = 0.61062179\n",
            "Iteration 294, loss = 0.61046193\n",
            "Iteration 295, loss = 0.61040489\n",
            "Iteration 296, loss = 0.61030862\n",
            "Iteration 297, loss = 0.61034279\n",
            "Iteration 298, loss = 0.61022855\n",
            "Iteration 299, loss = 0.61010386\n",
            "Iteration 300, loss = 0.60999955\n",
            "Iteration 301, loss = 0.60996039\n",
            "Iteration 302, loss = 0.60989614\n",
            "Iteration 303, loss = 0.60981835\n",
            "Iteration 304, loss = 0.60966681\n",
            "Iteration 305, loss = 0.60967053\n",
            "Iteration 306, loss = 0.60955809\n",
            "Iteration 307, loss = 0.60948282\n",
            "Iteration 308, loss = 0.60942772\n",
            "Iteration 309, loss = 0.60937873\n",
            "Iteration 310, loss = 0.60921770\n",
            "Iteration 311, loss = 0.60915810\n",
            "Iteration 312, loss = 0.60916593\n",
            "Iteration 313, loss = 0.60899527\n",
            "Iteration 314, loss = 0.60895421\n",
            "Iteration 315, loss = 0.60890093\n",
            "Iteration 316, loss = 0.60878355\n",
            "Iteration 317, loss = 0.60877062\n",
            "Iteration 318, loss = 0.60872227\n",
            "Iteration 319, loss = 0.60856839\n",
            "Iteration 320, loss = 0.60851505\n",
            "Iteration 321, loss = 0.60847053\n",
            "Iteration 322, loss = 0.60842877\n",
            "Iteration 323, loss = 0.60834872\n",
            "Iteration 324, loss = 0.60832633\n",
            "Iteration 325, loss = 0.60818708\n",
            "Iteration 326, loss = 0.60811173\n",
            "Iteration 327, loss = 0.60803433\n",
            "Iteration 328, loss = 0.60808402\n",
            "Iteration 329, loss = 0.60791728\n",
            "Iteration 330, loss = 0.60789220\n",
            "Iteration 331, loss = 0.60781524\n",
            "Iteration 332, loss = 0.60774264\n",
            "Iteration 333, loss = 0.60767291\n",
            "Iteration 334, loss = 0.60760938\n",
            "Iteration 335, loss = 0.60754359\n",
            "Iteration 336, loss = 0.60747330\n",
            "Iteration 337, loss = 0.60739599\n",
            "Iteration 338, loss = 0.60732238\n",
            "Iteration 339, loss = 0.60730941\n",
            "Iteration 340, loss = 0.60724486\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67771300\n",
            "Iteration 2, loss = 0.67658278\n",
            "Iteration 3, loss = 0.67507875\n",
            "Iteration 4, loss = 0.67421309\n",
            "Iteration 5, loss = 0.67299777\n",
            "Iteration 6, loss = 0.67206103\n",
            "Iteration 7, loss = 0.67161109\n",
            "Iteration 8, loss = 0.67109342\n",
            "Iteration 9, loss = 0.67073509\n",
            "Iteration 10, loss = 0.67038495\n",
            "Iteration 11, loss = 0.67004866\n",
            "Iteration 12, loss = 0.66981217\n",
            "Iteration 13, loss = 0.66952830\n",
            "Iteration 14, loss = 0.66920167\n",
            "Iteration 15, loss = 0.66899390\n",
            "Iteration 16, loss = 0.66872088\n",
            "Iteration 17, loss = 0.66846018\n",
            "Iteration 18, loss = 0.66828640\n",
            "Iteration 19, loss = 0.66789799\n",
            "Iteration 20, loss = 0.66767858\n",
            "Iteration 21, loss = 0.66737792\n",
            "Iteration 22, loss = 0.66706322\n",
            "Iteration 23, loss = 0.66687312\n",
            "Iteration 24, loss = 0.66650503\n",
            "Iteration 25, loss = 0.66630357\n",
            "Iteration 26, loss = 0.66601058\n",
            "Iteration 27, loss = 0.66570348\n",
            "Iteration 28, loss = 0.66542519\n",
            "Iteration 29, loss = 0.66513423\n",
            "Iteration 30, loss = 0.66487311\n",
            "Iteration 31, loss = 0.66459547\n",
            "Iteration 32, loss = 0.66431781\n",
            "Iteration 33, loss = 0.66405246\n",
            "Iteration 34, loss = 0.66379320\n",
            "Iteration 35, loss = 0.66356516\n",
            "Iteration 36, loss = 0.66329302\n",
            "Iteration 37, loss = 0.66301224\n",
            "Iteration 38, loss = 0.66285101\n",
            "Iteration 39, loss = 0.66249133\n",
            "Iteration 40, loss = 0.66225412\n",
            "Iteration 41, loss = 0.66201801\n",
            "Iteration 42, loss = 0.66180413\n",
            "Iteration 43, loss = 0.66157868\n",
            "Iteration 44, loss = 0.66135725\n",
            "Iteration 45, loss = 0.66116785\n",
            "Iteration 46, loss = 0.66101117\n",
            "Iteration 47, loss = 0.66075575\n",
            "Iteration 48, loss = 0.66051176\n",
            "Iteration 49, loss = 0.66035846\n",
            "Iteration 50, loss = 0.66011190\n",
            "Iteration 51, loss = 0.65993129\n",
            "Iteration 52, loss = 0.65970410\n",
            "Iteration 53, loss = 0.65954532\n",
            "Iteration 54, loss = 0.65931713\n",
            "Iteration 55, loss = 0.65910922\n",
            "Iteration 56, loss = 0.65895093\n",
            "Iteration 57, loss = 0.65874001\n",
            "Iteration 58, loss = 0.65857545\n",
            "Iteration 59, loss = 0.65836340\n",
            "Iteration 60, loss = 0.65815846\n",
            "Iteration 61, loss = 0.65794483\n",
            "Iteration 62, loss = 0.65778923\n",
            "Iteration 63, loss = 0.65775181\n",
            "Iteration 64, loss = 0.65743344\n",
            "Iteration 65, loss = 0.65717830\n",
            "Iteration 66, loss = 0.65700669\n",
            "Iteration 67, loss = 0.65679972\n",
            "Iteration 68, loss = 0.65664313\n",
            "Iteration 69, loss = 0.65638457\n",
            "Iteration 70, loss = 0.65623286\n",
            "Iteration 71, loss = 0.65600182\n",
            "Iteration 72, loss = 0.65586797\n",
            "Iteration 73, loss = 0.65565210\n",
            "Iteration 74, loss = 0.65556214\n",
            "Iteration 75, loss = 0.65529240\n",
            "Iteration 76, loss = 0.65512848\n",
            "Iteration 77, loss = 0.65492814\n",
            "Iteration 78, loss = 0.65473948\n",
            "Iteration 79, loss = 0.65458705\n",
            "Iteration 80, loss = 0.65441973\n",
            "Iteration 81, loss = 0.65419962\n",
            "Iteration 82, loss = 0.65413029\n",
            "Iteration 83, loss = 0.65382154\n",
            "Iteration 84, loss = 0.65363057\n",
            "Iteration 85, loss = 0.65346271\n",
            "Iteration 86, loss = 0.65331256\n",
            "Iteration 87, loss = 0.65308273\n",
            "Iteration 88, loss = 0.65293465\n",
            "Iteration 89, loss = 0.65276495\n",
            "Iteration 90, loss = 0.65257291\n",
            "Iteration 91, loss = 0.65238790\n",
            "Iteration 92, loss = 0.65219439\n",
            "Iteration 93, loss = 0.65206403\n",
            "Iteration 94, loss = 0.65187638\n",
            "Iteration 95, loss = 0.65169915\n",
            "Iteration 96, loss = 0.65153432\n",
            "Iteration 97, loss = 0.65132934\n",
            "Iteration 98, loss = 0.65116332\n",
            "Iteration 99, loss = 0.65105193\n",
            "Iteration 100, loss = 0.65082354\n",
            "Iteration 101, loss = 0.65067117\n",
            "Iteration 102, loss = 0.65054379\n",
            "Iteration 103, loss = 0.65025150\n",
            "Iteration 104, loss = 0.65009978\n",
            "Iteration 105, loss = 0.64988059\n",
            "Iteration 106, loss = 0.64970636\n",
            "Iteration 107, loss = 0.64961676\n",
            "Iteration 108, loss = 0.64952874\n",
            "Iteration 109, loss = 0.64917641\n",
            "Iteration 110, loss = 0.64903192\n",
            "Iteration 111, loss = 0.64881293\n",
            "Iteration 112, loss = 0.64866391\n",
            "Iteration 113, loss = 0.64856881\n",
            "Iteration 114, loss = 0.64829595\n",
            "Iteration 115, loss = 0.64815957\n",
            "Iteration 116, loss = 0.64798132\n",
            "Iteration 117, loss = 0.64783285\n",
            "Iteration 118, loss = 0.64762700\n",
            "Iteration 119, loss = 0.64743318\n",
            "Iteration 120, loss = 0.64722955\n",
            "Iteration 121, loss = 0.64708823\n",
            "Iteration 122, loss = 0.64688733\n",
            "Iteration 123, loss = 0.64671454\n",
            "Iteration 124, loss = 0.64656674\n",
            "Iteration 125, loss = 0.64641274\n",
            "Iteration 126, loss = 0.64622177\n",
            "Iteration 127, loss = 0.64604688\n",
            "Iteration 128, loss = 0.64586997\n",
            "Iteration 129, loss = 0.64570259\n",
            "Iteration 130, loss = 0.64550902\n",
            "Iteration 131, loss = 0.64538758\n",
            "Iteration 132, loss = 0.64519506\n",
            "Iteration 133, loss = 0.64500675\n",
            "Iteration 134, loss = 0.64483714\n",
            "Iteration 135, loss = 0.64468080\n",
            "Iteration 136, loss = 0.64452018\n",
            "Iteration 137, loss = 0.64449108\n",
            "Iteration 138, loss = 0.64425822\n",
            "Iteration 139, loss = 0.64402906\n",
            "Iteration 140, loss = 0.64395011\n",
            "Iteration 141, loss = 0.64371533\n",
            "Iteration 142, loss = 0.64353925\n",
            "Iteration 143, loss = 0.64344779\n",
            "Iteration 144, loss = 0.64318817\n",
            "Iteration 145, loss = 0.64303806\n",
            "Iteration 146, loss = 0.64288868\n",
            "Iteration 147, loss = 0.64272037\n",
            "Iteration 148, loss = 0.64259790\n",
            "Iteration 149, loss = 0.64246015\n",
            "Iteration 150, loss = 0.64226970\n",
            "Iteration 151, loss = 0.64211379\n",
            "Iteration 152, loss = 0.64191880\n",
            "Iteration 153, loss = 0.64183302\n",
            "Iteration 154, loss = 0.64159371\n",
            "Iteration 155, loss = 0.64144211\n",
            "Iteration 156, loss = 0.64127386\n",
            "Iteration 157, loss = 0.64120293\n",
            "Iteration 158, loss = 0.64097643\n",
            "Iteration 159, loss = 0.64082341\n",
            "Iteration 160, loss = 0.64066005\n",
            "Iteration 161, loss = 0.64051829\n",
            "Iteration 162, loss = 0.64035140\n",
            "Iteration 163, loss = 0.64020228\n",
            "Iteration 164, loss = 0.64002345\n",
            "Iteration 165, loss = 0.63994085\n",
            "Iteration 166, loss = 0.63987016\n",
            "Iteration 167, loss = 0.63956698\n",
            "Iteration 168, loss = 0.63939310\n",
            "Iteration 169, loss = 0.63930865\n",
            "Iteration 170, loss = 0.63915678\n",
            "Iteration 171, loss = 0.63907389\n",
            "Iteration 172, loss = 0.63887473\n",
            "Iteration 173, loss = 0.63868703\n",
            "Iteration 174, loss = 0.63857862\n",
            "Iteration 175, loss = 0.63846681\n",
            "Iteration 176, loss = 0.63825472\n",
            "Iteration 177, loss = 0.63817807\n",
            "Iteration 178, loss = 0.63801903\n",
            "Iteration 179, loss = 0.63790547\n",
            "Iteration 180, loss = 0.63772011\n",
            "Iteration 181, loss = 0.63757260\n",
            "Iteration 182, loss = 0.63743216\n",
            "Iteration 183, loss = 0.63730053\n",
            "Iteration 184, loss = 0.63712183\n",
            "Iteration 185, loss = 0.63696682\n",
            "Iteration 186, loss = 0.63684594\n",
            "Iteration 187, loss = 0.63666564\n",
            "Iteration 188, loss = 0.63655669\n",
            "Iteration 189, loss = 0.63642225\n",
            "Iteration 190, loss = 0.63627397\n",
            "Iteration 191, loss = 0.63614653\n",
            "Iteration 192, loss = 0.63599641\n",
            "Iteration 193, loss = 0.63598468\n",
            "Iteration 194, loss = 0.63568414\n",
            "Iteration 195, loss = 0.63581136\n",
            "Iteration 196, loss = 0.63545292\n",
            "Iteration 197, loss = 0.63532243\n",
            "Iteration 198, loss = 0.63523552\n",
            "Iteration 199, loss = 0.63509407\n",
            "Iteration 200, loss = 0.63498412\n",
            "Iteration 201, loss = 0.63479830\n",
            "Iteration 202, loss = 0.63464339\n",
            "Iteration 203, loss = 0.63455348\n",
            "Iteration 204, loss = 0.63462254\n",
            "Iteration 205, loss = 0.63424543\n",
            "Iteration 206, loss = 0.63414580\n",
            "Iteration 207, loss = 0.63406061\n",
            "Iteration 208, loss = 0.63387418\n",
            "Iteration 209, loss = 0.63383848\n",
            "Iteration 210, loss = 0.63364159\n",
            "Iteration 211, loss = 0.63353213\n",
            "Iteration 212, loss = 0.63345993\n",
            "Iteration 213, loss = 0.63337559\n",
            "Iteration 214, loss = 0.63314842\n",
            "Iteration 215, loss = 0.63303566\n",
            "Iteration 216, loss = 0.63292737\n",
            "Iteration 217, loss = 0.63279009\n",
            "Iteration 218, loss = 0.63266955\n",
            "Iteration 219, loss = 0.63254797\n",
            "Iteration 220, loss = 0.63239726\n",
            "Iteration 221, loss = 0.63230468\n",
            "Iteration 222, loss = 0.63219285\n",
            "Iteration 223, loss = 0.63206756\n",
            "Iteration 224, loss = 0.63201294\n",
            "Iteration 225, loss = 0.63185983\n",
            "Iteration 226, loss = 0.63168901\n",
            "Iteration 227, loss = 0.63162322\n",
            "Iteration 228, loss = 0.63144369\n",
            "Iteration 229, loss = 0.63134066\n",
            "Iteration 230, loss = 0.63125913\n",
            "Iteration 231, loss = 0.63111123\n",
            "Iteration 232, loss = 0.63109939\n",
            "Iteration 233, loss = 0.63090681\n",
            "Iteration 234, loss = 0.63081231\n",
            "Iteration 235, loss = 0.63081970\n",
            "Iteration 236, loss = 0.63057783\n",
            "Iteration 237, loss = 0.63046227\n",
            "Iteration 238, loss = 0.63037306\n",
            "Iteration 239, loss = 0.63028628\n",
            "Iteration 240, loss = 0.63013436\n",
            "Iteration 241, loss = 0.63005836\n",
            "Iteration 242, loss = 0.62994452\n",
            "Iteration 243, loss = 0.62982891\n",
            "Iteration 244, loss = 0.62976179\n",
            "Iteration 245, loss = 0.62966077\n",
            "Iteration 246, loss = 0.62951613\n",
            "Iteration 247, loss = 0.62943808\n",
            "Iteration 248, loss = 0.62933206\n",
            "Iteration 249, loss = 0.62922128\n",
            "Iteration 250, loss = 0.62914597\n",
            "Iteration 251, loss = 0.62902841\n",
            "Iteration 252, loss = 0.62891773\n",
            "Iteration 253, loss = 0.62883191\n",
            "Iteration 254, loss = 0.62871141\n",
            "Iteration 255, loss = 0.62861674\n",
            "Iteration 256, loss = 0.62858529\n",
            "Iteration 257, loss = 0.62843015\n",
            "Iteration 258, loss = 0.62844714\n",
            "Iteration 259, loss = 0.62821536\n",
            "Iteration 260, loss = 0.62821093\n",
            "Iteration 261, loss = 0.62812206\n",
            "Iteration 262, loss = 0.62796923\n",
            "Iteration 263, loss = 0.62790676\n",
            "Iteration 264, loss = 0.62782816\n",
            "Iteration 265, loss = 0.62769101\n",
            "Iteration 266, loss = 0.62760221\n",
            "Iteration 267, loss = 0.62749809\n",
            "Iteration 268, loss = 0.62742858\n",
            "Iteration 269, loss = 0.62739101\n",
            "Iteration 270, loss = 0.62728145\n",
            "Iteration 271, loss = 0.62711511\n",
            "Iteration 272, loss = 0.62712834\n",
            "Iteration 273, loss = 0.62691890\n",
            "Iteration 274, loss = 0.62687171\n",
            "Iteration 275, loss = 0.62679496\n",
            "Iteration 276, loss = 0.62670446\n",
            "Iteration 277, loss = 0.62662811\n",
            "Iteration 278, loss = 0.62651599\n",
            "Iteration 279, loss = 0.62655690\n",
            "Iteration 280, loss = 0.62635151\n",
            "Iteration 281, loss = 0.62653526\n",
            "Iteration 282, loss = 0.62622327\n",
            "Iteration 283, loss = 0.62606549\n",
            "Iteration 284, loss = 0.62600588\n",
            "Iteration 285, loss = 0.62600737\n",
            "Iteration 286, loss = 0.62583733\n",
            "Iteration 287, loss = 0.62580140\n",
            "Iteration 288, loss = 0.62570608\n",
            "Iteration 289, loss = 0.62565154\n",
            "Iteration 290, loss = 0.62555134\n",
            "Iteration 291, loss = 0.62548255\n",
            "Iteration 292, loss = 0.62539441\n",
            "Iteration 293, loss = 0.62529645\n",
            "Iteration 294, loss = 0.62521952\n",
            "Iteration 295, loss = 0.62513656\n",
            "Iteration 296, loss = 0.62508634\n",
            "Iteration 297, loss = 0.62505127\n",
            "Iteration 298, loss = 0.62490341\n",
            "Iteration 299, loss = 0.62481350\n",
            "Iteration 300, loss = 0.62475842\n",
            "Iteration 301, loss = 0.62470311\n",
            "Iteration 302, loss = 0.62461918\n",
            "Iteration 303, loss = 0.62464572\n",
            "Iteration 304, loss = 0.62453377\n",
            "Iteration 305, loss = 0.62442409\n",
            "Iteration 306, loss = 0.62436221\n",
            "Iteration 307, loss = 0.62434979\n",
            "Iteration 308, loss = 0.62421751\n",
            "Iteration 309, loss = 0.62422993\n",
            "Iteration 310, loss = 0.62408669\n",
            "Iteration 311, loss = 0.62402061\n",
            "Iteration 312, loss = 0.62399781\n",
            "Iteration 313, loss = 0.62385523\n",
            "Iteration 314, loss = 0.62386203\n",
            "Iteration 315, loss = 0.62388071\n",
            "Iteration 316, loss = 0.62362649\n",
            "Iteration 317, loss = 0.62355264\n",
            "Iteration 318, loss = 0.62349018\n",
            "Iteration 319, loss = 0.62349169\n",
            "Iteration 320, loss = 0.62341866\n",
            "Iteration 321, loss = 0.62341597\n",
            "Iteration 322, loss = 0.62327041\n",
            "Iteration 323, loss = 0.62323152\n",
            "Iteration 324, loss = 0.62314064\n",
            "Iteration 325, loss = 0.62308054\n",
            "Iteration 326, loss = 0.62301428\n",
            "Iteration 327, loss = 0.62296558\n",
            "Iteration 328, loss = 0.62292202\n",
            "Iteration 329, loss = 0.62278293\n",
            "Iteration 330, loss = 0.62277028\n",
            "Iteration 331, loss = 0.62268193\n",
            "Iteration 332, loss = 0.62270626\n",
            "Iteration 333, loss = 0.62258334\n",
            "Iteration 334, loss = 0.62250176\n",
            "Iteration 335, loss = 0.62244293\n",
            "Iteration 336, loss = 0.62237011\n",
            "Iteration 337, loss = 0.62227722\n",
            "Iteration 338, loss = 0.62221782\n",
            "Iteration 339, loss = 0.62218434\n",
            "Iteration 340, loss = 0.62220217\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68166696\n",
            "Iteration 2, loss = 0.67984043\n",
            "Iteration 3, loss = 0.67752502\n",
            "Iteration 4, loss = 0.67589759\n",
            "Iteration 5, loss = 0.67472971\n",
            "Iteration 6, loss = 0.67339220\n",
            "Iteration 7, loss = 0.67260819\n",
            "Iteration 8, loss = 0.67221138\n",
            "Iteration 9, loss = 0.67142332\n",
            "Iteration 10, loss = 0.67097364\n",
            "Iteration 11, loss = 0.67048901\n",
            "Iteration 12, loss = 0.67012898\n",
            "Iteration 13, loss = 0.66975769\n",
            "Iteration 14, loss = 0.66935807\n",
            "Iteration 15, loss = 0.66903617\n",
            "Iteration 16, loss = 0.66871588\n",
            "Iteration 17, loss = 0.66832421\n",
            "Iteration 18, loss = 0.66808594\n",
            "Iteration 19, loss = 0.66769786\n",
            "Iteration 20, loss = 0.66747892\n",
            "Iteration 21, loss = 0.66706938\n",
            "Iteration 22, loss = 0.66674578\n",
            "Iteration 23, loss = 0.66646045\n",
            "Iteration 24, loss = 0.66615012\n",
            "Iteration 25, loss = 0.66586519\n",
            "Iteration 26, loss = 0.66553542\n",
            "Iteration 27, loss = 0.66519916\n",
            "Iteration 28, loss = 0.66495731\n",
            "Iteration 29, loss = 0.66458903\n",
            "Iteration 30, loss = 0.66427613\n",
            "Iteration 31, loss = 0.66398944\n",
            "Iteration 32, loss = 0.66365939\n",
            "Iteration 33, loss = 0.66337612\n",
            "Iteration 34, loss = 0.66302034\n",
            "Iteration 35, loss = 0.66276247\n",
            "Iteration 36, loss = 0.66243380\n",
            "Iteration 37, loss = 0.66213242\n",
            "Iteration 38, loss = 0.66181013\n",
            "Iteration 39, loss = 0.66155087\n",
            "Iteration 40, loss = 0.66121653\n",
            "Iteration 41, loss = 0.66089951\n",
            "Iteration 42, loss = 0.66058409\n",
            "Iteration 43, loss = 0.66029592\n",
            "Iteration 44, loss = 0.65998776\n",
            "Iteration 45, loss = 0.65967664\n",
            "Iteration 46, loss = 0.65936095\n",
            "Iteration 47, loss = 0.65904678\n",
            "Iteration 48, loss = 0.65866174\n",
            "Iteration 49, loss = 0.65834829\n",
            "Iteration 50, loss = 0.65794094\n",
            "Iteration 51, loss = 0.65760744\n",
            "Iteration 52, loss = 0.65742849\n",
            "Iteration 53, loss = 0.65693829\n",
            "Iteration 54, loss = 0.65664722\n",
            "Iteration 55, loss = 0.65632391\n",
            "Iteration 56, loss = 0.65594687\n",
            "Iteration 57, loss = 0.65559558\n",
            "Iteration 58, loss = 0.65528288\n",
            "Iteration 59, loss = 0.65489502\n",
            "Iteration 60, loss = 0.65463912\n",
            "Iteration 61, loss = 0.65423001\n",
            "Iteration 62, loss = 0.65392691\n",
            "Iteration 63, loss = 0.65364108\n",
            "Iteration 64, loss = 0.65330735\n",
            "Iteration 65, loss = 0.65288134\n",
            "Iteration 66, loss = 0.65255518\n",
            "Iteration 67, loss = 0.65221794\n",
            "Iteration 68, loss = 0.65189075\n",
            "Iteration 69, loss = 0.65155388\n",
            "Iteration 70, loss = 0.65120530\n",
            "Iteration 71, loss = 0.65091402\n",
            "Iteration 72, loss = 0.65059618\n",
            "Iteration 73, loss = 0.65025039\n",
            "Iteration 74, loss = 0.64997883\n",
            "Iteration 75, loss = 0.64967052\n",
            "Iteration 76, loss = 0.64939857\n",
            "Iteration 77, loss = 0.64911448\n",
            "Iteration 78, loss = 0.64879124\n",
            "Iteration 79, loss = 0.64852419\n",
            "Iteration 80, loss = 0.64821726\n",
            "Iteration 81, loss = 0.64797385\n",
            "Iteration 82, loss = 0.64766143\n",
            "Iteration 83, loss = 0.64736124\n",
            "Iteration 84, loss = 0.64712173\n",
            "Iteration 85, loss = 0.64685020\n",
            "Iteration 86, loss = 0.64655707\n",
            "Iteration 87, loss = 0.64640454\n",
            "Iteration 88, loss = 0.64609336\n",
            "Iteration 89, loss = 0.64574383\n",
            "Iteration 90, loss = 0.64546997\n",
            "Iteration 91, loss = 0.64523953\n",
            "Iteration 92, loss = 0.64494123\n",
            "Iteration 93, loss = 0.64470942\n",
            "Iteration 94, loss = 0.64442769\n",
            "Iteration 95, loss = 0.64414780\n",
            "Iteration 96, loss = 0.64398765\n",
            "Iteration 97, loss = 0.64363096\n",
            "Iteration 98, loss = 0.64338480\n",
            "Iteration 99, loss = 0.64313655\n",
            "Iteration 100, loss = 0.64287457\n",
            "Iteration 101, loss = 0.64270664\n",
            "Iteration 102, loss = 0.64239634\n",
            "Iteration 103, loss = 0.64213830\n",
            "Iteration 104, loss = 0.64194405\n",
            "Iteration 105, loss = 0.64165926\n",
            "Iteration 106, loss = 0.64140491\n",
            "Iteration 107, loss = 0.64117267\n",
            "Iteration 108, loss = 0.64087249\n",
            "Iteration 109, loss = 0.64069494\n",
            "Iteration 110, loss = 0.64045108\n",
            "Iteration 111, loss = 0.64014907\n",
            "Iteration 112, loss = 0.63986808\n",
            "Iteration 113, loss = 0.63965608\n",
            "Iteration 114, loss = 0.63946260\n",
            "Iteration 115, loss = 0.63916789\n",
            "Iteration 116, loss = 0.63892846\n",
            "Iteration 117, loss = 0.63884284\n",
            "Iteration 118, loss = 0.63859332\n",
            "Iteration 119, loss = 0.63824271\n",
            "Iteration 120, loss = 0.63805981\n",
            "Iteration 121, loss = 0.63792915\n",
            "Iteration 122, loss = 0.63763627\n",
            "Iteration 123, loss = 0.63741486\n",
            "Iteration 124, loss = 0.63721516\n",
            "Iteration 125, loss = 0.63706525\n",
            "Iteration 126, loss = 0.63679905\n",
            "Iteration 127, loss = 0.63659082\n",
            "Iteration 128, loss = 0.63644940\n",
            "Iteration 129, loss = 0.63625823\n",
            "Iteration 130, loss = 0.63601895\n",
            "Iteration 131, loss = 0.63577709\n",
            "Iteration 132, loss = 0.63565292\n",
            "Iteration 133, loss = 0.63542340\n",
            "Iteration 134, loss = 0.63526078\n",
            "Iteration 135, loss = 0.63502529\n",
            "Iteration 136, loss = 0.63483923\n",
            "Iteration 137, loss = 0.63466322\n",
            "Iteration 138, loss = 0.63449351\n",
            "Iteration 139, loss = 0.63431119\n",
            "Iteration 140, loss = 0.63407221\n",
            "Iteration 141, loss = 0.63386810\n",
            "Iteration 142, loss = 0.63374414\n",
            "Iteration 143, loss = 0.63352688\n",
            "Iteration 144, loss = 0.63345935\n",
            "Iteration 145, loss = 0.63323586\n",
            "Iteration 146, loss = 0.63302032\n",
            "Iteration 147, loss = 0.63285695\n",
            "Iteration 148, loss = 0.63273825\n",
            "Iteration 149, loss = 0.63248913\n",
            "Iteration 150, loss = 0.63241019\n",
            "Iteration 151, loss = 0.63226880\n",
            "Iteration 152, loss = 0.63204601\n",
            "Iteration 153, loss = 0.63185401\n",
            "Iteration 154, loss = 0.63180782\n",
            "Iteration 155, loss = 0.63156385\n",
            "Iteration 156, loss = 0.63135037\n",
            "Iteration 157, loss = 0.63124835\n",
            "Iteration 158, loss = 0.63111612\n",
            "Iteration 159, loss = 0.63094145\n",
            "Iteration 160, loss = 0.63075597\n",
            "Iteration 161, loss = 0.63063023\n",
            "Iteration 162, loss = 0.63051613\n",
            "Iteration 163, loss = 0.63035898\n",
            "Iteration 164, loss = 0.63018309\n",
            "Iteration 165, loss = 0.63010638\n",
            "Iteration 166, loss = 0.62991612\n",
            "Iteration 167, loss = 0.62989406\n",
            "Iteration 168, loss = 0.62962743\n",
            "Iteration 169, loss = 0.62950542\n",
            "Iteration 170, loss = 0.62942580\n",
            "Iteration 171, loss = 0.62922129\n",
            "Iteration 172, loss = 0.62910494\n",
            "Iteration 173, loss = 0.62918207\n",
            "Iteration 174, loss = 0.62884395\n",
            "Iteration 175, loss = 0.62875254\n",
            "Iteration 176, loss = 0.62869004\n",
            "Iteration 177, loss = 0.62857865\n",
            "Iteration 178, loss = 0.62837505\n",
            "Iteration 179, loss = 0.62830106\n",
            "Iteration 180, loss = 0.62813769\n",
            "Iteration 181, loss = 0.62810213\n",
            "Iteration 182, loss = 0.62790986\n",
            "Iteration 183, loss = 0.62784229\n",
            "Iteration 184, loss = 0.62773529\n",
            "Iteration 185, loss = 0.62766060\n",
            "Iteration 186, loss = 0.62749075\n",
            "Iteration 187, loss = 0.62736015\n",
            "Iteration 188, loss = 0.62729394\n",
            "Iteration 189, loss = 0.62723334\n",
            "Iteration 190, loss = 0.62706342\n",
            "Iteration 191, loss = 0.62708104\n",
            "Iteration 192, loss = 0.62689917\n",
            "Iteration 193, loss = 0.62674932\n",
            "Iteration 194, loss = 0.62662984\n",
            "Iteration 195, loss = 0.62657375\n",
            "Iteration 196, loss = 0.62644031\n",
            "Iteration 197, loss = 0.62638469\n",
            "Iteration 198, loss = 0.62623422\n",
            "Iteration 199, loss = 0.62614438\n",
            "Iteration 200, loss = 0.62609728\n",
            "Iteration 201, loss = 0.62600630\n",
            "Iteration 202, loss = 0.62588911\n",
            "Iteration 203, loss = 0.62587904\n",
            "Iteration 204, loss = 0.62573487\n",
            "Iteration 205, loss = 0.62577657\n",
            "Iteration 206, loss = 0.62554681\n",
            "Iteration 207, loss = 0.62546609\n",
            "Iteration 208, loss = 0.62555420\n",
            "Iteration 209, loss = 0.62530007\n",
            "Iteration 210, loss = 0.62526896\n",
            "Iteration 211, loss = 0.62519747\n",
            "Iteration 212, loss = 0.62510271\n",
            "Iteration 213, loss = 0.62501704\n",
            "Iteration 214, loss = 0.62493751\n",
            "Iteration 215, loss = 0.62485154\n",
            "Iteration 216, loss = 0.62477847\n",
            "Iteration 217, loss = 0.62476277\n",
            "Iteration 218, loss = 0.62463801\n",
            "Iteration 219, loss = 0.62469102\n",
            "Iteration 220, loss = 0.62451386\n",
            "Iteration 221, loss = 0.62440997\n",
            "Iteration 222, loss = 0.62439078\n",
            "Iteration 223, loss = 0.62426451\n",
            "Iteration 224, loss = 0.62419045\n",
            "Iteration 225, loss = 0.62412920\n",
            "Iteration 226, loss = 0.62412430\n",
            "Iteration 227, loss = 0.62400435\n",
            "Iteration 228, loss = 0.62393877\n",
            "Iteration 229, loss = 0.62387335\n",
            "Iteration 230, loss = 0.62382451\n",
            "Iteration 231, loss = 0.62388979\n",
            "Iteration 232, loss = 0.62371641\n",
            "Iteration 233, loss = 0.62364042\n",
            "Iteration 234, loss = 0.62361859\n",
            "Iteration 235, loss = 0.62352809\n",
            "Iteration 236, loss = 0.62347610\n",
            "Iteration 237, loss = 0.62346279\n",
            "Iteration 238, loss = 0.62337522\n",
            "Iteration 239, loss = 0.62331730\n",
            "Iteration 240, loss = 0.62325940\n",
            "Iteration 241, loss = 0.62327312\n",
            "Iteration 242, loss = 0.62318460\n",
            "Iteration 243, loss = 0.62312219\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.86480517\n",
            "Iteration 2, loss = 0.71035869\n",
            "Iteration 3, loss = 0.60173205\n",
            "Iteration 4, loss = 0.58479737\n",
            "Iteration 5, loss = 0.59195735\n",
            "Iteration 6, loss = 0.58045405\n",
            "Iteration 7, loss = 0.58027590\n",
            "Iteration 8, loss = 0.58305673\n",
            "Iteration 9, loss = 0.60978209\n",
            "Iteration 10, loss = 0.59630487\n",
            "Iteration 11, loss = 0.59836833\n",
            "Iteration 12, loss = 0.59804077\n",
            "Iteration 13, loss = 0.59391566\n",
            "Iteration 14, loss = 0.58767678\n",
            "Iteration 15, loss = 0.58147217\n",
            "Iteration 16, loss = 0.57102747\n",
            "Iteration 17, loss = 0.58272351\n",
            "Iteration 18, loss = 0.57472346\n",
            "Iteration 19, loss = 0.57468740\n",
            "Iteration 20, loss = 0.58603102\n",
            "Iteration 21, loss = 0.56214046\n",
            "Iteration 22, loss = 0.56839602\n",
            "Iteration 23, loss = 0.58237065\n",
            "Iteration 24, loss = 0.57451773\n",
            "Iteration 25, loss = 0.56504176\n",
            "Iteration 26, loss = 0.56856482\n",
            "Iteration 27, loss = 0.58103418\n",
            "Iteration 28, loss = 0.56974165\n",
            "Iteration 29, loss = 0.56300206\n",
            "Iteration 30, loss = 0.57995897\n",
            "Iteration 31, loss = 0.55688217\n",
            "Iteration 32, loss = 0.55786767\n",
            "Iteration 33, loss = 0.58607922\n",
            "Iteration 34, loss = 0.56230169\n",
            "Iteration 35, loss = 0.55758413\n",
            "Iteration 36, loss = 0.56284140\n",
            "Iteration 37, loss = 0.56215722\n",
            "Iteration 38, loss = 0.56201797\n",
            "Iteration 39, loss = 0.55510267\n",
            "Iteration 40, loss = 0.55444587\n",
            "Iteration 41, loss = 0.54818012\n",
            "Iteration 42, loss = 0.55809125\n",
            "Iteration 43, loss = 0.55860045\n",
            "Iteration 44, loss = 0.55535473\n",
            "Iteration 45, loss = 0.54782143\n",
            "Iteration 46, loss = 0.54291061\n",
            "Iteration 47, loss = 0.55302600\n",
            "Iteration 48, loss = 0.54371733\n",
            "Iteration 49, loss = 0.55507183\n",
            "Iteration 50, loss = 0.55230961\n",
            "Iteration 51, loss = 0.55006904\n",
            "Iteration 52, loss = 0.56097869\n",
            "Iteration 53, loss = 0.56281678\n",
            "Iteration 54, loss = 0.55070988\n",
            "Iteration 55, loss = 0.55103108\n",
            "Iteration 56, loss = 0.61120729\n",
            "Iteration 57, loss = 0.55323296\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.79210404\n",
            "Iteration 2, loss = 0.69059833\n",
            "Iteration 3, loss = 0.63719931\n",
            "Iteration 4, loss = 0.63742154\n",
            "Iteration 5, loss = 0.64111303\n",
            "Iteration 6, loss = 0.63063701\n",
            "Iteration 7, loss = 0.59802985\n",
            "Iteration 8, loss = 0.60226877\n",
            "Iteration 9, loss = 0.59132291\n",
            "Iteration 10, loss = 0.59695214\n",
            "Iteration 11, loss = 0.58819147\n",
            "Iteration 12, loss = 0.58874286\n",
            "Iteration 13, loss = 0.57242634\n",
            "Iteration 14, loss = 0.58384595\n",
            "Iteration 15, loss = 0.59342428\n",
            "Iteration 16, loss = 0.59161140\n",
            "Iteration 17, loss = 0.57671563\n",
            "Iteration 18, loss = 0.57441523\n",
            "Iteration 19, loss = 0.58323384\n",
            "Iteration 20, loss = 0.57302409\n",
            "Iteration 21, loss = 0.60148729\n",
            "Iteration 22, loss = 0.58647591\n",
            "Iteration 23, loss = 0.56557907\n",
            "Iteration 24, loss = 0.56892606\n",
            "Iteration 25, loss = 0.56387384\n",
            "Iteration 26, loss = 0.57105845\n",
            "Iteration 27, loss = 0.55585365\n",
            "Iteration 28, loss = 0.57296438\n",
            "Iteration 29, loss = 0.55930119\n",
            "Iteration 30, loss = 0.56007158\n",
            "Iteration 31, loss = 0.60973893\n",
            "Iteration 32, loss = 0.55577565\n",
            "Iteration 33, loss = 0.57195686\n",
            "Iteration 34, loss = 0.54857161\n",
            "Iteration 35, loss = 0.55611148\n",
            "Iteration 36, loss = 0.59075989\n",
            "Iteration 37, loss = 0.60364779\n",
            "Iteration 38, loss = 0.68334428\n",
            "Iteration 39, loss = 0.55705991\n",
            "Iteration 40, loss = 0.55189960\n",
            "Iteration 41, loss = 0.54187288\n",
            "Iteration 42, loss = 0.53916872\n",
            "Iteration 43, loss = 0.54157461\n",
            "Iteration 44, loss = 0.52780862\n",
            "Iteration 45, loss = 0.53813790\n",
            "Iteration 46, loss = 0.54764250\n",
            "Iteration 47, loss = 0.56991298\n",
            "Iteration 48, loss = 0.55928466\n",
            "Iteration 49, loss = 0.54739282\n",
            "Iteration 50, loss = 0.53338661\n",
            "Iteration 51, loss = 0.54650856\n",
            "Iteration 52, loss = 0.60318995\n",
            "Iteration 53, loss = 0.54710340\n",
            "Iteration 54, loss = 0.53243225\n",
            "Iteration 55, loss = 0.56235642\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.82691579\n",
            "Iteration 2, loss = 0.66463227\n",
            "Iteration 3, loss = 0.61406795\n",
            "Iteration 4, loss = 0.60745757\n",
            "Iteration 5, loss = 0.59824397\n",
            "Iteration 6, loss = 0.59964607\n",
            "Iteration 7, loss = 0.59570820\n",
            "Iteration 8, loss = 0.61311910\n",
            "Iteration 9, loss = 0.58793318\n",
            "Iteration 10, loss = 0.59688607\n",
            "Iteration 11, loss = 0.58594587\n",
            "Iteration 12, loss = 0.59452159\n",
            "Iteration 13, loss = 0.59735611\n",
            "Iteration 14, loss = 0.62061450\n",
            "Iteration 15, loss = 0.59744048\n",
            "Iteration 16, loss = 0.58302238\n",
            "Iteration 17, loss = 0.57324644\n",
            "Iteration 18, loss = 0.59251359\n",
            "Iteration 19, loss = 0.57920697\n",
            "Iteration 20, loss = 0.57659943\n",
            "Iteration 21, loss = 0.56688073\n",
            "Iteration 22, loss = 0.56450403\n",
            "Iteration 23, loss = 0.56312604\n",
            "Iteration 24, loss = 0.57060598\n",
            "Iteration 25, loss = 0.56288568\n",
            "Iteration 26, loss = 0.56294544\n",
            "Iteration 27, loss = 0.57327432\n",
            "Iteration 28, loss = 0.58244194\n",
            "Iteration 29, loss = 0.56031441\n",
            "Iteration 30, loss = 0.55285696\n",
            "Iteration 31, loss = 0.55586075\n",
            "Iteration 32, loss = 0.56765518\n",
            "Iteration 33, loss = 0.55178737\n",
            "Iteration 34, loss = 0.55282647\n",
            "Iteration 35, loss = 0.55623417\n",
            "Iteration 36, loss = 0.54834227\n",
            "Iteration 37, loss = 0.57253500\n",
            "Iteration 38, loss = 0.56433593\n",
            "Iteration 39, loss = 0.54568733\n",
            "Iteration 40, loss = 0.54807248\n",
            "Iteration 41, loss = 0.56520365\n",
            "Iteration 42, loss = 0.56188688\n",
            "Iteration 43, loss = 0.57144572\n",
            "Iteration 44, loss = 0.54631215\n",
            "Iteration 45, loss = 0.53787882\n",
            "Iteration 46, loss = 0.59701593\n",
            "Iteration 47, loss = 0.54728110\n",
            "Iteration 48, loss = 0.53810318\n",
            "Iteration 49, loss = 0.53582770\n",
            "Iteration 50, loss = 0.54292529\n",
            "Iteration 51, loss = 0.54601831\n",
            "Iteration 52, loss = 0.55651031\n",
            "Iteration 53, loss = 0.54478737\n",
            "Iteration 54, loss = 0.54676400\n",
            "Iteration 55, loss = 0.53944957\n",
            "Iteration 56, loss = 0.53879180\n",
            "Iteration 57, loss = 0.54548156\n",
            "Iteration 58, loss = 0.53412050\n",
            "Iteration 59, loss = 0.52891852\n",
            "Iteration 60, loss = 0.52839864\n",
            "Iteration 61, loss = 0.53220622\n",
            "Iteration 62, loss = 0.53549496\n",
            "Iteration 63, loss = 0.53220117\n",
            "Iteration 64, loss = 0.53910057\n",
            "Iteration 65, loss = 0.54295375\n",
            "Iteration 66, loss = 0.56275485\n",
            "Iteration 67, loss = 0.53106592\n",
            "Iteration 68, loss = 0.52179567\n",
            "Iteration 69, loss = 0.57476360\n",
            "Iteration 70, loss = 0.53339269\n",
            "Iteration 71, loss = 0.54476097\n",
            "Iteration 72, loss = 0.53286734\n",
            "Iteration 73, loss = 0.52814459\n",
            "Iteration 74, loss = 0.51379236\n",
            "Iteration 75, loss = 0.50902407\n",
            "Iteration 76, loss = 0.54314128\n",
            "Iteration 77, loss = 0.52156157\n",
            "Iteration 78, loss = 0.51593523\n",
            "Iteration 79, loss = 0.51534565\n",
            "Iteration 80, loss = 0.54079125\n",
            "Iteration 81, loss = 0.52699869\n",
            "Iteration 82, loss = 0.51884780\n",
            "Iteration 83, loss = 0.51012664\n",
            "Iteration 84, loss = 0.52258643\n",
            "Iteration 85, loss = 0.51313444\n",
            "Iteration 86, loss = 0.51555613\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.86422763\n",
            "Iteration 2, loss = 0.67643040\n",
            "Iteration 3, loss = 1.03786475\n",
            "Iteration 4, loss = 0.77442989\n",
            "Iteration 5, loss = 0.69118579\n",
            "Iteration 6, loss = 0.61358556\n",
            "Iteration 7, loss = 0.61250222\n",
            "Iteration 8, loss = 0.64646651\n",
            "Iteration 9, loss = 0.62330704\n",
            "Iteration 10, loss = 0.61198777\n",
            "Iteration 11, loss = 0.67013992\n",
            "Iteration 12, loss = 0.61077463\n",
            "Iteration 13, loss = 0.59981218\n",
            "Iteration 14, loss = 0.59704252\n",
            "Iteration 15, loss = 0.57150788\n",
            "Iteration 16, loss = 0.61444850\n",
            "Iteration 17, loss = 0.56893256\n",
            "Iteration 18, loss = 0.58117813\n",
            "Iteration 19, loss = 0.63308207\n",
            "Iteration 20, loss = 0.65166165\n",
            "Iteration 21, loss = 0.60537559\n",
            "Iteration 22, loss = 0.57299239\n",
            "Iteration 23, loss = 0.58565625\n",
            "Iteration 24, loss = 0.57322119\n",
            "Iteration 25, loss = 0.64881837\n",
            "Iteration 26, loss = 0.55306130\n",
            "Iteration 27, loss = 0.61731301\n",
            "Iteration 28, loss = 0.57381296\n",
            "Iteration 29, loss = 0.56899082\n",
            "Iteration 30, loss = 0.55343861\n",
            "Iteration 31, loss = 0.55972636\n",
            "Iteration 32, loss = 0.55777383\n",
            "Iteration 33, loss = 0.56383363\n",
            "Iteration 34, loss = 0.57986986\n",
            "Iteration 35, loss = 0.57173231\n",
            "Iteration 36, loss = 0.56569093\n",
            "Iteration 37, loss = 0.67552239\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71269724\n",
            "Iteration 2, loss = 0.68568183\n",
            "Iteration 3, loss = 0.64387988\n",
            "Iteration 4, loss = 0.62546826\n",
            "Iteration 5, loss = 0.63479355\n",
            "Iteration 6, loss = 0.65364313\n",
            "Iteration 7, loss = 0.67536982\n",
            "Iteration 8, loss = 0.60427153\n",
            "Iteration 9, loss = 0.61293505\n",
            "Iteration 10, loss = 0.65667047\n",
            "Iteration 11, loss = 0.59309393\n",
            "Iteration 12, loss = 0.58961319\n",
            "Iteration 13, loss = 0.61742098\n",
            "Iteration 14, loss = 0.61692854\n",
            "Iteration 15, loss = 0.58816470\n",
            "Iteration 16, loss = 0.57782591\n",
            "Iteration 17, loss = 0.60865090\n",
            "Iteration 18, loss = 0.58232971\n",
            "Iteration 19, loss = 0.58779327\n",
            "Iteration 20, loss = 0.57640896\n",
            "Iteration 21, loss = 0.57686194\n",
            "Iteration 22, loss = 0.60323090\n",
            "Iteration 23, loss = 0.59091884\n",
            "Iteration 24, loss = 0.58057561\n",
            "Iteration 25, loss = 0.57616282\n",
            "Iteration 26, loss = 0.57132954\n",
            "Iteration 27, loss = 0.57865358\n",
            "Iteration 28, loss = 0.59820626\n",
            "Iteration 29, loss = 0.58277014\n",
            "Iteration 30, loss = 0.58497284\n",
            "Iteration 31, loss = 0.57616407\n",
            "Iteration 32, loss = 0.57650817\n",
            "Iteration 33, loss = 0.55277873\n",
            "Iteration 34, loss = 0.56841018\n",
            "Iteration 35, loss = 0.57736035\n",
            "Iteration 36, loss = 0.57449383\n",
            "Iteration 37, loss = 0.56961515\n",
            "Iteration 38, loss = 0.55978464\n",
            "Iteration 39, loss = 0.56369006\n",
            "Iteration 40, loss = 0.54570723\n",
            "Iteration 41, loss = 0.56467563\n",
            "Iteration 42, loss = 0.56017730\n",
            "Iteration 43, loss = 0.61036656\n",
            "Iteration 44, loss = 0.56221220\n",
            "Iteration 45, loss = 0.55521607\n",
            "Iteration 46, loss = 0.55722578\n",
            "Iteration 47, loss = 0.56943302\n",
            "Iteration 48, loss = 0.57746322\n",
            "Iteration 49, loss = 0.55233631\n",
            "Iteration 50, loss = 0.54499366\n",
            "Iteration 51, loss = 0.58283391\n",
            "Iteration 52, loss = 0.55246050\n",
            "Iteration 53, loss = 0.57863385\n",
            "Iteration 54, loss = 0.54073893\n",
            "Iteration 55, loss = 0.54647113\n",
            "Iteration 56, loss = 0.55110190\n",
            "Iteration 57, loss = 0.53912706\n",
            "Iteration 58, loss = 0.55471109\n",
            "Iteration 59, loss = 0.55561529\n",
            "Iteration 60, loss = 0.55637487\n",
            "Iteration 61, loss = 0.53211461\n",
            "Iteration 62, loss = 0.58270291\n",
            "Iteration 63, loss = 0.53886117\n",
            "Iteration 64, loss = 0.52782976\n",
            "Iteration 65, loss = 0.53404977\n",
            "Iteration 66, loss = 0.53246349\n",
            "Iteration 67, loss = 0.52756188\n",
            "Iteration 68, loss = 0.54864224\n",
            "Iteration 69, loss = 0.53066411\n",
            "Iteration 70, loss = 0.54387928\n",
            "Iteration 71, loss = 0.55823770\n",
            "Iteration 72, loss = 0.58238456\n",
            "Iteration 73, loss = 0.53773327\n",
            "Iteration 74, loss = 0.53219310\n",
            "Iteration 75, loss = 0.54213144\n",
            "Iteration 76, loss = 0.57342231\n",
            "Iteration 77, loss = 0.52121844\n",
            "Iteration 78, loss = 0.53494990\n",
            "Iteration 79, loss = 0.52768696\n",
            "Iteration 80, loss = 0.54878634\n",
            "Iteration 81, loss = 0.52899827\n",
            "Iteration 82, loss = 0.52015632\n",
            "Iteration 83, loss = 0.53195901\n",
            "Iteration 84, loss = 0.51641109\n",
            "Iteration 85, loss = 0.51260701\n",
            "Iteration 86, loss = 0.52484292\n",
            "Iteration 87, loss = 0.51876526\n",
            "Iteration 88, loss = 0.52119360\n",
            "Iteration 89, loss = 0.51544124\n",
            "Iteration 90, loss = 0.53602080\n",
            "Iteration 91, loss = 0.51430945\n",
            "Iteration 92, loss = 0.50922502\n",
            "Iteration 93, loss = 0.51836379\n",
            "Iteration 94, loss = 0.52698354\n",
            "Iteration 95, loss = 0.52978360\n",
            "Iteration 96, loss = 0.53846247\n",
            "Iteration 97, loss = 0.52875821\n",
            "Iteration 98, loss = 0.51833275\n",
            "Iteration 99, loss = 0.55608680\n",
            "Iteration 100, loss = 0.53663806\n",
            "Iteration 101, loss = 0.51583082\n",
            "Iteration 102, loss = 0.51606110\n",
            "Iteration 103, loss = 0.52353304\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.37957849\n",
            "Iteration 2, loss = 0.93857811\n",
            "Iteration 3, loss = 0.73924048\n",
            "Iteration 4, loss = 0.75930610\n",
            "Iteration 5, loss = 0.64602314\n",
            "Iteration 6, loss = 0.60430077\n",
            "Iteration 7, loss = 0.60384411\n",
            "Iteration 8, loss = 0.58694119\n",
            "Iteration 9, loss = 0.58243317\n",
            "Iteration 10, loss = 0.57682368\n",
            "Iteration 11, loss = 0.58302012\n",
            "Iteration 12, loss = 0.57591393\n",
            "Iteration 13, loss = 0.57240099\n",
            "Iteration 14, loss = 0.56792282\n",
            "Iteration 15, loss = 0.56135375\n",
            "Iteration 16, loss = 0.56490143\n",
            "Iteration 17, loss = 0.55533817\n",
            "Iteration 18, loss = 0.55579033\n",
            "Iteration 19, loss = 0.56108658\n",
            "Iteration 20, loss = 0.56079115\n",
            "Iteration 21, loss = 0.55909931\n",
            "Iteration 22, loss = 0.55456826\n",
            "Iteration 23, loss = 0.56435669\n",
            "Iteration 24, loss = 0.54086096\n",
            "Iteration 25, loss = 0.53703946\n",
            "Iteration 26, loss = 0.53405458\n",
            "Iteration 27, loss = 0.53048412\n",
            "Iteration 28, loss = 0.53147996\n",
            "Iteration 29, loss = 0.52767008\n",
            "Iteration 30, loss = 0.52540569\n",
            "Iteration 31, loss = 0.52269120\n",
            "Iteration 32, loss = 0.52297264\n",
            "Iteration 33, loss = 0.52591835\n",
            "Iteration 34, loss = 0.51919650\n",
            "Iteration 35, loss = 0.51295974\n",
            "Iteration 36, loss = 0.51582789\n",
            "Iteration 37, loss = 0.51741294\n",
            "Iteration 38, loss = 0.50924752\n",
            "Iteration 39, loss = 0.51683958\n",
            "Iteration 40, loss = 0.50565314\n",
            "Iteration 41, loss = 0.50444151\n",
            "Iteration 42, loss = 0.51479077\n",
            "Iteration 43, loss = 0.50556663\n",
            "Iteration 44, loss = 0.50335789\n",
            "Iteration 45, loss = 0.50050082\n",
            "Iteration 46, loss = 0.49578857\n",
            "Iteration 47, loss = 0.48928877\n",
            "Iteration 48, loss = 0.50059633\n",
            "Iteration 49, loss = 0.49188538\n",
            "Iteration 50, loss = 0.48464644\n",
            "Iteration 51, loss = 0.48239127\n",
            "Iteration 52, loss = 0.51089240\n",
            "Iteration 53, loss = 0.49691421\n",
            "Iteration 54, loss = 0.49105939\n",
            "Iteration 55, loss = 0.49530304\n",
            "Iteration 56, loss = 0.47345312\n",
            "Iteration 57, loss = 0.47687460\n",
            "Iteration 58, loss = 0.47130026\n",
            "Iteration 59, loss = 0.46744685\n",
            "Iteration 60, loss = 0.47833135\n",
            "Iteration 61, loss = 0.48404548\n",
            "Iteration 62, loss = 0.46506743\n",
            "Iteration 63, loss = 0.46453054\n",
            "Iteration 64, loss = 0.46624674\n",
            "Iteration 65, loss = 0.45882284\n",
            "Iteration 66, loss = 0.45589818\n",
            "Iteration 67, loss = 0.45936386\n",
            "Iteration 68, loss = 0.45924643\n",
            "Iteration 69, loss = 0.46811019\n",
            "Iteration 70, loss = 0.45305960\n",
            "Iteration 71, loss = 0.45485073\n",
            "Iteration 72, loss = 0.44817245\n",
            "Iteration 73, loss = 0.44594283\n",
            "Iteration 74, loss = 0.44922969\n",
            "Iteration 75, loss = 0.44648832\n",
            "Iteration 76, loss = 0.44592756\n",
            "Iteration 77, loss = 0.44220685\n",
            "Iteration 78, loss = 0.45536535\n",
            "Iteration 79, loss = 0.47044630\n",
            "Iteration 80, loss = 0.45143227\n",
            "Iteration 81, loss = 0.45278615\n",
            "Iteration 82, loss = 0.44144631\n",
            "Iteration 83, loss = 0.43896408\n",
            "Iteration 84, loss = 0.44258546\n",
            "Iteration 85, loss = 0.43340456\n",
            "Iteration 86, loss = 0.43883869\n",
            "Iteration 87, loss = 0.43302010\n",
            "Iteration 88, loss = 0.43670208\n",
            "Iteration 89, loss = 0.43534380\n",
            "Iteration 90, loss = 0.43192970\n",
            "Iteration 91, loss = 0.42955622\n",
            "Iteration 92, loss = 0.42212990\n",
            "Iteration 93, loss = 0.45170340\n",
            "Iteration 94, loss = 0.42463396\n",
            "Iteration 95, loss = 0.42325539\n",
            "Iteration 96, loss = 0.42363301\n",
            "Iteration 97, loss = 0.42245492\n",
            "Iteration 98, loss = 0.42016692\n",
            "Iteration 99, loss = 0.41793032\n",
            "Iteration 100, loss = 0.43100388\n",
            "Iteration 101, loss = 0.44327301\n",
            "Iteration 102, loss = 0.42591042\n",
            "Iteration 103, loss = 0.44860521\n",
            "Iteration 104, loss = 0.45576341\n",
            "Iteration 105, loss = 0.43372587\n",
            "Iteration 106, loss = 0.44537334\n",
            "Iteration 107, loss = 0.43041521\n",
            "Iteration 108, loss = 0.43004480\n",
            "Iteration 109, loss = 0.42153940\n",
            "Iteration 110, loss = 0.41405828\n",
            "Iteration 111, loss = 0.41708541\n",
            "Iteration 112, loss = 0.41332854\n",
            "Iteration 113, loss = 0.42372584\n",
            "Iteration 114, loss = 0.42187649\n",
            "Iteration 115, loss = 0.41994544\n",
            "Iteration 116, loss = 0.41082324\n",
            "Iteration 117, loss = 0.41365985\n",
            "Iteration 118, loss = 0.41082004\n",
            "Iteration 119, loss = 0.40802267\n",
            "Iteration 120, loss = 0.41509397\n",
            "Iteration 121, loss = 0.41523940\n",
            "Iteration 122, loss = 0.41057622\n",
            "Iteration 123, loss = 0.41103912\n",
            "Iteration 124, loss = 0.42015130\n",
            "Iteration 125, loss = 0.41585940\n",
            "Iteration 126, loss = 0.42460675\n",
            "Iteration 127, loss = 0.40910945\n",
            "Iteration 128, loss = 0.41284146\n",
            "Iteration 129, loss = 0.41566487\n",
            "Iteration 130, loss = 0.41653276\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.93958925\n",
            "Iteration 2, loss = 2.50266272\n",
            "Iteration 3, loss = 1.20753958\n",
            "Iteration 4, loss = 0.79662921\n",
            "Iteration 5, loss = 0.63288594\n",
            "Iteration 6, loss = 0.63518957\n",
            "Iteration 7, loss = 0.60434729\n",
            "Iteration 8, loss = 0.58980333\n",
            "Iteration 9, loss = 0.58075896\n",
            "Iteration 10, loss = 0.58313626\n",
            "Iteration 11, loss = 0.58300731\n",
            "Iteration 12, loss = 0.57343709\n",
            "Iteration 13, loss = 0.57097444\n",
            "Iteration 14, loss = 0.56761986\n",
            "Iteration 15, loss = 0.56515372\n",
            "Iteration 16, loss = 0.56904909\n",
            "Iteration 17, loss = 0.56140240\n",
            "Iteration 18, loss = 0.55918529\n",
            "Iteration 19, loss = 0.55849146\n",
            "Iteration 20, loss = 0.55839794\n",
            "Iteration 21, loss = 0.55436799\n",
            "Iteration 22, loss = 0.55181864\n",
            "Iteration 23, loss = 0.55440002\n",
            "Iteration 24, loss = 0.55305437\n",
            "Iteration 25, loss = 0.54743586\n",
            "Iteration 26, loss = 0.54591986\n",
            "Iteration 27, loss = 0.54470672\n",
            "Iteration 28, loss = 0.54405378\n",
            "Iteration 29, loss = 0.54296102\n",
            "Iteration 30, loss = 0.54256978\n",
            "Iteration 31, loss = 0.54248511\n",
            "Iteration 32, loss = 0.53618234\n",
            "Iteration 33, loss = 0.53672436\n",
            "Iteration 34, loss = 0.53429823\n",
            "Iteration 35, loss = 0.53848505\n",
            "Iteration 36, loss = 0.53437950\n",
            "Iteration 37, loss = 0.53582498\n",
            "Iteration 38, loss = 0.53167044\n",
            "Iteration 39, loss = 0.52862257\n",
            "Iteration 40, loss = 0.52357421\n",
            "Iteration 41, loss = 0.53031675\n",
            "Iteration 42, loss = 0.52116742\n",
            "Iteration 43, loss = 0.52580802\n",
            "Iteration 44, loss = 0.51909044\n",
            "Iteration 45, loss = 0.53485085\n",
            "Iteration 46, loss = 0.51715303\n",
            "Iteration 47, loss = 0.52614716\n",
            "Iteration 48, loss = 0.51792546\n",
            "Iteration 49, loss = 0.52168599\n",
            "Iteration 50, loss = 0.51983377\n",
            "Iteration 51, loss = 0.51436137\n",
            "Iteration 52, loss = 0.51639255\n",
            "Iteration 53, loss = 0.51442477\n",
            "Iteration 54, loss = 0.50681281\n",
            "Iteration 55, loss = 0.50458045\n",
            "Iteration 56, loss = 0.50609033\n",
            "Iteration 57, loss = 0.49780756\n",
            "Iteration 58, loss = 0.51535270\n",
            "Iteration 59, loss = 0.50523657\n",
            "Iteration 60, loss = 0.51727937\n",
            "Iteration 61, loss = 0.52616400\n",
            "Iteration 62, loss = 0.51413102\n",
            "Iteration 63, loss = 0.50163492\n",
            "Iteration 64, loss = 0.48969774\n",
            "Iteration 65, loss = 0.50212502\n",
            "Iteration 66, loss = 0.48493472\n",
            "Iteration 67, loss = 0.47942758\n",
            "Iteration 68, loss = 0.47764702\n",
            "Iteration 69, loss = 0.47901175\n",
            "Iteration 70, loss = 0.47714283\n",
            "Iteration 71, loss = 0.47888585\n",
            "Iteration 72, loss = 0.47245597\n",
            "Iteration 73, loss = 0.47088654\n",
            "Iteration 74, loss = 0.47842660\n",
            "Iteration 75, loss = 0.48220498\n",
            "Iteration 76, loss = 0.49918141\n",
            "Iteration 77, loss = 0.47084565\n",
            "Iteration 78, loss = 0.46137264\n",
            "Iteration 79, loss = 0.45955757\n",
            "Iteration 80, loss = 0.45972937\n",
            "Iteration 81, loss = 0.45602336\n",
            "Iteration 82, loss = 0.46119910\n",
            "Iteration 83, loss = 0.45477108\n",
            "Iteration 84, loss = 0.45905132\n",
            "Iteration 85, loss = 0.47107865\n",
            "Iteration 86, loss = 0.46065418\n",
            "Iteration 87, loss = 0.45317785\n",
            "Iteration 88, loss = 0.44635340\n",
            "Iteration 89, loss = 0.45048278\n",
            "Iteration 90, loss = 0.44506430\n",
            "Iteration 91, loss = 0.44806360\n",
            "Iteration 92, loss = 0.44591360\n",
            "Iteration 93, loss = 0.44242684\n",
            "Iteration 94, loss = 0.44604096\n",
            "Iteration 95, loss = 0.43961804\n",
            "Iteration 96, loss = 0.44370893\n",
            "Iteration 97, loss = 0.44147039\n",
            "Iteration 98, loss = 0.43847765\n",
            "Iteration 99, loss = 0.43679455\n",
            "Iteration 100, loss = 0.45007534\n",
            "Iteration 101, loss = 0.47332515\n",
            "Iteration 102, loss = 0.49926151\n",
            "Iteration 103, loss = 0.46934235\n",
            "Iteration 104, loss = 0.45945330\n",
            "Iteration 105, loss = 0.44174079\n",
            "Iteration 106, loss = 0.43967764\n",
            "Iteration 107, loss = 0.43312554\n",
            "Iteration 108, loss = 0.43330056\n",
            "Iteration 109, loss = 0.42946688\n",
            "Iteration 110, loss = 0.43458985\n",
            "Iteration 111, loss = 0.43050371\n",
            "Iteration 112, loss = 0.42663461\n",
            "Iteration 113, loss = 0.42967725\n",
            "Iteration 114, loss = 0.42786430\n",
            "Iteration 115, loss = 0.42649085\n",
            "Iteration 116, loss = 0.42689853\n",
            "Iteration 117, loss = 0.43015438\n",
            "Iteration 118, loss = 0.43408834\n",
            "Iteration 119, loss = 0.44341577\n",
            "Iteration 120, loss = 0.43291989\n",
            "Iteration 121, loss = 0.45303326\n",
            "Iteration 122, loss = 0.43104373\n",
            "Iteration 123, loss = 0.43165524\n",
            "Iteration 124, loss = 0.42110933\n",
            "Iteration 125, loss = 0.41924188\n",
            "Iteration 126, loss = 0.42477269\n",
            "Iteration 127, loss = 0.42390781\n",
            "Iteration 128, loss = 0.42566824\n",
            "Iteration 129, loss = 0.42837735\n",
            "Iteration 130, loss = 0.42471508\n",
            "Iteration 131, loss = 0.41925262\n",
            "Iteration 132, loss = 0.41875520\n",
            "Iteration 133, loss = 0.41544980\n",
            "Iteration 134, loss = 0.41833243\n",
            "Iteration 135, loss = 0.43284986\n",
            "Iteration 136, loss = 0.41642523\n",
            "Iteration 137, loss = 0.41482873\n",
            "Iteration 138, loss = 0.41366208\n",
            "Iteration 139, loss = 0.41351487\n",
            "Iteration 140, loss = 0.41986333\n",
            "Iteration 141, loss = 0.45388364\n",
            "Iteration 142, loss = 0.42777689\n",
            "Iteration 143, loss = 0.42089086\n",
            "Iteration 144, loss = 0.42771418\n",
            "Iteration 145, loss = 0.42235118\n",
            "Iteration 146, loss = 0.42213836\n",
            "Iteration 147, loss = 0.43084348\n",
            "Iteration 148, loss = 0.41890159\n",
            "Iteration 149, loss = 0.40927578\n",
            "Iteration 150, loss = 0.40885933\n",
            "Iteration 151, loss = 0.41022268\n",
            "Iteration 152, loss = 0.40705758\n",
            "Iteration 153, loss = 0.40590713\n",
            "Iteration 154, loss = 0.41023603\n",
            "Iteration 155, loss = 0.41067721\n",
            "Iteration 156, loss = 0.41060920\n",
            "Iteration 157, loss = 0.40596029\n",
            "Iteration 158, loss = 0.40592334\n",
            "Iteration 159, loss = 0.41090238\n",
            "Iteration 160, loss = 0.40330497\n",
            "Iteration 161, loss = 0.40527983\n",
            "Iteration 162, loss = 0.40443661\n",
            "Iteration 163, loss = 0.40311279\n",
            "Iteration 164, loss = 0.40548767\n",
            "Iteration 165, loss = 0.41259690\n",
            "Iteration 166, loss = 0.41367699\n",
            "Iteration 167, loss = 0.42567666\n",
            "Iteration 168, loss = 0.42655650\n",
            "Iteration 169, loss = 0.41473018\n",
            "Iteration 170, loss = 0.41163694\n",
            "Iteration 171, loss = 0.40178873\n",
            "Iteration 172, loss = 0.39995144\n",
            "Iteration 173, loss = 0.40199547\n",
            "Iteration 174, loss = 0.40044175\n",
            "Iteration 175, loss = 0.40121996\n",
            "Iteration 176, loss = 0.39877561\n",
            "Iteration 177, loss = 0.40042409\n",
            "Iteration 178, loss = 0.39511576\n",
            "Iteration 179, loss = 0.39833339\n",
            "Iteration 180, loss = 0.39744304\n",
            "Iteration 181, loss = 0.39539873\n",
            "Iteration 182, loss = 0.42266784\n",
            "Iteration 183, loss = 0.41079784\n",
            "Iteration 184, loss = 0.40475636\n",
            "Iteration 185, loss = 0.40501585\n",
            "Iteration 186, loss = 0.39448478\n",
            "Iteration 187, loss = 0.39861719\n",
            "Iteration 188, loss = 0.39330017\n",
            "Iteration 189, loss = 0.39356414\n",
            "Iteration 190, loss = 0.39509531\n",
            "Iteration 191, loss = 0.39544974\n",
            "Iteration 192, loss = 0.39304797\n",
            "Iteration 193, loss = 0.39648898\n",
            "Iteration 194, loss = 0.41169583\n",
            "Iteration 195, loss = 0.39167923\n",
            "Iteration 196, loss = 0.40403677\n",
            "Iteration 197, loss = 0.39630094\n",
            "Iteration 198, loss = 0.41093633\n",
            "Iteration 199, loss = 0.39794332\n",
            "Iteration 200, loss = 0.39983044\n",
            "Iteration 201, loss = 0.39437439\n",
            "Iteration 202, loss = 0.38775687\n",
            "Iteration 203, loss = 0.39452785\n",
            "Iteration 204, loss = 0.38767386\n",
            "Iteration 205, loss = 0.38709702\n",
            "Iteration 206, loss = 0.39035523\n",
            "Iteration 207, loss = 0.38523258\n",
            "Iteration 208, loss = 0.38918413\n",
            "Iteration 209, loss = 0.38873643\n",
            "Iteration 210, loss = 0.39184168\n",
            "Iteration 211, loss = 0.38644448\n",
            "Iteration 212, loss = 0.38521256\n",
            "Iteration 213, loss = 0.38838408\n",
            "Iteration 214, loss = 0.39024102\n",
            "Iteration 215, loss = 0.38415648\n",
            "Iteration 216, loss = 0.38453140\n",
            "Iteration 217, loss = 0.38806720\n",
            "Iteration 218, loss = 0.40098136\n",
            "Iteration 219, loss = 0.39177472\n",
            "Iteration 220, loss = 0.38347038\n",
            "Iteration 221, loss = 0.38990491\n",
            "Iteration 222, loss = 0.38528078\n",
            "Iteration 223, loss = 0.38374581\n",
            "Iteration 224, loss = 0.38332518\n",
            "Iteration 225, loss = 0.39360344\n",
            "Iteration 226, loss = 0.38183192\n",
            "Iteration 227, loss = 0.39007593\n",
            "Iteration 228, loss = 0.44435943\n",
            "Iteration 229, loss = 0.43972286\n",
            "Iteration 230, loss = 0.42841760\n",
            "Iteration 231, loss = 0.40634648\n",
            "Iteration 232, loss = 0.41088426\n",
            "Iteration 233, loss = 0.38511982\n",
            "Iteration 234, loss = 0.39213118\n",
            "Iteration 235, loss = 0.39000852\n",
            "Iteration 236, loss = 0.38808719\n",
            "Iteration 237, loss = 0.40101214\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 5.65928349\n",
            "Iteration 2, loss = 3.54743102\n",
            "Iteration 3, loss = 1.65913133\n",
            "Iteration 4, loss = 1.14098272\n",
            "Iteration 5, loss = 0.84432307\n",
            "Iteration 6, loss = 0.75868986\n",
            "Iteration 7, loss = 0.70710750\n",
            "Iteration 8, loss = 0.65618750\n",
            "Iteration 9, loss = 0.61732569\n",
            "Iteration 10, loss = 0.61339424\n",
            "Iteration 11, loss = 0.60181689\n",
            "Iteration 12, loss = 0.59298707\n",
            "Iteration 13, loss = 0.58792478\n",
            "Iteration 14, loss = 0.58500941\n",
            "Iteration 15, loss = 0.57996184\n",
            "Iteration 16, loss = 0.57289683\n",
            "Iteration 17, loss = 0.56882425\n",
            "Iteration 18, loss = 0.56652758\n",
            "Iteration 19, loss = 0.56550375\n",
            "Iteration 20, loss = 0.56082549\n",
            "Iteration 21, loss = 0.55778919\n",
            "Iteration 22, loss = 0.56378578\n",
            "Iteration 23, loss = 0.55539166\n",
            "Iteration 24, loss = 0.54899337\n",
            "Iteration 25, loss = 0.54807423\n",
            "Iteration 26, loss = 0.54943006\n",
            "Iteration 27, loss = 0.53999016\n",
            "Iteration 28, loss = 0.54233055\n",
            "Iteration 29, loss = 0.54092176\n",
            "Iteration 30, loss = 0.53943431\n",
            "Iteration 31, loss = 0.53395808\n",
            "Iteration 32, loss = 0.53142163\n",
            "Iteration 33, loss = 0.52847267\n",
            "Iteration 34, loss = 0.52535421\n",
            "Iteration 35, loss = 0.52698872\n",
            "Iteration 36, loss = 0.52378299\n",
            "Iteration 37, loss = 0.52638245\n",
            "Iteration 38, loss = 0.51890997\n",
            "Iteration 39, loss = 0.51595221\n",
            "Iteration 40, loss = 0.51481630\n",
            "Iteration 41, loss = 0.51650417\n",
            "Iteration 42, loss = 0.50978515\n",
            "Iteration 43, loss = 0.50562100\n",
            "Iteration 44, loss = 0.50439121\n",
            "Iteration 45, loss = 0.50533627\n",
            "Iteration 46, loss = 0.50068667\n",
            "Iteration 47, loss = 0.49735839\n",
            "Iteration 48, loss = 0.50311920\n",
            "Iteration 49, loss = 0.49521175\n",
            "Iteration 50, loss = 0.50049589\n",
            "Iteration 51, loss = 0.50124814\n",
            "Iteration 52, loss = 0.50000531\n",
            "Iteration 53, loss = 0.48732515\n",
            "Iteration 54, loss = 0.48841800\n",
            "Iteration 55, loss = 0.48738766\n",
            "Iteration 56, loss = 0.48182166\n",
            "Iteration 57, loss = 0.48784424\n",
            "Iteration 58, loss = 0.48809571\n",
            "Iteration 59, loss = 0.48761734\n",
            "Iteration 60, loss = 0.48420732\n",
            "Iteration 61, loss = 0.48989547\n",
            "Iteration 62, loss = 0.47554886\n",
            "Iteration 63, loss = 0.47181938\n",
            "Iteration 64, loss = 0.46536906\n",
            "Iteration 65, loss = 0.46743978\n",
            "Iteration 66, loss = 0.47098956\n",
            "Iteration 67, loss = 0.46929582\n",
            "Iteration 68, loss = 0.46064061\n",
            "Iteration 69, loss = 0.46138183\n",
            "Iteration 70, loss = 0.45912788\n",
            "Iteration 71, loss = 0.45964424\n",
            "Iteration 72, loss = 0.46695215\n",
            "Iteration 73, loss = 0.45705613\n",
            "Iteration 74, loss = 0.45644168\n",
            "Iteration 75, loss = 0.47617560\n",
            "Iteration 76, loss = 0.47386010\n",
            "Iteration 77, loss = 0.46406132\n",
            "Iteration 78, loss = 0.46229611\n",
            "Iteration 79, loss = 0.45349091\n",
            "Iteration 80, loss = 0.45094505\n",
            "Iteration 81, loss = 0.44466120\n",
            "Iteration 82, loss = 0.44154782\n",
            "Iteration 83, loss = 0.44139903\n",
            "Iteration 84, loss = 0.44232958\n",
            "Iteration 85, loss = 0.44429125\n",
            "Iteration 86, loss = 0.43928520\n",
            "Iteration 87, loss = 0.44113087\n",
            "Iteration 88, loss = 0.43780598\n",
            "Iteration 89, loss = 0.43746094\n",
            "Iteration 90, loss = 0.43770624\n",
            "Iteration 91, loss = 0.42865808\n",
            "Iteration 92, loss = 0.43641197\n",
            "Iteration 93, loss = 0.43474703\n",
            "Iteration 94, loss = 0.42959710\n",
            "Iteration 95, loss = 0.43452711\n",
            "Iteration 96, loss = 0.43989289\n",
            "Iteration 97, loss = 0.43212915\n",
            "Iteration 98, loss = 0.43350285\n",
            "Iteration 99, loss = 0.42986851\n",
            "Iteration 100, loss = 0.42678773\n",
            "Iteration 101, loss = 0.42632989\n",
            "Iteration 102, loss = 0.42287458\n",
            "Iteration 103, loss = 0.42598038\n",
            "Iteration 104, loss = 0.41981150\n",
            "Iteration 105, loss = 0.42322003\n",
            "Iteration 106, loss = 0.42578911\n",
            "Iteration 107, loss = 0.42570164\n",
            "Iteration 108, loss = 0.42706651\n",
            "Iteration 109, loss = 0.42296769\n",
            "Iteration 110, loss = 0.42141998\n",
            "Iteration 111, loss = 0.41951921\n",
            "Iteration 112, loss = 0.42626665\n",
            "Iteration 113, loss = 0.42350040\n",
            "Iteration 114, loss = 0.42396446\n",
            "Iteration 115, loss = 0.42397263\n",
            "Iteration 116, loss = 0.42225274\n",
            "Iteration 117, loss = 0.41553726\n",
            "Iteration 118, loss = 0.42334097\n",
            "Iteration 119, loss = 0.41622105\n",
            "Iteration 120, loss = 0.41323670\n",
            "Iteration 121, loss = 0.41545087\n",
            "Iteration 122, loss = 0.41067247\n",
            "Iteration 123, loss = 0.41503355\n",
            "Iteration 124, loss = 0.41758583\n",
            "Iteration 125, loss = 0.41777908\n",
            "Iteration 126, loss = 0.42058860\n",
            "Iteration 127, loss = 0.41148989\n",
            "Iteration 128, loss = 0.41098360\n",
            "Iteration 129, loss = 0.41204036\n",
            "Iteration 130, loss = 0.41015286\n",
            "Iteration 131, loss = 0.42707586\n",
            "Iteration 132, loss = 0.43962112\n",
            "Iteration 133, loss = 0.42720199\n",
            "Iteration 134, loss = 0.42728057\n",
            "Iteration 135, loss = 0.41843884\n",
            "Iteration 136, loss = 0.41963557\n",
            "Iteration 137, loss = 0.41507441\n",
            "Iteration 138, loss = 0.40929661\n",
            "Iteration 139, loss = 0.41019106\n",
            "Iteration 140, loss = 0.41061688\n",
            "Iteration 141, loss = 0.40961850\n",
            "Iteration 142, loss = 0.41721108\n",
            "Iteration 143, loss = 0.41774390\n",
            "Iteration 144, loss = 0.41149290\n",
            "Iteration 145, loss = 0.41921251\n",
            "Iteration 146, loss = 0.40928139\n",
            "Iteration 147, loss = 0.40935235\n",
            "Iteration 148, loss = 0.41374525\n",
            "Iteration 149, loss = 0.40864084\n",
            "Iteration 150, loss = 0.41291911\n",
            "Iteration 151, loss = 0.40694636\n",
            "Iteration 152, loss = 0.40483501\n",
            "Iteration 153, loss = 0.40507920\n",
            "Iteration 154, loss = 0.40714386\n",
            "Iteration 155, loss = 0.40821820\n",
            "Iteration 156, loss = 0.40610364\n",
            "Iteration 157, loss = 0.40634708\n",
            "Iteration 158, loss = 0.41043712\n",
            "Iteration 159, loss = 0.41048349\n",
            "Iteration 160, loss = 0.41184345\n",
            "Iteration 161, loss = 0.40662522\n",
            "Iteration 162, loss = 0.40760384\n",
            "Iteration 163, loss = 0.40525303\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71296505\n",
            "Iteration 2, loss = 0.62431732\n",
            "Iteration 3, loss = 0.62552533\n",
            "Iteration 4, loss = 0.62933795\n",
            "Iteration 5, loss = 0.60841188\n",
            "Iteration 6, loss = 0.60299327\n",
            "Iteration 7, loss = 0.59839835\n",
            "Iteration 8, loss = 0.59065946\n",
            "Iteration 9, loss = 0.58271531\n",
            "Iteration 10, loss = 0.58174812\n",
            "Iteration 11, loss = 0.58975213\n",
            "Iteration 12, loss = 0.57656059\n",
            "Iteration 13, loss = 0.57049204\n",
            "Iteration 14, loss = 0.56558074\n",
            "Iteration 15, loss = 0.56287663\n",
            "Iteration 16, loss = 0.56652381\n",
            "Iteration 17, loss = 0.55648370\n",
            "Iteration 18, loss = 0.55188705\n",
            "Iteration 19, loss = 0.55101385\n",
            "Iteration 20, loss = 0.57264564\n",
            "Iteration 21, loss = 0.56129762\n",
            "Iteration 22, loss = 0.56096815\n",
            "Iteration 23, loss = 0.56009214\n",
            "Iteration 24, loss = 0.55089039\n",
            "Iteration 25, loss = 0.53505559\n",
            "Iteration 26, loss = 0.54093570\n",
            "Iteration 27, loss = 0.53723465\n",
            "Iteration 28, loss = 0.55234443\n",
            "Iteration 29, loss = 0.54136027\n",
            "Iteration 30, loss = 0.52735948\n",
            "Iteration 31, loss = 0.52893988\n",
            "Iteration 32, loss = 0.53419910\n",
            "Iteration 33, loss = 0.52327622\n",
            "Iteration 34, loss = 0.52559429\n",
            "Iteration 35, loss = 0.52759545\n",
            "Iteration 36, loss = 0.53303819\n",
            "Iteration 37, loss = 0.52040984\n",
            "Iteration 38, loss = 0.52309811\n",
            "Iteration 39, loss = 0.51780862\n",
            "Iteration 40, loss = 0.52542122\n",
            "Iteration 41, loss = 0.52948624\n",
            "Iteration 42, loss = 0.51173427\n",
            "Iteration 43, loss = 0.51827266\n",
            "Iteration 44, loss = 0.50612068\n",
            "Iteration 45, loss = 0.50348993\n",
            "Iteration 46, loss = 0.50360100\n",
            "Iteration 47, loss = 0.50247727\n",
            "Iteration 48, loss = 0.49739458\n",
            "Iteration 49, loss = 0.49751972\n",
            "Iteration 50, loss = 0.49034383\n",
            "Iteration 51, loss = 0.49033332\n",
            "Iteration 52, loss = 0.49918632\n",
            "Iteration 53, loss = 0.50205420\n",
            "Iteration 54, loss = 0.50495193\n",
            "Iteration 55, loss = 0.48305872\n",
            "Iteration 56, loss = 0.48638709\n",
            "Iteration 57, loss = 0.48245335\n",
            "Iteration 58, loss = 0.48137116\n",
            "Iteration 59, loss = 0.47967415\n",
            "Iteration 60, loss = 0.48449079\n",
            "Iteration 61, loss = 0.47943890\n",
            "Iteration 62, loss = 0.47276227\n",
            "Iteration 63, loss = 0.47217570\n",
            "Iteration 64, loss = 0.47242896\n",
            "Iteration 65, loss = 0.46852850\n",
            "Iteration 66, loss = 0.46856352\n",
            "Iteration 67, loss = 0.47155130\n",
            "Iteration 68, loss = 0.47429804\n",
            "Iteration 69, loss = 0.46749996\n",
            "Iteration 70, loss = 0.47007131\n",
            "Iteration 71, loss = 0.46776347\n",
            "Iteration 72, loss = 0.47355245\n",
            "Iteration 73, loss = 0.46067105\n",
            "Iteration 74, loss = 0.46164710\n",
            "Iteration 75, loss = 0.46986408\n",
            "Iteration 76, loss = 0.48258849\n",
            "Iteration 77, loss = 0.46516536\n",
            "Iteration 78, loss = 0.45432769\n",
            "Iteration 79, loss = 0.46116656\n",
            "Iteration 80, loss = 0.48528471\n",
            "Iteration 81, loss = 0.49273818\n",
            "Iteration 82, loss = 0.47907245\n",
            "Iteration 83, loss = 0.46239887\n",
            "Iteration 84, loss = 0.46077360\n",
            "Iteration 85, loss = 0.45632496\n",
            "Iteration 86, loss = 0.45684991\n",
            "Iteration 87, loss = 0.45454782\n",
            "Iteration 88, loss = 0.45036199\n",
            "Iteration 89, loss = 0.46122962\n",
            "Iteration 90, loss = 0.45198932\n",
            "Iteration 91, loss = 0.45436477\n",
            "Iteration 92, loss = 0.45232431\n",
            "Iteration 93, loss = 0.45319829\n",
            "Iteration 94, loss = 0.47065479\n",
            "Iteration 95, loss = 0.45276246\n",
            "Iteration 96, loss = 0.44277089\n",
            "Iteration 97, loss = 0.43873784\n",
            "Iteration 98, loss = 0.44463700\n",
            "Iteration 99, loss = 0.44308068\n",
            "Iteration 100, loss = 0.44653487\n",
            "Iteration 101, loss = 0.43124783\n",
            "Iteration 102, loss = 0.44081855\n",
            "Iteration 103, loss = 0.43644136\n",
            "Iteration 104, loss = 0.43624477\n",
            "Iteration 105, loss = 0.44113002\n",
            "Iteration 106, loss = 0.44501847\n",
            "Iteration 107, loss = 0.43879265\n",
            "Iteration 108, loss = 0.43519302\n",
            "Iteration 109, loss = 0.43672478\n",
            "Iteration 110, loss = 0.45211051\n",
            "Iteration 111, loss = 0.43065846\n",
            "Iteration 112, loss = 0.43567956\n",
            "Iteration 113, loss = 0.43073680\n",
            "Iteration 114, loss = 0.44157459\n",
            "Iteration 115, loss = 0.45893508\n",
            "Iteration 116, loss = 0.43271961\n",
            "Iteration 117, loss = 0.43410913\n",
            "Iteration 118, loss = 0.44605242\n",
            "Iteration 119, loss = 0.43194129\n",
            "Iteration 120, loss = 0.43273119\n",
            "Iteration 121, loss = 0.43628191\n",
            "Iteration 122, loss = 0.42960015\n",
            "Iteration 123, loss = 0.42617776\n",
            "Iteration 124, loss = 0.43256713\n",
            "Iteration 125, loss = 0.42550256\n",
            "Iteration 126, loss = 0.43539737\n",
            "Iteration 127, loss = 0.43484834\n",
            "Iteration 128, loss = 0.43694720\n",
            "Iteration 129, loss = 0.43937348\n",
            "Iteration 130, loss = 0.42681451\n",
            "Iteration 131, loss = 0.42333286\n",
            "Iteration 132, loss = 0.42039340\n",
            "Iteration 133, loss = 0.42068478\n",
            "Iteration 134, loss = 0.42268297\n",
            "Iteration 135, loss = 0.42337374\n",
            "Iteration 136, loss = 0.42688002\n",
            "Iteration 137, loss = 0.42242814\n",
            "Iteration 138, loss = 0.41857504\n",
            "Iteration 139, loss = 0.42028533\n",
            "Iteration 140, loss = 0.43086467\n",
            "Iteration 141, loss = 0.42143121\n",
            "Iteration 142, loss = 0.42444585\n",
            "Iteration 143, loss = 0.41776964\n",
            "Iteration 144, loss = 0.42152990\n",
            "Iteration 145, loss = 0.43096268\n",
            "Iteration 146, loss = 0.42731658\n",
            "Iteration 147, loss = 0.43356442\n",
            "Iteration 148, loss = 0.41677947\n",
            "Iteration 149, loss = 0.41556314\n",
            "Iteration 150, loss = 0.41310381\n",
            "Iteration 151, loss = 0.42622480\n",
            "Iteration 152, loss = 0.44276195\n",
            "Iteration 153, loss = 0.41486341\n",
            "Iteration 154, loss = 0.42576283\n",
            "Iteration 155, loss = 0.42346753\n",
            "Iteration 156, loss = 0.42667671\n",
            "Iteration 157, loss = 0.42772013\n",
            "Iteration 158, loss = 0.41718551\n",
            "Iteration 159, loss = 0.42137849\n",
            "Iteration 160, loss = 0.42168586\n",
            "Iteration 161, loss = 0.41627175\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 4.73268130\n",
            "Iteration 2, loss = 2.52083297\n",
            "Iteration 3, loss = 1.10142759\n",
            "Iteration 4, loss = 0.98882480\n",
            "Iteration 5, loss = 0.78137847\n",
            "Iteration 6, loss = 0.74039960\n",
            "Iteration 7, loss = 0.70070915\n",
            "Iteration 8, loss = 0.65868106\n",
            "Iteration 9, loss = 0.64314576\n",
            "Iteration 10, loss = 0.62930898\n",
            "Iteration 11, loss = 0.62397334\n",
            "Iteration 12, loss = 0.61557629\n",
            "Iteration 13, loss = 0.61050647\n",
            "Iteration 14, loss = 0.60644164\n",
            "Iteration 15, loss = 0.60234499\n",
            "Iteration 16, loss = 0.59974039\n",
            "Iteration 17, loss = 0.59533291\n",
            "Iteration 18, loss = 0.59237486\n",
            "Iteration 19, loss = 0.58715025\n",
            "Iteration 20, loss = 0.58758453\n",
            "Iteration 21, loss = 0.58493659\n",
            "Iteration 22, loss = 0.58471706\n",
            "Iteration 23, loss = 0.58000527\n",
            "Iteration 24, loss = 0.57667158\n",
            "Iteration 25, loss = 0.57375612\n",
            "Iteration 26, loss = 0.57135125\n",
            "Iteration 27, loss = 0.57142186\n",
            "Iteration 28, loss = 0.56829765\n",
            "Iteration 29, loss = 0.56509998\n",
            "Iteration 30, loss = 0.56153657\n",
            "Iteration 31, loss = 0.56174001\n",
            "Iteration 32, loss = 0.56411778\n",
            "Iteration 33, loss = 0.55847046\n",
            "Iteration 34, loss = 0.55780209\n",
            "Iteration 35, loss = 0.55278692\n",
            "Iteration 36, loss = 0.54956710\n",
            "Iteration 37, loss = 0.54844604\n",
            "Iteration 38, loss = 0.54844112\n",
            "Iteration 39, loss = 0.54590561\n",
            "Iteration 40, loss = 0.54432875\n",
            "Iteration 41, loss = 0.54741956\n",
            "Iteration 42, loss = 0.55481252\n",
            "Iteration 43, loss = 0.55533451\n",
            "Iteration 44, loss = 0.53820751\n",
            "Iteration 45, loss = 0.53406687\n",
            "Iteration 46, loss = 0.53485633\n",
            "Iteration 47, loss = 0.53340289\n",
            "Iteration 48, loss = 0.52820002\n",
            "Iteration 49, loss = 0.52714160\n",
            "Iteration 50, loss = 0.52774473\n",
            "Iteration 51, loss = 0.52649032\n",
            "Iteration 52, loss = 0.52321032\n",
            "Iteration 53, loss = 0.52046045\n",
            "Iteration 54, loss = 0.51923938\n",
            "Iteration 55, loss = 0.51700633\n",
            "Iteration 56, loss = 0.51886870\n",
            "Iteration 57, loss = 0.52542790\n",
            "Iteration 58, loss = 0.51644223\n",
            "Iteration 59, loss = 0.51455254\n",
            "Iteration 60, loss = 0.52800023\n",
            "Iteration 61, loss = 0.53324709\n",
            "Iteration 62, loss = 0.54679929\n",
            "Iteration 63, loss = 0.52525148\n",
            "Iteration 64, loss = 0.52644797\n",
            "Iteration 65, loss = 0.53388120\n",
            "Iteration 66, loss = 0.50580168\n",
            "Iteration 67, loss = 0.50901404\n",
            "Iteration 68, loss = 0.49828798\n",
            "Iteration 69, loss = 0.50488834\n",
            "Iteration 70, loss = 0.49880734\n",
            "Iteration 71, loss = 0.49285642\n",
            "Iteration 72, loss = 0.49508932\n",
            "Iteration 73, loss = 0.49910687\n",
            "Iteration 74, loss = 0.49367707\n",
            "Iteration 75, loss = 0.49441781\n",
            "Iteration 76, loss = 0.49661865\n",
            "Iteration 77, loss = 0.48681454\n",
            "Iteration 78, loss = 0.48635966\n",
            "Iteration 79, loss = 0.48465123\n",
            "Iteration 80, loss = 0.48129508\n",
            "Iteration 81, loss = 0.48414274\n",
            "Iteration 82, loss = 0.48739010\n",
            "Iteration 83, loss = 0.48408604\n",
            "Iteration 84, loss = 0.47903678\n",
            "Iteration 85, loss = 0.48340337\n",
            "Iteration 86, loss = 0.48028611\n",
            "Iteration 87, loss = 0.47356440\n",
            "Iteration 88, loss = 0.47377402\n",
            "Iteration 89, loss = 0.47577456\n",
            "Iteration 90, loss = 0.47498159\n",
            "Iteration 91, loss = 0.47222831\n",
            "Iteration 92, loss = 0.47009079\n",
            "Iteration 93, loss = 0.47507133\n",
            "Iteration 94, loss = 0.46848816\n",
            "Iteration 95, loss = 0.46760378\n",
            "Iteration 96, loss = 0.46576418\n",
            "Iteration 97, loss = 0.46335369\n",
            "Iteration 98, loss = 0.46368715\n",
            "Iteration 99, loss = 0.46526361\n",
            "Iteration 100, loss = 0.47237454\n",
            "Iteration 101, loss = 0.46443992\n",
            "Iteration 102, loss = 0.46211948\n",
            "Iteration 103, loss = 0.46210912\n",
            "Iteration 104, loss = 0.45882096\n",
            "Iteration 105, loss = 0.45724496\n",
            "Iteration 106, loss = 0.45870297\n",
            "Iteration 107, loss = 0.45626868\n",
            "Iteration 108, loss = 0.45664420\n",
            "Iteration 109, loss = 0.45505182\n",
            "Iteration 110, loss = 0.45486965\n",
            "Iteration 111, loss = 0.45491360\n",
            "Iteration 112, loss = 0.45035794\n",
            "Iteration 113, loss = 0.45497471\n",
            "Iteration 114, loss = 0.45372162\n",
            "Iteration 115, loss = 0.45156319\n",
            "Iteration 116, loss = 0.44930452\n",
            "Iteration 117, loss = 0.45314382\n",
            "Iteration 118, loss = 0.44712390\n",
            "Iteration 119, loss = 0.44867053\n",
            "Iteration 120, loss = 0.44719596\n",
            "Iteration 121, loss = 0.44585732\n",
            "Iteration 122, loss = 0.44490879\n",
            "Iteration 123, loss = 0.44634338\n",
            "Iteration 124, loss = 0.44582796\n",
            "Iteration 125, loss = 0.44538413\n",
            "Iteration 126, loss = 0.44674767\n",
            "Iteration 127, loss = 0.44801564\n",
            "Iteration 128, loss = 0.44765182\n",
            "Iteration 129, loss = 0.44714719\n",
            "Iteration 130, loss = 0.44499337\n",
            "Iteration 131, loss = 0.44914773\n",
            "Iteration 132, loss = 0.43941975\n",
            "Iteration 133, loss = 0.43847253\n",
            "Iteration 134, loss = 0.44317207\n",
            "Iteration 135, loss = 0.43978248\n",
            "Iteration 136, loss = 0.43881623\n",
            "Iteration 137, loss = 0.43801418\n",
            "Iteration 138, loss = 0.44463469\n",
            "Iteration 139, loss = 0.43577023\n",
            "Iteration 140, loss = 0.43658414\n",
            "Iteration 141, loss = 0.43914634\n",
            "Iteration 142, loss = 0.44180053\n",
            "Iteration 143, loss = 0.43893576\n",
            "Iteration 144, loss = 0.43961588\n",
            "Iteration 145, loss = 0.43737104\n",
            "Iteration 146, loss = 0.43151710\n",
            "Iteration 147, loss = 0.43195742\n",
            "Iteration 148, loss = 0.43070611\n",
            "Iteration 149, loss = 0.43170090\n",
            "Iteration 150, loss = 0.43249004\n",
            "Iteration 151, loss = 0.43973634\n",
            "Iteration 152, loss = 0.43705362\n",
            "Iteration 153, loss = 0.43507994\n",
            "Iteration 154, loss = 0.43692937\n",
            "Iteration 155, loss = 0.43384562\n",
            "Iteration 156, loss = 0.42933363\n",
            "Iteration 157, loss = 0.42994152\n",
            "Iteration 158, loss = 0.42507274\n",
            "Iteration 159, loss = 0.43466478\n",
            "Iteration 160, loss = 0.43647547\n",
            "Iteration 161, loss = 0.42700773\n",
            "Iteration 162, loss = 0.42598233\n",
            "Iteration 163, loss = 0.42481612\n",
            "Iteration 164, loss = 0.42402364\n",
            "Iteration 165, loss = 0.42412749\n",
            "Iteration 166, loss = 0.42405423\n",
            "Iteration 167, loss = 0.42997689\n",
            "Iteration 168, loss = 0.42253405\n",
            "Iteration 169, loss = 0.42644626\n",
            "Iteration 170, loss = 0.42208143\n",
            "Iteration 171, loss = 0.42007814\n",
            "Iteration 172, loss = 0.43756553\n",
            "Iteration 173, loss = 0.43786783\n",
            "Iteration 174, loss = 0.42987889\n",
            "Iteration 175, loss = 0.42706704\n",
            "Iteration 176, loss = 0.42445646\n",
            "Iteration 177, loss = 0.42141208\n",
            "Iteration 178, loss = 0.42218608\n",
            "Iteration 179, loss = 0.41706410\n",
            "Iteration 180, loss = 0.41885207\n",
            "Iteration 181, loss = 0.41994522\n",
            "Iteration 182, loss = 0.41749597\n",
            "Iteration 183, loss = 0.42218406\n",
            "Iteration 184, loss = 0.42156345\n",
            "Iteration 185, loss = 0.42054233\n",
            "Iteration 186, loss = 0.41496436\n",
            "Iteration 187, loss = 0.41426453\n",
            "Iteration 188, loss = 0.41420064\n",
            "Iteration 189, loss = 0.41426039\n",
            "Iteration 190, loss = 0.41254489\n",
            "Iteration 191, loss = 0.41425273\n",
            "Iteration 192, loss = 0.41376277\n",
            "Iteration 193, loss = 0.41584668\n",
            "Iteration 194, loss = 0.41388495\n",
            "Iteration 195, loss = 0.41761399\n",
            "Iteration 196, loss = 0.43034357\n",
            "Iteration 197, loss = 0.42752508\n",
            "Iteration 198, loss = 0.41958145\n",
            "Iteration 199, loss = 0.42534773\n",
            "Iteration 200, loss = 0.41285840\n",
            "Iteration 201, loss = 0.41536269\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66738062\n",
            "Iteration 2, loss = 0.63967934\n",
            "Iteration 3, loss = 0.62300674\n",
            "Iteration 4, loss = 0.61069108\n",
            "Iteration 5, loss = 0.60394833\n",
            "Iteration 6, loss = 0.59816924\n",
            "Iteration 7, loss = 0.59726736\n",
            "Iteration 8, loss = 0.59480521\n",
            "Iteration 9, loss = 0.59551302\n",
            "Iteration 10, loss = 0.58946738\n",
            "Iteration 11, loss = 0.58750635\n",
            "Iteration 12, loss = 0.58539628\n",
            "Iteration 13, loss = 0.58407908\n",
            "Iteration 14, loss = 0.58101449\n",
            "Iteration 15, loss = 0.57772936\n",
            "Iteration 16, loss = 0.57491894\n",
            "Iteration 17, loss = 0.57100746\n",
            "Iteration 18, loss = 0.57047220\n",
            "Iteration 19, loss = 0.56589648\n",
            "Iteration 20, loss = 0.56055308\n",
            "Iteration 21, loss = 0.55867084\n",
            "Iteration 22, loss = 0.55572350\n",
            "Iteration 23, loss = 0.54978698\n",
            "Iteration 24, loss = 0.54660316\n",
            "Iteration 25, loss = 0.54013298\n",
            "Iteration 26, loss = 0.53757507\n",
            "Iteration 27, loss = 0.54080170\n",
            "Iteration 28, loss = 0.53186183\n",
            "Iteration 29, loss = 0.53016250\n",
            "Iteration 30, loss = 0.52806581\n",
            "Iteration 31, loss = 0.51483006\n",
            "Iteration 32, loss = 0.50966437\n",
            "Iteration 33, loss = 0.50476510\n",
            "Iteration 34, loss = 0.50299639\n",
            "Iteration 35, loss = 0.50253783\n",
            "Iteration 36, loss = 0.49347796\n",
            "Iteration 37, loss = 0.48873157\n",
            "Iteration 38, loss = 0.48402662\n",
            "Iteration 39, loss = 0.48326572\n",
            "Iteration 40, loss = 0.48303067\n",
            "Iteration 41, loss = 0.47338466\n",
            "Iteration 42, loss = 0.47313701\n",
            "Iteration 43, loss = 0.47023894\n",
            "Iteration 44, loss = 0.46925305\n",
            "Iteration 45, loss = 0.46954482\n",
            "Iteration 46, loss = 0.45983109\n",
            "Iteration 47, loss = 0.46220752\n",
            "Iteration 48, loss = 0.45862213\n",
            "Iteration 49, loss = 0.46415231\n",
            "Iteration 50, loss = 0.45187207\n",
            "Iteration 51, loss = 0.45550014\n",
            "Iteration 52, loss = 0.45451919\n",
            "Iteration 53, loss = 0.45432681\n",
            "Iteration 54, loss = 0.45233868\n",
            "Iteration 55, loss = 0.45470990\n",
            "Iteration 56, loss = 0.44661953\n",
            "Iteration 57, loss = 0.44891854\n",
            "Iteration 58, loss = 0.44539169\n",
            "Iteration 59, loss = 0.44614827\n",
            "Iteration 60, loss = 0.45269622\n",
            "Iteration 61, loss = 0.45369383\n",
            "Iteration 62, loss = 0.44697206\n",
            "Iteration 63, loss = 0.44611007\n",
            "Iteration 64, loss = 0.43986277\n",
            "Iteration 65, loss = 0.44109614\n",
            "Iteration 66, loss = 0.43988657\n",
            "Iteration 67, loss = 0.43991469\n",
            "Iteration 68, loss = 0.45157262\n",
            "Iteration 69, loss = 0.43650633\n",
            "Iteration 70, loss = 0.43832311\n",
            "Iteration 71, loss = 0.43697981\n",
            "Iteration 72, loss = 0.44072225\n",
            "Iteration 73, loss = 0.44543902\n",
            "Iteration 74, loss = 0.43852713\n",
            "Iteration 75, loss = 0.43787444\n",
            "Iteration 76, loss = 0.43013097\n",
            "Iteration 77, loss = 0.43356651\n",
            "Iteration 78, loss = 0.43470852\n",
            "Iteration 79, loss = 0.44031816\n",
            "Iteration 80, loss = 0.44661203\n",
            "Iteration 81, loss = 0.43232660\n",
            "Iteration 82, loss = 0.43004451\n",
            "Iteration 83, loss = 0.42946963\n",
            "Iteration 84, loss = 0.42770844\n",
            "Iteration 85, loss = 0.43014353\n",
            "Iteration 86, loss = 0.44190453\n",
            "Iteration 87, loss = 0.43175053\n",
            "Iteration 88, loss = 0.42729169\n",
            "Iteration 89, loss = 0.43217472\n",
            "Iteration 90, loss = 0.42568874\n",
            "Iteration 91, loss = 0.43075354\n",
            "Iteration 92, loss = 0.42895103\n",
            "Iteration 93, loss = 0.42449947\n",
            "Iteration 94, loss = 0.42832065\n",
            "Iteration 95, loss = 0.42690664\n",
            "Iteration 96, loss = 0.42395472\n",
            "Iteration 97, loss = 0.42863952\n",
            "Iteration 98, loss = 0.42809518\n",
            "Iteration 99, loss = 0.42382130\n",
            "Iteration 100, loss = 0.42363024\n",
            "Iteration 101, loss = 0.42233960\n",
            "Iteration 102, loss = 0.42446905\n",
            "Iteration 103, loss = 0.42186419\n",
            "Iteration 104, loss = 0.41981075\n",
            "Iteration 105, loss = 0.42157195\n",
            "Iteration 106, loss = 0.42124823\n",
            "Iteration 107, loss = 0.41832128\n",
            "Iteration 108, loss = 0.42871653\n",
            "Iteration 109, loss = 0.41877554\n",
            "Iteration 110, loss = 0.42955930\n",
            "Iteration 111, loss = 0.42179508\n",
            "Iteration 112, loss = 0.41788432\n",
            "Iteration 113, loss = 0.42050717\n",
            "Iteration 114, loss = 0.41904553\n",
            "Iteration 115, loss = 0.41777841\n",
            "Iteration 116, loss = 0.41849392\n",
            "Iteration 117, loss = 0.42128768\n",
            "Iteration 118, loss = 0.42221247\n",
            "Iteration 119, loss = 0.41966936\n",
            "Iteration 120, loss = 0.42338633\n",
            "Iteration 121, loss = 0.41457469\n",
            "Iteration 122, loss = 0.41522080\n",
            "Iteration 123, loss = 0.41604051\n",
            "Iteration 124, loss = 0.41565978\n",
            "Iteration 125, loss = 0.41585289\n",
            "Iteration 126, loss = 0.41423344\n",
            "Iteration 127, loss = 0.41263489\n",
            "Iteration 128, loss = 0.41415451\n",
            "Iteration 129, loss = 0.41844062\n",
            "Iteration 130, loss = 0.41031675\n",
            "Iteration 131, loss = 0.41441070\n",
            "Iteration 132, loss = 0.41259971\n",
            "Iteration 133, loss = 0.41198161\n",
            "Iteration 134, loss = 0.41161085\n",
            "Iteration 135, loss = 0.41915392\n",
            "Iteration 136, loss = 0.41391251\n",
            "Iteration 137, loss = 0.41263217\n",
            "Iteration 138, loss = 0.42316820\n",
            "Iteration 139, loss = 0.41212479\n",
            "Iteration 140, loss = 0.42091749\n",
            "Iteration 141, loss = 0.41633179\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66297122\n",
            "Iteration 2, loss = 0.64894018\n",
            "Iteration 3, loss = 0.62130537\n",
            "Iteration 4, loss = 0.61082856\n",
            "Iteration 5, loss = 0.60513057\n",
            "Iteration 6, loss = 0.59862714\n",
            "Iteration 7, loss = 0.59957604\n",
            "Iteration 8, loss = 0.59675460\n",
            "Iteration 9, loss = 0.59513580\n",
            "Iteration 10, loss = 0.59480406\n",
            "Iteration 11, loss = 0.59045342\n",
            "Iteration 12, loss = 0.58926392\n",
            "Iteration 13, loss = 0.58790334\n",
            "Iteration 14, loss = 0.58515192\n",
            "Iteration 15, loss = 0.58259800\n",
            "Iteration 16, loss = 0.58607692\n",
            "Iteration 17, loss = 0.57583750\n",
            "Iteration 18, loss = 0.58048450\n",
            "Iteration 19, loss = 0.57216462\n",
            "Iteration 20, loss = 0.56981145\n",
            "Iteration 21, loss = 0.56742623\n",
            "Iteration 22, loss = 0.56544866\n",
            "Iteration 23, loss = 0.56063325\n",
            "Iteration 24, loss = 0.55817632\n",
            "Iteration 25, loss = 0.55273855\n",
            "Iteration 26, loss = 0.54861493\n",
            "Iteration 27, loss = 0.54847092\n",
            "Iteration 28, loss = 0.55036829\n",
            "Iteration 29, loss = 0.54426928\n",
            "Iteration 30, loss = 0.54120917\n",
            "Iteration 31, loss = 0.53456690\n",
            "Iteration 32, loss = 0.52313726\n",
            "Iteration 33, loss = 0.52087564\n",
            "Iteration 34, loss = 0.52091809\n",
            "Iteration 35, loss = 0.51661220\n",
            "Iteration 36, loss = 0.50504024\n",
            "Iteration 37, loss = 0.50137992\n",
            "Iteration 38, loss = 0.49992931\n",
            "Iteration 39, loss = 0.50355074\n",
            "Iteration 40, loss = 0.49236958\n",
            "Iteration 41, loss = 0.48672095\n",
            "Iteration 42, loss = 0.48837709\n",
            "Iteration 43, loss = 0.48024277\n",
            "Iteration 44, loss = 0.47522158\n",
            "Iteration 45, loss = 0.47398928\n",
            "Iteration 46, loss = 0.46484158\n",
            "Iteration 47, loss = 0.46043763\n",
            "Iteration 48, loss = 0.45806674\n",
            "Iteration 49, loss = 0.45527563\n",
            "Iteration 50, loss = 0.45318080\n",
            "Iteration 51, loss = 0.44829268\n",
            "Iteration 52, loss = 0.44736972\n",
            "Iteration 53, loss = 0.44245987\n",
            "Iteration 54, loss = 0.45300036\n",
            "Iteration 55, loss = 0.44050302\n",
            "Iteration 56, loss = 0.44423431\n",
            "Iteration 57, loss = 0.43644004\n",
            "Iteration 58, loss = 0.43885224\n",
            "Iteration 59, loss = 0.43568826\n",
            "Iteration 60, loss = 0.43582339\n",
            "Iteration 61, loss = 0.43146521\n",
            "Iteration 62, loss = 0.43054055\n",
            "Iteration 63, loss = 0.42688600\n",
            "Iteration 64, loss = 0.43013704\n",
            "Iteration 65, loss = 0.42902637\n",
            "Iteration 66, loss = 0.42598369\n",
            "Iteration 67, loss = 0.43735256\n",
            "Iteration 68, loss = 0.42270693\n",
            "Iteration 69, loss = 0.42130102\n",
            "Iteration 70, loss = 0.42321339\n",
            "Iteration 71, loss = 0.41767683\n",
            "Iteration 72, loss = 0.41763412\n",
            "Iteration 73, loss = 0.41727146\n",
            "Iteration 74, loss = 0.41989213\n",
            "Iteration 75, loss = 0.41850907\n",
            "Iteration 76, loss = 0.41995417\n",
            "Iteration 77, loss = 0.41804269\n",
            "Iteration 78, loss = 0.41472076\n",
            "Iteration 79, loss = 0.41963817\n",
            "Iteration 80, loss = 0.41541438\n",
            "Iteration 81, loss = 0.41758737\n",
            "Iteration 82, loss = 0.42316334\n",
            "Iteration 83, loss = 0.41966360\n",
            "Iteration 84, loss = 0.41289525\n",
            "Iteration 85, loss = 0.41267158\n",
            "Iteration 86, loss = 0.41462451\n",
            "Iteration 87, loss = 0.41139775\n",
            "Iteration 88, loss = 0.41463126\n",
            "Iteration 89, loss = 0.41524205\n",
            "Iteration 90, loss = 0.41296719\n",
            "Iteration 91, loss = 0.41210585\n",
            "Iteration 92, loss = 0.41433593\n",
            "Iteration 93, loss = 0.41245380\n",
            "Iteration 94, loss = 0.41154768\n",
            "Iteration 95, loss = 0.41073689\n",
            "Iteration 96, loss = 0.40969901\n",
            "Iteration 97, loss = 0.40734820\n",
            "Iteration 98, loss = 0.40830713\n",
            "Iteration 99, loss = 0.41520127\n",
            "Iteration 100, loss = 0.41453056\n",
            "Iteration 101, loss = 0.41428921\n",
            "Iteration 102, loss = 0.41569488\n",
            "Iteration 103, loss = 0.41017743\n",
            "Iteration 104, loss = 0.40391194\n",
            "Iteration 105, loss = 0.40432306\n",
            "Iteration 106, loss = 0.40616994\n",
            "Iteration 107, loss = 0.40209697\n",
            "Iteration 108, loss = 0.40433545\n",
            "Iteration 109, loss = 0.40377695\n",
            "Iteration 110, loss = 0.42352395\n",
            "Iteration 111, loss = 0.40243834\n",
            "Iteration 112, loss = 0.40666930\n",
            "Iteration 113, loss = 0.40374600\n",
            "Iteration 114, loss = 0.40481168\n",
            "Iteration 115, loss = 0.40848670\n",
            "Iteration 116, loss = 0.40986376\n",
            "Iteration 117, loss = 0.40218014\n",
            "Iteration 118, loss = 0.40556067\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70257660\n",
            "Iteration 2, loss = 0.66382136\n",
            "Iteration 3, loss = 0.64446475\n",
            "Iteration 4, loss = 0.62790394\n",
            "Iteration 5, loss = 0.61418802\n",
            "Iteration 6, loss = 0.60505772\n",
            "Iteration 7, loss = 0.60202859\n",
            "Iteration 8, loss = 0.59753553\n",
            "Iteration 9, loss = 0.59525909\n",
            "Iteration 10, loss = 0.59464409\n",
            "Iteration 11, loss = 0.59047794\n",
            "Iteration 12, loss = 0.59000270\n",
            "Iteration 13, loss = 0.58531594\n",
            "Iteration 14, loss = 0.58302949\n",
            "Iteration 15, loss = 0.58122400\n",
            "Iteration 16, loss = 0.57736073\n",
            "Iteration 17, loss = 0.57339873\n",
            "Iteration 18, loss = 0.57654762\n",
            "Iteration 19, loss = 0.56939347\n",
            "Iteration 20, loss = 0.56596294\n",
            "Iteration 21, loss = 0.56269383\n",
            "Iteration 22, loss = 0.55810449\n",
            "Iteration 23, loss = 0.55537190\n",
            "Iteration 24, loss = 0.54967869\n",
            "Iteration 25, loss = 0.54472905\n",
            "Iteration 26, loss = 0.54463780\n",
            "Iteration 27, loss = 0.53983024\n",
            "Iteration 28, loss = 0.53094341\n",
            "Iteration 29, loss = 0.52458310\n",
            "Iteration 30, loss = 0.52356355\n",
            "Iteration 31, loss = 0.51586648\n",
            "Iteration 32, loss = 0.50859350\n",
            "Iteration 33, loss = 0.50762611\n",
            "Iteration 34, loss = 0.50500543\n",
            "Iteration 35, loss = 0.49474642\n",
            "Iteration 36, loss = 0.48731873\n",
            "Iteration 37, loss = 0.48438077\n",
            "Iteration 38, loss = 0.47649490\n",
            "Iteration 39, loss = 0.47188812\n",
            "Iteration 40, loss = 0.47224075\n",
            "Iteration 41, loss = 0.46353877\n",
            "Iteration 42, loss = 0.45773499\n",
            "Iteration 43, loss = 0.45325410\n",
            "Iteration 44, loss = 0.45523190\n",
            "Iteration 45, loss = 0.45617202\n",
            "Iteration 46, loss = 0.44617211\n",
            "Iteration 47, loss = 0.44515607\n",
            "Iteration 48, loss = 0.43815301\n",
            "Iteration 49, loss = 0.43575865\n",
            "Iteration 50, loss = 0.43259375\n",
            "Iteration 51, loss = 0.43089800\n",
            "Iteration 52, loss = 0.43541275\n",
            "Iteration 53, loss = 0.44807368\n",
            "Iteration 54, loss = 0.42879804\n",
            "Iteration 55, loss = 0.42945775\n",
            "Iteration 56, loss = 0.42667875\n",
            "Iteration 57, loss = 0.42646452\n",
            "Iteration 58, loss = 0.41916381\n",
            "Iteration 59, loss = 0.42113232\n",
            "Iteration 60, loss = 0.41927076\n",
            "Iteration 61, loss = 0.41851893\n",
            "Iteration 62, loss = 0.42147646\n",
            "Iteration 63, loss = 0.41560408\n",
            "Iteration 64, loss = 0.41395145\n",
            "Iteration 65, loss = 0.41888952\n",
            "Iteration 66, loss = 0.41460244\n",
            "Iteration 67, loss = 0.41313520\n",
            "Iteration 68, loss = 0.41157202\n",
            "Iteration 69, loss = 0.41301936\n",
            "Iteration 70, loss = 0.41119167\n",
            "Iteration 71, loss = 0.41279821\n",
            "Iteration 72, loss = 0.41210964\n",
            "Iteration 73, loss = 0.40926203\n",
            "Iteration 74, loss = 0.41444367\n",
            "Iteration 75, loss = 0.41244788\n",
            "Iteration 76, loss = 0.41211800\n",
            "Iteration 77, loss = 0.42302319\n",
            "Iteration 78, loss = 0.41617813\n",
            "Iteration 79, loss = 0.40899510\n",
            "Iteration 80, loss = 0.41004129\n",
            "Iteration 81, loss = 0.40851495\n",
            "Iteration 82, loss = 0.40809679\n",
            "Iteration 83, loss = 0.40945050\n",
            "Iteration 84, loss = 0.40944578\n",
            "Iteration 85, loss = 0.40392948\n",
            "Iteration 86, loss = 0.40725912\n",
            "Iteration 87, loss = 0.40382536\n",
            "Iteration 88, loss = 0.40374815\n",
            "Iteration 89, loss = 0.40775642\n",
            "Iteration 90, loss = 0.41583362\n",
            "Iteration 91, loss = 0.41048106\n",
            "Iteration 92, loss = 0.41067374\n",
            "Iteration 93, loss = 0.40538788\n",
            "Iteration 94, loss = 0.40218039\n",
            "Iteration 95, loss = 0.39948733\n",
            "Iteration 96, loss = 0.39839592\n",
            "Iteration 97, loss = 0.40196580\n",
            "Iteration 98, loss = 0.41294555\n",
            "Iteration 99, loss = 0.40045975\n",
            "Iteration 100, loss = 0.39904908\n",
            "Iteration 101, loss = 0.39670557\n",
            "Iteration 102, loss = 0.39775111\n",
            "Iteration 103, loss = 0.39688652\n",
            "Iteration 104, loss = 0.40060344\n",
            "Iteration 105, loss = 0.40293098\n",
            "Iteration 106, loss = 0.39495331\n",
            "Iteration 107, loss = 0.39584691\n",
            "Iteration 108, loss = 0.39594270\n",
            "Iteration 109, loss = 0.39372936\n",
            "Iteration 110, loss = 0.39352030\n",
            "Iteration 111, loss = 0.39254018\n",
            "Iteration 112, loss = 0.39225361\n",
            "Iteration 113, loss = 0.39396071\n",
            "Iteration 114, loss = 0.39132158\n",
            "Iteration 115, loss = 0.39092376\n",
            "Iteration 116, loss = 0.39145492\n",
            "Iteration 117, loss = 0.39156136\n",
            "Iteration 118, loss = 0.39443992\n",
            "Iteration 119, loss = 0.39584158\n",
            "Iteration 120, loss = 0.39151332\n",
            "Iteration 121, loss = 0.38951664\n",
            "Iteration 122, loss = 0.39126195\n",
            "Iteration 123, loss = 0.38833948\n",
            "Iteration 124, loss = 0.39292542\n",
            "Iteration 125, loss = 0.38811653\n",
            "Iteration 126, loss = 0.38961172\n",
            "Iteration 127, loss = 0.39490955\n",
            "Iteration 128, loss = 0.39503481\n",
            "Iteration 129, loss = 0.40189490\n",
            "Iteration 130, loss = 0.39278723\n",
            "Iteration 131, loss = 0.39515402\n",
            "Iteration 132, loss = 0.38983666\n",
            "Iteration 133, loss = 0.39158573\n",
            "Iteration 134, loss = 0.38806584\n",
            "Iteration 135, loss = 0.38776596\n",
            "Iteration 136, loss = 0.39380276\n",
            "Iteration 137, loss = 0.38298402\n",
            "Iteration 138, loss = 0.38764314\n",
            "Iteration 139, loss = 0.38347360\n",
            "Iteration 140, loss = 0.38509094\n",
            "Iteration 141, loss = 0.38640622\n",
            "Iteration 142, loss = 0.38723992\n",
            "Iteration 143, loss = 0.39132684\n",
            "Iteration 144, loss = 0.39355926\n",
            "Iteration 145, loss = 0.39085695\n",
            "Iteration 146, loss = 0.38945991\n",
            "Iteration 147, loss = 0.38481197\n",
            "Iteration 148, loss = 0.38998409\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66517064\n",
            "Iteration 2, loss = 0.64833397\n",
            "Iteration 3, loss = 0.63431773\n",
            "Iteration 4, loss = 0.62798502\n",
            "Iteration 5, loss = 0.61880717\n",
            "Iteration 6, loss = 0.61591538\n",
            "Iteration 7, loss = 0.61402600\n",
            "Iteration 8, loss = 0.61733707\n",
            "Iteration 9, loss = 0.61089093\n",
            "Iteration 10, loss = 0.60836439\n",
            "Iteration 11, loss = 0.60801660\n",
            "Iteration 12, loss = 0.60344607\n",
            "Iteration 13, loss = 0.60665397\n",
            "Iteration 14, loss = 0.60270912\n",
            "Iteration 15, loss = 0.59714660\n",
            "Iteration 16, loss = 0.59486266\n",
            "Iteration 17, loss = 0.59147539\n",
            "Iteration 18, loss = 0.58768453\n",
            "Iteration 19, loss = 0.58571024\n",
            "Iteration 20, loss = 0.58062805\n",
            "Iteration 21, loss = 0.57564672\n",
            "Iteration 22, loss = 0.57322336\n",
            "Iteration 23, loss = 0.56978467\n",
            "Iteration 24, loss = 0.57227145\n",
            "Iteration 25, loss = 0.55895644\n",
            "Iteration 26, loss = 0.55179866\n",
            "Iteration 27, loss = 0.54978143\n",
            "Iteration 28, loss = 0.53888437\n",
            "Iteration 29, loss = 0.53356338\n",
            "Iteration 30, loss = 0.52779683\n",
            "Iteration 31, loss = 0.52551636\n",
            "Iteration 32, loss = 0.51604676\n",
            "Iteration 33, loss = 0.51855416\n",
            "Iteration 34, loss = 0.50944069\n",
            "Iteration 35, loss = 0.49743891\n",
            "Iteration 36, loss = 0.49662646\n",
            "Iteration 37, loss = 0.49192456\n",
            "Iteration 38, loss = 0.48377410\n",
            "Iteration 39, loss = 0.48207891\n",
            "Iteration 40, loss = 0.47583051\n",
            "Iteration 41, loss = 0.48346925\n",
            "Iteration 42, loss = 0.47062769\n",
            "Iteration 43, loss = 0.47527510\n",
            "Iteration 44, loss = 0.45823259\n",
            "Iteration 45, loss = 0.46484352\n",
            "Iteration 46, loss = 0.45692603\n",
            "Iteration 47, loss = 0.45690242\n",
            "Iteration 48, loss = 0.45461171\n",
            "Iteration 49, loss = 0.45462020\n",
            "Iteration 50, loss = 0.45544186\n",
            "Iteration 51, loss = 0.45583080\n",
            "Iteration 52, loss = 0.45782351\n",
            "Iteration 53, loss = 0.45621342\n",
            "Iteration 54, loss = 0.45184086\n",
            "Iteration 55, loss = 0.45094259\n",
            "Iteration 56, loss = 0.44399079\n",
            "Iteration 57, loss = 0.44414274\n",
            "Iteration 58, loss = 0.44410056\n",
            "Iteration 59, loss = 0.44882613\n",
            "Iteration 60, loss = 0.44174889\n",
            "Iteration 61, loss = 0.44576928\n",
            "Iteration 62, loss = 0.43764480\n",
            "Iteration 63, loss = 0.43756196\n",
            "Iteration 64, loss = 0.44432403\n",
            "Iteration 65, loss = 0.43934194\n",
            "Iteration 66, loss = 0.43569884\n",
            "Iteration 67, loss = 0.43470486\n",
            "Iteration 68, loss = 0.43605517\n",
            "Iteration 69, loss = 0.43757695\n",
            "Iteration 70, loss = 0.43684364\n",
            "Iteration 71, loss = 0.43371983\n",
            "Iteration 72, loss = 0.43240843\n",
            "Iteration 73, loss = 0.43382036\n",
            "Iteration 74, loss = 0.43440682\n",
            "Iteration 75, loss = 0.43804034\n",
            "Iteration 76, loss = 0.43235438\n",
            "Iteration 77, loss = 0.44041567\n",
            "Iteration 78, loss = 0.43352756\n",
            "Iteration 79, loss = 0.43097006\n",
            "Iteration 80, loss = 0.42980855\n",
            "Iteration 81, loss = 0.43223986\n",
            "Iteration 82, loss = 0.43385250\n",
            "Iteration 83, loss = 0.43441793\n",
            "Iteration 84, loss = 0.42937383\n",
            "Iteration 85, loss = 0.43245969\n",
            "Iteration 86, loss = 0.43412493\n",
            "Iteration 87, loss = 0.44002071\n",
            "Iteration 88, loss = 0.42721446\n",
            "Iteration 89, loss = 0.43191180\n",
            "Iteration 90, loss = 0.42726133\n",
            "Iteration 91, loss = 0.43017889\n",
            "Iteration 92, loss = 0.42483489\n",
            "Iteration 93, loss = 0.42411523\n",
            "Iteration 94, loss = 0.42570721\n",
            "Iteration 95, loss = 0.42922376\n",
            "Iteration 96, loss = 0.42642588\n",
            "Iteration 97, loss = 0.43195814\n",
            "Iteration 98, loss = 0.43824037\n",
            "Iteration 99, loss = 0.43116379\n",
            "Iteration 100, loss = 0.44609697\n",
            "Iteration 101, loss = 0.42435612\n",
            "Iteration 102, loss = 0.42406006\n",
            "Iteration 103, loss = 0.42528658\n",
            "Iteration 104, loss = 0.42352628\n",
            "Iteration 105, loss = 0.42689223\n",
            "Iteration 106, loss = 0.43271246\n",
            "Iteration 107, loss = 0.42876673\n",
            "Iteration 108, loss = 0.42307258\n",
            "Iteration 109, loss = 0.42349832\n",
            "Iteration 110, loss = 0.42884933\n",
            "Iteration 111, loss = 0.43128953\n",
            "Iteration 112, loss = 0.42677609\n",
            "Iteration 113, loss = 0.42218099\n",
            "Iteration 114, loss = 0.42955640\n",
            "Iteration 115, loss = 0.42185530\n",
            "Iteration 116, loss = 0.42128091\n",
            "Iteration 117, loss = 0.41965668\n",
            "Iteration 118, loss = 0.42083115\n",
            "Iteration 119, loss = 0.42023033\n",
            "Iteration 120, loss = 0.41935259\n",
            "Iteration 121, loss = 0.41944917\n",
            "Iteration 122, loss = 0.41859236\n",
            "Iteration 123, loss = 0.41875561\n",
            "Iteration 124, loss = 0.42240763\n",
            "Iteration 125, loss = 0.43792805\n",
            "Iteration 126, loss = 0.43412323\n",
            "Iteration 127, loss = 0.42295690\n",
            "Iteration 128, loss = 0.42049092\n",
            "Iteration 129, loss = 0.42798099\n",
            "Iteration 130, loss = 0.41937340\n",
            "Iteration 131, loss = 0.42068253\n",
            "Iteration 132, loss = 0.41902362\n",
            "Iteration 133, loss = 0.41643473\n",
            "Iteration 134, loss = 0.41597416\n",
            "Iteration 135, loss = 0.41498341\n",
            "Iteration 136, loss = 0.41428611\n",
            "Iteration 137, loss = 0.41499895\n",
            "Iteration 138, loss = 0.41465190\n",
            "Iteration 139, loss = 0.41370291\n",
            "Iteration 140, loss = 0.41556586\n",
            "Iteration 141, loss = 0.41774323\n",
            "Iteration 142, loss = 0.41917863\n",
            "Iteration 143, loss = 0.42069858\n",
            "Iteration 144, loss = 0.41593742\n",
            "Iteration 145, loss = 0.41124988\n",
            "Iteration 146, loss = 0.41136311\n",
            "Iteration 147, loss = 0.41579863\n",
            "Iteration 148, loss = 0.41319577\n",
            "Iteration 149, loss = 0.41226439\n",
            "Iteration 150, loss = 0.41839761\n",
            "Iteration 151, loss = 0.41673203\n",
            "Iteration 152, loss = 0.41428709\n",
            "Iteration 153, loss = 0.41477417\n",
            "Iteration 154, loss = 0.41535870\n",
            "Iteration 155, loss = 0.41812173\n",
            "Iteration 156, loss = 0.41426586\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67666050\n",
            "Iteration 2, loss = 0.65355521\n",
            "Iteration 3, loss = 0.63813720\n",
            "Iteration 4, loss = 0.62755254\n",
            "Iteration 5, loss = 0.61925246\n",
            "Iteration 6, loss = 0.61704191\n",
            "Iteration 7, loss = 0.61427143\n",
            "Iteration 8, loss = 0.61090815\n",
            "Iteration 9, loss = 0.60978830\n",
            "Iteration 10, loss = 0.60784920\n",
            "Iteration 11, loss = 0.60651460\n",
            "Iteration 12, loss = 0.60500952\n",
            "Iteration 13, loss = 0.60259035\n",
            "Iteration 14, loss = 0.59890250\n",
            "Iteration 15, loss = 0.60072288\n",
            "Iteration 16, loss = 0.59458937\n",
            "Iteration 17, loss = 0.59435547\n",
            "Iteration 18, loss = 0.58963950\n",
            "Iteration 19, loss = 0.58761977\n",
            "Iteration 20, loss = 0.58329645\n",
            "Iteration 21, loss = 0.57918633\n",
            "Iteration 22, loss = 0.57842554\n",
            "Iteration 23, loss = 0.57350725\n",
            "Iteration 24, loss = 0.57161646\n",
            "Iteration 25, loss = 0.56410698\n",
            "Iteration 26, loss = 0.55992533\n",
            "Iteration 27, loss = 0.55513530\n",
            "Iteration 28, loss = 0.55117004\n",
            "Iteration 29, loss = 0.54687468\n",
            "Iteration 30, loss = 0.54272610\n",
            "Iteration 31, loss = 0.53658608\n",
            "Iteration 32, loss = 0.53080846\n",
            "Iteration 33, loss = 0.52775669\n",
            "Iteration 34, loss = 0.52044513\n",
            "Iteration 35, loss = 0.52064749\n",
            "Iteration 36, loss = 0.51226970\n",
            "Iteration 37, loss = 0.50887273\n",
            "Iteration 38, loss = 0.50278818\n",
            "Iteration 39, loss = 0.50040942\n",
            "Iteration 40, loss = 0.49434621\n",
            "Iteration 41, loss = 0.48942728\n",
            "Iteration 42, loss = 0.48543213\n",
            "Iteration 43, loss = 0.48212390\n",
            "Iteration 44, loss = 0.48049940\n",
            "Iteration 45, loss = 0.47776539\n",
            "Iteration 46, loss = 0.47701218\n",
            "Iteration 47, loss = 0.48389640\n",
            "Iteration 48, loss = 0.47240901\n",
            "Iteration 49, loss = 0.46754489\n",
            "Iteration 50, loss = 0.47211519\n",
            "Iteration 51, loss = 0.46904442\n",
            "Iteration 52, loss = 0.46547733\n",
            "Iteration 53, loss = 0.46261734\n",
            "Iteration 54, loss = 0.46563061\n",
            "Iteration 55, loss = 0.45863533\n",
            "Iteration 56, loss = 0.45952431\n",
            "Iteration 57, loss = 0.46245332\n",
            "Iteration 58, loss = 0.45539193\n",
            "Iteration 59, loss = 0.45451498\n",
            "Iteration 60, loss = 0.46116816\n",
            "Iteration 61, loss = 0.46385527\n",
            "Iteration 62, loss = 0.46086524\n",
            "Iteration 63, loss = 0.45437628\n",
            "Iteration 64, loss = 0.45030633\n",
            "Iteration 65, loss = 0.45328733\n",
            "Iteration 66, loss = 0.44977000\n",
            "Iteration 67, loss = 0.45084575\n",
            "Iteration 68, loss = 0.45233860\n",
            "Iteration 69, loss = 0.45725518\n",
            "Iteration 70, loss = 0.45129323\n",
            "Iteration 71, loss = 0.46019498\n",
            "Iteration 72, loss = 0.44445751\n",
            "Iteration 73, loss = 0.44980111\n",
            "Iteration 74, loss = 0.47205922\n",
            "Iteration 75, loss = 0.44948691\n",
            "Iteration 76, loss = 0.44802412\n",
            "Iteration 77, loss = 0.45298823\n",
            "Iteration 78, loss = 0.45396457\n",
            "Iteration 79, loss = 0.45309294\n",
            "Iteration 80, loss = 0.45316884\n",
            "Iteration 81, loss = 0.44303667\n",
            "Iteration 82, loss = 0.44554163\n",
            "Iteration 83, loss = 0.44376844\n",
            "Iteration 84, loss = 0.44708303\n",
            "Iteration 85, loss = 0.43929374\n",
            "Iteration 86, loss = 0.44390394\n",
            "Iteration 87, loss = 0.44052222\n",
            "Iteration 88, loss = 0.44160430\n",
            "Iteration 89, loss = 0.45228114\n",
            "Iteration 90, loss = 0.43978780\n",
            "Iteration 91, loss = 0.43952697\n",
            "Iteration 92, loss = 0.44242635\n",
            "Iteration 93, loss = 0.43894471\n",
            "Iteration 94, loss = 0.44506949\n",
            "Iteration 95, loss = 0.44906975\n",
            "Iteration 96, loss = 0.45220809\n",
            "Iteration 97, loss = 0.46310151\n",
            "Iteration 98, loss = 0.44881054\n",
            "Iteration 99, loss = 0.43882473\n",
            "Iteration 100, loss = 0.43665254\n",
            "Iteration 101, loss = 0.43777375\n",
            "Iteration 102, loss = 0.43440794\n",
            "Iteration 103, loss = 0.43575957\n",
            "Iteration 104, loss = 0.43438072\n",
            "Iteration 105, loss = 0.43385471\n",
            "Iteration 106, loss = 0.43430968\n",
            "Iteration 107, loss = 0.43636421\n",
            "Iteration 108, loss = 0.43253209\n",
            "Iteration 109, loss = 0.43284622\n",
            "Iteration 110, loss = 0.43348552\n",
            "Iteration 111, loss = 0.44110722\n",
            "Iteration 112, loss = 0.43391705\n",
            "Iteration 113, loss = 0.42950829\n",
            "Iteration 114, loss = 0.42973309\n",
            "Iteration 115, loss = 0.42918022\n",
            "Iteration 116, loss = 0.43072153\n",
            "Iteration 117, loss = 0.42817279\n",
            "Iteration 118, loss = 0.42992075\n",
            "Iteration 119, loss = 0.42797733\n",
            "Iteration 120, loss = 0.42987809\n",
            "Iteration 121, loss = 0.43527532\n",
            "Iteration 122, loss = 0.43076785\n",
            "Iteration 123, loss = 0.42628395\n",
            "Iteration 124, loss = 0.42969528\n",
            "Iteration 125, loss = 0.43157572\n",
            "Iteration 126, loss = 0.43905403\n",
            "Iteration 127, loss = 0.43423509\n",
            "Iteration 128, loss = 0.43550604\n",
            "Iteration 129, loss = 0.42880242\n",
            "Iteration 130, loss = 0.42497120\n",
            "Iteration 131, loss = 0.42740008\n",
            "Iteration 132, loss = 0.42483800\n",
            "Iteration 133, loss = 0.42418989\n",
            "Iteration 134, loss = 0.42323136\n",
            "Iteration 135, loss = 0.42489281\n",
            "Iteration 136, loss = 0.43356885\n",
            "Iteration 137, loss = 0.43175354\n",
            "Iteration 138, loss = 0.44349756\n",
            "Iteration 139, loss = 0.43428843\n",
            "Iteration 140, loss = 0.42272615\n",
            "Iteration 141, loss = 0.42622954\n",
            "Iteration 142, loss = 0.42498338\n",
            "Iteration 143, loss = 0.42676938\n",
            "Iteration 144, loss = 0.42097225\n",
            "Iteration 145, loss = 0.41996574\n",
            "Iteration 146, loss = 0.42113498\n",
            "Iteration 147, loss = 0.41757244\n",
            "Iteration 148, loss = 0.42042880\n",
            "Iteration 149, loss = 0.42108081\n",
            "Iteration 150, loss = 0.41813822\n",
            "Iteration 151, loss = 0.41802298\n",
            "Iteration 152, loss = 0.42118984\n",
            "Iteration 153, loss = 0.42055261\n",
            "Iteration 154, loss = 0.43702645\n",
            "Iteration 155, loss = 0.43086770\n",
            "Iteration 156, loss = 0.42433073\n",
            "Iteration 157, loss = 0.41771969\n",
            "Iteration 158, loss = 0.41339602\n",
            "Iteration 159, loss = 0.41637922\n",
            "Iteration 160, loss = 0.41699297\n",
            "Iteration 161, loss = 0.41881864\n",
            "Iteration 162, loss = 0.41482437\n",
            "Iteration 163, loss = 0.41378494\n",
            "Iteration 164, loss = 0.41375041\n",
            "Iteration 165, loss = 0.41506142\n",
            "Iteration 166, loss = 0.41285322\n",
            "Iteration 167, loss = 0.41275411\n",
            "Iteration 168, loss = 0.41218170\n",
            "Iteration 169, loss = 0.41261058\n",
            "Iteration 170, loss = 0.41228352\n",
            "Iteration 171, loss = 0.41732706\n",
            "Iteration 172, loss = 0.42275438\n",
            "Iteration 173, loss = 0.42351052\n",
            "Iteration 174, loss = 0.41532559\n",
            "Iteration 175, loss = 0.41808632\n",
            "Iteration 176, loss = 0.41539189\n",
            "Iteration 177, loss = 0.41135009\n",
            "Iteration 178, loss = 0.40847812\n",
            "Iteration 179, loss = 0.41194176\n",
            "Iteration 180, loss = 0.41866896\n",
            "Iteration 181, loss = 0.41123332\n",
            "Iteration 182, loss = 0.40807033\n",
            "Iteration 183, loss = 0.41281983\n",
            "Iteration 184, loss = 0.41221334\n",
            "Iteration 185, loss = 0.41449760\n",
            "Iteration 186, loss = 0.40888650\n",
            "Iteration 187, loss = 0.41496158\n",
            "Iteration 188, loss = 0.40822684\n",
            "Iteration 189, loss = 0.42157049\n",
            "Iteration 190, loss = 0.40746060\n",
            "Iteration 191, loss = 0.41295124\n",
            "Iteration 192, loss = 0.40765005\n",
            "Iteration 193, loss = 0.41056006\n",
            "Iteration 194, loss = 0.41559132\n",
            "Iteration 195, loss = 0.40822923\n",
            "Iteration 196, loss = 0.41353474\n",
            "Iteration 197, loss = 0.41054609\n",
            "Iteration 198, loss = 0.40951973\n",
            "Iteration 199, loss = 0.41012910\n",
            "Iteration 200, loss = 0.40487431\n",
            "Iteration 1, loss = 0.67763880\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss = 0.67674147\n",
            "Iteration 3, loss = 0.67559258\n",
            "Iteration 4, loss = 0.67480660\n",
            "Iteration 5, loss = 0.67326318\n",
            "Iteration 6, loss = 0.67256427\n",
            "Iteration 7, loss = 0.67122205\n",
            "Iteration 8, loss = 0.67073105\n",
            "Iteration 9, loss = 0.66871544\n",
            "Iteration 10, loss = 0.66769366\n",
            "Iteration 11, loss = 0.66672397\n",
            "Iteration 12, loss = 0.66593942\n",
            "Iteration 13, loss = 0.66449423\n",
            "Iteration 14, loss = 0.66371927\n",
            "Iteration 15, loss = 0.66244470\n",
            "Iteration 16, loss = 0.66150912\n",
            "Iteration 17, loss = 0.66090968\n",
            "Iteration 18, loss = 0.66003548\n",
            "Iteration 19, loss = 0.65885448\n",
            "Iteration 20, loss = 0.65773150\n",
            "Iteration 21, loss = 0.65686671\n",
            "Iteration 22, loss = 0.65606473\n",
            "Iteration 23, loss = 0.65530735\n",
            "Iteration 24, loss = 0.65423826\n",
            "Iteration 25, loss = 0.65321438\n",
            "Iteration 26, loss = 0.65253767\n",
            "Iteration 27, loss = 0.65157799\n",
            "Iteration 28, loss = 0.65069578\n",
            "Iteration 29, loss = 0.65001592\n",
            "Iteration 30, loss = 0.64893883\n",
            "Iteration 31, loss = 0.64804408\n",
            "Iteration 32, loss = 0.64723393\n",
            "Iteration 33, loss = 0.64636401\n",
            "Iteration 34, loss = 0.64535271\n",
            "Iteration 35, loss = 0.64449369\n",
            "Iteration 36, loss = 0.64355613\n",
            "Iteration 37, loss = 0.64298723\n",
            "Iteration 38, loss = 0.64236335\n",
            "Iteration 39, loss = 0.64168134\n",
            "Iteration 40, loss = 0.64032903\n",
            "Iteration 41, loss = 0.63974737\n",
            "Iteration 42, loss = 0.63872870\n",
            "Iteration 43, loss = 0.63816173\n",
            "Iteration 44, loss = 0.63706657\n",
            "Iteration 45, loss = 0.63653539\n",
            "Iteration 46, loss = 0.63554838\n",
            "Iteration 47, loss = 0.63484295\n",
            "Iteration 48, loss = 0.63445663\n",
            "Iteration 49, loss = 0.63362203\n",
            "Iteration 50, loss = 0.63256184\n",
            "Iteration 51, loss = 0.63180834\n",
            "Iteration 52, loss = 0.63146963\n",
            "Iteration 53, loss = 0.63034806\n",
            "Iteration 54, loss = 0.62967962\n",
            "Iteration 55, loss = 0.62916813\n",
            "Iteration 56, loss = 0.62869181\n",
            "Iteration 57, loss = 0.62759456\n",
            "Iteration 58, loss = 0.62729124\n",
            "Iteration 59, loss = 0.62645377\n",
            "Iteration 60, loss = 0.62611895\n",
            "Iteration 61, loss = 0.62511487\n",
            "Iteration 62, loss = 0.62428626\n",
            "Iteration 63, loss = 0.62366879\n",
            "Iteration 64, loss = 0.62311797\n",
            "Iteration 65, loss = 0.62244436\n",
            "Iteration 66, loss = 0.62215407\n",
            "Iteration 67, loss = 0.62160120\n",
            "Iteration 68, loss = 0.62104240\n",
            "Iteration 69, loss = 0.62022674\n",
            "Iteration 70, loss = 0.62047053\n",
            "Iteration 71, loss = 0.61931004\n",
            "Iteration 72, loss = 0.61902733\n",
            "Iteration 73, loss = 0.61829542\n",
            "Iteration 74, loss = 0.61723366\n",
            "Iteration 75, loss = 0.61781457\n",
            "Iteration 76, loss = 0.61652452\n",
            "Iteration 77, loss = 0.61609601\n",
            "Iteration 78, loss = 0.61527551\n",
            "Iteration 79, loss = 0.61510923\n",
            "Iteration 80, loss = 0.61508567\n",
            "Iteration 81, loss = 0.61407691\n",
            "Iteration 82, loss = 0.61367015\n",
            "Iteration 83, loss = 0.61347988\n",
            "Iteration 84, loss = 0.61264951\n",
            "Iteration 85, loss = 0.61257979\n",
            "Iteration 86, loss = 0.61228321\n",
            "Iteration 87, loss = 0.61218992\n",
            "Iteration 88, loss = 0.61145028\n",
            "Iteration 89, loss = 0.61082572\n",
            "Iteration 90, loss = 0.61080844\n",
            "Iteration 91, loss = 0.61000996\n",
            "Iteration 92, loss = 0.61007738\n",
            "Iteration 93, loss = 0.60974989\n",
            "Iteration 94, loss = 0.60932996\n",
            "Iteration 95, loss = 0.60883806\n",
            "Iteration 96, loss = 0.60846798\n",
            "Iteration 97, loss = 0.60828935\n",
            "Iteration 98, loss = 0.60775933\n",
            "Iteration 99, loss = 0.60772267\n",
            "Iteration 100, loss = 0.60811757\n",
            "Iteration 101, loss = 0.60714902\n",
            "Iteration 102, loss = 0.60699822\n",
            "Iteration 103, loss = 0.60672596\n",
            "Iteration 104, loss = 0.60627555\n",
            "Iteration 105, loss = 0.60623824\n",
            "Iteration 106, loss = 0.60579852\n",
            "Iteration 107, loss = 0.60613768\n",
            "Iteration 108, loss = 0.60539873\n",
            "Iteration 109, loss = 0.60531430\n",
            "Iteration 110, loss = 0.60521530\n",
            "Iteration 111, loss = 0.60482061\n",
            "Iteration 112, loss = 0.60458386\n",
            "Iteration 113, loss = 0.60507503\n",
            "Iteration 114, loss = 0.60408785\n",
            "Iteration 115, loss = 0.60468306\n",
            "Iteration 116, loss = 0.60381750\n",
            "Iteration 117, loss = 0.60405445\n",
            "Iteration 118, loss = 0.60330770\n",
            "Iteration 119, loss = 0.60343982\n",
            "Iteration 120, loss = 0.60330106\n",
            "Iteration 121, loss = 0.60329015\n",
            "Iteration 122, loss = 0.60285401\n",
            "Iteration 123, loss = 0.60309697\n",
            "Iteration 124, loss = 0.60268662\n",
            "Iteration 125, loss = 0.60255500\n",
            "Iteration 126, loss = 0.60262459\n",
            "Iteration 127, loss = 0.60303730\n",
            "Iteration 128, loss = 0.60220846\n",
            "Iteration 129, loss = 0.60205475\n",
            "Iteration 130, loss = 0.60221737\n",
            "Iteration 131, loss = 0.60193197\n",
            "Iteration 132, loss = 0.60177333\n",
            "Iteration 133, loss = 0.60184713\n",
            "Iteration 134, loss = 0.60162555\n",
            "Iteration 135, loss = 0.60144210\n",
            "Iteration 136, loss = 0.60119047\n",
            "Iteration 137, loss = 0.60118406\n",
            "Iteration 138, loss = 0.60123553\n",
            "Iteration 139, loss = 0.60122006\n",
            "Iteration 140, loss = 0.60080438\n",
            "Iteration 141, loss = 0.60077529\n",
            "Iteration 142, loss = 0.60132235\n",
            "Iteration 143, loss = 0.60084211\n",
            "Iteration 144, loss = 0.60117828\n",
            "Iteration 145, loss = 0.60069601\n",
            "Iteration 146, loss = 0.60079666\n",
            "Iteration 147, loss = 0.60109851\n",
            "Iteration 148, loss = 0.60073310\n",
            "Iteration 149, loss = 0.60045566\n",
            "Iteration 150, loss = 0.60096786\n",
            "Iteration 151, loss = 0.60009718\n",
            "Iteration 152, loss = 0.60012649\n",
            "Iteration 153, loss = 0.60100215\n",
            "Iteration 154, loss = 0.60030721\n",
            "Iteration 155, loss = 0.59993138\n",
            "Iteration 156, loss = 0.59994220\n",
            "Iteration 157, loss = 0.60027426\n",
            "Iteration 158, loss = 0.59984968\n",
            "Iteration 159, loss = 0.59996292\n",
            "Iteration 160, loss = 0.59967246\n",
            "Iteration 161, loss = 0.59979888\n",
            "Iteration 162, loss = 0.59975856\n",
            "Iteration 163, loss = 0.59946990\n",
            "Iteration 164, loss = 0.59948139\n",
            "Iteration 165, loss = 0.59941238\n",
            "Iteration 166, loss = 0.59961072\n",
            "Iteration 167, loss = 0.59975861\n",
            "Iteration 168, loss = 0.59942097\n",
            "Iteration 169, loss = 0.60020610\n",
            "Iteration 170, loss = 0.59906889\n",
            "Iteration 171, loss = 0.59931921\n",
            "Iteration 172, loss = 0.59919834\n",
            "Iteration 173, loss = 0.59907253\n",
            "Iteration 174, loss = 0.59938648\n",
            "Iteration 175, loss = 0.59906582\n",
            "Iteration 176, loss = 0.59891074\n",
            "Iteration 177, loss = 0.59909520\n",
            "Iteration 178, loss = 0.59887994\n",
            "Iteration 179, loss = 0.59874980\n",
            "Iteration 180, loss = 0.59882020\n",
            "Iteration 181, loss = 0.59899277\n",
            "Iteration 182, loss = 0.59859442\n",
            "Iteration 183, loss = 0.59875524\n",
            "Iteration 184, loss = 0.59886021\n",
            "Iteration 185, loss = 0.59865872\n",
            "Iteration 186, loss = 0.59850085\n",
            "Iteration 187, loss = 0.59844897\n",
            "Iteration 188, loss = 0.59858798\n",
            "Iteration 189, loss = 0.59872659\n",
            "Iteration 190, loss = 0.59956528\n",
            "Iteration 191, loss = 0.59891051\n",
            "Iteration 192, loss = 0.59858069\n",
            "Iteration 193, loss = 0.59833260\n",
            "Iteration 194, loss = 0.59850714\n",
            "Iteration 195, loss = 0.59839405\n",
            "Iteration 196, loss = 0.59825540\n",
            "Iteration 197, loss = 0.59834334\n",
            "Iteration 198, loss = 0.59822187\n",
            "Iteration 199, loss = 0.59834823\n",
            "Iteration 200, loss = 0.59841055\n",
            "Iteration 201, loss = 0.59880121\n",
            "Iteration 202, loss = 0.59824977\n",
            "Iteration 203, loss = 0.59801996\n",
            "Iteration 204, loss = 0.59797958\n",
            "Iteration 205, loss = 0.59834488\n",
            "Iteration 206, loss = 0.59786845\n",
            "Iteration 207, loss = 0.59786819\n",
            "Iteration 208, loss = 0.59780524\n",
            "Iteration 209, loss = 0.59795845\n",
            "Iteration 210, loss = 0.59781438\n",
            "Iteration 211, loss = 0.59791364\n",
            "Iteration 212, loss = 0.59793495\n",
            "Iteration 213, loss = 0.59751599\n",
            "Iteration 214, loss = 0.59758492\n",
            "Iteration 215, loss = 0.59767915\n",
            "Iteration 216, loss = 0.59745398\n",
            "Iteration 217, loss = 0.59762039\n",
            "Iteration 218, loss = 0.59764462\n",
            "Iteration 219, loss = 0.59796799\n",
            "Iteration 220, loss = 0.59756548\n",
            "Iteration 221, loss = 0.59787101\n",
            "Iteration 222, loss = 0.59770073\n",
            "Iteration 223, loss = 0.59759173\n",
            "Iteration 224, loss = 0.59730144\n",
            "Iteration 225, loss = 0.59717554\n",
            "Iteration 226, loss = 0.59788688\n",
            "Iteration 227, loss = 0.59752656\n",
            "Iteration 228, loss = 0.59718893\n",
            "Iteration 229, loss = 0.59751840\n",
            "Iteration 230, loss = 0.59736795\n",
            "Iteration 231, loss = 0.59717748\n",
            "Iteration 232, loss = 0.59721161\n",
            "Iteration 233, loss = 0.59746875\n",
            "Iteration 234, loss = 0.59710703\n",
            "Iteration 235, loss = 0.59721655\n",
            "Iteration 236, loss = 0.59699803\n",
            "Iteration 237, loss = 0.59707355\n",
            "Iteration 238, loss = 0.59674858\n",
            "Iteration 239, loss = 0.59691212\n",
            "Iteration 240, loss = 0.59674022\n",
            "Iteration 241, loss = 0.59677528\n",
            "Iteration 242, loss = 0.59719958\n",
            "Iteration 243, loss = 0.59676324\n",
            "Iteration 244, loss = 0.59679100\n",
            "Iteration 245, loss = 0.59707025\n",
            "Iteration 246, loss = 0.59718071\n",
            "Iteration 247, loss = 0.59659560\n",
            "Iteration 248, loss = 0.59686867\n",
            "Iteration 249, loss = 0.59666388\n",
            "Iteration 250, loss = 0.59700662\n",
            "Iteration 251, loss = 0.59657790\n",
            "Iteration 252, loss = 0.59643346\n",
            "Iteration 253, loss = 0.59698422\n",
            "Iteration 254, loss = 0.59646232\n",
            "Iteration 255, loss = 0.59662868\n",
            "Iteration 256, loss = 0.59637047\n",
            "Iteration 257, loss = 0.59673220\n",
            "Iteration 258, loss = 0.59624701\n",
            "Iteration 259, loss = 0.59712165\n",
            "Iteration 260, loss = 0.59648174\n",
            "Iteration 261, loss = 0.59653651\n",
            "Iteration 262, loss = 0.59630026\n",
            "Iteration 263, loss = 0.59622252\n",
            "Iteration 264, loss = 0.59627044\n",
            "Iteration 265, loss = 0.59606678\n",
            "Iteration 266, loss = 0.59649719\n",
            "Iteration 267, loss = 0.59607069\n",
            "Iteration 268, loss = 0.59605185\n",
            "Iteration 269, loss = 0.59610816\n",
            "Iteration 270, loss = 0.59626139\n",
            "Iteration 271, loss = 0.59613118\n",
            "Iteration 272, loss = 0.59589601\n",
            "Iteration 273, loss = 0.59627002\n",
            "Iteration 274, loss = 0.59597563\n",
            "Iteration 275, loss = 0.59608892\n",
            "Iteration 276, loss = 0.59565154\n",
            "Iteration 277, loss = 0.59636555\n",
            "Iteration 278, loss = 0.59592555\n",
            "Iteration 279, loss = 0.59564837\n",
            "Iteration 280, loss = 0.59571612\n",
            "Iteration 281, loss = 0.59616370\n",
            "Iteration 282, loss = 0.59627563\n",
            "Iteration 283, loss = 0.59606981\n",
            "Iteration 284, loss = 0.59627211\n",
            "Iteration 285, loss = 0.59548439\n",
            "Iteration 286, loss = 0.59645964\n",
            "Iteration 287, loss = 0.59648291\n",
            "Iteration 288, loss = 0.59559501\n",
            "Iteration 289, loss = 0.59548195\n",
            "Iteration 290, loss = 0.59546440\n",
            "Iteration 291, loss = 0.59563578\n",
            "Iteration 292, loss = 0.59530055\n",
            "Iteration 293, loss = 0.59544640\n",
            "Iteration 294, loss = 0.59563296\n",
            "Iteration 295, loss = 0.59518211\n",
            "Iteration 296, loss = 0.59518011\n",
            "Iteration 297, loss = 0.59518141\n",
            "Iteration 298, loss = 0.59549798\n",
            "Iteration 299, loss = 0.59551457\n",
            "Iteration 300, loss = 0.59515011\n",
            "Iteration 301, loss = 0.59504974\n",
            "Iteration 302, loss = 0.59555276\n",
            "Iteration 303, loss = 0.59553056\n",
            "Iteration 304, loss = 0.59498272\n",
            "Iteration 305, loss = 0.59499533\n",
            "Iteration 306, loss = 0.59516803\n",
            "Iteration 307, loss = 0.59480447\n",
            "Iteration 308, loss = 0.59534215\n",
            "Iteration 309, loss = 0.59463315\n",
            "Iteration 310, loss = 0.59488326\n",
            "Iteration 311, loss = 0.59476754\n",
            "Iteration 312, loss = 0.59464984\n",
            "Iteration 313, loss = 0.59477804\n",
            "Iteration 314, loss = 0.59472318\n",
            "Iteration 315, loss = 0.59474845\n",
            "Iteration 316, loss = 0.59478047\n",
            "Iteration 317, loss = 0.59488784\n",
            "Iteration 318, loss = 0.59490540\n",
            "Iteration 319, loss = 0.59463245\n",
            "Iteration 320, loss = 0.59450324\n",
            "Iteration 321, loss = 0.59462260\n",
            "Iteration 322, loss = 0.59470162\n",
            "Iteration 323, loss = 0.59470440\n",
            "Iteration 324, loss = 0.59444238\n",
            "Iteration 325, loss = 0.59444168\n",
            "Iteration 326, loss = 0.59478554\n",
            "Iteration 327, loss = 0.59449381\n",
            "Iteration 328, loss = 0.59443862\n",
            "Iteration 329, loss = 0.59440903\n",
            "Iteration 330, loss = 0.59437361\n",
            "Iteration 331, loss = 0.59438757\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.77631062\n",
            "Iteration 2, loss = 0.69344014\n",
            "Iteration 3, loss = 0.67933911\n",
            "Iteration 4, loss = 0.67992603\n",
            "Iteration 5, loss = 0.67645622\n",
            "Iteration 6, loss = 0.67519008\n",
            "Iteration 7, loss = 0.67412448\n",
            "Iteration 8, loss = 0.67277500\n",
            "Iteration 9, loss = 0.67232181\n",
            "Iteration 10, loss = 0.67074020\n",
            "Iteration 11, loss = 0.66967143\n",
            "Iteration 12, loss = 0.66911193\n",
            "Iteration 13, loss = 0.66785774\n",
            "Iteration 14, loss = 0.66693041\n",
            "Iteration 15, loss = 0.66625225\n",
            "Iteration 16, loss = 0.66487076\n",
            "Iteration 17, loss = 0.66444040\n",
            "Iteration 18, loss = 0.66366242\n",
            "Iteration 19, loss = 0.66219289\n",
            "Iteration 20, loss = 0.66176047\n",
            "Iteration 21, loss = 0.66086955\n",
            "Iteration 22, loss = 0.66006088\n",
            "Iteration 23, loss = 0.65889814\n",
            "Iteration 24, loss = 0.65828314\n",
            "Iteration 25, loss = 0.65726876\n",
            "Iteration 26, loss = 0.65658124\n",
            "Iteration 27, loss = 0.65568521\n",
            "Iteration 28, loss = 0.65508567\n",
            "Iteration 29, loss = 0.65433510\n",
            "Iteration 30, loss = 0.65325957\n",
            "Iteration 31, loss = 0.65246125\n",
            "Iteration 32, loss = 0.65203767\n",
            "Iteration 33, loss = 0.65097664\n",
            "Iteration 34, loss = 0.65030961\n",
            "Iteration 35, loss = 0.65023615\n",
            "Iteration 36, loss = 0.64865559\n",
            "Iteration 37, loss = 0.64846920\n",
            "Iteration 38, loss = 0.64726753\n",
            "Iteration 39, loss = 0.64675841\n",
            "Iteration 40, loss = 0.64594765\n",
            "Iteration 41, loss = 0.64504451\n",
            "Iteration 42, loss = 0.64452779\n",
            "Iteration 43, loss = 0.64367148\n",
            "Iteration 44, loss = 0.64275047\n",
            "Iteration 45, loss = 0.64207183\n",
            "Iteration 46, loss = 0.64163340\n",
            "Iteration 47, loss = 0.64055043\n",
            "Iteration 48, loss = 0.63991402\n",
            "Iteration 49, loss = 0.63909507\n",
            "Iteration 50, loss = 0.63846175\n",
            "Iteration 51, loss = 0.63769670\n",
            "Iteration 52, loss = 0.63698948\n",
            "Iteration 53, loss = 0.63656833\n",
            "Iteration 54, loss = 0.63566988\n",
            "Iteration 55, loss = 0.63649595\n",
            "Iteration 56, loss = 0.63561696\n",
            "Iteration 57, loss = 0.63354935\n",
            "Iteration 58, loss = 0.63284338\n",
            "Iteration 59, loss = 0.63276502\n",
            "Iteration 60, loss = 0.63197535\n",
            "Iteration 61, loss = 0.63107756\n",
            "Iteration 62, loss = 0.63025735\n",
            "Iteration 63, loss = 0.62991243\n",
            "Iteration 64, loss = 0.62941023\n",
            "Iteration 65, loss = 0.62871993\n",
            "Iteration 66, loss = 0.62778710\n",
            "Iteration 67, loss = 0.62723487\n",
            "Iteration 68, loss = 0.62673981\n",
            "Iteration 69, loss = 0.62621406\n",
            "Iteration 70, loss = 0.62608653\n",
            "Iteration 71, loss = 0.62525546\n",
            "Iteration 72, loss = 0.62467752\n",
            "Iteration 73, loss = 0.62412981\n",
            "Iteration 74, loss = 0.62389618\n",
            "Iteration 75, loss = 0.62320858\n",
            "Iteration 76, loss = 0.62245897\n",
            "Iteration 77, loss = 0.62199701\n",
            "Iteration 78, loss = 0.62137997\n",
            "Iteration 79, loss = 0.62140213\n",
            "Iteration 80, loss = 0.62058524\n",
            "Iteration 81, loss = 0.62044315\n",
            "Iteration 82, loss = 0.61962492\n",
            "Iteration 83, loss = 0.61919718\n",
            "Iteration 84, loss = 0.61875985\n",
            "Iteration 85, loss = 0.61856139\n",
            "Iteration 86, loss = 0.61795406\n",
            "Iteration 87, loss = 0.61786939\n",
            "Iteration 88, loss = 0.61710787\n",
            "Iteration 89, loss = 0.61703949\n",
            "Iteration 90, loss = 0.61668720\n",
            "Iteration 91, loss = 0.61615129\n",
            "Iteration 92, loss = 0.61566668\n",
            "Iteration 93, loss = 0.61563750\n",
            "Iteration 94, loss = 0.61494760\n",
            "Iteration 95, loss = 0.61468409\n",
            "Iteration 96, loss = 0.61458042\n",
            "Iteration 97, loss = 0.61394639\n",
            "Iteration 98, loss = 0.61370371\n",
            "Iteration 99, loss = 0.61363155\n",
            "Iteration 100, loss = 0.61347562\n",
            "Iteration 101, loss = 0.61317454\n",
            "Iteration 102, loss = 0.61243442\n",
            "Iteration 103, loss = 0.61212565\n",
            "Iteration 104, loss = 0.61198037\n",
            "Iteration 105, loss = 0.61237644\n",
            "Iteration 106, loss = 0.61140757\n",
            "Iteration 107, loss = 0.61119155\n",
            "Iteration 108, loss = 0.61089043\n",
            "Iteration 109, loss = 0.61073578\n",
            "Iteration 110, loss = 0.61046498\n",
            "Iteration 111, loss = 0.61110170\n",
            "Iteration 112, loss = 0.60995120\n",
            "Iteration 113, loss = 0.60966822\n",
            "Iteration 114, loss = 0.60950191\n",
            "Iteration 115, loss = 0.60936294\n",
            "Iteration 116, loss = 0.60913725\n",
            "Iteration 117, loss = 0.60901998\n",
            "Iteration 118, loss = 0.60857055\n",
            "Iteration 119, loss = 0.60921275\n",
            "Iteration 120, loss = 0.60858076\n",
            "Iteration 121, loss = 0.60841834\n",
            "Iteration 122, loss = 0.60814432\n",
            "Iteration 123, loss = 0.60817168\n",
            "Iteration 124, loss = 0.60758235\n",
            "Iteration 125, loss = 0.60765742\n",
            "Iteration 126, loss = 0.60766391\n",
            "Iteration 127, loss = 0.60791627\n",
            "Iteration 128, loss = 0.60704281\n",
            "Iteration 129, loss = 0.60704338\n",
            "Iteration 130, loss = 0.60713415\n",
            "Iteration 131, loss = 0.60683394\n",
            "Iteration 132, loss = 0.60695330\n",
            "Iteration 133, loss = 0.60721396\n",
            "Iteration 134, loss = 0.60635518\n",
            "Iteration 135, loss = 0.60626553\n",
            "Iteration 136, loss = 0.60648312\n",
            "Iteration 137, loss = 0.60617162\n",
            "Iteration 138, loss = 0.60617542\n",
            "Iteration 139, loss = 0.60602231\n",
            "Iteration 140, loss = 0.60582773\n",
            "Iteration 141, loss = 0.60551026\n",
            "Iteration 142, loss = 0.60551241\n",
            "Iteration 143, loss = 0.60538239\n",
            "Iteration 144, loss = 0.60558166\n",
            "Iteration 145, loss = 0.60519183\n",
            "Iteration 146, loss = 0.60520234\n",
            "Iteration 147, loss = 0.60518121\n",
            "Iteration 148, loss = 0.60553335\n",
            "Iteration 149, loss = 0.60478652\n",
            "Iteration 150, loss = 0.60482518\n",
            "Iteration 151, loss = 0.60517307\n",
            "Iteration 152, loss = 0.60523719\n",
            "Iteration 153, loss = 0.60453904\n",
            "Iteration 154, loss = 0.60484911\n",
            "Iteration 155, loss = 0.60445371\n",
            "Iteration 156, loss = 0.60433911\n",
            "Iteration 157, loss = 0.60438132\n",
            "Iteration 158, loss = 0.60442157\n",
            "Iteration 159, loss = 0.60441260\n",
            "Iteration 160, loss = 0.60451819\n",
            "Iteration 161, loss = 0.60429954\n",
            "Iteration 162, loss = 0.60443493\n",
            "Iteration 163, loss = 0.60397849\n",
            "Iteration 164, loss = 0.60430343\n",
            "Iteration 165, loss = 0.60446868\n",
            "Iteration 166, loss = 0.60408479\n",
            "Iteration 167, loss = 0.60389685\n",
            "Iteration 168, loss = 0.60431080\n",
            "Iteration 169, loss = 0.60397328\n",
            "Iteration 170, loss = 0.60347548\n",
            "Iteration 171, loss = 0.60358895\n",
            "Iteration 172, loss = 0.60375291\n",
            "Iteration 173, loss = 0.60393188\n",
            "Iteration 174, loss = 0.60375185\n",
            "Iteration 175, loss = 0.60351813\n",
            "Iteration 176, loss = 0.60389801\n",
            "Iteration 177, loss = 0.60299751\n",
            "Iteration 178, loss = 0.60310226\n",
            "Iteration 179, loss = 0.60322969\n",
            "Iteration 180, loss = 0.60307362\n",
            "Iteration 181, loss = 0.60338432\n",
            "Iteration 182, loss = 0.60322799\n",
            "Iteration 183, loss = 0.60300823\n",
            "Iteration 184, loss = 0.60287239\n",
            "Iteration 185, loss = 0.60274708\n",
            "Iteration 186, loss = 0.60296402\n",
            "Iteration 187, loss = 0.60278098\n",
            "Iteration 188, loss = 0.60338892\n",
            "Iteration 189, loss = 0.60282829\n",
            "Iteration 190, loss = 0.60268841\n",
            "Iteration 191, loss = 0.60248212\n",
            "Iteration 192, loss = 0.60283731\n",
            "Iteration 193, loss = 0.60251583\n",
            "Iteration 194, loss = 0.60244813\n",
            "Iteration 195, loss = 0.60268500\n",
            "Iteration 196, loss = 0.60237391\n",
            "Iteration 197, loss = 0.60234094\n",
            "Iteration 198, loss = 0.60229181\n",
            "Iteration 199, loss = 0.60222074\n",
            "Iteration 200, loss = 0.60251449\n",
            "Iteration 201, loss = 0.60214566\n",
            "Iteration 202, loss = 0.60216420\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69398613\n",
            "Iteration 2, loss = 0.67857896\n",
            "Iteration 3, loss = 0.67559221\n",
            "Iteration 4, loss = 0.67518022\n",
            "Iteration 5, loss = 0.67397842\n",
            "Iteration 6, loss = 0.67264441\n",
            "Iteration 7, loss = 0.67162873\n",
            "Iteration 8, loss = 0.67061715\n",
            "Iteration 9, loss = 0.66987096\n",
            "Iteration 10, loss = 0.66836388\n",
            "Iteration 11, loss = 0.66741575\n",
            "Iteration 12, loss = 0.66632122\n",
            "Iteration 13, loss = 0.66556177\n",
            "Iteration 14, loss = 0.66458160\n",
            "Iteration 15, loss = 0.66395753\n",
            "Iteration 16, loss = 0.66260568\n",
            "Iteration 17, loss = 0.66170395\n",
            "Iteration 18, loss = 0.66042273\n",
            "Iteration 19, loss = 0.66016532\n",
            "Iteration 20, loss = 0.65905194\n",
            "Iteration 21, loss = 0.65812013\n",
            "Iteration 22, loss = 0.65689921\n",
            "Iteration 23, loss = 0.65586120\n",
            "Iteration 24, loss = 0.65510034\n",
            "Iteration 25, loss = 0.65421128\n",
            "Iteration 26, loss = 0.65324857\n",
            "Iteration 27, loss = 0.65236931\n",
            "Iteration 28, loss = 0.65169103\n",
            "Iteration 29, loss = 0.65066813\n",
            "Iteration 30, loss = 0.65053388\n",
            "Iteration 31, loss = 0.64922765\n",
            "Iteration 32, loss = 0.64813310\n",
            "Iteration 33, loss = 0.64728384\n",
            "Iteration 34, loss = 0.64666479\n",
            "Iteration 35, loss = 0.64545643\n",
            "Iteration 36, loss = 0.64481555\n",
            "Iteration 37, loss = 0.64385600\n",
            "Iteration 38, loss = 0.64291126\n",
            "Iteration 39, loss = 0.64233601\n",
            "Iteration 40, loss = 0.64139105\n",
            "Iteration 41, loss = 0.64048423\n",
            "Iteration 42, loss = 0.63978179\n",
            "Iteration 43, loss = 0.63904646\n",
            "Iteration 44, loss = 0.63833822\n",
            "Iteration 45, loss = 0.63752349\n",
            "Iteration 46, loss = 0.63665740\n",
            "Iteration 47, loss = 0.63581653\n",
            "Iteration 48, loss = 0.63568350\n",
            "Iteration 49, loss = 0.63466763\n",
            "Iteration 50, loss = 0.63380524\n",
            "Iteration 51, loss = 0.63283139\n",
            "Iteration 52, loss = 0.63236588\n",
            "Iteration 53, loss = 0.63173983\n",
            "Iteration 54, loss = 0.63081717\n",
            "Iteration 55, loss = 0.63015828\n",
            "Iteration 56, loss = 0.62961392\n",
            "Iteration 57, loss = 0.62878951\n",
            "Iteration 58, loss = 0.62869525\n",
            "Iteration 59, loss = 0.62759938\n",
            "Iteration 60, loss = 0.62716292\n",
            "Iteration 61, loss = 0.62649617\n",
            "Iteration 62, loss = 0.62576750\n",
            "Iteration 63, loss = 0.62528792\n",
            "Iteration 64, loss = 0.62492018\n",
            "Iteration 65, loss = 0.62441741\n",
            "Iteration 66, loss = 0.62386105\n",
            "Iteration 67, loss = 0.62301810\n",
            "Iteration 68, loss = 0.62265352\n",
            "Iteration 69, loss = 0.62198927\n",
            "Iteration 70, loss = 0.62161301\n",
            "Iteration 71, loss = 0.62127072\n",
            "Iteration 72, loss = 0.62082194\n",
            "Iteration 73, loss = 0.62015470\n",
            "Iteration 74, loss = 0.61952124\n",
            "Iteration 75, loss = 0.61883143\n",
            "Iteration 76, loss = 0.61850363\n",
            "Iteration 77, loss = 0.61819444\n",
            "Iteration 78, loss = 0.61821047\n",
            "Iteration 79, loss = 0.61750097\n",
            "Iteration 80, loss = 0.61721438\n",
            "Iteration 81, loss = 0.61644938\n",
            "Iteration 82, loss = 0.61615621\n",
            "Iteration 83, loss = 0.61609756\n",
            "Iteration 84, loss = 0.61519776\n",
            "Iteration 85, loss = 0.61478545\n",
            "Iteration 86, loss = 0.61454903\n",
            "Iteration 87, loss = 0.61419064\n",
            "Iteration 88, loss = 0.61370662\n",
            "Iteration 89, loss = 0.61348557\n",
            "Iteration 90, loss = 0.61341575\n",
            "Iteration 91, loss = 0.61314276\n",
            "Iteration 92, loss = 0.61260242\n",
            "Iteration 93, loss = 0.61224635\n",
            "Iteration 94, loss = 0.61199681\n",
            "Iteration 95, loss = 0.61163077\n",
            "Iteration 96, loss = 0.61147325\n",
            "Iteration 97, loss = 0.61109169\n",
            "Iteration 98, loss = 0.61093901\n",
            "Iteration 99, loss = 0.61148502\n",
            "Iteration 100, loss = 0.61102536\n",
            "Iteration 101, loss = 0.61011782\n",
            "Iteration 102, loss = 0.60986357\n",
            "Iteration 103, loss = 0.60979003\n",
            "Iteration 104, loss = 0.60993999\n",
            "Iteration 105, loss = 0.60972651\n",
            "Iteration 106, loss = 0.60927416\n",
            "Iteration 107, loss = 0.60903471\n",
            "Iteration 108, loss = 0.60872091\n",
            "Iteration 109, loss = 0.60875327\n",
            "Iteration 110, loss = 0.60838293\n",
            "Iteration 111, loss = 0.60878104\n",
            "Iteration 112, loss = 0.60822576\n",
            "Iteration 113, loss = 0.60815006\n",
            "Iteration 114, loss = 0.60763851\n",
            "Iteration 115, loss = 0.60756692\n",
            "Iteration 116, loss = 0.60807356\n",
            "Iteration 117, loss = 0.60717175\n",
            "Iteration 118, loss = 0.60708398\n",
            "Iteration 119, loss = 0.60697642\n",
            "Iteration 120, loss = 0.60697822\n",
            "Iteration 121, loss = 0.60677698\n",
            "Iteration 122, loss = 0.60686312\n",
            "Iteration 123, loss = 0.60646433\n",
            "Iteration 124, loss = 0.60664955\n",
            "Iteration 125, loss = 0.60636838\n",
            "Iteration 126, loss = 0.60653939\n",
            "Iteration 127, loss = 0.60604476\n",
            "Iteration 128, loss = 0.60597902\n",
            "Iteration 129, loss = 0.60636254\n",
            "Iteration 130, loss = 0.60587888\n",
            "Iteration 131, loss = 0.60595983\n",
            "Iteration 132, loss = 0.60565804\n",
            "Iteration 133, loss = 0.60553792\n",
            "Iteration 134, loss = 0.60538131\n",
            "Iteration 135, loss = 0.60542480\n",
            "Iteration 136, loss = 0.60530797\n",
            "Iteration 137, loss = 0.60508440\n",
            "Iteration 138, loss = 0.60514984\n",
            "Iteration 139, loss = 0.60525662\n",
            "Iteration 140, loss = 0.60533185\n",
            "Iteration 141, loss = 0.60476558\n",
            "Iteration 142, loss = 0.60503762\n",
            "Iteration 143, loss = 0.60483749\n",
            "Iteration 144, loss = 0.60478562\n",
            "Iteration 145, loss = 0.60451019\n",
            "Iteration 146, loss = 0.60442905\n",
            "Iteration 147, loss = 0.60447443\n",
            "Iteration 148, loss = 0.60430860\n",
            "Iteration 149, loss = 0.60441985\n",
            "Iteration 150, loss = 0.60469070\n",
            "Iteration 151, loss = 0.60415015\n",
            "Iteration 152, loss = 0.60403355\n",
            "Iteration 153, loss = 0.60414195\n",
            "Iteration 154, loss = 0.60406466\n",
            "Iteration 155, loss = 0.60427066\n",
            "Iteration 156, loss = 0.60397661\n",
            "Iteration 157, loss = 0.60376958\n",
            "Iteration 158, loss = 0.60385318\n",
            "Iteration 159, loss = 0.60382955\n",
            "Iteration 160, loss = 0.60416724\n",
            "Iteration 161, loss = 0.60391696\n",
            "Iteration 162, loss = 0.60442055\n",
            "Iteration 163, loss = 0.60371302\n",
            "Iteration 164, loss = 0.60385730\n",
            "Iteration 165, loss = 0.60356254\n",
            "Iteration 166, loss = 0.60379933\n",
            "Iteration 167, loss = 0.60352731\n",
            "Iteration 168, loss = 0.60337910\n",
            "Iteration 169, loss = 0.60344514\n",
            "Iteration 170, loss = 0.60338402\n",
            "Iteration 171, loss = 0.60331149\n",
            "Iteration 172, loss = 0.60334043\n",
            "Iteration 173, loss = 0.60317470\n",
            "Iteration 174, loss = 0.60328424\n",
            "Iteration 175, loss = 0.60300532\n",
            "Iteration 176, loss = 0.60298329\n",
            "Iteration 177, loss = 0.60311140\n",
            "Iteration 178, loss = 0.60292727\n",
            "Iteration 179, loss = 0.60286922\n",
            "Iteration 180, loss = 0.60287787\n",
            "Iteration 181, loss = 0.60356336\n",
            "Iteration 182, loss = 0.60301603\n",
            "Iteration 183, loss = 0.60290142\n",
            "Iteration 184, loss = 0.60292389\n",
            "Iteration 185, loss = 0.60283484\n",
            "Iteration 186, loss = 0.60278503\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68404087\n",
            "Iteration 2, loss = 0.67897611\n",
            "Iteration 3, loss = 0.67681373\n",
            "Iteration 4, loss = 0.67633384\n",
            "Iteration 5, loss = 0.67535336\n",
            "Iteration 6, loss = 0.67449242\n",
            "Iteration 7, loss = 0.67428119\n",
            "Iteration 8, loss = 0.67285987\n",
            "Iteration 9, loss = 0.67206613\n",
            "Iteration 10, loss = 0.67135042\n",
            "Iteration 11, loss = 0.67058030\n",
            "Iteration 12, loss = 0.67009924\n",
            "Iteration 13, loss = 0.66920001\n",
            "Iteration 14, loss = 0.66862565\n",
            "Iteration 15, loss = 0.66754276\n",
            "Iteration 16, loss = 0.66719315\n",
            "Iteration 17, loss = 0.66659647\n",
            "Iteration 18, loss = 0.66564054\n",
            "Iteration 19, loss = 0.66486926\n",
            "Iteration 20, loss = 0.66425083\n",
            "Iteration 21, loss = 0.66372417\n",
            "Iteration 22, loss = 0.66329849\n",
            "Iteration 23, loss = 0.66249127\n",
            "Iteration 24, loss = 0.66238899\n",
            "Iteration 25, loss = 0.66141112\n",
            "Iteration 26, loss = 0.66061770\n",
            "Iteration 27, loss = 0.65983987\n",
            "Iteration 28, loss = 0.65958147\n",
            "Iteration 29, loss = 0.65871134\n",
            "Iteration 30, loss = 0.65790955\n",
            "Iteration 31, loss = 0.65746006\n",
            "Iteration 32, loss = 0.65701825\n",
            "Iteration 33, loss = 0.65642646\n",
            "Iteration 34, loss = 0.65607637\n",
            "Iteration 35, loss = 0.65508169\n",
            "Iteration 36, loss = 0.65530944\n",
            "Iteration 37, loss = 0.65397333\n",
            "Iteration 38, loss = 0.65350972\n",
            "Iteration 39, loss = 0.65284295\n",
            "Iteration 40, loss = 0.65234948\n",
            "Iteration 41, loss = 0.65173108\n",
            "Iteration 42, loss = 0.65119570\n",
            "Iteration 43, loss = 0.65065266\n",
            "Iteration 44, loss = 0.65042073\n",
            "Iteration 45, loss = 0.64940807\n",
            "Iteration 46, loss = 0.64893369\n",
            "Iteration 47, loss = 0.64845029\n",
            "Iteration 48, loss = 0.64781211\n",
            "Iteration 49, loss = 0.64778902\n",
            "Iteration 50, loss = 0.64705993\n",
            "Iteration 51, loss = 0.64638051\n",
            "Iteration 52, loss = 0.64623557\n",
            "Iteration 53, loss = 0.64543555\n",
            "Iteration 54, loss = 0.64497846\n",
            "Iteration 55, loss = 0.64474329\n",
            "Iteration 56, loss = 0.64433616\n",
            "Iteration 57, loss = 0.64323748\n",
            "Iteration 58, loss = 0.64289835\n",
            "Iteration 59, loss = 0.64228514\n",
            "Iteration 60, loss = 0.64217558\n",
            "Iteration 61, loss = 0.64155305\n",
            "Iteration 62, loss = 0.64090490\n",
            "Iteration 63, loss = 0.64064618\n",
            "Iteration 64, loss = 0.64026122\n",
            "Iteration 65, loss = 0.63982580\n",
            "Iteration 66, loss = 0.63924402\n",
            "Iteration 67, loss = 0.63885935\n",
            "Iteration 68, loss = 0.63854893\n",
            "Iteration 69, loss = 0.63885823\n",
            "Iteration 70, loss = 0.63755462\n",
            "Iteration 71, loss = 0.63734388\n",
            "Iteration 72, loss = 0.63736551\n",
            "Iteration 73, loss = 0.63708736\n",
            "Iteration 74, loss = 0.63639141\n",
            "Iteration 75, loss = 0.63565655\n",
            "Iteration 76, loss = 0.63537295\n",
            "Iteration 77, loss = 0.63544044\n",
            "Iteration 78, loss = 0.63471581\n",
            "Iteration 79, loss = 0.63437895\n",
            "Iteration 80, loss = 0.63378308\n",
            "Iteration 81, loss = 0.63375039\n",
            "Iteration 82, loss = 0.63295959\n",
            "Iteration 83, loss = 0.63318817\n",
            "Iteration 84, loss = 0.63287879\n",
            "Iteration 85, loss = 0.63201728\n",
            "Iteration 86, loss = 0.63169570\n",
            "Iteration 87, loss = 0.63170136\n",
            "Iteration 88, loss = 0.63094660\n",
            "Iteration 89, loss = 0.63084322\n",
            "Iteration 90, loss = 0.63074296\n",
            "Iteration 91, loss = 0.63028576\n",
            "Iteration 92, loss = 0.63011236\n",
            "Iteration 93, loss = 0.62997906\n",
            "Iteration 94, loss = 0.62969750\n",
            "Iteration 95, loss = 0.62928773\n",
            "Iteration 96, loss = 0.62872580\n",
            "Iteration 97, loss = 0.62856718\n",
            "Iteration 98, loss = 0.62854406\n",
            "Iteration 99, loss = 0.62836104\n",
            "Iteration 100, loss = 0.62790408\n",
            "Iteration 101, loss = 0.62800079\n",
            "Iteration 102, loss = 0.62919194\n",
            "Iteration 103, loss = 0.62706945\n",
            "Iteration 104, loss = 0.62697674\n",
            "Iteration 105, loss = 0.62665469\n",
            "Iteration 106, loss = 0.62688349\n",
            "Iteration 107, loss = 0.62669774\n",
            "Iteration 108, loss = 0.62643822\n",
            "Iteration 109, loss = 0.62601941\n",
            "Iteration 110, loss = 0.62581514\n",
            "Iteration 111, loss = 0.62559215\n",
            "Iteration 112, loss = 0.62539056\n",
            "Iteration 113, loss = 0.62546665\n",
            "Iteration 114, loss = 0.62499146\n",
            "Iteration 115, loss = 0.62507748\n",
            "Iteration 116, loss = 0.62464788\n",
            "Iteration 117, loss = 0.62476432\n",
            "Iteration 118, loss = 0.62438228\n",
            "Iteration 119, loss = 0.62532290\n",
            "Iteration 120, loss = 0.62448027\n",
            "Iteration 121, loss = 0.62389474\n",
            "Iteration 122, loss = 0.62405558\n",
            "Iteration 123, loss = 0.62377994\n",
            "Iteration 124, loss = 0.62434979\n",
            "Iteration 125, loss = 0.62399311\n",
            "Iteration 126, loss = 0.62330359\n",
            "Iteration 127, loss = 0.62316782\n",
            "Iteration 128, loss = 0.62307574\n",
            "Iteration 129, loss = 0.62282955\n",
            "Iteration 130, loss = 0.62284025\n",
            "Iteration 131, loss = 0.62255385\n",
            "Iteration 132, loss = 0.62263087\n",
            "Iteration 133, loss = 0.62246344\n",
            "Iteration 134, loss = 0.62247959\n",
            "Iteration 135, loss = 0.62237866\n",
            "Iteration 136, loss = 0.62234138\n",
            "Iteration 137, loss = 0.62229041\n",
            "Iteration 138, loss = 0.62163860\n",
            "Iteration 139, loss = 0.62194296\n",
            "Iteration 140, loss = 0.62174217\n",
            "Iteration 141, loss = 0.62151572\n",
            "Iteration 142, loss = 0.62185241\n",
            "Iteration 143, loss = 0.62156021\n",
            "Iteration 144, loss = 0.62163650\n",
            "Iteration 145, loss = 0.62153554\n",
            "Iteration 146, loss = 0.62119372\n",
            "Iteration 147, loss = 0.62091494\n",
            "Iteration 148, loss = 0.62104190\n",
            "Iteration 149, loss = 0.62092198\n",
            "Iteration 150, loss = 0.62124823\n",
            "Iteration 151, loss = 0.62059985\n",
            "Iteration 152, loss = 0.62084741\n",
            "Iteration 153, loss = 0.62110468\n",
            "Iteration 154, loss = 0.62051705\n",
            "Iteration 155, loss = 0.62068279\n",
            "Iteration 156, loss = 0.62055279\n",
            "Iteration 157, loss = 0.62041524\n",
            "Iteration 158, loss = 0.62029079\n",
            "Iteration 159, loss = 0.62062203\n",
            "Iteration 160, loss = 0.62067377\n",
            "Iteration 161, loss = 0.62010721\n",
            "Iteration 162, loss = 0.61995675\n",
            "Iteration 163, loss = 0.62023935\n",
            "Iteration 164, loss = 0.62020989\n",
            "Iteration 165, loss = 0.62002287\n",
            "Iteration 166, loss = 0.62002417\n",
            "Iteration 167, loss = 0.61973483\n",
            "Iteration 168, loss = 0.61994390\n",
            "Iteration 169, loss = 0.62089544\n",
            "Iteration 170, loss = 0.61957038\n",
            "Iteration 171, loss = 0.61962288\n",
            "Iteration 172, loss = 0.62002099\n",
            "Iteration 173, loss = 0.61935039\n",
            "Iteration 174, loss = 0.61938818\n",
            "Iteration 175, loss = 0.61955238\n",
            "Iteration 176, loss = 0.61933324\n",
            "Iteration 177, loss = 0.61943143\n",
            "Iteration 178, loss = 0.61911425\n",
            "Iteration 179, loss = 0.61939880\n",
            "Iteration 180, loss = 0.61938597\n",
            "Iteration 181, loss = 0.61912419\n",
            "Iteration 182, loss = 0.61919289\n",
            "Iteration 183, loss = 0.61886087\n",
            "Iteration 184, loss = 0.61885567\n",
            "Iteration 185, loss = 0.61953338\n",
            "Iteration 186, loss = 0.61897726\n",
            "Iteration 187, loss = 0.61904378\n",
            "Iteration 188, loss = 0.61864111\n",
            "Iteration 189, loss = 0.61901963\n",
            "Iteration 190, loss = 0.61872163\n",
            "Iteration 191, loss = 0.61854913\n",
            "Iteration 192, loss = 0.61872180\n",
            "Iteration 193, loss = 0.61875642\n",
            "Iteration 194, loss = 0.61849095\n",
            "Iteration 195, loss = 0.61844969\n",
            "Iteration 196, loss = 0.61874831\n",
            "Iteration 197, loss = 0.61872591\n",
            "Iteration 198, loss = 0.61860726\n",
            "Iteration 199, loss = 0.61850587\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68419377\n",
            "Iteration 2, loss = 0.67663327\n",
            "Iteration 3, loss = 0.67347937\n",
            "Iteration 4, loss = 0.67380747\n",
            "Iteration 5, loss = 0.67201294\n",
            "Iteration 6, loss = 0.67124389\n",
            "Iteration 7, loss = 0.67014225\n",
            "Iteration 8, loss = 0.66915483\n",
            "Iteration 9, loss = 0.66815684\n",
            "Iteration 10, loss = 0.66737499\n",
            "Iteration 11, loss = 0.66667554\n",
            "Iteration 12, loss = 0.66521368\n",
            "Iteration 13, loss = 0.66442240\n",
            "Iteration 14, loss = 0.66391051\n",
            "Iteration 15, loss = 0.66281303\n",
            "Iteration 16, loss = 0.66220065\n",
            "Iteration 17, loss = 0.66090714\n",
            "Iteration 18, loss = 0.66029558\n",
            "Iteration 19, loss = 0.65949717\n",
            "Iteration 20, loss = 0.65881607\n",
            "Iteration 21, loss = 0.65799397\n",
            "Iteration 22, loss = 0.65885014\n",
            "Iteration 23, loss = 0.65644365\n",
            "Iteration 24, loss = 0.65598993\n",
            "Iteration 25, loss = 0.65575065\n",
            "Iteration 26, loss = 0.65418587\n",
            "Iteration 27, loss = 0.65366176\n",
            "Iteration 28, loss = 0.65245249\n",
            "Iteration 29, loss = 0.65164854\n",
            "Iteration 30, loss = 0.65088787\n",
            "Iteration 31, loss = 0.65054252\n",
            "Iteration 32, loss = 0.64975514\n",
            "Iteration 33, loss = 0.64895189\n",
            "Iteration 34, loss = 0.64819324\n",
            "Iteration 35, loss = 0.64764038\n",
            "Iteration 36, loss = 0.64673300\n",
            "Iteration 37, loss = 0.64635507\n",
            "Iteration 38, loss = 0.64536855\n",
            "Iteration 39, loss = 0.64468577\n",
            "Iteration 40, loss = 0.64427429\n",
            "Iteration 41, loss = 0.64359432\n",
            "Iteration 42, loss = 0.64280866\n",
            "Iteration 43, loss = 0.64235731\n",
            "Iteration 44, loss = 0.64144042\n",
            "Iteration 45, loss = 0.64065122\n",
            "Iteration 46, loss = 0.64004796\n",
            "Iteration 47, loss = 0.63967220\n",
            "Iteration 48, loss = 0.63889505\n",
            "Iteration 49, loss = 0.63860245\n",
            "Iteration 50, loss = 0.63767951\n",
            "Iteration 51, loss = 0.63730258\n",
            "Iteration 52, loss = 0.63675454\n",
            "Iteration 53, loss = 0.63608439\n",
            "Iteration 54, loss = 0.63565976\n",
            "Iteration 55, loss = 0.63523528\n",
            "Iteration 56, loss = 0.63462775\n",
            "Iteration 57, loss = 0.63455518\n",
            "Iteration 58, loss = 0.63386560\n",
            "Iteration 59, loss = 0.63286118\n",
            "Iteration 60, loss = 0.63242542\n",
            "Iteration 61, loss = 0.63186522\n",
            "Iteration 62, loss = 0.63137651\n",
            "Iteration 63, loss = 0.63103940\n",
            "Iteration 64, loss = 0.63054532\n",
            "Iteration 65, loss = 0.63007895\n",
            "Iteration 66, loss = 0.62959085\n",
            "Iteration 67, loss = 0.62912992\n",
            "Iteration 68, loss = 0.62894275\n",
            "Iteration 69, loss = 0.62832249\n",
            "Iteration 70, loss = 0.62893627\n",
            "Iteration 71, loss = 0.63029402\n",
            "Iteration 72, loss = 0.62718549\n",
            "Iteration 73, loss = 0.62660232\n",
            "Iteration 74, loss = 0.62696207\n",
            "Iteration 75, loss = 0.62669705\n",
            "Iteration 76, loss = 0.62584830\n",
            "Iteration 77, loss = 0.62526466\n",
            "Iteration 78, loss = 0.62536876\n",
            "Iteration 79, loss = 0.62480635\n",
            "Iteration 80, loss = 0.62444346\n",
            "Iteration 81, loss = 0.62392296\n",
            "Iteration 82, loss = 0.62372465\n",
            "Iteration 83, loss = 0.62337304\n",
            "Iteration 84, loss = 0.62331026\n",
            "Iteration 85, loss = 0.62389473\n",
            "Iteration 86, loss = 0.62228521\n",
            "Iteration 87, loss = 0.62231836\n",
            "Iteration 88, loss = 0.62228896\n",
            "Iteration 89, loss = 0.62165001\n",
            "Iteration 90, loss = 0.62145831\n",
            "Iteration 91, loss = 0.62258333\n",
            "Iteration 92, loss = 0.62127349\n",
            "Iteration 93, loss = 0.62081919\n",
            "Iteration 94, loss = 0.62059789\n",
            "Iteration 95, loss = 0.62031447\n",
            "Iteration 96, loss = 0.62039485\n",
            "Iteration 97, loss = 0.62004366\n",
            "Iteration 98, loss = 0.61987379\n",
            "Iteration 99, loss = 0.62080922\n",
            "Iteration 100, loss = 0.62008897\n",
            "Iteration 101, loss = 0.61927059\n",
            "Iteration 102, loss = 0.61934986\n",
            "Iteration 103, loss = 0.61948199\n",
            "Iteration 104, loss = 0.61911400\n",
            "Iteration 105, loss = 0.61866783\n",
            "Iteration 106, loss = 0.61856776\n",
            "Iteration 107, loss = 0.61839320\n",
            "Iteration 108, loss = 0.61841471\n",
            "Iteration 109, loss = 0.61822657\n",
            "Iteration 110, loss = 0.61790458\n",
            "Iteration 111, loss = 0.61806050\n",
            "Iteration 112, loss = 0.61782903\n",
            "Iteration 113, loss = 0.61750025\n",
            "Iteration 114, loss = 0.61768445\n",
            "Iteration 115, loss = 0.61745390\n",
            "Iteration 116, loss = 0.61772544\n",
            "Iteration 117, loss = 0.61736126\n",
            "Iteration 118, loss = 0.61743391\n",
            "Iteration 119, loss = 0.61704175\n",
            "Iteration 120, loss = 0.61731176\n",
            "Iteration 121, loss = 0.61741862\n",
            "Iteration 122, loss = 0.61709028\n",
            "Iteration 123, loss = 0.61664788\n",
            "Iteration 124, loss = 0.61665257\n",
            "Iteration 125, loss = 0.61708075\n",
            "Iteration 126, loss = 0.61628607\n",
            "Iteration 127, loss = 0.61654230\n",
            "Iteration 128, loss = 0.61662914\n",
            "Iteration 129, loss = 0.61745981\n",
            "Iteration 130, loss = 0.61615912\n",
            "Iteration 131, loss = 0.61621993\n",
            "Iteration 132, loss = 0.61785526\n",
            "Iteration 133, loss = 0.61591410\n",
            "Iteration 134, loss = 0.61579987\n",
            "Iteration 135, loss = 0.61607876\n",
            "Iteration 136, loss = 0.61593696\n",
            "Iteration 137, loss = 0.61587292\n",
            "Iteration 138, loss = 0.61571392\n",
            "Iteration 139, loss = 0.61572925\n",
            "Iteration 140, loss = 0.61552853\n",
            "Iteration 141, loss = 0.61558074\n",
            "Iteration 142, loss = 0.61554867\n",
            "Iteration 143, loss = 0.61558609\n",
            "Iteration 144, loss = 0.61570025\n",
            "Iteration 145, loss = 0.61543331\n",
            "Iteration 146, loss = 0.61547911\n",
            "Iteration 147, loss = 0.61537681\n",
            "Iteration 148, loss = 0.61530561\n",
            "Iteration 149, loss = 0.61520845\n",
            "Iteration 150, loss = 0.61543420\n",
            "Iteration 151, loss = 0.61560535\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.63786845\n",
            "Iteration 2, loss = 0.60400545\n",
            "Iteration 3, loss = 0.59884894\n",
            "Iteration 4, loss = 0.60070053\n",
            "Iteration 5, loss = 0.59113954\n",
            "Iteration 6, loss = 0.58713875\n",
            "Iteration 7, loss = 0.58354983\n",
            "Iteration 8, loss = 0.57665829\n",
            "Iteration 9, loss = 0.57312269\n",
            "Iteration 10, loss = 0.57400469\n",
            "Iteration 11, loss = 0.57048072\n",
            "Iteration 12, loss = 0.56919955\n",
            "Iteration 13, loss = 0.55707572\n",
            "Iteration 14, loss = 0.58020154\n",
            "Iteration 15, loss = 0.58151005\n",
            "Iteration 16, loss = 0.57156453\n",
            "Iteration 17, loss = 0.56297172\n",
            "Iteration 18, loss = 0.56810557\n",
            "Iteration 19, loss = 0.56773803\n",
            "Iteration 20, loss = 0.55456223\n",
            "Iteration 21, loss = 0.56115753\n",
            "Iteration 22, loss = 0.56289005\n",
            "Iteration 23, loss = 0.55694708\n",
            "Iteration 24, loss = 0.56441835\n",
            "Iteration 25, loss = 0.55938801\n",
            "Iteration 26, loss = 0.55642154\n",
            "Iteration 27, loss = 0.54297574\n",
            "Iteration 28, loss = 0.54700470\n",
            "Iteration 29, loss = 0.54100754\n",
            "Iteration 30, loss = 0.53025389\n",
            "Iteration 31, loss = 0.54158934\n",
            "Iteration 32, loss = 0.54191023\n",
            "Iteration 33, loss = 0.55016917\n",
            "Iteration 34, loss = 0.54907601\n",
            "Iteration 35, loss = 0.54024334\n",
            "Iteration 36, loss = 0.54143791\n",
            "Iteration 37, loss = 0.54270609\n",
            "Iteration 38, loss = 0.54006611\n",
            "Iteration 39, loss = 0.54880070\n",
            "Iteration 40, loss = 0.52494621\n",
            "Iteration 41, loss = 0.54272915\n",
            "Iteration 42, loss = 0.53936311\n",
            "Iteration 43, loss = 0.54836693\n",
            "Iteration 44, loss = 0.52964708\n",
            "Iteration 45, loss = 0.53201438\n",
            "Iteration 46, loss = 0.52626563\n",
            "Iteration 47, loss = 0.54545691\n",
            "Iteration 48, loss = 0.54256429\n",
            "Iteration 49, loss = 0.54820013\n",
            "Iteration 50, loss = 0.55425035\n",
            "Iteration 51, loss = 0.54541523\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66076898\n",
            "Iteration 2, loss = 0.60161874\n",
            "Iteration 3, loss = 0.59693213\n",
            "Iteration 4, loss = 0.59304057\n",
            "Iteration 5, loss = 0.58837758\n",
            "Iteration 6, loss = 0.58368044\n",
            "Iteration 7, loss = 0.58223058\n",
            "Iteration 8, loss = 0.58051352\n",
            "Iteration 9, loss = 0.57500795\n",
            "Iteration 10, loss = 0.58259180\n",
            "Iteration 11, loss = 0.57371881\n",
            "Iteration 12, loss = 0.56893536\n",
            "Iteration 13, loss = 0.57626668\n",
            "Iteration 14, loss = 0.57021995\n",
            "Iteration 15, loss = 0.57413355\n",
            "Iteration 16, loss = 0.56903577\n",
            "Iteration 17, loss = 0.56388864\n",
            "Iteration 18, loss = 0.57149279\n",
            "Iteration 19, loss = 0.56556625\n",
            "Iteration 20, loss = 0.55871643\n",
            "Iteration 21, loss = 0.56063606\n",
            "Iteration 22, loss = 0.56227701\n",
            "Iteration 23, loss = 0.57086965\n",
            "Iteration 24, loss = 0.55815075\n",
            "Iteration 25, loss = 0.55594688\n",
            "Iteration 26, loss = 0.55418663\n",
            "Iteration 27, loss = 0.55850351\n",
            "Iteration 28, loss = 0.56307805\n",
            "Iteration 29, loss = 0.54503684\n",
            "Iteration 30, loss = 0.54886753\n",
            "Iteration 31, loss = 0.53822235\n",
            "Iteration 32, loss = 0.54248939\n",
            "Iteration 33, loss = 0.54926713\n",
            "Iteration 34, loss = 0.54100633\n",
            "Iteration 35, loss = 0.54132418\n",
            "Iteration 36, loss = 0.54022093\n",
            "Iteration 37, loss = 0.54388541\n",
            "Iteration 38, loss = 0.53828582\n",
            "Iteration 39, loss = 0.53691172\n",
            "Iteration 40, loss = 0.53289017\n",
            "Iteration 41, loss = 0.54468915\n",
            "Iteration 42, loss = 0.53916368\n",
            "Iteration 43, loss = 0.54403428\n",
            "Iteration 44, loss = 0.54425928\n",
            "Iteration 45, loss = 0.53575557\n",
            "Iteration 46, loss = 0.54303328\n",
            "Iteration 47, loss = 0.54434135\n",
            "Iteration 48, loss = 0.53742992\n",
            "Iteration 49, loss = 0.52957125\n",
            "Iteration 50, loss = 0.52432385\n",
            "Iteration 51, loss = 0.54527571\n",
            "Iteration 52, loss = 0.53031847\n",
            "Iteration 53, loss = 0.52795184\n",
            "Iteration 54, loss = 0.52397442\n",
            "Iteration 55, loss = 0.55144652\n",
            "Iteration 56, loss = 0.52831063\n",
            "Iteration 57, loss = 0.52255238\n",
            "Iteration 58, loss = 0.51563553\n",
            "Iteration 59, loss = 0.52124713\n",
            "Iteration 60, loss = 0.51883021\n",
            "Iteration 61, loss = 0.52439545\n",
            "Iteration 62, loss = 0.51894993\n",
            "Iteration 63, loss = 0.52040323\n",
            "Iteration 64, loss = 0.49976571\n",
            "Iteration 65, loss = 0.53483722\n",
            "Iteration 66, loss = 0.53739829\n",
            "Iteration 67, loss = 0.53700619\n",
            "Iteration 68, loss = 0.52526872\n",
            "Iteration 69, loss = 0.51997896\n",
            "Iteration 70, loss = 0.54545336\n",
            "Iteration 71, loss = 0.52155956\n",
            "Iteration 72, loss = 0.53326923\n",
            "Iteration 73, loss = 0.51643193\n",
            "Iteration 74, loss = 0.53844250\n",
            "Iteration 75, loss = 0.51861691\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.63102283\n",
            "Iteration 2, loss = 0.60428277\n",
            "Iteration 3, loss = 0.60373916\n",
            "Iteration 4, loss = 0.59804814\n",
            "Iteration 5, loss = 0.58936788\n",
            "Iteration 6, loss = 0.59026299\n",
            "Iteration 7, loss = 0.58461903\n",
            "Iteration 8, loss = 0.58143574\n",
            "Iteration 9, loss = 0.57312866\n",
            "Iteration 10, loss = 0.57135031\n",
            "Iteration 11, loss = 0.58347431\n",
            "Iteration 12, loss = 0.58133249\n",
            "Iteration 13, loss = 0.57994166\n",
            "Iteration 14, loss = 0.57493207\n",
            "Iteration 15, loss = 0.57110678\n",
            "Iteration 16, loss = 0.57164578\n",
            "Iteration 17, loss = 0.56473723\n",
            "Iteration 18, loss = 0.56235173\n",
            "Iteration 19, loss = 0.56307422\n",
            "Iteration 20, loss = 0.56305247\n",
            "Iteration 21, loss = 0.56394807\n",
            "Iteration 22, loss = 0.56181637\n",
            "Iteration 23, loss = 0.56465751\n",
            "Iteration 24, loss = 0.55949458\n",
            "Iteration 25, loss = 0.55432562\n",
            "Iteration 26, loss = 0.55812666\n",
            "Iteration 27, loss = 0.56000438\n",
            "Iteration 28, loss = 0.55221917\n",
            "Iteration 29, loss = 0.55236566\n",
            "Iteration 30, loss = 0.54432182\n",
            "Iteration 31, loss = 0.54403313\n",
            "Iteration 32, loss = 0.53666866\n",
            "Iteration 33, loss = 0.55856041\n",
            "Iteration 34, loss = 0.55678843\n",
            "Iteration 35, loss = 0.55346223\n",
            "Iteration 36, loss = 0.54628812\n",
            "Iteration 37, loss = 0.54811261\n",
            "Iteration 38, loss = 0.55791830\n",
            "Iteration 39, loss = 0.55672553\n",
            "Iteration 40, loss = 0.53401288\n",
            "Iteration 41, loss = 0.54122731\n",
            "Iteration 42, loss = 0.53074491\n",
            "Iteration 43, loss = 0.53489077\n",
            "Iteration 44, loss = 0.54184311\n",
            "Iteration 45, loss = 0.54588063\n",
            "Iteration 46, loss = 0.54642674\n",
            "Iteration 47, loss = 0.52707911\n",
            "Iteration 48, loss = 0.54118271\n",
            "Iteration 49, loss = 0.53106297\n",
            "Iteration 50, loss = 0.53861708\n",
            "Iteration 51, loss = 0.52659257\n",
            "Iteration 52, loss = 0.52246926\n",
            "Iteration 53, loss = 0.53026302\n",
            "Iteration 54, loss = 0.51583213\n",
            "Iteration 55, loss = 0.50743448\n",
            "Iteration 56, loss = 0.51578160\n",
            "Iteration 57, loss = 0.52443639\n",
            "Iteration 58, loss = 0.51848651\n",
            "Iteration 59, loss = 0.51620322\n",
            "Iteration 60, loss = 0.50673102\n",
            "Iteration 61, loss = 0.56634333\n",
            "Iteration 62, loss = 0.53366450\n",
            "Iteration 63, loss = 0.52948729\n",
            "Iteration 64, loss = 0.52313349\n",
            "Iteration 65, loss = 0.51677031\n",
            "Iteration 66, loss = 0.50960986\n",
            "Iteration 67, loss = 0.50439654\n",
            "Iteration 68, loss = 0.51476773\n",
            "Iteration 69, loss = 0.51188738\n",
            "Iteration 70, loss = 0.51964326\n",
            "Iteration 71, loss = 0.51237362\n",
            "Iteration 72, loss = 0.52484744\n",
            "Iteration 73, loss = 0.52686156\n",
            "Iteration 74, loss = 0.53770280\n",
            "Iteration 75, loss = 0.50819631\n",
            "Iteration 76, loss = 0.50743572\n",
            "Iteration 77, loss = 0.51640934\n",
            "Iteration 78, loss = 0.50458920\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64673477\n",
            "Iteration 2, loss = 0.61119811\n",
            "Iteration 3, loss = 0.61350741\n",
            "Iteration 4, loss = 0.61115807\n",
            "Iteration 5, loss = 0.60577413\n",
            "Iteration 6, loss = 0.60134419\n",
            "Iteration 7, loss = 0.60241738\n",
            "Iteration 8, loss = 0.60780992\n",
            "Iteration 9, loss = 0.60303468\n",
            "Iteration 10, loss = 0.60166071\n",
            "Iteration 11, loss = 0.59922980\n",
            "Iteration 12, loss = 0.58523716\n",
            "Iteration 13, loss = 0.59504812\n",
            "Iteration 14, loss = 0.58798693\n",
            "Iteration 15, loss = 0.58232422\n",
            "Iteration 16, loss = 0.58780275\n",
            "Iteration 17, loss = 0.57604543\n",
            "Iteration 18, loss = 0.57878600\n",
            "Iteration 19, loss = 0.57533708\n",
            "Iteration 20, loss = 0.57175645\n",
            "Iteration 21, loss = 0.57235282\n",
            "Iteration 22, loss = 0.56492645\n",
            "Iteration 23, loss = 0.57588473\n",
            "Iteration 24, loss = 0.56156198\n",
            "Iteration 25, loss = 0.55976244\n",
            "Iteration 26, loss = 0.55231742\n",
            "Iteration 27, loss = 0.59093711\n",
            "Iteration 28, loss = 0.57852525\n",
            "Iteration 29, loss = 0.58275450\n",
            "Iteration 30, loss = 0.56413149\n",
            "Iteration 31, loss = 0.56417250\n",
            "Iteration 32, loss = 0.56364466\n",
            "Iteration 33, loss = 0.55785611\n",
            "Iteration 34, loss = 0.55787926\n",
            "Iteration 35, loss = 0.55813317\n",
            "Iteration 36, loss = 0.55160153\n",
            "Iteration 37, loss = 0.54493037\n",
            "Iteration 38, loss = 0.55153400\n",
            "Iteration 39, loss = 0.55245949\n",
            "Iteration 40, loss = 0.55290807\n",
            "Iteration 41, loss = 0.54052202\n",
            "Iteration 42, loss = 0.52944524\n",
            "Iteration 43, loss = 0.55277574\n",
            "Iteration 44, loss = 0.53410946\n",
            "Iteration 45, loss = 0.54595243\n",
            "Iteration 46, loss = 0.54775135\n",
            "Iteration 47, loss = 0.52442022\n",
            "Iteration 48, loss = 0.53716184\n",
            "Iteration 49, loss = 0.52942126\n",
            "Iteration 50, loss = 0.55037134\n",
            "Iteration 51, loss = 0.53282550\n",
            "Iteration 52, loss = 0.54706336\n",
            "Iteration 53, loss = 0.53825943\n",
            "Iteration 54, loss = 0.52056541\n",
            "Iteration 55, loss = 0.53200506\n",
            "Iteration 56, loss = 0.52227532\n",
            "Iteration 57, loss = 0.51674897\n",
            "Iteration 58, loss = 0.52714131\n",
            "Iteration 59, loss = 0.53137385\n",
            "Iteration 60, loss = 0.52395303\n",
            "Iteration 61, loss = 0.52474871\n",
            "Iteration 62, loss = 0.51706826\n",
            "Iteration 63, loss = 0.55497873\n",
            "Iteration 64, loss = 0.51989476\n",
            "Iteration 65, loss = 0.53228414\n",
            "Iteration 66, loss = 0.50714916\n",
            "Iteration 67, loss = 0.51461765\n",
            "Iteration 68, loss = 0.52277422\n",
            "Iteration 69, loss = 0.51325290\n",
            "Iteration 70, loss = 0.51438823\n",
            "Iteration 71, loss = 0.49838850\n",
            "Iteration 72, loss = 0.52351023\n",
            "Iteration 73, loss = 0.51682326\n",
            "Iteration 74, loss = 0.52318960\n",
            "Iteration 75, loss = 0.52785130\n",
            "Iteration 76, loss = 0.52628505\n",
            "Iteration 77, loss = 0.51574197\n",
            "Iteration 78, loss = 0.49606578\n",
            "Iteration 79, loss = 0.50128471\n",
            "Iteration 80, loss = 0.50848598\n",
            "Iteration 81, loss = 0.50633014\n",
            "Iteration 82, loss = 0.50874842\n",
            "Iteration 83, loss = 0.51236731\n",
            "Iteration 84, loss = 0.51039806\n",
            "Iteration 85, loss = 0.51516784\n",
            "Iteration 86, loss = 0.50383243\n",
            "Iteration 87, loss = 0.49054128\n",
            "Iteration 88, loss = 0.51209994\n",
            "Iteration 89, loss = 0.50373565\n",
            "Iteration 90, loss = 0.50185055\n",
            "Iteration 91, loss = 0.49278785\n",
            "Iteration 92, loss = 0.51013746\n",
            "Iteration 93, loss = 0.50482508\n",
            "Iteration 94, loss = 0.52003061\n",
            "Iteration 95, loss = 0.50378836\n",
            "Iteration 96, loss = 0.50322814\n",
            "Iteration 97, loss = 0.51389187\n",
            "Iteration 98, loss = 0.51278261\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68450560\n",
            "Iteration 2, loss = 0.62835192\n",
            "Iteration 3, loss = 0.62439314\n",
            "Iteration 4, loss = 0.61867965\n",
            "Iteration 5, loss = 0.61368079\n",
            "Iteration 6, loss = 0.61654176\n",
            "Iteration 7, loss = 0.61358163\n",
            "Iteration 8, loss = 0.60521829\n",
            "Iteration 9, loss = 0.60278418\n",
            "Iteration 10, loss = 0.59878309\n",
            "Iteration 11, loss = 0.60336859\n",
            "Iteration 12, loss = 0.59774282\n",
            "Iteration 13, loss = 0.59480834\n",
            "Iteration 14, loss = 0.59855549\n",
            "Iteration 15, loss = 0.59452194\n",
            "Iteration 16, loss = 0.59475074\n",
            "Iteration 17, loss = 0.60135172\n",
            "Iteration 18, loss = 0.58969926\n",
            "Iteration 19, loss = 0.59365244\n",
            "Iteration 20, loss = 0.59186872\n",
            "Iteration 21, loss = 0.58497258\n",
            "Iteration 22, loss = 0.57872366\n",
            "Iteration 23, loss = 0.58371074\n",
            "Iteration 24, loss = 0.58869590\n",
            "Iteration 25, loss = 0.57449296\n",
            "Iteration 26, loss = 0.59227081\n",
            "Iteration 27, loss = 0.57430941\n",
            "Iteration 28, loss = 0.57554690\n",
            "Iteration 29, loss = 0.57852241\n",
            "Iteration 30, loss = 0.57083917\n",
            "Iteration 31, loss = 0.57475298\n",
            "Iteration 32, loss = 0.57250041\n",
            "Iteration 33, loss = 0.56967897\n",
            "Iteration 34, loss = 0.57169716\n",
            "Iteration 35, loss = 0.57845840\n",
            "Iteration 36, loss = 0.58656886\n",
            "Iteration 37, loss = 0.56335484\n",
            "Iteration 38, loss = 0.58665051\n",
            "Iteration 39, loss = 0.57993830\n",
            "Iteration 40, loss = 0.56692132\n",
            "Iteration 41, loss = 0.56666780\n",
            "Iteration 42, loss = 0.57843821\n",
            "Iteration 43, loss = 0.57113782\n",
            "Iteration 44, loss = 0.55929065\n",
            "Iteration 45, loss = 0.56344978\n",
            "Iteration 46, loss = 0.56298003\n",
            "Iteration 47, loss = 0.55841616\n",
            "Iteration 48, loss = 0.55953080\n",
            "Iteration 49, loss = 0.56317355\n",
            "Iteration 50, loss = 0.57042469\n",
            "Iteration 51, loss = 0.55655789\n",
            "Iteration 52, loss = 0.54824774\n",
            "Iteration 53, loss = 0.55398643\n",
            "Iteration 54, loss = 0.54492141\n",
            "Iteration 55, loss = 0.58688578\n",
            "Iteration 56, loss = 0.55544965\n",
            "Iteration 57, loss = 0.55453575\n",
            "Iteration 58, loss = 0.55497819\n",
            "Iteration 59, loss = 0.54649024\n",
            "Iteration 60, loss = 0.55834591\n",
            "Iteration 61, loss = 0.55641755\n",
            "Iteration 62, loss = 0.53225459\n",
            "Iteration 63, loss = 0.57024014\n",
            "Iteration 64, loss = 0.55943177\n",
            "Iteration 65, loss = 0.54321687\n",
            "Iteration 66, loss = 0.55056491\n",
            "Iteration 67, loss = 0.54786026\n",
            "Iteration 68, loss = 0.55289633\n",
            "Iteration 69, loss = 0.56381855\n",
            "Iteration 70, loss = 0.54897512\n",
            "Iteration 71, loss = 0.58184110\n",
            "Iteration 72, loss = 0.57167702\n",
            "Iteration 73, loss = 0.55046218\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.14182458\n",
            "Iteration 2, loss = 0.68335439\n",
            "Iteration 3, loss = 0.64211596\n",
            "Iteration 4, loss = 0.60213227\n",
            "Iteration 5, loss = 0.60215563\n",
            "Iteration 6, loss = 0.60361116\n",
            "Iteration 7, loss = 0.66438164\n",
            "Iteration 8, loss = 0.58870314\n",
            "Iteration 9, loss = 0.59137863\n",
            "Iteration 10, loss = 0.57883168\n",
            "Iteration 11, loss = 0.58450116\n",
            "Iteration 12, loss = 0.62125604\n",
            "Iteration 13, loss = 0.58598386\n",
            "Iteration 14, loss = 0.57657460\n",
            "Iteration 15, loss = 0.57672963\n",
            "Iteration 16, loss = 0.57984585\n",
            "Iteration 17, loss = 0.59571074\n",
            "Iteration 18, loss = 0.57453834\n",
            "Iteration 19, loss = 0.56430360\n",
            "Iteration 20, loss = 0.57257886\n",
            "Iteration 21, loss = 0.56634255\n",
            "Iteration 22, loss = 0.55770138\n",
            "Iteration 23, loss = 0.56259650\n",
            "Iteration 24, loss = 0.56852854\n",
            "Iteration 25, loss = 0.59491015\n",
            "Iteration 26, loss = 0.55668855\n",
            "Iteration 27, loss = 0.55682480\n",
            "Iteration 28, loss = 0.55225765\n",
            "Iteration 29, loss = 0.55807913\n",
            "Iteration 30, loss = 0.57414341\n",
            "Iteration 31, loss = 0.56216781\n",
            "Iteration 32, loss = 0.56033142\n",
            "Iteration 33, loss = 0.55887121\n",
            "Iteration 34, loss = 0.56123214\n",
            "Iteration 35, loss = 0.54239091\n",
            "Iteration 36, loss = 0.53693780\n",
            "Iteration 37, loss = 0.54899671\n",
            "Iteration 38, loss = 0.54955941\n",
            "Iteration 39, loss = 0.54955219\n",
            "Iteration 40, loss = 0.54551504\n",
            "Iteration 41, loss = 0.54830998\n",
            "Iteration 42, loss = 0.53659903\n",
            "Iteration 43, loss = 0.54511783\n",
            "Iteration 44, loss = 0.54060258\n",
            "Iteration 45, loss = 0.53058589\n",
            "Iteration 46, loss = 0.52681202\n",
            "Iteration 47, loss = 0.53879406\n",
            "Iteration 48, loss = 0.52724408\n",
            "Iteration 49, loss = 0.51753626\n",
            "Iteration 50, loss = 0.57122367\n",
            "Iteration 51, loss = 0.57076325\n",
            "Iteration 52, loss = 0.57005118\n",
            "Iteration 53, loss = 0.53883765\n",
            "Iteration 54, loss = 0.52051060\n",
            "Iteration 55, loss = 0.51827848\n",
            "Iteration 56, loss = 0.54940936\n",
            "Iteration 57, loss = 0.51563772\n",
            "Iteration 58, loss = 0.52810456\n",
            "Iteration 59, loss = 0.52264745\n",
            "Iteration 60, loss = 0.52498741\n",
            "Iteration 61, loss = 0.51424462\n",
            "Iteration 62, loss = 0.52557981\n",
            "Iteration 63, loss = 0.53543081\n",
            "Iteration 64, loss = 0.50742278\n",
            "Iteration 65, loss = 0.52024397\n",
            "Iteration 66, loss = 0.50559427\n",
            "Iteration 67, loss = 0.52047548\n",
            "Iteration 68, loss = 0.51307331\n",
            "Iteration 69, loss = 0.50632918\n",
            "Iteration 70, loss = 0.52449939\n",
            "Iteration 71, loss = 0.52080801\n",
            "Iteration 72, loss = 0.52221762\n",
            "Iteration 73, loss = 0.49805575\n",
            "Iteration 74, loss = 0.50328395\n",
            "Iteration 75, loss = 0.51168230\n",
            "Iteration 76, loss = 0.50543056\n",
            "Iteration 77, loss = 0.49526134\n",
            "Iteration 78, loss = 0.49799899\n",
            "Iteration 79, loss = 0.48949819\n",
            "Iteration 80, loss = 0.50446275\n",
            "Iteration 81, loss = 0.51736276\n",
            "Iteration 82, loss = 0.52132986\n",
            "Iteration 83, loss = 0.49901454\n",
            "Iteration 84, loss = 0.49942002\n",
            "Iteration 85, loss = 0.48520210\n",
            "Iteration 86, loss = 0.50224915\n",
            "Iteration 87, loss = 0.48247315\n",
            "Iteration 88, loss = 0.49628726\n",
            "Iteration 89, loss = 0.48637516\n",
            "Iteration 90, loss = 0.50047304\n",
            "Iteration 91, loss = 0.48775439\n",
            "Iteration 92, loss = 0.48707948\n",
            "Iteration 93, loss = 0.51604951\n",
            "Iteration 94, loss = 0.50405354\n",
            "Iteration 95, loss = 0.46907555\n",
            "Iteration 96, loss = 0.48732225\n",
            "Iteration 97, loss = 0.49594086\n",
            "Iteration 98, loss = 0.48492659\n",
            "Iteration 99, loss = 0.48429627\n",
            "Iteration 100, loss = 0.50529459\n",
            "Iteration 101, loss = 0.48604750\n",
            "Iteration 102, loss = 0.48898049\n",
            "Iteration 103, loss = 0.47468464\n",
            "Iteration 104, loss = 0.48421570\n",
            "Iteration 105, loss = 0.48411290\n",
            "Iteration 106, loss = 0.49060937\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.98938445\n",
            "Iteration 2, loss = 0.73211368\n",
            "Iteration 3, loss = 0.63825455\n",
            "Iteration 4, loss = 0.66201439\n",
            "Iteration 5, loss = 0.64569346\n",
            "Iteration 6, loss = 0.60409272\n",
            "Iteration 7, loss = 0.60123621\n",
            "Iteration 8, loss = 0.59897617\n",
            "Iteration 9, loss = 0.60075163\n",
            "Iteration 10, loss = 0.58941357\n",
            "Iteration 11, loss = 0.59178942\n",
            "Iteration 12, loss = 0.58410668\n",
            "Iteration 13, loss = 0.59044778\n",
            "Iteration 14, loss = 0.58896592\n",
            "Iteration 15, loss = 0.59644984\n",
            "Iteration 16, loss = 0.58340672\n",
            "Iteration 17, loss = 0.58567481\n",
            "Iteration 18, loss = 0.58714451\n",
            "Iteration 19, loss = 0.58971745\n",
            "Iteration 20, loss = 0.59080884\n",
            "Iteration 21, loss = 0.58388178\n",
            "Iteration 22, loss = 0.58983820\n",
            "Iteration 23, loss = 0.58542982\n",
            "Iteration 24, loss = 0.58119259\n",
            "Iteration 25, loss = 0.58865377\n",
            "Iteration 26, loss = 0.57948348\n",
            "Iteration 27, loss = 0.57280135\n",
            "Iteration 28, loss = 0.57836279\n",
            "Iteration 29, loss = 0.57271694\n",
            "Iteration 30, loss = 0.58381753\n",
            "Iteration 31, loss = 0.57653993\n",
            "Iteration 32, loss = 0.56361370\n",
            "Iteration 33, loss = 0.58216498\n",
            "Iteration 34, loss = 0.57467575\n",
            "Iteration 35, loss = 0.56836039\n",
            "Iteration 36, loss = 0.56337376\n",
            "Iteration 37, loss = 0.55958504\n",
            "Iteration 38, loss = 0.56161991\n",
            "Iteration 39, loss = 0.56008846\n",
            "Iteration 40, loss = 0.56595479\n",
            "Iteration 41, loss = 0.55595724\n",
            "Iteration 42, loss = 0.54089704\n",
            "Iteration 43, loss = 0.59863238\n",
            "Iteration 44, loss = 0.56304388\n",
            "Iteration 45, loss = 0.55573761\n",
            "Iteration 46, loss = 0.55447127\n",
            "Iteration 47, loss = 0.55777135\n",
            "Iteration 48, loss = 0.55168081\n",
            "Iteration 49, loss = 0.54634363\n",
            "Iteration 50, loss = 0.54002117\n",
            "Iteration 51, loss = 0.54681206\n",
            "Iteration 52, loss = 0.54539116\n",
            "Iteration 53, loss = 0.55458915\n",
            "Iteration 54, loss = 0.54058801\n",
            "Iteration 55, loss = 0.55603789\n",
            "Iteration 56, loss = 0.53941605\n",
            "Iteration 57, loss = 0.54226930\n",
            "Iteration 58, loss = 0.57468055\n",
            "Iteration 59, loss = 0.53998435\n",
            "Iteration 60, loss = 0.55978405\n",
            "Iteration 61, loss = 0.54416289\n",
            "Iteration 62, loss = 0.54760760\n",
            "Iteration 63, loss = 0.52741478\n",
            "Iteration 64, loss = 0.53836475\n",
            "Iteration 65, loss = 0.53333329\n",
            "Iteration 66, loss = 0.54939534\n",
            "Iteration 67, loss = 0.53891044\n",
            "Iteration 68, loss = 0.52069858\n",
            "Iteration 69, loss = 0.52903964\n",
            "Iteration 70, loss = 0.51999307\n",
            "Iteration 71, loss = 0.50611219\n",
            "Iteration 72, loss = 0.55396928\n",
            "Iteration 73, loss = 0.54039757\n",
            "Iteration 74, loss = 0.51711585\n",
            "Iteration 75, loss = 0.51473051\n",
            "Iteration 76, loss = 0.50676731\n",
            "Iteration 77, loss = 0.52290035\n",
            "Iteration 78, loss = 0.50815268\n",
            "Iteration 79, loss = 0.50511016\n",
            "Iteration 80, loss = 0.49858459\n",
            "Iteration 81, loss = 0.52050110\n",
            "Iteration 82, loss = 0.53056668\n",
            "Iteration 83, loss = 0.50737819\n",
            "Iteration 84, loss = 0.50179639\n",
            "Iteration 85, loss = 0.50388610\n",
            "Iteration 86, loss = 0.50439418\n",
            "Iteration 87, loss = 0.52523960\n",
            "Iteration 88, loss = 0.50064726\n",
            "Iteration 89, loss = 0.52625648\n",
            "Iteration 90, loss = 0.51402786\n",
            "Iteration 91, loss = 0.50558866\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.38558379\n",
            "Iteration 2, loss = 0.71815090\n",
            "Iteration 3, loss = 0.67540786\n",
            "Iteration 4, loss = 0.65727139\n",
            "Iteration 5, loss = 0.66598777\n",
            "Iteration 6, loss = 0.64491453\n",
            "Iteration 7, loss = 0.61087318\n",
            "Iteration 8, loss = 0.62400377\n",
            "Iteration 9, loss = 0.60312742\n",
            "Iteration 10, loss = 0.64439926\n",
            "Iteration 11, loss = 0.60537656\n",
            "Iteration 12, loss = 0.60495259\n",
            "Iteration 13, loss = 0.60550911\n",
            "Iteration 14, loss = 0.59488962\n",
            "Iteration 15, loss = 0.59563787\n",
            "Iteration 16, loss = 0.59566343\n",
            "Iteration 17, loss = 0.59829293\n",
            "Iteration 18, loss = 0.58755273\n",
            "Iteration 19, loss = 0.59018238\n",
            "Iteration 20, loss = 0.58071724\n",
            "Iteration 21, loss = 0.59283539\n",
            "Iteration 22, loss = 0.59775476\n",
            "Iteration 23, loss = 0.58357642\n",
            "Iteration 24, loss = 0.58148146\n",
            "Iteration 25, loss = 0.57727078\n",
            "Iteration 26, loss = 0.58353426\n",
            "Iteration 27, loss = 0.57278080\n",
            "Iteration 28, loss = 0.57283315\n",
            "Iteration 29, loss = 0.57440572\n",
            "Iteration 30, loss = 0.57538280\n",
            "Iteration 31, loss = 0.57228334\n",
            "Iteration 32, loss = 0.59457867\n",
            "Iteration 33, loss = 0.57392775\n",
            "Iteration 34, loss = 0.56932811\n",
            "Iteration 35, loss = 0.58297890\n",
            "Iteration 36, loss = 0.56976931\n",
            "Iteration 37, loss = 0.58860665\n",
            "Iteration 38, loss = 0.56804729\n",
            "Iteration 39, loss = 0.56401880\n",
            "Iteration 40, loss = 0.57123588\n",
            "Iteration 41, loss = 0.56110915\n",
            "Iteration 42, loss = 0.56151515\n",
            "Iteration 43, loss = 0.55501869\n",
            "Iteration 44, loss = 0.55496282\n",
            "Iteration 45, loss = 0.55293105\n",
            "Iteration 46, loss = 0.54960164\n",
            "Iteration 47, loss = 0.54898828\n",
            "Iteration 48, loss = 0.55164446\n",
            "Iteration 49, loss = 0.56119068\n",
            "Iteration 50, loss = 0.55353353\n",
            "Iteration 51, loss = 0.55934255\n",
            "Iteration 52, loss = 0.56402380\n",
            "Iteration 53, loss = 0.55327740\n",
            "Iteration 54, loss = 0.56135323\n",
            "Iteration 55, loss = 0.55411098\n",
            "Iteration 56, loss = 0.55356466\n",
            "Iteration 57, loss = 0.55229078\n",
            "Iteration 58, loss = 0.54755463\n",
            "Iteration 59, loss = 0.55158484\n",
            "Iteration 60, loss = 0.55444888\n",
            "Iteration 61, loss = 0.55473592\n",
            "Iteration 62, loss = 0.53797433\n",
            "Iteration 63, loss = 0.54202095\n",
            "Iteration 64, loss = 0.55568958\n",
            "Iteration 65, loss = 0.55722490\n",
            "Iteration 66, loss = 0.54502966\n",
            "Iteration 67, loss = 0.53861917\n",
            "Iteration 68, loss = 0.56321712\n",
            "Iteration 69, loss = 0.54282393\n",
            "Iteration 70, loss = 0.53307673\n",
            "Iteration 71, loss = 0.53090823\n",
            "Iteration 72, loss = 0.52754003\n",
            "Iteration 73, loss = 0.52784808\n",
            "Iteration 74, loss = 0.53305489\n",
            "Iteration 75, loss = 0.52136353\n",
            "Iteration 76, loss = 0.52511184\n",
            "Iteration 77, loss = 0.53184494\n",
            "Iteration 78, loss = 0.54668548\n",
            "Iteration 79, loss = 0.52918645\n",
            "Iteration 80, loss = 0.53267427\n",
            "Iteration 81, loss = 0.52346880\n",
            "Iteration 82, loss = 0.51194007\n",
            "Iteration 83, loss = 0.50608056\n",
            "Iteration 84, loss = 0.52818134\n",
            "Iteration 85, loss = 0.52332390\n",
            "Iteration 86, loss = 0.50938153\n",
            "Iteration 87, loss = 0.51771439\n",
            "Iteration 88, loss = 0.50651955\n",
            "Iteration 89, loss = 0.51268947\n",
            "Iteration 90, loss = 0.51932066\n",
            "Iteration 91, loss = 0.50362000\n",
            "Iteration 92, loss = 0.51847531\n",
            "Iteration 93, loss = 0.50093502\n",
            "Iteration 94, loss = 0.56565920\n",
            "Iteration 95, loss = 0.56618067\n",
            "Iteration 96, loss = 0.51773378\n",
            "Iteration 97, loss = 0.51007341\n",
            "Iteration 98, loss = 0.51802431\n",
            "Iteration 99, loss = 0.49324844\n",
            "Iteration 100, loss = 0.49037207\n",
            "Iteration 101, loss = 0.49007945\n",
            "Iteration 102, loss = 0.51897883\n",
            "Iteration 103, loss = 0.48865641\n",
            "Iteration 104, loss = 0.50112845\n",
            "Iteration 105, loss = 0.47976966\n",
            "Iteration 106, loss = 0.47681058\n",
            "Iteration 107, loss = 0.50288640\n",
            "Iteration 108, loss = 0.48105318\n",
            "Iteration 109, loss = 0.49452102\n",
            "Iteration 110, loss = 0.47859735\n",
            "Iteration 111, loss = 0.49743366\n",
            "Iteration 112, loss = 0.48318486\n",
            "Iteration 113, loss = 0.48684585\n",
            "Iteration 114, loss = 0.47595695\n",
            "Iteration 115, loss = 0.47826290\n",
            "Iteration 116, loss = 0.52078106\n",
            "Iteration 117, loss = 0.49714292\n",
            "Iteration 118, loss = 0.50364390\n",
            "Iteration 119, loss = 0.47915812\n",
            "Iteration 120, loss = 0.51601481\n",
            "Iteration 121, loss = 0.47424756\n",
            "Iteration 122, loss = 0.48130839\n",
            "Iteration 123, loss = 0.47634868\n",
            "Iteration 124, loss = 0.47233017\n",
            "Iteration 125, loss = 0.46935208\n",
            "Iteration 126, loss = 0.48868365\n",
            "Iteration 127, loss = 0.46779072\n",
            "Iteration 128, loss = 0.47173465\n",
            "Iteration 129, loss = 0.49040842\n",
            "Iteration 130, loss = 0.47398285\n",
            "Iteration 131, loss = 0.47264729\n",
            "Iteration 132, loss = 0.47171603\n",
            "Iteration 133, loss = 0.46732589\n",
            "Iteration 134, loss = 0.48043832\n",
            "Iteration 135, loss = 0.47947991\n",
            "Iteration 136, loss = 0.46533005\n",
            "Iteration 137, loss = 0.50512500\n",
            "Iteration 138, loss = 0.48309590\n",
            "Iteration 139, loss = 0.48308239\n",
            "Iteration 140, loss = 0.48423989\n",
            "Iteration 141, loss = 0.46248497\n",
            "Iteration 142, loss = 0.47820879\n",
            "Iteration 143, loss = 0.47049570\n",
            "Iteration 144, loss = 0.46191567\n",
            "Iteration 145, loss = 0.45242999\n",
            "Iteration 146, loss = 0.47087370\n",
            "Iteration 147, loss = 0.47581156\n",
            "Iteration 148, loss = 0.47631709\n",
            "Iteration 149, loss = 0.46496058\n",
            "Iteration 150, loss = 0.46163485\n",
            "Iteration 151, loss = 0.45905063\n",
            "Iteration 152, loss = 0.46319673\n",
            "Iteration 153, loss = 0.45624868\n",
            "Iteration 154, loss = 0.51081403\n",
            "Iteration 155, loss = 0.46101887\n",
            "Iteration 156, loss = 0.46103877\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.05227961\n",
            "Iteration 2, loss = 0.64959536\n",
            "Iteration 3, loss = 0.63587683\n",
            "Iteration 4, loss = 0.62022522\n",
            "Iteration 5, loss = 0.62223155\n",
            "Iteration 6, loss = 0.62022733\n",
            "Iteration 7, loss = 0.60447910\n",
            "Iteration 8, loss = 0.62528250\n",
            "Iteration 9, loss = 0.60626832\n",
            "Iteration 10, loss = 0.61333238\n",
            "Iteration 11, loss = 0.60947207\n",
            "Iteration 12, loss = 0.60854664\n",
            "Iteration 13, loss = 0.60201627\n",
            "Iteration 14, loss = 0.60192462\n",
            "Iteration 15, loss = 0.60786211\n",
            "Iteration 16, loss = 0.59835584\n",
            "Iteration 17, loss = 0.59544061\n",
            "Iteration 18, loss = 0.61610222\n",
            "Iteration 19, loss = 0.60236141\n",
            "Iteration 20, loss = 0.60397595\n",
            "Iteration 21, loss = 0.59842507\n",
            "Iteration 22, loss = 0.60688986\n",
            "Iteration 23, loss = 0.60216762\n",
            "Iteration 24, loss = 0.59650520\n",
            "Iteration 25, loss = 0.58801207\n",
            "Iteration 26, loss = 0.59837275\n",
            "Iteration 27, loss = 0.58587374\n",
            "Iteration 28, loss = 0.59351165\n",
            "Iteration 29, loss = 0.59492462\n",
            "Iteration 30, loss = 0.57861262\n",
            "Iteration 31, loss = 0.57871701\n",
            "Iteration 32, loss = 0.57281938\n",
            "Iteration 33, loss = 0.60405582\n",
            "Iteration 34, loss = 0.58357002\n",
            "Iteration 35, loss = 0.57947589\n",
            "Iteration 36, loss = 0.57693792\n",
            "Iteration 37, loss = 0.56988301\n",
            "Iteration 38, loss = 0.57003601\n",
            "Iteration 39, loss = 0.56388364\n",
            "Iteration 40, loss = 0.57825728\n",
            "Iteration 41, loss = 0.57787630\n",
            "Iteration 42, loss = 0.57741892\n",
            "Iteration 43, loss = 0.55930108\n",
            "Iteration 44, loss = 0.57530233\n",
            "Iteration 45, loss = 0.55678007\n",
            "Iteration 46, loss = 0.55677762\n",
            "Iteration 47, loss = 0.56336253\n",
            "Iteration 48, loss = 0.55353992\n",
            "Iteration 49, loss = 0.56116783\n",
            "Iteration 50, loss = 0.55683178\n",
            "Iteration 51, loss = 0.55162505\n",
            "Iteration 52, loss = 0.56216775\n",
            "Iteration 53, loss = 0.54468900\n",
            "Iteration 54, loss = 0.54076286\n",
            "Iteration 55, loss = 0.55819915\n",
            "Iteration 56, loss = 0.54028164\n",
            "Iteration 57, loss = 0.54427081\n",
            "Iteration 58, loss = 0.55612611\n",
            "Iteration 59, loss = 0.54929898\n",
            "Iteration 60, loss = 0.54112547\n",
            "Iteration 61, loss = 0.53955180\n",
            "Iteration 62, loss = 0.53254283\n",
            "Iteration 63, loss = 0.54660637\n",
            "Iteration 64, loss = 0.53893074\n",
            "Iteration 65, loss = 0.54380684\n",
            "Iteration 66, loss = 0.53310219\n",
            "Iteration 67, loss = 0.55342538\n",
            "Iteration 68, loss = 0.54590784\n",
            "Iteration 69, loss = 0.52739201\n",
            "Iteration 70, loss = 0.59571547\n",
            "Iteration 71, loss = 0.52888573\n",
            "Iteration 72, loss = 0.52039970\n",
            "Iteration 73, loss = 0.50521588\n",
            "Iteration 74, loss = 0.52294020\n",
            "Iteration 75, loss = 0.51149220\n",
            "Iteration 76, loss = 0.53259414\n",
            "Iteration 77, loss = 0.52401873\n",
            "Iteration 78, loss = 0.53286200\n",
            "Iteration 79, loss = 0.51402338\n",
            "Iteration 80, loss = 0.52898222\n",
            "Iteration 81, loss = 0.52244215\n",
            "Iteration 82, loss = 0.51239234\n",
            "Iteration 83, loss = 0.53603487\n",
            "Iteration 84, loss = 0.52374710\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.96008695\n",
            "Iteration 2, loss = 0.69303234\n",
            "Iteration 3, loss = 0.63200706\n",
            "Iteration 4, loss = 0.65334955\n",
            "Iteration 5, loss = 0.61998212\n",
            "Iteration 6, loss = 0.62502640\n",
            "Iteration 7, loss = 0.60954785\n",
            "Iteration 8, loss = 0.62379139\n",
            "Iteration 9, loss = 0.60531722\n",
            "Iteration 10, loss = 0.60840513\n",
            "Iteration 11, loss = 0.60569600\n",
            "Iteration 12, loss = 0.60487850\n",
            "Iteration 13, loss = 0.59757282\n",
            "Iteration 14, loss = 0.60661351\n",
            "Iteration 15, loss = 0.60047561\n",
            "Iteration 16, loss = 0.59754718\n",
            "Iteration 17, loss = 0.59872024\n",
            "Iteration 18, loss = 0.59731295\n",
            "Iteration 19, loss = 0.59109063\n",
            "Iteration 20, loss = 0.59024624\n",
            "Iteration 21, loss = 0.60166697\n",
            "Iteration 22, loss = 0.59148278\n",
            "Iteration 23, loss = 0.58723134\n",
            "Iteration 24, loss = 0.57957919\n",
            "Iteration 25, loss = 0.58533602\n",
            "Iteration 26, loss = 0.58701619\n",
            "Iteration 27, loss = 0.59285399\n",
            "Iteration 28, loss = 0.58712182\n",
            "Iteration 29, loss = 0.57975426\n",
            "Iteration 30, loss = 0.57909082\n",
            "Iteration 31, loss = 0.58441496\n",
            "Iteration 32, loss = 0.58134564\n",
            "Iteration 33, loss = 0.57838554\n",
            "Iteration 34, loss = 0.56413837\n",
            "Iteration 35, loss = 0.57313195\n",
            "Iteration 36, loss = 0.56535262\n",
            "Iteration 37, loss = 0.57069162\n",
            "Iteration 38, loss = 0.58167838\n",
            "Iteration 39, loss = 0.57709009\n",
            "Iteration 40, loss = 0.56784697\n",
            "Iteration 41, loss = 0.56505557\n",
            "Iteration 42, loss = 0.56052828\n",
            "Iteration 43, loss = 0.58456766\n",
            "Iteration 44, loss = 0.56860567\n",
            "Iteration 45, loss = 0.56599818\n",
            "Iteration 46, loss = 0.56144554\n",
            "Iteration 47, loss = 0.54737626\n",
            "Iteration 48, loss = 0.56026027\n",
            "Iteration 49, loss = 0.55386920\n",
            "Iteration 50, loss = 0.58470074\n",
            "Iteration 51, loss = 0.56976288\n",
            "Iteration 52, loss = 0.56809955\n",
            "Iteration 53, loss = 0.54712378\n",
            "Iteration 54, loss = 0.60037588\n",
            "Iteration 55, loss = 0.56180548\n",
            "Iteration 56, loss = 0.55760179\n",
            "Iteration 57, loss = 0.57190587\n",
            "Iteration 58, loss = 0.54677701\n",
            "Iteration 59, loss = 0.53820368\n",
            "Iteration 60, loss = 0.56308768\n",
            "Iteration 61, loss = 0.54879070\n",
            "Iteration 62, loss = 0.58844356\n",
            "Iteration 63, loss = 0.55299159\n",
            "Iteration 64, loss = 0.54726334\n",
            "Iteration 65, loss = 0.54565289\n",
            "Iteration 66, loss = 0.54678001\n",
            "Iteration 67, loss = 0.54646513\n",
            "Iteration 68, loss = 0.54353039\n",
            "Iteration 69, loss = 0.56617137\n",
            "Iteration 70, loss = 0.54338130\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69621686\n",
            "Iteration 2, loss = 0.67800536\n",
            "Iteration 3, loss = 0.67476913\n",
            "Iteration 4, loss = 0.67287134\n",
            "Iteration 5, loss = 0.67094020\n",
            "Iteration 6, loss = 0.66992477\n",
            "Iteration 7, loss = 0.66924011\n",
            "Iteration 8, loss = 0.66868126\n",
            "Iteration 9, loss = 0.66735337\n",
            "Iteration 10, loss = 0.66629021\n",
            "Iteration 11, loss = 0.66525965\n",
            "Iteration 12, loss = 0.66424654\n",
            "Iteration 13, loss = 0.66330959\n",
            "Iteration 14, loss = 0.66268453\n",
            "Iteration 15, loss = 0.66140403\n",
            "Iteration 16, loss = 0.66045662\n",
            "Iteration 17, loss = 0.65954355\n",
            "Iteration 18, loss = 0.65860477\n",
            "Iteration 19, loss = 0.65841390\n",
            "Iteration 20, loss = 0.65672533\n",
            "Iteration 21, loss = 0.65585105\n",
            "Iteration 22, loss = 0.65493248\n",
            "Iteration 23, loss = 0.65453142\n",
            "Iteration 24, loss = 0.65310990\n",
            "Iteration 25, loss = 0.65260564\n",
            "Iteration 26, loss = 0.65134839\n",
            "Iteration 27, loss = 0.65069327\n",
            "Iteration 28, loss = 0.64947588\n",
            "Iteration 29, loss = 0.64904059\n",
            "Iteration 30, loss = 0.64777450\n",
            "Iteration 31, loss = 0.64737308\n",
            "Iteration 32, loss = 0.64614031\n",
            "Iteration 33, loss = 0.64536945\n",
            "Iteration 34, loss = 0.64432593\n",
            "Iteration 35, loss = 0.64376341\n",
            "Iteration 36, loss = 0.64305030\n",
            "Iteration 37, loss = 0.64236628\n",
            "Iteration 38, loss = 0.64154263\n",
            "Iteration 39, loss = 0.64052094\n",
            "Iteration 40, loss = 0.63953349\n",
            "Iteration 41, loss = 0.63893812\n",
            "Iteration 42, loss = 0.63833525\n",
            "Iteration 43, loss = 0.63739518\n",
            "Iteration 44, loss = 0.63664562\n",
            "Iteration 45, loss = 0.63580824\n",
            "Iteration 46, loss = 0.63496435\n",
            "Iteration 47, loss = 0.63418430\n",
            "Iteration 48, loss = 0.63346299\n",
            "Iteration 49, loss = 0.63287675\n",
            "Iteration 50, loss = 0.63239909\n",
            "Iteration 51, loss = 0.63142295\n",
            "Iteration 52, loss = 0.63083839\n",
            "Iteration 53, loss = 0.62976590\n",
            "Iteration 54, loss = 0.62924644\n",
            "Iteration 55, loss = 0.62826688\n",
            "Iteration 56, loss = 0.62758297\n",
            "Iteration 57, loss = 0.62677144\n",
            "Iteration 58, loss = 0.62670147\n",
            "Iteration 59, loss = 0.62557636\n",
            "Iteration 60, loss = 0.62497047\n",
            "Iteration 61, loss = 0.62477730\n",
            "Iteration 62, loss = 0.62346514\n",
            "Iteration 63, loss = 0.62279752\n",
            "Iteration 64, loss = 0.62278128\n",
            "Iteration 65, loss = 0.62153311\n",
            "Iteration 66, loss = 0.62104821\n",
            "Iteration 67, loss = 0.62041244\n",
            "Iteration 68, loss = 0.62023593\n",
            "Iteration 69, loss = 0.61915806\n",
            "Iteration 70, loss = 0.61886715\n",
            "Iteration 71, loss = 0.61816877\n",
            "Iteration 72, loss = 0.61770407\n",
            "Iteration 73, loss = 0.61702482\n",
            "Iteration 74, loss = 0.61656924\n",
            "Iteration 75, loss = 0.61612796\n",
            "Iteration 76, loss = 0.61553107\n",
            "Iteration 77, loss = 0.61514580\n",
            "Iteration 78, loss = 0.61463781\n",
            "Iteration 79, loss = 0.61439610\n",
            "Iteration 80, loss = 0.61366826\n",
            "Iteration 81, loss = 0.61389131\n",
            "Iteration 82, loss = 0.61340271\n",
            "Iteration 83, loss = 0.61229652\n",
            "Iteration 84, loss = 0.61192283\n",
            "Iteration 85, loss = 0.61155458\n",
            "Iteration 86, loss = 0.61124831\n",
            "Iteration 87, loss = 0.61098547\n",
            "Iteration 88, loss = 0.61026043\n",
            "Iteration 89, loss = 0.61000914\n",
            "Iteration 90, loss = 0.60992893\n",
            "Iteration 91, loss = 0.60940782\n",
            "Iteration 92, loss = 0.60930333\n",
            "Iteration 93, loss = 0.60893728\n",
            "Iteration 94, loss = 0.60850093\n",
            "Iteration 95, loss = 0.60824918\n",
            "Iteration 96, loss = 0.60800312\n",
            "Iteration 97, loss = 0.60755716\n",
            "Iteration 98, loss = 0.60764868\n",
            "Iteration 99, loss = 0.60683089\n",
            "Iteration 100, loss = 0.60733095\n",
            "Iteration 101, loss = 0.60656816\n",
            "Iteration 102, loss = 0.60631209\n",
            "Iteration 103, loss = 0.60620921\n",
            "Iteration 104, loss = 0.60581460\n",
            "Iteration 105, loss = 0.60570397\n",
            "Iteration 106, loss = 0.60538629\n",
            "Iteration 107, loss = 0.60506927\n",
            "Iteration 108, loss = 0.60542543\n",
            "Iteration 109, loss = 0.60474404\n",
            "Iteration 110, loss = 0.60477949\n",
            "Iteration 111, loss = 0.60500030\n",
            "Iteration 112, loss = 0.60453875\n",
            "Iteration 113, loss = 0.60428074\n",
            "Iteration 114, loss = 0.60393864\n",
            "Iteration 115, loss = 0.60376243\n",
            "Iteration 116, loss = 0.60381276\n",
            "Iteration 117, loss = 0.60355864\n",
            "Iteration 118, loss = 0.60347477\n",
            "Iteration 119, loss = 0.60303557\n",
            "Iteration 120, loss = 0.60302047\n",
            "Iteration 121, loss = 0.60279483\n",
            "Iteration 122, loss = 0.60294890\n",
            "Iteration 123, loss = 0.60262555\n",
            "Iteration 124, loss = 0.60257541\n",
            "Iteration 125, loss = 0.60239902\n",
            "Iteration 126, loss = 0.60218629\n",
            "Iteration 127, loss = 0.60223775\n",
            "Iteration 128, loss = 0.60269953\n",
            "Iteration 129, loss = 0.60226739\n",
            "Iteration 130, loss = 0.60175745\n",
            "Iteration 131, loss = 0.60186773\n",
            "Iteration 132, loss = 0.60191949\n",
            "Iteration 133, loss = 0.60161675\n",
            "Iteration 134, loss = 0.60204209\n",
            "Iteration 135, loss = 0.60129060\n",
            "Iteration 136, loss = 0.60152872\n",
            "Iteration 137, loss = 0.60134144\n",
            "Iteration 138, loss = 0.60119038\n",
            "Iteration 139, loss = 0.60107380\n",
            "Iteration 140, loss = 0.60114731\n",
            "Iteration 141, loss = 0.60094208\n",
            "Iteration 142, loss = 0.60078484\n",
            "Iteration 143, loss = 0.60067798\n",
            "Iteration 144, loss = 0.60102146\n",
            "Iteration 145, loss = 0.60095766\n",
            "Iteration 146, loss = 0.60085176\n",
            "Iteration 147, loss = 0.60107530\n",
            "Iteration 148, loss = 0.60089206\n",
            "Iteration 149, loss = 0.60034454\n",
            "Iteration 150, loss = 0.60021366\n",
            "Iteration 151, loss = 0.60075772\n",
            "Iteration 152, loss = 0.60023158\n",
            "Iteration 153, loss = 0.60005761\n",
            "Iteration 154, loss = 0.60022449\n",
            "Iteration 155, loss = 0.60003553\n",
            "Iteration 156, loss = 0.60006735\n",
            "Iteration 157, loss = 0.60023014\n",
            "Iteration 158, loss = 0.60001392\n",
            "Iteration 159, loss = 0.59981891\n",
            "Iteration 160, loss = 0.60005492\n",
            "Iteration 161, loss = 0.59994116\n",
            "Iteration 162, loss = 0.59964469\n",
            "Iteration 163, loss = 0.59965844\n",
            "Iteration 164, loss = 0.59963075\n",
            "Iteration 165, loss = 0.59955994\n",
            "Iteration 166, loss = 0.59946920\n",
            "Iteration 167, loss = 0.59940466\n",
            "Iteration 168, loss = 0.59966476\n",
            "Iteration 169, loss = 0.59945363\n",
            "Iteration 170, loss = 0.60027859\n",
            "Iteration 171, loss = 0.59957031\n",
            "Iteration 172, loss = 0.59952638\n",
            "Iteration 173, loss = 0.59950963\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67671497\n",
            "Iteration 2, loss = 0.67586900\n",
            "Iteration 3, loss = 0.67442231\n",
            "Iteration 4, loss = 0.67297482\n",
            "Iteration 5, loss = 0.67189866\n",
            "Iteration 6, loss = 0.67057046\n",
            "Iteration 7, loss = 0.66973519\n",
            "Iteration 8, loss = 0.66800157\n",
            "Iteration 9, loss = 0.66717212\n",
            "Iteration 10, loss = 0.66611289\n",
            "Iteration 11, loss = 0.66491599\n",
            "Iteration 12, loss = 0.66413824\n",
            "Iteration 13, loss = 0.66300798\n",
            "Iteration 14, loss = 0.66180755\n",
            "Iteration 15, loss = 0.66128400\n",
            "Iteration 16, loss = 0.66015642\n",
            "Iteration 17, loss = 0.65880249\n",
            "Iteration 18, loss = 0.65771390\n",
            "Iteration 19, loss = 0.65679980\n",
            "Iteration 20, loss = 0.65579144\n",
            "Iteration 21, loss = 0.65494347\n",
            "Iteration 22, loss = 0.65486869\n",
            "Iteration 23, loss = 0.65341844\n",
            "Iteration 24, loss = 0.65220162\n",
            "Iteration 25, loss = 0.65155084\n",
            "Iteration 26, loss = 0.65107940\n",
            "Iteration 27, loss = 0.64988776\n",
            "Iteration 28, loss = 0.64906604\n",
            "Iteration 29, loss = 0.64778453\n",
            "Iteration 30, loss = 0.64655997\n",
            "Iteration 31, loss = 0.64574826\n",
            "Iteration 32, loss = 0.64502490\n",
            "Iteration 33, loss = 0.64410758\n",
            "Iteration 34, loss = 0.64308241\n",
            "Iteration 35, loss = 0.64247918\n",
            "Iteration 36, loss = 0.64174792\n",
            "Iteration 37, loss = 0.64123609\n",
            "Iteration 38, loss = 0.63985063\n",
            "Iteration 39, loss = 0.63921572\n",
            "Iteration 40, loss = 0.63836096\n",
            "Iteration 41, loss = 0.63755690\n",
            "Iteration 42, loss = 0.63713352\n",
            "Iteration 43, loss = 0.63607058\n",
            "Iteration 44, loss = 0.63580022\n",
            "Iteration 45, loss = 0.63488521\n",
            "Iteration 46, loss = 0.63405640\n",
            "Iteration 47, loss = 0.63333391\n",
            "Iteration 48, loss = 0.63256448\n",
            "Iteration 49, loss = 0.63181264\n",
            "Iteration 50, loss = 0.63123939\n",
            "Iteration 51, loss = 0.63038845\n",
            "Iteration 52, loss = 0.62951138\n",
            "Iteration 53, loss = 0.62953450\n",
            "Iteration 54, loss = 0.62826656\n",
            "Iteration 55, loss = 0.62859965\n",
            "Iteration 56, loss = 0.62811792\n",
            "Iteration 57, loss = 0.62650096\n",
            "Iteration 58, loss = 0.62620814\n",
            "Iteration 59, loss = 0.62541821\n",
            "Iteration 60, loss = 0.62446318\n",
            "Iteration 61, loss = 0.62408023\n",
            "Iteration 62, loss = 0.62330934\n",
            "Iteration 63, loss = 0.62288595\n",
            "Iteration 64, loss = 0.62244043\n",
            "Iteration 65, loss = 0.62197341\n",
            "Iteration 66, loss = 0.62116673\n",
            "Iteration 67, loss = 0.62149047\n",
            "Iteration 68, loss = 0.62036196\n",
            "Iteration 69, loss = 0.62086656\n",
            "Iteration 70, loss = 0.61942989\n",
            "Iteration 71, loss = 0.61861322\n",
            "Iteration 72, loss = 0.61828069\n",
            "Iteration 73, loss = 0.61777155\n",
            "Iteration 74, loss = 0.61753654\n",
            "Iteration 75, loss = 0.61703974\n",
            "Iteration 76, loss = 0.61657677\n",
            "Iteration 77, loss = 0.61596044\n",
            "Iteration 78, loss = 0.61550651\n",
            "Iteration 79, loss = 0.61578460\n",
            "Iteration 80, loss = 0.61486277\n",
            "Iteration 81, loss = 0.61459094\n",
            "Iteration 82, loss = 0.61479474\n",
            "Iteration 83, loss = 0.61381834\n",
            "Iteration 84, loss = 0.61342448\n",
            "Iteration 85, loss = 0.61334696\n",
            "Iteration 86, loss = 0.61278651\n",
            "Iteration 87, loss = 0.61217051\n",
            "Iteration 88, loss = 0.61209469\n",
            "Iteration 89, loss = 0.61175186\n",
            "Iteration 90, loss = 0.61134406\n",
            "Iteration 91, loss = 0.61113852\n",
            "Iteration 92, loss = 0.61117549\n",
            "Iteration 93, loss = 0.61063899\n",
            "Iteration 94, loss = 0.61020043\n",
            "Iteration 95, loss = 0.61007921\n",
            "Iteration 96, loss = 0.60982172\n",
            "Iteration 97, loss = 0.60959851\n",
            "Iteration 98, loss = 0.60909497\n",
            "Iteration 99, loss = 0.60942890\n",
            "Iteration 100, loss = 0.60899602\n",
            "Iteration 101, loss = 0.60855864\n",
            "Iteration 102, loss = 0.60827814\n",
            "Iteration 103, loss = 0.60837557\n",
            "Iteration 104, loss = 0.60814063\n",
            "Iteration 105, loss = 0.60794893\n",
            "Iteration 106, loss = 0.60765108\n",
            "Iteration 107, loss = 0.60732750\n",
            "Iteration 108, loss = 0.60716183\n",
            "Iteration 109, loss = 0.60739546\n",
            "Iteration 110, loss = 0.60682656\n",
            "Iteration 111, loss = 0.60712140\n",
            "Iteration 112, loss = 0.60716853\n",
            "Iteration 113, loss = 0.60690513\n",
            "Iteration 114, loss = 0.60625943\n",
            "Iteration 115, loss = 0.60610815\n",
            "Iteration 116, loss = 0.60594384\n",
            "Iteration 117, loss = 0.60591832\n",
            "Iteration 118, loss = 0.60594301\n",
            "Iteration 119, loss = 0.60569890\n",
            "Iteration 120, loss = 0.60549518\n",
            "Iteration 121, loss = 0.60525371\n",
            "Iteration 122, loss = 0.60582647\n",
            "Iteration 123, loss = 0.60486515\n",
            "Iteration 124, loss = 0.60507410\n",
            "Iteration 125, loss = 0.60495252\n",
            "Iteration 126, loss = 0.60459745\n",
            "Iteration 127, loss = 0.60497366\n",
            "Iteration 128, loss = 0.60456543\n",
            "Iteration 129, loss = 0.60473191\n",
            "Iteration 130, loss = 0.60447590\n",
            "Iteration 131, loss = 0.60543688\n",
            "Iteration 132, loss = 0.60433764\n",
            "Iteration 133, loss = 0.60416943\n",
            "Iteration 134, loss = 0.60387165\n",
            "Iteration 135, loss = 0.60398708\n",
            "Iteration 136, loss = 0.60385809\n",
            "Iteration 137, loss = 0.60443445\n",
            "Iteration 138, loss = 0.60384639\n",
            "Iteration 139, loss = 0.60344004\n",
            "Iteration 140, loss = 0.60362227\n",
            "Iteration 141, loss = 0.60345458\n",
            "Iteration 142, loss = 0.60388175\n",
            "Iteration 143, loss = 0.60506733\n",
            "Iteration 144, loss = 0.60475268\n",
            "Iteration 145, loss = 0.60315768\n",
            "Iteration 146, loss = 0.60319696\n",
            "Iteration 147, loss = 0.60349708\n",
            "Iteration 148, loss = 0.60368073\n",
            "Iteration 149, loss = 0.60328170\n",
            "Iteration 150, loss = 0.60278661\n",
            "Iteration 151, loss = 0.60369746\n",
            "Iteration 152, loss = 0.60290154\n",
            "Iteration 153, loss = 0.60290134\n",
            "Iteration 154, loss = 0.60264838\n",
            "Iteration 155, loss = 0.60271456\n",
            "Iteration 156, loss = 0.60266408\n",
            "Iteration 157, loss = 0.60238592\n",
            "Iteration 158, loss = 0.60252685\n",
            "Iteration 159, loss = 0.60248456\n",
            "Iteration 160, loss = 0.60234979\n",
            "Iteration 161, loss = 0.60224223\n",
            "Iteration 162, loss = 0.60223393\n",
            "Iteration 163, loss = 0.60223512\n",
            "Iteration 164, loss = 0.60228041\n",
            "Iteration 165, loss = 0.60245495\n",
            "Iteration 166, loss = 0.60219025\n",
            "Iteration 167, loss = 0.60217334\n",
            "Iteration 168, loss = 0.60247300\n",
            "Iteration 169, loss = 0.60237771\n",
            "Iteration 170, loss = 0.60199666\n",
            "Iteration 171, loss = 0.60195605\n",
            "Iteration 172, loss = 0.60167171\n",
            "Iteration 173, loss = 0.60171746\n",
            "Iteration 174, loss = 0.60192298\n",
            "Iteration 175, loss = 0.60205920\n",
            "Iteration 176, loss = 0.60138697\n",
            "Iteration 177, loss = 0.60167326\n",
            "Iteration 178, loss = 0.60181260\n",
            "Iteration 179, loss = 0.60144891\n",
            "Iteration 180, loss = 0.60151669\n",
            "Iteration 181, loss = 0.60160786\n",
            "Iteration 182, loss = 0.60131556\n",
            "Iteration 183, loss = 0.60139071\n",
            "Iteration 184, loss = 0.60124999\n",
            "Iteration 185, loss = 0.60155697\n",
            "Iteration 186, loss = 0.60116463\n",
            "Iteration 187, loss = 0.60131629\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69237930\n",
            "Iteration 2, loss = 0.67201834\n",
            "Iteration 3, loss = 0.67044262\n",
            "Iteration 4, loss = 0.67009007\n",
            "Iteration 5, loss = 0.66778790\n",
            "Iteration 6, loss = 0.66700052\n",
            "Iteration 7, loss = 0.66608745\n",
            "Iteration 8, loss = 0.66460530\n",
            "Iteration 9, loss = 0.66413023\n",
            "Iteration 10, loss = 0.66290388\n",
            "Iteration 11, loss = 0.66167885\n",
            "Iteration 12, loss = 0.66070193\n",
            "Iteration 13, loss = 0.65995841\n",
            "Iteration 14, loss = 0.65871021\n",
            "Iteration 15, loss = 0.65752376\n",
            "Iteration 16, loss = 0.65659799\n",
            "Iteration 17, loss = 0.65554894\n",
            "Iteration 18, loss = 0.65468992\n",
            "Iteration 19, loss = 0.65373897\n",
            "Iteration 20, loss = 0.65345718\n",
            "Iteration 21, loss = 0.65200404\n",
            "Iteration 22, loss = 0.65134663\n",
            "Iteration 23, loss = 0.65022009\n",
            "Iteration 24, loss = 0.64939702\n",
            "Iteration 25, loss = 0.64829488\n",
            "Iteration 26, loss = 0.64728750\n",
            "Iteration 27, loss = 0.64708153\n",
            "Iteration 28, loss = 0.64585835\n",
            "Iteration 29, loss = 0.64538411\n",
            "Iteration 30, loss = 0.64413822\n",
            "Iteration 31, loss = 0.64302904\n",
            "Iteration 32, loss = 0.64293221\n",
            "Iteration 33, loss = 0.64174396\n",
            "Iteration 34, loss = 0.64071973\n",
            "Iteration 35, loss = 0.63980578\n",
            "Iteration 36, loss = 0.64026542\n",
            "Iteration 37, loss = 0.63781690\n",
            "Iteration 38, loss = 0.63729131\n",
            "Iteration 39, loss = 0.63681554\n",
            "Iteration 40, loss = 0.63583369\n",
            "Iteration 41, loss = 0.63511829\n",
            "Iteration 42, loss = 0.63424738\n",
            "Iteration 43, loss = 0.63384078\n",
            "Iteration 44, loss = 0.63278758\n",
            "Iteration 45, loss = 0.63199070\n",
            "Iteration 46, loss = 0.63148756\n",
            "Iteration 47, loss = 0.63101058\n",
            "Iteration 48, loss = 0.63006952\n",
            "Iteration 49, loss = 0.62971432\n",
            "Iteration 50, loss = 0.62972772\n",
            "Iteration 51, loss = 0.62880916\n",
            "Iteration 52, loss = 0.62773117\n",
            "Iteration 53, loss = 0.62734731\n",
            "Iteration 54, loss = 0.62597044\n",
            "Iteration 55, loss = 0.62550331\n",
            "Iteration 56, loss = 0.62509614\n",
            "Iteration 57, loss = 0.62452398\n",
            "Iteration 58, loss = 0.62393095\n",
            "Iteration 59, loss = 0.62335687\n",
            "Iteration 60, loss = 0.62276372\n",
            "Iteration 61, loss = 0.62211708\n",
            "Iteration 62, loss = 0.62167905\n",
            "Iteration 63, loss = 0.62131185\n",
            "Iteration 64, loss = 0.62108764\n",
            "Iteration 65, loss = 0.62043836\n",
            "Iteration 66, loss = 0.61975025\n",
            "Iteration 67, loss = 0.61919898\n",
            "Iteration 68, loss = 0.61924225\n",
            "Iteration 69, loss = 0.61852983\n",
            "Iteration 70, loss = 0.61814347\n",
            "Iteration 71, loss = 0.61763022\n",
            "Iteration 72, loss = 0.61727496\n",
            "Iteration 73, loss = 0.61691865\n",
            "Iteration 74, loss = 0.61654294\n",
            "Iteration 75, loss = 0.61593314\n",
            "Iteration 76, loss = 0.61586929\n",
            "Iteration 77, loss = 0.61556990\n",
            "Iteration 78, loss = 0.61579004\n",
            "Iteration 79, loss = 0.61466666\n",
            "Iteration 80, loss = 0.61429467\n",
            "Iteration 81, loss = 0.61383189\n",
            "Iteration 82, loss = 0.61359659\n",
            "Iteration 83, loss = 0.61335522\n",
            "Iteration 84, loss = 0.61301662\n",
            "Iteration 85, loss = 0.61332075\n",
            "Iteration 86, loss = 0.61262189\n",
            "Iteration 87, loss = 0.61251736\n",
            "Iteration 88, loss = 0.61194618\n",
            "Iteration 89, loss = 0.61179784\n",
            "Iteration 90, loss = 0.61146587\n",
            "Iteration 91, loss = 0.61146824\n",
            "Iteration 92, loss = 0.61090326\n",
            "Iteration 93, loss = 0.61076961\n",
            "Iteration 94, loss = 0.61057381\n",
            "Iteration 95, loss = 0.61062934\n",
            "Iteration 96, loss = 0.61011725\n",
            "Iteration 97, loss = 0.60998092\n",
            "Iteration 98, loss = 0.60993978\n",
            "Iteration 99, loss = 0.61060666\n",
            "Iteration 100, loss = 0.60936977\n",
            "Iteration 101, loss = 0.60943224\n",
            "Iteration 102, loss = 0.60921435\n",
            "Iteration 103, loss = 0.60892387\n",
            "Iteration 104, loss = 0.60884700\n",
            "Iteration 105, loss = 0.60891671\n",
            "Iteration 106, loss = 0.60875514\n",
            "Iteration 107, loss = 0.60852651\n",
            "Iteration 108, loss = 0.60826909\n",
            "Iteration 109, loss = 0.60897467\n",
            "Iteration 110, loss = 0.60894733\n",
            "Iteration 111, loss = 0.60814437\n",
            "Iteration 112, loss = 0.60778147\n",
            "Iteration 113, loss = 0.60789723\n",
            "Iteration 114, loss = 0.60784527\n",
            "Iteration 115, loss = 0.60766257\n",
            "Iteration 116, loss = 0.60733100\n",
            "Iteration 117, loss = 0.60751618\n",
            "Iteration 118, loss = 0.60742804\n",
            "Iteration 119, loss = 0.60766133\n",
            "Iteration 120, loss = 0.60701583\n",
            "Iteration 121, loss = 0.60686151\n",
            "Iteration 122, loss = 0.60692642\n",
            "Iteration 123, loss = 0.60677643\n",
            "Iteration 124, loss = 0.60669706\n",
            "Iteration 125, loss = 0.60650970\n",
            "Iteration 126, loss = 0.60651748\n",
            "Iteration 127, loss = 0.60702791\n",
            "Iteration 128, loss = 0.60632114\n",
            "Iteration 129, loss = 0.60628857\n",
            "Iteration 130, loss = 0.60660839\n",
            "Iteration 131, loss = 0.60608072\n",
            "Iteration 132, loss = 0.60598757\n",
            "Iteration 133, loss = 0.60659466\n",
            "Iteration 134, loss = 0.60581980\n",
            "Iteration 135, loss = 0.60588909\n",
            "Iteration 136, loss = 0.60591600\n",
            "Iteration 137, loss = 0.60560535\n",
            "Iteration 138, loss = 0.60621281\n",
            "Iteration 139, loss = 0.60651460\n",
            "Iteration 140, loss = 0.60565345\n",
            "Iteration 141, loss = 0.60568073\n",
            "Iteration 142, loss = 0.60571346\n",
            "Iteration 143, loss = 0.60592535\n",
            "Iteration 144, loss = 0.60511877\n",
            "Iteration 145, loss = 0.60583904\n",
            "Iteration 146, loss = 0.60548647\n",
            "Iteration 147, loss = 0.60514077\n",
            "Iteration 148, loss = 0.60516674\n",
            "Iteration 149, loss = 0.60512282\n",
            "Iteration 150, loss = 0.60517494\n",
            "Iteration 151, loss = 0.60503256\n",
            "Iteration 152, loss = 0.60504399\n",
            "Iteration 153, loss = 0.60492300\n",
            "Iteration 154, loss = 0.60522440\n",
            "Iteration 155, loss = 0.60512703\n",
            "Iteration 156, loss = 0.60542613\n",
            "Iteration 157, loss = 0.60464878\n",
            "Iteration 158, loss = 0.60482613\n",
            "Iteration 159, loss = 0.60463774\n",
            "Iteration 160, loss = 0.60451012\n",
            "Iteration 161, loss = 0.60445345\n",
            "Iteration 162, loss = 0.60466774\n",
            "Iteration 163, loss = 0.60460169\n",
            "Iteration 164, loss = 0.60492647\n",
            "Iteration 165, loss = 0.60445547\n",
            "Iteration 166, loss = 0.60542949\n",
            "Iteration 167, loss = 0.60477039\n",
            "Iteration 168, loss = 0.60463374\n",
            "Iteration 169, loss = 0.60454203\n",
            "Iteration 170, loss = 0.60475081\n",
            "Iteration 171, loss = 0.60428890\n",
            "Iteration 172, loss = 0.60565974\n",
            "Iteration 173, loss = 0.60387405\n",
            "Iteration 174, loss = 0.60444857\n",
            "Iteration 175, loss = 0.60432238\n",
            "Iteration 176, loss = 0.60398784\n",
            "Iteration 177, loss = 0.60402246\n",
            "Iteration 178, loss = 0.60420231\n",
            "Iteration 179, loss = 0.60496029\n",
            "Iteration 180, loss = 0.60387751\n",
            "Iteration 181, loss = 0.60396424\n",
            "Iteration 182, loss = 0.60428709\n",
            "Iteration 183, loss = 0.60373281\n",
            "Iteration 184, loss = 0.60372737\n",
            "Iteration 185, loss = 0.60416427\n",
            "Iteration 186, loss = 0.60401466\n",
            "Iteration 187, loss = 0.60381306\n",
            "Iteration 188, loss = 0.60382776\n",
            "Iteration 189, loss = 0.60420070\n",
            "Iteration 190, loss = 0.60400833\n",
            "Iteration 191, loss = 0.60397941\n",
            "Iteration 192, loss = 0.60452237\n",
            "Iteration 193, loss = 0.60363461\n",
            "Iteration 194, loss = 0.60391068\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.67669295\n",
            "Iteration 2, loss = 0.67530349\n",
            "Iteration 3, loss = 0.67442371\n",
            "Iteration 4, loss = 0.67386830\n",
            "Iteration 5, loss = 0.67322966\n",
            "Iteration 6, loss = 0.67251002\n",
            "Iteration 7, loss = 0.67152549\n",
            "Iteration 8, loss = 0.67098243\n",
            "Iteration 9, loss = 0.67022240\n",
            "Iteration 10, loss = 0.66945919\n",
            "Iteration 11, loss = 0.66885210\n",
            "Iteration 12, loss = 0.66813733\n",
            "Iteration 13, loss = 0.66756605\n",
            "Iteration 14, loss = 0.66686492\n",
            "Iteration 15, loss = 0.66655444\n",
            "Iteration 16, loss = 0.66555307\n",
            "Iteration 17, loss = 0.66557621\n",
            "Iteration 18, loss = 0.66431482\n",
            "Iteration 19, loss = 0.66409842\n",
            "Iteration 20, loss = 0.66322487\n",
            "Iteration 21, loss = 0.66290024\n",
            "Iteration 22, loss = 0.66209108\n",
            "Iteration 23, loss = 0.66198003\n",
            "Iteration 24, loss = 0.66090842\n",
            "Iteration 25, loss = 0.66041013\n",
            "Iteration 26, loss = 0.66031688\n",
            "Iteration 27, loss = 0.65929974\n",
            "Iteration 28, loss = 0.65868323\n",
            "Iteration 29, loss = 0.65830077\n",
            "Iteration 30, loss = 0.65790125\n",
            "Iteration 31, loss = 0.65748645\n",
            "Iteration 32, loss = 0.65652655\n",
            "Iteration 33, loss = 0.65590907\n",
            "Iteration 34, loss = 0.65554338\n",
            "Iteration 35, loss = 0.65522801\n",
            "Iteration 36, loss = 0.65444964\n",
            "Iteration 37, loss = 0.65382484\n",
            "Iteration 38, loss = 0.65367203\n",
            "Iteration 39, loss = 0.65315889\n",
            "Iteration 40, loss = 0.65266383\n",
            "Iteration 41, loss = 0.65212657\n",
            "Iteration 42, loss = 0.65195598\n",
            "Iteration 43, loss = 0.65111819\n",
            "Iteration 44, loss = 0.65137875\n",
            "Iteration 45, loss = 0.64980740\n",
            "Iteration 46, loss = 0.64960905\n",
            "Iteration 47, loss = 0.64885241\n",
            "Iteration 48, loss = 0.64825746\n",
            "Iteration 49, loss = 0.64774371\n",
            "Iteration 50, loss = 0.64745442\n",
            "Iteration 51, loss = 0.64722509\n",
            "Iteration 52, loss = 0.64645058\n",
            "Iteration 53, loss = 0.64626504\n",
            "Iteration 54, loss = 0.64545022\n",
            "Iteration 55, loss = 0.64516547\n",
            "Iteration 56, loss = 0.64453488\n",
            "Iteration 57, loss = 0.64437521\n",
            "Iteration 58, loss = 0.64405579\n",
            "Iteration 59, loss = 0.64380116\n",
            "Iteration 60, loss = 0.64312711\n",
            "Iteration 61, loss = 0.64255550\n",
            "Iteration 62, loss = 0.64224660\n",
            "Iteration 63, loss = 0.64126336\n",
            "Iteration 64, loss = 0.64103651\n",
            "Iteration 65, loss = 0.64068542\n",
            "Iteration 66, loss = 0.64027694\n",
            "Iteration 67, loss = 0.63993972\n",
            "Iteration 68, loss = 0.63972691\n",
            "Iteration 69, loss = 0.63917256\n",
            "Iteration 70, loss = 0.63851023\n",
            "Iteration 71, loss = 0.63835886\n",
            "Iteration 72, loss = 0.63788111\n",
            "Iteration 73, loss = 0.63744407\n",
            "Iteration 74, loss = 0.63711447\n",
            "Iteration 75, loss = 0.63701778\n",
            "Iteration 76, loss = 0.63636047\n",
            "Iteration 77, loss = 0.63605517\n",
            "Iteration 78, loss = 0.63591520\n",
            "Iteration 79, loss = 0.63648233\n",
            "Iteration 80, loss = 0.63516836\n",
            "Iteration 81, loss = 0.63448251\n",
            "Iteration 82, loss = 0.63427477\n",
            "Iteration 83, loss = 0.63388780\n",
            "Iteration 84, loss = 0.63427887\n",
            "Iteration 85, loss = 0.63337544\n",
            "Iteration 86, loss = 0.63297633\n",
            "Iteration 87, loss = 0.63298051\n",
            "Iteration 88, loss = 0.63239756\n",
            "Iteration 89, loss = 0.63284316\n",
            "Iteration 90, loss = 0.63183861\n",
            "Iteration 91, loss = 0.63143654\n",
            "Iteration 92, loss = 0.63106867\n",
            "Iteration 93, loss = 0.63090054\n",
            "Iteration 94, loss = 0.63057027\n",
            "Iteration 95, loss = 0.63050679\n",
            "Iteration 96, loss = 0.63023485\n",
            "Iteration 97, loss = 0.62973361\n",
            "Iteration 98, loss = 0.62968152\n",
            "Iteration 99, loss = 0.62929260\n",
            "Iteration 100, loss = 0.62919767\n",
            "Iteration 101, loss = 0.62929995\n",
            "Iteration 102, loss = 0.62846980\n",
            "Iteration 103, loss = 0.62828436\n",
            "Iteration 104, loss = 0.62813945\n",
            "Iteration 105, loss = 0.62784577\n",
            "Iteration 106, loss = 0.62748416\n",
            "Iteration 107, loss = 0.62743762\n",
            "Iteration 108, loss = 0.62739207\n",
            "Iteration 109, loss = 0.62704475\n",
            "Iteration 110, loss = 0.62747125\n",
            "Iteration 111, loss = 0.62700150\n",
            "Iteration 112, loss = 0.62663682\n",
            "Iteration 113, loss = 0.62707945\n",
            "Iteration 114, loss = 0.62599271\n",
            "Iteration 115, loss = 0.62606170\n",
            "Iteration 116, loss = 0.62592907\n",
            "Iteration 117, loss = 0.62544989\n",
            "Iteration 118, loss = 0.62560467\n",
            "Iteration 119, loss = 0.62508452\n",
            "Iteration 120, loss = 0.62528463\n",
            "Iteration 121, loss = 0.62480483\n",
            "Iteration 122, loss = 0.62466480\n",
            "Iteration 123, loss = 0.62450681\n",
            "Iteration 124, loss = 0.62424615\n",
            "Iteration 125, loss = 0.62424880\n",
            "Iteration 126, loss = 0.62411223\n",
            "Iteration 127, loss = 0.62399299\n",
            "Iteration 128, loss = 0.62390306\n",
            "Iteration 129, loss = 0.62420086\n",
            "Iteration 130, loss = 0.62385650\n",
            "Iteration 131, loss = 0.62363813\n",
            "Iteration 132, loss = 0.62339220\n",
            "Iteration 133, loss = 0.62327199\n",
            "Iteration 134, loss = 0.62329063\n",
            "Iteration 135, loss = 0.62307291\n",
            "Iteration 136, loss = 0.62295194\n",
            "Iteration 137, loss = 0.62253411\n",
            "Iteration 138, loss = 0.62250101\n",
            "Iteration 139, loss = 0.62283832\n",
            "Iteration 140, loss = 0.62241375\n",
            "Iteration 141, loss = 0.62234342\n",
            "Iteration 142, loss = 0.62251279\n",
            "Iteration 143, loss = 0.62200209\n",
            "Iteration 144, loss = 0.62202021\n",
            "Iteration 145, loss = 0.62193530\n",
            "Iteration 146, loss = 0.62181139\n",
            "Iteration 147, loss = 0.62197363\n",
            "Iteration 148, loss = 0.62156219\n",
            "Iteration 149, loss = 0.62189016\n",
            "Iteration 150, loss = 0.62152539\n",
            "Iteration 151, loss = 0.62160663\n",
            "Iteration 152, loss = 0.62144733\n",
            "Iteration 153, loss = 0.62167163\n",
            "Iteration 154, loss = 0.62101484\n",
            "Iteration 155, loss = 0.62142935\n",
            "Iteration 156, loss = 0.62174694\n",
            "Iteration 157, loss = 0.62106256\n",
            "Iteration 158, loss = 0.62098680\n",
            "Iteration 159, loss = 0.62138091\n",
            "Iteration 160, loss = 0.62075677\n",
            "Iteration 161, loss = 0.62075933\n",
            "Iteration 162, loss = 0.62064173\n",
            "Iteration 163, loss = 0.62117973\n",
            "Iteration 164, loss = 0.62068856\n",
            "Iteration 165, loss = 0.62088971\n",
            "Iteration 166, loss = 0.62033609\n",
            "Iteration 167, loss = 0.62052919\n",
            "Iteration 168, loss = 0.62052050\n",
            "Iteration 169, loss = 0.62017254\n",
            "Iteration 170, loss = 0.62038587\n",
            "Iteration 171, loss = 0.62034474\n",
            "Iteration 172, loss = 0.62027143\n",
            "Iteration 173, loss = 0.62001217\n",
            "Iteration 174, loss = 0.61967977\n",
            "Iteration 175, loss = 0.61977450\n",
            "Iteration 176, loss = 0.62037249\n",
            "Iteration 177, loss = 0.61962984\n",
            "Iteration 178, loss = 0.62063339\n",
            "Iteration 179, loss = 0.61945421\n",
            "Iteration 180, loss = 0.62022862\n",
            "Iteration 181, loss = 0.61977746\n",
            "Iteration 182, loss = 0.61955647\n",
            "Iteration 183, loss = 0.61961157\n",
            "Iteration 184, loss = 0.61956708\n",
            "Iteration 185, loss = 0.61974497\n",
            "Iteration 186, loss = 0.61954706\n",
            "Iteration 187, loss = 0.61938032\n",
            "Iteration 188, loss = 0.61958987\n",
            "Iteration 189, loss = 0.61959075\n",
            "Iteration 190, loss = 0.61924724\n",
            "Iteration 191, loss = 0.61917750\n",
            "Iteration 192, loss = 0.61907927\n",
            "Iteration 193, loss = 0.61907884\n",
            "Iteration 194, loss = 0.61886238\n",
            "Iteration 195, loss = 0.61893410\n",
            "Iteration 196, loss = 0.61899567\n",
            "Iteration 197, loss = 0.61890252\n",
            "Iteration 198, loss = 0.61879617\n",
            "Iteration 199, loss = 0.61918479\n",
            "Iteration 200, loss = 0.61904707\n",
            "Iteration 1, loss = 0.69012620\n",
            "Iteration 2, loss = 0.68028254\n",
            "Iteration 3, loss = 0.67972369\n",
            "Iteration 4, loss = 0.67834871\n",
            "Iteration 5, loss = 0.67652724\n",
            "Iteration 6, loss = 0.67556759\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 7, loss = 0.67459202\n",
            "Iteration 8, loss = 0.67358348\n",
            "Iteration 9, loss = 0.67263320\n",
            "Iteration 10, loss = 0.67175143\n",
            "Iteration 11, loss = 0.67062279\n",
            "Iteration 12, loss = 0.66969981\n",
            "Iteration 13, loss = 0.66880353\n",
            "Iteration 14, loss = 0.66797085\n",
            "Iteration 15, loss = 0.66788750\n",
            "Iteration 16, loss = 0.66660569\n",
            "Iteration 17, loss = 0.66558768\n",
            "Iteration 18, loss = 0.66451988\n",
            "Iteration 19, loss = 0.66435775\n",
            "Iteration 20, loss = 0.66313865\n",
            "Iteration 21, loss = 0.66230261\n",
            "Iteration 22, loss = 0.66147097\n",
            "Iteration 23, loss = 0.66109872\n",
            "Iteration 24, loss = 0.66104686\n",
            "Iteration 25, loss = 0.66015952\n",
            "Iteration 26, loss = 0.65872443\n",
            "Iteration 27, loss = 0.65817690\n",
            "Iteration 28, loss = 0.65715785\n",
            "Iteration 29, loss = 0.65662837\n",
            "Iteration 30, loss = 0.65622124\n",
            "Iteration 31, loss = 0.65521162\n",
            "Iteration 32, loss = 0.65468842\n",
            "Iteration 33, loss = 0.65381849\n",
            "Iteration 34, loss = 0.65306415\n",
            "Iteration 35, loss = 0.65230521\n",
            "Iteration 36, loss = 0.65204972\n",
            "Iteration 37, loss = 0.65119608\n",
            "Iteration 38, loss = 0.65065532\n",
            "Iteration 39, loss = 0.65035667\n",
            "Iteration 40, loss = 0.64923115\n",
            "Iteration 41, loss = 0.64867054\n",
            "Iteration 42, loss = 0.64849256\n",
            "Iteration 43, loss = 0.64737301\n",
            "Iteration 44, loss = 0.64709142\n",
            "Iteration 45, loss = 0.64662103\n",
            "Iteration 46, loss = 0.64545731\n",
            "Iteration 47, loss = 0.64499023\n",
            "Iteration 48, loss = 0.64445701\n",
            "Iteration 49, loss = 0.64467931\n",
            "Iteration 50, loss = 0.64302828\n",
            "Iteration 51, loss = 0.64268997\n",
            "Iteration 52, loss = 0.64199554\n",
            "Iteration 53, loss = 0.64176448\n",
            "Iteration 54, loss = 0.64083026\n",
            "Iteration 55, loss = 0.64059521\n",
            "Iteration 56, loss = 0.64006700\n",
            "Iteration 57, loss = 0.63977194\n",
            "Iteration 58, loss = 0.63868001\n",
            "Iteration 59, loss = 0.63802631\n",
            "Iteration 60, loss = 0.63779301\n",
            "Iteration 61, loss = 0.63698649\n",
            "Iteration 62, loss = 0.63661055\n",
            "Iteration 63, loss = 0.63619642\n",
            "Iteration 64, loss = 0.63577898\n",
            "Iteration 65, loss = 0.63532073\n",
            "Iteration 66, loss = 0.63452848\n",
            "Iteration 67, loss = 0.63414137\n",
            "Iteration 68, loss = 0.63363701\n",
            "Iteration 69, loss = 0.63317883\n",
            "Iteration 70, loss = 0.63312361\n",
            "Iteration 71, loss = 0.63231143\n",
            "Iteration 72, loss = 0.63196655\n",
            "Iteration 73, loss = 0.63146379\n",
            "Iteration 74, loss = 0.63116101\n",
            "Iteration 75, loss = 0.63065981\n",
            "Iteration 76, loss = 0.63091944\n",
            "Iteration 77, loss = 0.62973710\n",
            "Iteration 78, loss = 0.62939874\n",
            "Iteration 79, loss = 0.62969617\n",
            "Iteration 80, loss = 0.62864213\n",
            "Iteration 81, loss = 0.62873115\n",
            "Iteration 82, loss = 0.62878220\n",
            "Iteration 83, loss = 0.62818383\n",
            "Iteration 84, loss = 0.62729646\n",
            "Iteration 85, loss = 0.62681288\n",
            "Iteration 86, loss = 0.62690637\n",
            "Iteration 87, loss = 0.62642109\n",
            "Iteration 88, loss = 0.62631004\n",
            "Iteration 89, loss = 0.62588076\n",
            "Iteration 90, loss = 0.62555349\n",
            "Iteration 91, loss = 0.62523638\n",
            "Iteration 92, loss = 0.62504079\n",
            "Iteration 93, loss = 0.62442245\n",
            "Iteration 94, loss = 0.62450609\n",
            "Iteration 95, loss = 0.62416161\n",
            "Iteration 96, loss = 0.62393195\n",
            "Iteration 97, loss = 0.62345176\n",
            "Iteration 98, loss = 0.62334964\n",
            "Iteration 99, loss = 0.62352141\n",
            "Iteration 100, loss = 0.62300446\n",
            "Iteration 101, loss = 0.62300459\n",
            "Iteration 102, loss = 0.62231405\n",
            "Iteration 103, loss = 0.62226545\n",
            "Iteration 104, loss = 0.62255460\n",
            "Iteration 105, loss = 0.62189077\n",
            "Iteration 106, loss = 0.62173974\n",
            "Iteration 107, loss = 0.62191494\n",
            "Iteration 108, loss = 0.62103173\n",
            "Iteration 109, loss = 0.62107871\n",
            "Iteration 110, loss = 0.62107651\n",
            "Iteration 111, loss = 0.62067753\n",
            "Iteration 112, loss = 0.62036877\n",
            "Iteration 113, loss = 0.62023837\n",
            "Iteration 114, loss = 0.62015909\n",
            "Iteration 115, loss = 0.61994375\n",
            "Iteration 116, loss = 0.62026214\n",
            "Iteration 117, loss = 0.61984818\n",
            "Iteration 118, loss = 0.61956862\n",
            "Iteration 119, loss = 0.61945709\n",
            "Iteration 120, loss = 0.61926923\n",
            "Iteration 121, loss = 0.61925645\n",
            "Iteration 122, loss = 0.61898475\n",
            "Iteration 123, loss = 0.61897164\n",
            "Iteration 124, loss = 0.61926947\n",
            "Iteration 125, loss = 0.61885010\n",
            "Iteration 126, loss = 0.61865249\n",
            "Iteration 127, loss = 0.61828821\n",
            "Iteration 128, loss = 0.61846791\n",
            "Iteration 129, loss = 0.61876342\n",
            "Iteration 130, loss = 0.61856623\n",
            "Iteration 131, loss = 0.61799604\n",
            "Iteration 132, loss = 0.61792378\n",
            "Iteration 133, loss = 0.61812617\n",
            "Iteration 134, loss = 0.61761550\n",
            "Iteration 135, loss = 0.61767200\n",
            "Iteration 136, loss = 0.61765532\n",
            "Iteration 137, loss = 0.61756993\n",
            "Iteration 138, loss = 0.61743797\n",
            "Iteration 139, loss = 0.61721264\n",
            "Iteration 140, loss = 0.61725276\n",
            "Iteration 141, loss = 0.61709950\n",
            "Iteration 142, loss = 0.61781859\n",
            "Iteration 143, loss = 0.61733696\n",
            "Iteration 144, loss = 0.61709438\n",
            "Iteration 145, loss = 0.61696386\n",
            "Iteration 146, loss = 0.61735495\n",
            "Iteration 147, loss = 0.61737731\n",
            "Iteration 148, loss = 0.61776094\n",
            "Iteration 149, loss = 0.61666378\n",
            "Iteration 150, loss = 0.61657829\n",
            "Iteration 151, loss = 0.61691021\n",
            "Iteration 152, loss = 0.61650262\n",
            "Iteration 153, loss = 0.61683866\n",
            "Iteration 154, loss = 0.61670582\n",
            "Iteration 155, loss = 0.61649435\n",
            "Iteration 156, loss = 0.61660216\n",
            "Iteration 157, loss = 0.61651956\n",
            "Iteration 158, loss = 0.61612733\n",
            "Iteration 159, loss = 0.61645639\n",
            "Iteration 160, loss = 0.61668182\n",
            "Iteration 161, loss = 0.61684075\n",
            "Iteration 162, loss = 0.61596891\n",
            "Iteration 163, loss = 0.61605039\n",
            "Iteration 164, loss = 0.61600912\n",
            "Iteration 165, loss = 0.61604677\n",
            "Iteration 166, loss = 0.61606786\n",
            "Iteration 167, loss = 0.61586823\n",
            "Iteration 168, loss = 0.61611597\n",
            "Iteration 169, loss = 0.61584352\n",
            "Iteration 170, loss = 0.61598524\n",
            "Iteration 171, loss = 0.61577221\n",
            "Iteration 172, loss = 0.61585067\n",
            "Iteration 173, loss = 0.61685423\n",
            "Iteration 174, loss = 0.61592146\n",
            "Iteration 175, loss = 0.61544584\n",
            "Iteration 176, loss = 0.61582303\n",
            "Iteration 177, loss = 0.61600985\n",
            "Iteration 178, loss = 0.61569765\n",
            "Iteration 179, loss = 0.61549329\n",
            "Iteration 180, loss = 0.61566285\n",
            "Iteration 181, loss = 0.61561238\n",
            "Iteration 182, loss = 0.61565157\n",
            "Iteration 183, loss = 0.61646718\n",
            "Iteration 184, loss = 0.61564398\n",
            "Iteration 185, loss = 0.61555472\n",
            "Iteration 186, loss = 0.61553977\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64414549\n",
            "Iteration 2, loss = 0.60858650\n",
            "Iteration 3, loss = 0.60350251\n",
            "Iteration 4, loss = 0.59473288\n",
            "Iteration 5, loss = 0.58894800\n",
            "Iteration 6, loss = 0.58379043\n",
            "Iteration 7, loss = 0.58252821\n",
            "Iteration 8, loss = 0.57113687\n",
            "Iteration 9, loss = 0.56050078\n",
            "Iteration 10, loss = 0.54756150\n",
            "Iteration 11, loss = 0.54225115\n",
            "Iteration 12, loss = 0.53259695\n",
            "Iteration 13, loss = 0.50538022\n",
            "Iteration 14, loss = 0.51530354\n",
            "Iteration 15, loss = 0.48729652\n",
            "Iteration 16, loss = 0.47868549\n",
            "Iteration 17, loss = 0.47644086\n",
            "Iteration 18, loss = 0.47124698\n",
            "Iteration 19, loss = 0.47713509\n",
            "Iteration 20, loss = 0.47328211\n",
            "Iteration 21, loss = 0.46382643\n",
            "Iteration 22, loss = 0.46026942\n",
            "Iteration 23, loss = 0.45573390\n",
            "Iteration 24, loss = 0.45194143\n",
            "Iteration 25, loss = 0.45082343\n",
            "Iteration 26, loss = 0.44548920\n",
            "Iteration 27, loss = 0.45405926\n",
            "Iteration 28, loss = 0.44133024\n",
            "Iteration 29, loss = 0.44629843\n",
            "Iteration 30, loss = 0.45018151\n",
            "Iteration 31, loss = 0.44689200\n",
            "Iteration 32, loss = 0.43666677\n",
            "Iteration 33, loss = 0.45117308\n",
            "Iteration 34, loss = 0.43672717\n",
            "Iteration 35, loss = 0.44309501\n",
            "Iteration 36, loss = 0.44733840\n",
            "Iteration 37, loss = 0.45861960\n",
            "Iteration 38, loss = 0.42960106\n",
            "Iteration 39, loss = 0.43979994\n",
            "Iteration 40, loss = 0.43020321\n",
            "Iteration 41, loss = 0.43096092\n",
            "Iteration 42, loss = 0.42569658\n",
            "Iteration 43, loss = 0.43226695\n",
            "Iteration 44, loss = 0.43365710\n",
            "Iteration 45, loss = 0.43931025\n",
            "Iteration 46, loss = 0.44927950\n",
            "Iteration 47, loss = 0.42463945\n",
            "Iteration 48, loss = 0.42560328\n",
            "Iteration 49, loss = 0.43308204\n",
            "Iteration 50, loss = 0.43576829\n",
            "Iteration 51, loss = 0.42616721\n",
            "Iteration 52, loss = 0.42319585\n",
            "Iteration 53, loss = 0.42496772\n",
            "Iteration 54, loss = 0.42396413\n",
            "Iteration 55, loss = 0.41599372\n",
            "Iteration 56, loss = 0.41791994\n",
            "Iteration 57, loss = 0.42838392\n",
            "Iteration 58, loss = 0.41947593\n",
            "Iteration 59, loss = 0.42332320\n",
            "Iteration 60, loss = 0.41521212\n",
            "Iteration 61, loss = 0.41246027\n",
            "Iteration 62, loss = 0.41754292\n",
            "Iteration 63, loss = 0.41969046\n",
            "Iteration 64, loss = 0.41407542\n",
            "Iteration 65, loss = 0.40992384\n",
            "Iteration 66, loss = 0.41528856\n",
            "Iteration 67, loss = 0.41262532\n",
            "Iteration 68, loss = 0.41069496\n",
            "Iteration 69, loss = 0.41680639\n",
            "Iteration 70, loss = 0.41355502\n",
            "Iteration 71, loss = 0.40550098\n",
            "Iteration 72, loss = 0.41641644\n",
            "Iteration 73, loss = 0.41033312\n",
            "Iteration 74, loss = 0.41321469\n",
            "Iteration 75, loss = 0.41070561\n",
            "Iteration 76, loss = 0.41070569\n",
            "Iteration 77, loss = 0.41000602\n",
            "Iteration 78, loss = 0.40203485\n",
            "Iteration 79, loss = 0.40959666\n",
            "Iteration 80, loss = 0.41036068\n",
            "Iteration 81, loss = 0.40182787\n",
            "Iteration 82, loss = 0.40503638\n",
            "Iteration 83, loss = 0.39964195\n",
            "Iteration 84, loss = 0.40200301\n",
            "Iteration 85, loss = 0.41136984\n",
            "Iteration 86, loss = 0.39460338\n",
            "Iteration 87, loss = 0.40539181\n",
            "Iteration 88, loss = 0.40681972\n",
            "Iteration 89, loss = 0.39929393\n",
            "Iteration 90, loss = 0.40385878\n",
            "Iteration 91, loss = 0.40370535\n",
            "Iteration 92, loss = 0.40050695\n",
            "Iteration 93, loss = 0.41239785\n",
            "Iteration 94, loss = 0.39717267\n",
            "Iteration 95, loss = 0.39825938\n",
            "Iteration 96, loss = 0.40272066\n",
            "Iteration 97, loss = 0.39953833\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64619502\n",
            "Iteration 2, loss = 0.60701995\n",
            "Iteration 3, loss = 0.60314107\n",
            "Iteration 4, loss = 0.59286172\n",
            "Iteration 5, loss = 0.59643989\n",
            "Iteration 6, loss = 0.58154214\n",
            "Iteration 7, loss = 0.58816836\n",
            "Iteration 8, loss = 0.57007618\n",
            "Iteration 9, loss = 0.56476015\n",
            "Iteration 10, loss = 0.54714559\n",
            "Iteration 11, loss = 0.54515264\n",
            "Iteration 12, loss = 0.53106671\n",
            "Iteration 13, loss = 0.51933651\n",
            "Iteration 14, loss = 0.52527645\n",
            "Iteration 15, loss = 0.49471984\n",
            "Iteration 16, loss = 0.49254067\n",
            "Iteration 17, loss = 0.47204385\n",
            "Iteration 18, loss = 0.47039713\n",
            "Iteration 19, loss = 0.45880073\n",
            "Iteration 20, loss = 0.46521689\n",
            "Iteration 21, loss = 0.44745774\n",
            "Iteration 22, loss = 0.45146509\n",
            "Iteration 23, loss = 0.45186372\n",
            "Iteration 24, loss = 0.45542507\n",
            "Iteration 25, loss = 0.44522547\n",
            "Iteration 26, loss = 0.43770783\n",
            "Iteration 27, loss = 0.44175425\n",
            "Iteration 28, loss = 0.42869830\n",
            "Iteration 29, loss = 0.43576266\n",
            "Iteration 30, loss = 0.44271978\n",
            "Iteration 31, loss = 0.43165222\n",
            "Iteration 32, loss = 0.42589353\n",
            "Iteration 33, loss = 0.43771707\n",
            "Iteration 34, loss = 0.43466041\n",
            "Iteration 35, loss = 0.43150236\n",
            "Iteration 36, loss = 0.43531054\n",
            "Iteration 37, loss = 0.42213009\n",
            "Iteration 38, loss = 0.43548560\n",
            "Iteration 39, loss = 0.42063160\n",
            "Iteration 40, loss = 0.42207912\n",
            "Iteration 41, loss = 0.42288620\n",
            "Iteration 42, loss = 0.44296249\n",
            "Iteration 43, loss = 0.42293073\n",
            "Iteration 44, loss = 0.42328751\n",
            "Iteration 45, loss = 0.41733650\n",
            "Iteration 46, loss = 0.41464493\n",
            "Iteration 47, loss = 0.41693367\n",
            "Iteration 48, loss = 0.41419723\n",
            "Iteration 49, loss = 0.41724025\n",
            "Iteration 50, loss = 0.41275648\n",
            "Iteration 51, loss = 0.40572754\n",
            "Iteration 52, loss = 0.42060756\n",
            "Iteration 53, loss = 0.41274049\n",
            "Iteration 54, loss = 0.41004415\n",
            "Iteration 55, loss = 0.40998807\n",
            "Iteration 56, loss = 0.41523495\n",
            "Iteration 57, loss = 0.40620012\n",
            "Iteration 58, loss = 0.40551752\n",
            "Iteration 59, loss = 0.40999966\n",
            "Iteration 60, loss = 0.41500733\n",
            "Iteration 61, loss = 0.40012391\n",
            "Iteration 62, loss = 0.40088236\n",
            "Iteration 63, loss = 0.40036059\n",
            "Iteration 64, loss = 0.41466084\n",
            "Iteration 65, loss = 0.40501027\n",
            "Iteration 66, loss = 0.40221496\n",
            "Iteration 67, loss = 0.40369472\n",
            "Iteration 68, loss = 0.40930282\n",
            "Iteration 69, loss = 0.39373146\n",
            "Iteration 70, loss = 0.39566057\n",
            "Iteration 71, loss = 0.39414045\n",
            "Iteration 72, loss = 0.41221695\n",
            "Iteration 73, loss = 0.40581028\n",
            "Iteration 74, loss = 0.40628348\n",
            "Iteration 75, loss = 0.39732144\n",
            "Iteration 76, loss = 0.39714732\n",
            "Iteration 77, loss = 0.39062666\n",
            "Iteration 78, loss = 0.42133701\n",
            "Iteration 79, loss = 0.39559140\n",
            "Iteration 80, loss = 0.39310813\n",
            "Iteration 81, loss = 0.39034229\n",
            "Iteration 82, loss = 0.40118156\n",
            "Iteration 83, loss = 0.39980468\n",
            "Iteration 84, loss = 0.39478053\n",
            "Iteration 85, loss = 0.38683115\n",
            "Iteration 86, loss = 0.39776544\n",
            "Iteration 87, loss = 0.38871165\n",
            "Iteration 88, loss = 0.39541936\n",
            "Iteration 89, loss = 0.38609295\n",
            "Iteration 90, loss = 0.39785550\n",
            "Iteration 91, loss = 0.38434862\n",
            "Iteration 92, loss = 0.38744894\n",
            "Iteration 93, loss = 0.39381077\n",
            "Iteration 94, loss = 0.38881249\n",
            "Iteration 95, loss = 0.38625578\n",
            "Iteration 96, loss = 0.39001817\n",
            "Iteration 97, loss = 0.38268199\n",
            "Iteration 98, loss = 0.38820322\n",
            "Iteration 99, loss = 0.38491329\n",
            "Iteration 100, loss = 0.38153183\n",
            "Iteration 101, loss = 0.38458001\n",
            "Iteration 102, loss = 0.39359635\n",
            "Iteration 103, loss = 0.37680771\n",
            "Iteration 104, loss = 0.38348442\n",
            "Iteration 105, loss = 0.37866355\n",
            "Iteration 106, loss = 0.38719164\n",
            "Iteration 107, loss = 0.38843497\n",
            "Iteration 108, loss = 0.38860102\n",
            "Iteration 109, loss = 0.37834424\n",
            "Iteration 110, loss = 0.37747720\n",
            "Iteration 111, loss = 0.37750507\n",
            "Iteration 112, loss = 0.38301054\n",
            "Iteration 113, loss = 0.38494447\n",
            "Iteration 114, loss = 0.38165013\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.65246348\n",
            "Iteration 2, loss = 0.61480145\n",
            "Iteration 3, loss = 0.59922521\n",
            "Iteration 4, loss = 0.59726555\n",
            "Iteration 5, loss = 0.58893262\n",
            "Iteration 6, loss = 0.58354451\n",
            "Iteration 7, loss = 0.57610903\n",
            "Iteration 8, loss = 0.55732888\n",
            "Iteration 9, loss = 0.54543653\n",
            "Iteration 10, loss = 0.52986298\n",
            "Iteration 11, loss = 0.51513137\n",
            "Iteration 12, loss = 0.50618155\n",
            "Iteration 13, loss = 0.48614897\n",
            "Iteration 14, loss = 0.47798102\n",
            "Iteration 15, loss = 0.46095104\n",
            "Iteration 16, loss = 0.45691064\n",
            "Iteration 17, loss = 0.44833349\n",
            "Iteration 18, loss = 0.43562762\n",
            "Iteration 19, loss = 0.45578076\n",
            "Iteration 20, loss = 0.43434589\n",
            "Iteration 21, loss = 0.43492410\n",
            "Iteration 22, loss = 0.43707401\n",
            "Iteration 23, loss = 0.42929671\n",
            "Iteration 24, loss = 0.43047703\n",
            "Iteration 25, loss = 0.42336260\n",
            "Iteration 26, loss = 0.41205793\n",
            "Iteration 27, loss = 0.43382977\n",
            "Iteration 28, loss = 0.43732832\n",
            "Iteration 29, loss = 0.42701055\n",
            "Iteration 30, loss = 0.41905880\n",
            "Iteration 31, loss = 0.41795440\n",
            "Iteration 32, loss = 0.41317259\n",
            "Iteration 33, loss = 0.43037824\n",
            "Iteration 34, loss = 0.42985814\n",
            "Iteration 35, loss = 0.41309622\n",
            "Iteration 36, loss = 0.42064106\n",
            "Iteration 37, loss = 0.41293557\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.65165566\n",
            "Iteration 2, loss = 0.62227298\n",
            "Iteration 3, loss = 0.61575680\n",
            "Iteration 4, loss = 0.60964053\n",
            "Iteration 5, loss = 0.60207388\n",
            "Iteration 6, loss = 0.59803670\n",
            "Iteration 7, loss = 0.58791164\n",
            "Iteration 8, loss = 0.56952999\n",
            "Iteration 9, loss = 0.55497647\n",
            "Iteration 10, loss = 0.53712155\n",
            "Iteration 11, loss = 0.53510724\n",
            "Iteration 12, loss = 0.51107082\n",
            "Iteration 13, loss = 0.49903371\n",
            "Iteration 14, loss = 0.49534632\n",
            "Iteration 15, loss = 0.47085111\n",
            "Iteration 16, loss = 0.47942697\n",
            "Iteration 17, loss = 0.47510514\n",
            "Iteration 18, loss = 0.45739032\n",
            "Iteration 19, loss = 0.46303625\n",
            "Iteration 20, loss = 0.45979076\n",
            "Iteration 21, loss = 0.44712893\n",
            "Iteration 22, loss = 0.44521941\n",
            "Iteration 23, loss = 0.45592540\n",
            "Iteration 24, loss = 0.44565219\n",
            "Iteration 25, loss = 0.44968578\n",
            "Iteration 26, loss = 0.43991823\n",
            "Iteration 27, loss = 0.44456535\n",
            "Iteration 28, loss = 0.43991945\n",
            "Iteration 29, loss = 0.44252449\n",
            "Iteration 30, loss = 0.44267541\n",
            "Iteration 31, loss = 0.43857923\n",
            "Iteration 32, loss = 0.43215204\n",
            "Iteration 33, loss = 0.43454668\n",
            "Iteration 34, loss = 0.42677718\n",
            "Iteration 35, loss = 0.43756268\n",
            "Iteration 36, loss = 0.43111771\n",
            "Iteration 37, loss = 0.43207140\n",
            "Iteration 38, loss = 0.43696215\n",
            "Iteration 39, loss = 0.43162622\n",
            "Iteration 40, loss = 0.42972400\n",
            "Iteration 41, loss = 0.42364622\n",
            "Iteration 42, loss = 0.41814875\n",
            "Iteration 43, loss = 0.42598730\n",
            "Iteration 44, loss = 0.43850676\n",
            "Iteration 45, loss = 0.42030833\n",
            "Iteration 46, loss = 0.42953636\n",
            "Iteration 47, loss = 0.43225753\n",
            "Iteration 48, loss = 0.43019879\n",
            "Iteration 49, loss = 0.42550154\n",
            "Iteration 50, loss = 0.42140952\n",
            "Iteration 51, loss = 0.41660045\n",
            "Iteration 52, loss = 0.41879614\n",
            "Iteration 53, loss = 0.41057728\n",
            "Iteration 54, loss = 0.42173185\n",
            "Iteration 55, loss = 0.41577890\n",
            "Iteration 56, loss = 0.41679139\n",
            "Iteration 57, loss = 0.40817886\n",
            "Iteration 58, loss = 0.40956473\n",
            "Iteration 59, loss = 0.42123640\n",
            "Iteration 60, loss = 0.40736223\n",
            "Iteration 61, loss = 0.41847761\n",
            "Iteration 62, loss = 0.41518446\n",
            "Iteration 63, loss = 0.40536872\n",
            "Iteration 64, loss = 0.41195829\n",
            "Iteration 65, loss = 0.40504632\n",
            "Iteration 66, loss = 0.41253446\n",
            "Iteration 67, loss = 0.40501877\n",
            "Iteration 68, loss = 0.40490862\n",
            "Iteration 69, loss = 0.42206924\n",
            "Iteration 70, loss = 0.40543264\n",
            "Iteration 71, loss = 0.40374862\n",
            "Iteration 72, loss = 0.40136800\n",
            "Iteration 73, loss = 0.40839591\n",
            "Iteration 74, loss = 0.40312940\n",
            "Iteration 75, loss = 0.41087639\n",
            "Iteration 76, loss = 0.40898025\n",
            "Iteration 77, loss = 0.39950049\n",
            "Iteration 78, loss = 0.40931798\n",
            "Iteration 79, loss = 0.40350555\n",
            "Iteration 80, loss = 0.41273083\n",
            "Iteration 81, loss = 0.41596957\n",
            "Iteration 82, loss = 0.40027667\n",
            "Iteration 83, loss = 0.40374098\n",
            "Iteration 84, loss = 0.40012399\n",
            "Iteration 85, loss = 0.39977395\n",
            "Iteration 86, loss = 0.39216808\n",
            "Iteration 87, loss = 0.39712151\n",
            "Iteration 88, loss = 0.41048584\n",
            "Iteration 89, loss = 0.39702061\n",
            "Iteration 90, loss = 0.39430038\n",
            "Iteration 91, loss = 0.40374531\n",
            "Iteration 92, loss = 0.39373020\n",
            "Iteration 93, loss = 0.40972663\n",
            "Iteration 94, loss = 0.39918429\n",
            "Iteration 95, loss = 0.39893083\n",
            "Iteration 96, loss = 0.39526009\n",
            "Iteration 97, loss = 0.40019938\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.65753240\n",
            "Iteration 2, loss = 0.62919332\n",
            "Iteration 3, loss = 0.61202666\n",
            "Iteration 4, loss = 0.60710515\n",
            "Iteration 5, loss = 0.59790697\n",
            "Iteration 6, loss = 0.59454208\n",
            "Iteration 7, loss = 0.58781110\n",
            "Iteration 8, loss = 0.57865012\n",
            "Iteration 9, loss = 0.56381547\n",
            "Iteration 10, loss = 0.55613535\n",
            "Iteration 11, loss = 0.54915280\n",
            "Iteration 12, loss = 0.52541901\n",
            "Iteration 13, loss = 0.51794806\n",
            "Iteration 14, loss = 0.50336870\n",
            "Iteration 15, loss = 0.51384629\n",
            "Iteration 16, loss = 0.48502205\n",
            "Iteration 17, loss = 0.49059835\n",
            "Iteration 18, loss = 0.47632627\n",
            "Iteration 19, loss = 0.47211901\n",
            "Iteration 20, loss = 0.47029555\n",
            "Iteration 21, loss = 0.46775774\n",
            "Iteration 22, loss = 0.45188830\n",
            "Iteration 23, loss = 0.45613197\n",
            "Iteration 24, loss = 0.45655393\n",
            "Iteration 25, loss = 0.47659472\n",
            "Iteration 26, loss = 0.46455582\n",
            "Iteration 27, loss = 0.45383043\n",
            "Iteration 28, loss = 0.45065821\n",
            "Iteration 29, loss = 0.45319702\n",
            "Iteration 30, loss = 0.45276844\n",
            "Iteration 31, loss = 0.45026706\n",
            "Iteration 32, loss = 0.45170574\n",
            "Iteration 33, loss = 0.44735682\n",
            "Iteration 34, loss = 0.45856911\n",
            "Iteration 35, loss = 0.44337551\n",
            "Iteration 36, loss = 0.43474970\n",
            "Iteration 37, loss = 0.45156331\n",
            "Iteration 38, loss = 0.45110849\n",
            "Iteration 39, loss = 0.46074348\n",
            "Iteration 40, loss = 0.43734615\n",
            "Iteration 41, loss = 0.43398509\n",
            "Iteration 42, loss = 0.44976716\n",
            "Iteration 43, loss = 0.43553440\n",
            "Iteration 44, loss = 0.44387120\n",
            "Iteration 45, loss = 0.43164009\n",
            "Iteration 46, loss = 0.43523846\n",
            "Iteration 47, loss = 0.43125047\n",
            "Iteration 48, loss = 0.43073889\n",
            "Iteration 49, loss = 0.43126334\n",
            "Iteration 50, loss = 0.42705811\n",
            "Iteration 51, loss = 0.42710311\n",
            "Iteration 52, loss = 0.44268873\n",
            "Iteration 53, loss = 0.42006283\n",
            "Iteration 54, loss = 0.42562703\n",
            "Iteration 55, loss = 0.42683586\n",
            "Iteration 56, loss = 0.42379740\n",
            "Iteration 57, loss = 0.42845560\n",
            "Iteration 58, loss = 0.42728697\n",
            "Iteration 59, loss = 0.43191231\n",
            "Iteration 60, loss = 0.42431603\n",
            "Iteration 61, loss = 0.42305241\n",
            "Iteration 62, loss = 0.44017427\n",
            "Iteration 63, loss = 0.42115362\n",
            "Iteration 64, loss = 0.42358449\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69616264\n",
            "Iteration 2, loss = 0.67409709\n",
            "Iteration 3, loss = 0.66930479\n",
            "Iteration 4, loss = 0.66566228\n",
            "Iteration 5, loss = 0.66370056\n",
            "Iteration 6, loss = 0.65709511\n",
            "Iteration 7, loss = 0.65555138\n",
            "Iteration 8, loss = 0.65381610\n",
            "Iteration 9, loss = 0.64928701\n",
            "Iteration 10, loss = 0.64380173\n",
            "Iteration 11, loss = 0.64396058\n",
            "Iteration 12, loss = 0.63802899\n",
            "Iteration 13, loss = 0.63396887\n",
            "Iteration 14, loss = 0.63174930\n",
            "Iteration 15, loss = 0.62802647\n",
            "Iteration 16, loss = 0.62723207\n",
            "Iteration 17, loss = 0.62320283\n",
            "Iteration 18, loss = 0.62221307\n",
            "Iteration 19, loss = 0.61898140\n",
            "Iteration 20, loss = 0.61795907\n",
            "Iteration 21, loss = 0.61566122\n",
            "Iteration 22, loss = 0.61611934\n",
            "Iteration 23, loss = 0.61519209\n",
            "Iteration 24, loss = 0.61031476\n",
            "Iteration 25, loss = 0.61062961\n",
            "Iteration 26, loss = 0.60920654\n",
            "Iteration 27, loss = 0.60780862\n",
            "Iteration 28, loss = 0.60737053\n",
            "Iteration 29, loss = 0.60730838\n",
            "Iteration 30, loss = 0.60702502\n",
            "Iteration 31, loss = 0.60705602\n",
            "Iteration 32, loss = 0.60714799\n",
            "Iteration 33, loss = 0.60595056\n",
            "Iteration 34, loss = 0.60371197\n",
            "Iteration 35, loss = 0.60526655\n",
            "Iteration 36, loss = 0.60426603\n",
            "Iteration 37, loss = 0.60378851\n",
            "Iteration 38, loss = 0.60274869\n",
            "Iteration 39, loss = 0.60302943\n",
            "Iteration 40, loss = 0.60326135\n",
            "Iteration 41, loss = 0.60342025\n",
            "Iteration 42, loss = 0.60281824\n",
            "Iteration 43, loss = 0.60207875\n",
            "Iteration 44, loss = 0.60183408\n",
            "Iteration 45, loss = 0.60220452\n",
            "Iteration 46, loss = 0.60217687\n",
            "Iteration 47, loss = 0.60357161\n",
            "Iteration 48, loss = 0.60009951\n",
            "Iteration 49, loss = 0.60055968\n",
            "Iteration 50, loss = 0.59975551\n",
            "Iteration 51, loss = 0.60096403\n",
            "Iteration 52, loss = 0.60010095\n",
            "Iteration 53, loss = 0.60261980\n",
            "Iteration 54, loss = 0.60061805\n",
            "Iteration 55, loss = 0.60158135\n",
            "Iteration 56, loss = 0.60155368\n",
            "Iteration 57, loss = 0.60392936\n",
            "Iteration 58, loss = 0.59840451\n",
            "Iteration 59, loss = 0.59890674\n",
            "Iteration 60, loss = 0.59884959\n",
            "Iteration 61, loss = 0.59889483\n",
            "Iteration 62, loss = 0.59773020\n",
            "Iteration 63, loss = 0.59806280\n",
            "Iteration 64, loss = 0.59806059\n",
            "Iteration 65, loss = 0.59661106\n",
            "Iteration 66, loss = 0.59875908\n",
            "Iteration 67, loss = 0.59720195\n",
            "Iteration 68, loss = 0.59664126\n",
            "Iteration 69, loss = 0.59690157\n",
            "Iteration 70, loss = 0.59649921\n",
            "Iteration 71, loss = 0.59589657\n",
            "Iteration 72, loss = 0.59728531\n",
            "Iteration 73, loss = 0.59800144\n",
            "Iteration 74, loss = 0.59565852\n",
            "Iteration 75, loss = 0.59647868\n",
            "Iteration 76, loss = 0.59736375\n",
            "Iteration 77, loss = 0.59529261\n",
            "Iteration 78, loss = 0.59883422\n",
            "Iteration 79, loss = 0.59525576\n",
            "Iteration 80, loss = 0.59562576\n",
            "Iteration 81, loss = 0.59694051\n",
            "Iteration 82, loss = 0.59419305\n",
            "Iteration 83, loss = 0.59481952\n",
            "Iteration 84, loss = 0.59396130\n",
            "Iteration 85, loss = 0.59744247\n",
            "Iteration 86, loss = 0.59468509\n",
            "Iteration 87, loss = 0.59402686\n",
            "Iteration 88, loss = 0.59326611\n",
            "Iteration 89, loss = 0.59280192\n",
            "Iteration 90, loss = 0.59349185\n",
            "Iteration 91, loss = 0.59286278\n",
            "Iteration 92, loss = 0.59590578\n",
            "Iteration 93, loss = 0.59112871\n",
            "Iteration 94, loss = 0.59397556\n",
            "Iteration 95, loss = 0.59150907\n",
            "Iteration 96, loss = 0.59345293\n",
            "Iteration 97, loss = 0.59162046\n",
            "Iteration 98, loss = 0.59167222\n",
            "Iteration 99, loss = 0.59335420\n",
            "Iteration 100, loss = 0.59156991\n",
            "Iteration 101, loss = 0.59308619\n",
            "Iteration 102, loss = 0.59389690\n",
            "Iteration 103, loss = 0.59109742\n",
            "Iteration 104, loss = 0.59111315\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.68423067\n",
            "Iteration 2, loss = 0.67456723\n",
            "Iteration 3, loss = 0.67118077\n",
            "Iteration 4, loss = 0.66780286\n",
            "Iteration 5, loss = 0.66308967\n",
            "Iteration 6, loss = 0.65837954\n",
            "Iteration 7, loss = 0.65496484\n",
            "Iteration 8, loss = 0.65112785\n",
            "Iteration 9, loss = 0.64764824\n",
            "Iteration 10, loss = 0.64649830\n",
            "Iteration 11, loss = 0.64149819\n",
            "Iteration 12, loss = 0.63674289\n",
            "Iteration 13, loss = 0.63509641\n",
            "Iteration 14, loss = 0.63236404\n",
            "Iteration 15, loss = 0.62941896\n",
            "Iteration 16, loss = 0.62664032\n",
            "Iteration 17, loss = 0.62518329\n",
            "Iteration 18, loss = 0.62429797\n",
            "Iteration 19, loss = 0.61994554\n",
            "Iteration 20, loss = 0.61904217\n",
            "Iteration 21, loss = 0.61594347\n",
            "Iteration 22, loss = 0.61713361\n",
            "Iteration 23, loss = 0.61432622\n",
            "Iteration 24, loss = 0.61170829\n",
            "Iteration 25, loss = 0.61189218\n",
            "Iteration 26, loss = 0.61139273\n",
            "Iteration 27, loss = 0.60863324\n",
            "Iteration 28, loss = 0.61153646\n",
            "Iteration 29, loss = 0.60835039\n",
            "Iteration 30, loss = 0.60946646\n",
            "Iteration 31, loss = 0.60850738\n",
            "Iteration 32, loss = 0.60655966\n",
            "Iteration 33, loss = 0.60679196\n",
            "Iteration 34, loss = 0.60627489\n",
            "Iteration 35, loss = 0.60720511\n",
            "Iteration 36, loss = 0.60624282\n",
            "Iteration 37, loss = 0.60538640\n",
            "Iteration 38, loss = 0.60457623\n",
            "Iteration 39, loss = 0.60416402\n",
            "Iteration 40, loss = 0.60112511\n",
            "Iteration 41, loss = 0.60521221\n",
            "Iteration 42, loss = 0.60341420\n",
            "Iteration 43, loss = 0.60272352\n",
            "Iteration 44, loss = 0.60314080\n",
            "Iteration 45, loss = 0.60435270\n",
            "Iteration 46, loss = 0.60242716\n",
            "Iteration 47, loss = 0.60290345\n",
            "Iteration 48, loss = 0.60096583\n",
            "Iteration 49, loss = 0.60231728\n",
            "Iteration 50, loss = 0.60059936\n",
            "Iteration 51, loss = 0.60121213\n",
            "Iteration 52, loss = 0.60145709\n",
            "Iteration 53, loss = 0.60078614\n",
            "Iteration 54, loss = 0.60048288\n",
            "Iteration 55, loss = 0.60290948\n",
            "Iteration 56, loss = 0.60045802\n",
            "Iteration 57, loss = 0.59988110\n",
            "Iteration 58, loss = 0.60054651\n",
            "Iteration 59, loss = 0.59957993\n",
            "Iteration 60, loss = 0.59834205\n",
            "Iteration 61, loss = 0.59937440\n",
            "Iteration 62, loss = 0.60013316\n",
            "Iteration 63, loss = 0.59955492\n",
            "Iteration 64, loss = 0.60094203\n",
            "Iteration 65, loss = 0.59727261\n",
            "Iteration 66, loss = 0.59821402\n",
            "Iteration 67, loss = 0.59723717\n",
            "Iteration 68, loss = 0.59875373\n",
            "Iteration 69, loss = 0.59763324\n",
            "Iteration 70, loss = 0.59906444\n",
            "Iteration 71, loss = 0.59823901\n",
            "Iteration 72, loss = 0.59752286\n",
            "Iteration 73, loss = 0.59781064\n",
            "Iteration 74, loss = 0.59817421\n",
            "Iteration 75, loss = 0.59907511\n",
            "Iteration 76, loss = 0.59706467\n",
            "Iteration 77, loss = 0.59861770\n",
            "Iteration 78, loss = 0.59598898\n",
            "Iteration 79, loss = 0.59549780\n",
            "Iteration 80, loss = 0.59950239\n",
            "Iteration 81, loss = 0.59764686\n",
            "Iteration 82, loss = 0.59444517\n",
            "Iteration 83, loss = 0.59612758\n",
            "Iteration 84, loss = 0.59462941\n",
            "Iteration 85, loss = 0.59576575\n",
            "Iteration 86, loss = 0.59528419\n",
            "Iteration 87, loss = 0.59551835\n",
            "Iteration 88, loss = 0.59410110\n",
            "Iteration 89, loss = 0.59404095\n",
            "Iteration 90, loss = 0.59360271\n",
            "Iteration 91, loss = 0.59395052\n",
            "Iteration 92, loss = 0.59437835\n",
            "Iteration 93, loss = 0.59394928\n",
            "Iteration 94, loss = 0.59577391\n",
            "Iteration 95, loss = 0.59415534\n",
            "Iteration 96, loss = 0.59470732\n",
            "Iteration 97, loss = 0.59152917\n",
            "Iteration 98, loss = 0.59921483\n",
            "Iteration 99, loss = 0.59734803\n",
            "Iteration 100, loss = 0.59181968\n",
            "Iteration 101, loss = 0.59396191\n",
            "Iteration 102, loss = 0.59143127\n",
            "Iteration 103, loss = 0.59337025\n",
            "Iteration 104, loss = 0.59276040\n",
            "Iteration 105, loss = 0.59136095\n",
            "Iteration 106, loss = 0.59206006\n",
            "Iteration 107, loss = 0.59185320\n",
            "Iteration 108, loss = 0.59049071\n",
            "Iteration 109, loss = 0.59252726\n",
            "Iteration 110, loss = 0.59158909\n",
            "Iteration 111, loss = 0.59117417\n",
            "Iteration 112, loss = 0.59124152\n",
            "Iteration 113, loss = 0.58934438\n",
            "Iteration 114, loss = 0.59277621\n",
            "Iteration 115, loss = 0.58967765\n",
            "Iteration 116, loss = 0.59108262\n",
            "Iteration 117, loss = 0.59140498\n",
            "Iteration 118, loss = 0.59116522\n",
            "Iteration 119, loss = 0.58988059\n",
            "Iteration 120, loss = 0.59092006\n",
            "Iteration 121, loss = 0.58958324\n",
            "Iteration 122, loss = 0.59069490\n",
            "Iteration 123, loss = 0.58851749\n",
            "Iteration 124, loss = 0.58986930\n",
            "Iteration 125, loss = 0.58995173\n",
            "Iteration 126, loss = 0.59024891\n",
            "Iteration 127, loss = 0.58768048\n",
            "Iteration 128, loss = 0.59088823\n",
            "Iteration 129, loss = 0.58885576\n",
            "Iteration 130, loss = 0.58979071\n",
            "Iteration 131, loss = 0.58994208\n",
            "Iteration 132, loss = 0.58842023\n",
            "Iteration 133, loss = 0.58781281\n",
            "Iteration 134, loss = 0.58936767\n",
            "Iteration 135, loss = 0.58711569\n",
            "Iteration 136, loss = 0.58622095\n",
            "Iteration 137, loss = 0.59009178\n",
            "Iteration 138, loss = 0.58599109\n",
            "Iteration 139, loss = 0.58725593\n",
            "Iteration 140, loss = 0.58641650\n",
            "Iteration 141, loss = 0.58521787\n",
            "Iteration 142, loss = 0.58701595\n",
            "Iteration 143, loss = 0.58717504\n",
            "Iteration 144, loss = 0.58567918\n",
            "Iteration 145, loss = 0.58520465\n",
            "Iteration 146, loss = 0.58461362\n",
            "Iteration 147, loss = 0.58532186\n",
            "Iteration 148, loss = 0.58871188\n",
            "Iteration 149, loss = 0.58732767\n",
            "Iteration 150, loss = 0.58562746\n",
            "Iteration 151, loss = 0.58545916\n",
            "Iteration 152, loss = 0.58370896\n",
            "Iteration 153, loss = 0.58501085\n",
            "Iteration 154, loss = 0.58489875\n",
            "Iteration 155, loss = 0.58388359\n",
            "Iteration 156, loss = 0.58614797\n",
            "Iteration 157, loss = 0.58504010\n",
            "Iteration 158, loss = 0.58325001\n",
            "Iteration 159, loss = 0.58394923\n",
            "Iteration 160, loss = 0.58525640\n",
            "Iteration 161, loss = 0.58247153\n",
            "Iteration 162, loss = 0.58622713\n",
            "Iteration 163, loss = 0.58255378\n",
            "Iteration 164, loss = 0.58193451\n",
            "Iteration 165, loss = 0.58424110\n",
            "Iteration 166, loss = 0.58000088\n",
            "Iteration 167, loss = 0.58304329\n",
            "Iteration 168, loss = 0.58291121\n",
            "Iteration 169, loss = 0.58032840\n",
            "Iteration 170, loss = 0.58064432\n",
            "Iteration 171, loss = 0.58165043\n",
            "Iteration 172, loss = 0.58108351\n",
            "Iteration 173, loss = 0.58056575\n",
            "Iteration 174, loss = 0.58182629\n",
            "Iteration 175, loss = 0.58455346\n",
            "Iteration 176, loss = 0.57827777\n",
            "Iteration 177, loss = 0.58057345\n",
            "Iteration 178, loss = 0.58112710\n",
            "Iteration 179, loss = 0.57846879\n",
            "Iteration 180, loss = 0.58139165\n",
            "Iteration 181, loss = 0.57970790\n",
            "Iteration 182, loss = 0.58065923\n",
            "Iteration 183, loss = 0.57864163\n",
            "Iteration 184, loss = 0.57762487\n",
            "Iteration 185, loss = 0.57873937\n",
            "Iteration 186, loss = 0.57932289\n",
            "Iteration 187, loss = 0.57682123\n",
            "Iteration 188, loss = 0.57906450\n",
            "Iteration 189, loss = 0.57931237\n",
            "Iteration 190, loss = 0.57763652\n",
            "Iteration 191, loss = 0.58166687\n",
            "Iteration 192, loss = 0.57622451\n",
            "Iteration 193, loss = 0.57778302\n",
            "Iteration 194, loss = 0.57960484\n",
            "Iteration 195, loss = 0.58133917\n",
            "Iteration 196, loss = 0.57388653\n",
            "Iteration 197, loss = 0.57820591\n",
            "Iteration 198, loss = 0.57281406\n",
            "Iteration 199, loss = 0.58260654\n",
            "Iteration 200, loss = 0.57753362\n",
            "Iteration 1, loss = 0.68443840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss = 0.67730883\n",
            "Iteration 3, loss = 0.67383296\n",
            "Iteration 4, loss = 0.67050046\n",
            "Iteration 5, loss = 0.66699514\n",
            "Iteration 6, loss = 0.66405080\n",
            "Iteration 7, loss = 0.66038692\n",
            "Iteration 8, loss = 0.65792657\n",
            "Iteration 9, loss = 0.65505827\n",
            "Iteration 10, loss = 0.65303076\n",
            "Iteration 11, loss = 0.65065003\n",
            "Iteration 12, loss = 0.64667998\n",
            "Iteration 13, loss = 0.64470536\n",
            "Iteration 14, loss = 0.63988863\n",
            "Iteration 15, loss = 0.63716850\n",
            "Iteration 16, loss = 0.63512508\n",
            "Iteration 17, loss = 0.63230769\n",
            "Iteration 18, loss = 0.63122384\n",
            "Iteration 19, loss = 0.62795969\n",
            "Iteration 20, loss = 0.62427701\n",
            "Iteration 21, loss = 0.62291754\n",
            "Iteration 22, loss = 0.62152113\n",
            "Iteration 23, loss = 0.61991802\n",
            "Iteration 24, loss = 0.61839902\n",
            "Iteration 25, loss = 0.62086755\n",
            "Iteration 26, loss = 0.61542287\n",
            "Iteration 27, loss = 0.61502095\n",
            "Iteration 28, loss = 0.61385545\n",
            "Iteration 29, loss = 0.61212014\n",
            "Iteration 30, loss = 0.61148653\n",
            "Iteration 31, loss = 0.61234102\n",
            "Iteration 32, loss = 0.61018796\n",
            "Iteration 33, loss = 0.61113728\n",
            "Iteration 34, loss = 0.60884037\n",
            "Iteration 35, loss = 0.60996946\n",
            "Iteration 36, loss = 0.60988875\n",
            "Iteration 37, loss = 0.60805720\n",
            "Iteration 38, loss = 0.60662305\n",
            "Iteration 39, loss = 0.60608645\n",
            "Iteration 40, loss = 0.60695465\n",
            "Iteration 41, loss = 0.60657036\n",
            "Iteration 42, loss = 0.60813046\n",
            "Iteration 43, loss = 0.60732348\n",
            "Iteration 44, loss = 0.60762996\n",
            "Iteration 45, loss = 0.60640217\n",
            "Iteration 46, loss = 0.60649821\n",
            "Iteration 47, loss = 0.60515045\n",
            "Iteration 48, loss = 0.60432323\n",
            "Iteration 49, loss = 0.60431262\n",
            "Iteration 50, loss = 0.60489367\n",
            "Iteration 51, loss = 0.60437510\n",
            "Iteration 52, loss = 0.60389711\n",
            "Iteration 53, loss = 0.60383351\n",
            "Iteration 54, loss = 0.60438257\n",
            "Iteration 55, loss = 0.60395600\n",
            "Iteration 56, loss = 0.60262833\n",
            "Iteration 57, loss = 0.60314554\n",
            "Iteration 58, loss = 0.60216124\n",
            "Iteration 59, loss = 0.60095462\n",
            "Iteration 60, loss = 0.60398609\n",
            "Iteration 61, loss = 0.60259753\n",
            "Iteration 62, loss = 0.60197210\n",
            "Iteration 63, loss = 0.60179243\n",
            "Iteration 64, loss = 0.60036364\n",
            "Iteration 65, loss = 0.60268980\n",
            "Iteration 66, loss = 0.60053118\n",
            "Iteration 67, loss = 0.59986717\n",
            "Iteration 68, loss = 0.60000444\n",
            "Iteration 69, loss = 0.60034723\n",
            "Iteration 70, loss = 0.60119291\n",
            "Iteration 71, loss = 0.59854256\n",
            "Iteration 72, loss = 0.59951771\n",
            "Iteration 73, loss = 0.60044425\n",
            "Iteration 74, loss = 0.60060490\n",
            "Iteration 75, loss = 0.59957362\n",
            "Iteration 76, loss = 0.59852779\n",
            "Iteration 77, loss = 0.59771684\n",
            "Iteration 78, loss = 0.59761879\n",
            "Iteration 79, loss = 0.59755596\n",
            "Iteration 80, loss = 0.60143851\n",
            "Iteration 81, loss = 0.59712653\n",
            "Iteration 82, loss = 0.59667846\n",
            "Iteration 83, loss = 0.59688831\n",
            "Iteration 84, loss = 0.59585366\n",
            "Iteration 85, loss = 0.59701580\n",
            "Iteration 86, loss = 0.59655238\n",
            "Iteration 87, loss = 0.59633775\n",
            "Iteration 88, loss = 0.59523803\n",
            "Iteration 89, loss = 0.59750762\n",
            "Iteration 90, loss = 0.59535072\n",
            "Iteration 91, loss = 0.59528887\n",
            "Iteration 92, loss = 0.59524925\n",
            "Iteration 93, loss = 0.59587482\n",
            "Iteration 94, loss = 0.59461080\n",
            "Iteration 95, loss = 0.59397569\n",
            "Iteration 96, loss = 0.59590324\n",
            "Iteration 97, loss = 0.59336409\n",
            "Iteration 98, loss = 0.59204767\n",
            "Iteration 99, loss = 0.59366985\n",
            "Iteration 100, loss = 0.59311793\n",
            "Iteration 101, loss = 0.59433854\n",
            "Iteration 102, loss = 0.59297337\n",
            "Iteration 103, loss = 0.59244602\n",
            "Iteration 104, loss = 0.59155245\n",
            "Iteration 105, loss = 0.59241010\n",
            "Iteration 106, loss = 0.59273450\n",
            "Iteration 107, loss = 0.59246419\n",
            "Iteration 108, loss = 0.59276002\n",
            "Iteration 109, loss = 0.59164664\n",
            "Iteration 110, loss = 0.59131206\n",
            "Iteration 111, loss = 0.59165861\n",
            "Iteration 112, loss = 0.58941392\n",
            "Iteration 113, loss = 0.59034010\n",
            "Iteration 114, loss = 0.58906675\n",
            "Iteration 115, loss = 0.59086525\n",
            "Iteration 116, loss = 0.59165686\n",
            "Iteration 117, loss = 0.58969010\n",
            "Iteration 118, loss = 0.58895606\n",
            "Iteration 119, loss = 0.58867061\n",
            "Iteration 120, loss = 0.58777639\n",
            "Iteration 121, loss = 0.58938556\n",
            "Iteration 122, loss = 0.58740255\n",
            "Iteration 123, loss = 0.59173415\n",
            "Iteration 124, loss = 0.58757584\n",
            "Iteration 125, loss = 0.58863201\n",
            "Iteration 126, loss = 0.58842660\n",
            "Iteration 127, loss = 0.58902329\n",
            "Iteration 128, loss = 0.58908518\n",
            "Iteration 129, loss = 0.58935106\n",
            "Iteration 130, loss = 0.58796362\n",
            "Iteration 131, loss = 0.58619530\n",
            "Iteration 132, loss = 0.58702318\n",
            "Iteration 133, loss = 0.58674520\n",
            "Iteration 134, loss = 0.58500473\n",
            "Iteration 135, loss = 0.58695249\n",
            "Iteration 136, loss = 0.58591782\n",
            "Iteration 137, loss = 0.58479271\n",
            "Iteration 138, loss = 0.58476503\n",
            "Iteration 139, loss = 0.58564197\n",
            "Iteration 140, loss = 0.58299149\n",
            "Iteration 141, loss = 0.58333729\n",
            "Iteration 142, loss = 0.58304370\n",
            "Iteration 143, loss = 0.58425655\n",
            "Iteration 144, loss = 0.58536470\n",
            "Iteration 145, loss = 0.58499095\n",
            "Iteration 146, loss = 0.58599341\n",
            "Iteration 147, loss = 0.58468352\n",
            "Iteration 148, loss = 0.58194858\n",
            "Iteration 149, loss = 0.58077484\n",
            "Iteration 150, loss = 0.58042987\n",
            "Iteration 151, loss = 0.58489064\n",
            "Iteration 152, loss = 0.58544151\n",
            "Iteration 153, loss = 0.58169678\n",
            "Iteration 154, loss = 0.58378984\n",
            "Iteration 155, loss = 0.57996240\n",
            "Iteration 156, loss = 0.57866921\n",
            "Iteration 157, loss = 0.57816718\n",
            "Iteration 158, loss = 0.57896126\n",
            "Iteration 159, loss = 0.57917245\n",
            "Iteration 160, loss = 0.57931959\n",
            "Iteration 161, loss = 0.57770782\n",
            "Iteration 162, loss = 0.57841166\n",
            "Iteration 163, loss = 0.57886604\n",
            "Iteration 164, loss = 0.57495917\n",
            "Iteration 165, loss = 0.58027883\n",
            "Iteration 166, loss = 0.58149579\n",
            "Iteration 167, loss = 0.57674422\n",
            "Iteration 168, loss = 0.57958543\n",
            "Iteration 169, loss = 0.57526826\n",
            "Iteration 170, loss = 0.57688609\n",
            "Iteration 171, loss = 0.57443586\n",
            "Iteration 172, loss = 0.58079500\n",
            "Iteration 173, loss = 0.57370453\n",
            "Iteration 174, loss = 0.57304949\n",
            "Iteration 175, loss = 0.57674969\n",
            "Iteration 176, loss = 0.57400587\n",
            "Iteration 177, loss = 0.57223530\n",
            "Iteration 178, loss = 0.57600804\n",
            "Iteration 179, loss = 0.57147596\n",
            "Iteration 180, loss = 0.57377500\n",
            "Iteration 181, loss = 0.57077610\n",
            "Iteration 182, loss = 0.57182272\n",
            "Iteration 183, loss = 0.56976741\n",
            "Iteration 184, loss = 0.56988461\n",
            "Iteration 185, loss = 0.56881925\n",
            "Iteration 186, loss = 0.57075276\n",
            "Iteration 187, loss = 0.57047031\n",
            "Iteration 188, loss = 0.56854798\n",
            "Iteration 189, loss = 0.56989884\n",
            "Iteration 190, loss = 0.56648860\n",
            "Iteration 191, loss = 0.57187952\n",
            "Iteration 192, loss = 0.56887303\n",
            "Iteration 193, loss = 0.57135388\n",
            "Iteration 194, loss = 0.56718480\n",
            "Iteration 195, loss = 0.56491521\n",
            "Iteration 196, loss = 0.56719712\n",
            "Iteration 197, loss = 0.56724888\n",
            "Iteration 198, loss = 0.56607455\n",
            "Iteration 199, loss = 0.56645490\n",
            "Iteration 200, loss = 0.56432941\n",
            "Iteration 1, loss = 0.69505174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss = 0.68335962\n",
            "Iteration 3, loss = 0.67895240\n",
            "Iteration 4, loss = 0.67427583\n",
            "Iteration 5, loss = 0.67141129\n",
            "Iteration 6, loss = 0.66783123\n",
            "Iteration 7, loss = 0.66524510\n",
            "Iteration 8, loss = 0.66293469\n",
            "Iteration 9, loss = 0.66084689\n",
            "Iteration 10, loss = 0.65575327\n",
            "Iteration 11, loss = 0.65448502\n",
            "Iteration 12, loss = 0.65066283\n",
            "Iteration 13, loss = 0.65136610\n",
            "Iteration 14, loss = 0.64777788\n",
            "Iteration 15, loss = 0.64686496\n",
            "Iteration 16, loss = 0.64183622\n",
            "Iteration 17, loss = 0.64166106\n",
            "Iteration 18, loss = 0.64104492\n",
            "Iteration 19, loss = 0.63951945\n",
            "Iteration 20, loss = 0.63492502\n",
            "Iteration 21, loss = 0.63339098\n",
            "Iteration 22, loss = 0.63214075\n",
            "Iteration 23, loss = 0.63037773\n",
            "Iteration 24, loss = 0.63111036\n",
            "Iteration 25, loss = 0.62851322\n",
            "Iteration 26, loss = 0.62719276\n",
            "Iteration 27, loss = 0.62851325\n",
            "Iteration 28, loss = 0.62659920\n",
            "Iteration 29, loss = 0.62508528\n",
            "Iteration 30, loss = 0.62416951\n",
            "Iteration 31, loss = 0.62421456\n",
            "Iteration 32, loss = 0.62453828\n",
            "Iteration 33, loss = 0.62409209\n",
            "Iteration 34, loss = 0.62328242\n",
            "Iteration 35, loss = 0.62239188\n",
            "Iteration 36, loss = 0.62134240\n",
            "Iteration 37, loss = 0.62355133\n",
            "Iteration 38, loss = 0.62391060\n",
            "Iteration 39, loss = 0.62147731\n",
            "Iteration 40, loss = 0.62070983\n",
            "Iteration 41, loss = 0.62409197\n",
            "Iteration 42, loss = 0.62084200\n",
            "Iteration 43, loss = 0.62130090\n",
            "Iteration 44, loss = 0.62160728\n",
            "Iteration 45, loss = 0.62080884\n",
            "Iteration 46, loss = 0.61894042\n",
            "Iteration 47, loss = 0.62141013\n",
            "Iteration 48, loss = 0.61849378\n",
            "Iteration 49, loss = 0.62062678\n",
            "Iteration 50, loss = 0.62072094\n",
            "Iteration 51, loss = 0.61969128\n",
            "Iteration 52, loss = 0.61842070\n",
            "Iteration 53, loss = 0.61798125\n",
            "Iteration 54, loss = 0.61882447\n",
            "Iteration 55, loss = 0.61921070\n",
            "Iteration 56, loss = 0.61962672\n",
            "Iteration 57, loss = 0.61834176\n",
            "Iteration 58, loss = 0.61911924\n",
            "Iteration 59, loss = 0.61773067\n",
            "Iteration 60, loss = 0.61759013\n",
            "Iteration 61, loss = 0.61957447\n",
            "Iteration 62, loss = 0.61855778\n",
            "Iteration 63, loss = 0.61795589\n",
            "Iteration 64, loss = 0.61813912\n",
            "Iteration 65, loss = 0.61588682\n",
            "Iteration 66, loss = 0.61668648\n",
            "Iteration 67, loss = 0.61634241\n",
            "Iteration 68, loss = 0.61930911\n",
            "Iteration 69, loss = 0.61563603\n",
            "Iteration 70, loss = 0.61625560\n",
            "Iteration 71, loss = 0.61627809\n",
            "Iteration 72, loss = 0.61509561\n",
            "Iteration 73, loss = 0.61658106\n",
            "Iteration 74, loss = 0.61694798\n",
            "Iteration 75, loss = 0.61571951\n",
            "Iteration 76, loss = 0.61514325\n",
            "Iteration 77, loss = 0.61582946\n",
            "Iteration 78, loss = 0.61480444\n",
            "Iteration 79, loss = 0.61617682\n",
            "Iteration 80, loss = 0.61469737\n",
            "Iteration 81, loss = 0.61364595\n",
            "Iteration 82, loss = 0.61368204\n",
            "Iteration 83, loss = 0.61298357\n",
            "Iteration 84, loss = 0.61687270\n",
            "Iteration 85, loss = 0.61309974\n",
            "Iteration 86, loss = 0.61357069\n",
            "Iteration 87, loss = 0.61308238\n",
            "Iteration 88, loss = 0.61357631\n",
            "Iteration 89, loss = 0.61348383\n",
            "Iteration 90, loss = 0.61288148\n",
            "Iteration 91, loss = 0.61388539\n",
            "Iteration 92, loss = 0.61386632\n",
            "Iteration 93, loss = 0.61210377\n",
            "Iteration 94, loss = 0.61192140\n",
            "Iteration 95, loss = 0.61250744\n",
            "Iteration 96, loss = 0.61401538\n",
            "Iteration 97, loss = 0.61292870\n",
            "Iteration 98, loss = 0.61269491\n",
            "Iteration 99, loss = 0.61198302\n",
            "Iteration 100, loss = 0.61135124\n",
            "Iteration 101, loss = 0.61392030\n",
            "Iteration 102, loss = 0.61182678\n",
            "Iteration 103, loss = 0.61252614\n",
            "Iteration 104, loss = 0.61175506\n",
            "Iteration 105, loss = 0.61043183\n",
            "Iteration 106, loss = 0.61294003\n",
            "Iteration 107, loss = 0.60992907\n",
            "Iteration 108, loss = 0.61217538\n",
            "Iteration 109, loss = 0.61121452\n",
            "Iteration 110, loss = 0.61139587\n",
            "Iteration 111, loss = 0.61251431\n",
            "Iteration 112, loss = 0.60913491\n",
            "Iteration 113, loss = 0.61009707\n",
            "Iteration 114, loss = 0.60951559\n",
            "Iteration 115, loss = 0.61090425\n",
            "Iteration 116, loss = 0.61001022\n",
            "Iteration 117, loss = 0.61041727\n",
            "Iteration 118, loss = 0.60872545\n",
            "Iteration 119, loss = 0.60975825\n",
            "Iteration 120, loss = 0.61056910\n",
            "Iteration 121, loss = 0.60888996\n",
            "Iteration 122, loss = 0.60872909\n",
            "Iteration 123, loss = 0.60864654\n",
            "Iteration 124, loss = 0.60866045\n",
            "Iteration 125, loss = 0.60827670\n",
            "Iteration 126, loss = 0.60747854\n",
            "Iteration 127, loss = 0.60757459\n",
            "Iteration 128, loss = 0.60679271\n",
            "Iteration 129, loss = 0.60725768\n",
            "Iteration 130, loss = 0.60626832\n",
            "Iteration 131, loss = 0.60666425\n",
            "Iteration 132, loss = 0.60487304\n",
            "Iteration 133, loss = 0.60868857\n",
            "Iteration 134, loss = 0.60718022\n",
            "Iteration 135, loss = 0.60728881\n",
            "Iteration 136, loss = 0.60719572\n",
            "Iteration 137, loss = 0.60574836\n",
            "Iteration 138, loss = 0.60818903\n",
            "Iteration 139, loss = 0.60514933\n",
            "Iteration 140, loss = 0.60498696\n",
            "Iteration 141, loss = 0.61059965\n",
            "Iteration 142, loss = 0.60400121\n",
            "Iteration 143, loss = 0.60408254\n",
            "Iteration 144, loss = 0.60360536\n",
            "Iteration 145, loss = 0.60290369\n",
            "Iteration 146, loss = 0.60314273\n",
            "Iteration 147, loss = 0.60595193\n",
            "Iteration 148, loss = 0.60312790\n",
            "Iteration 149, loss = 0.60178818\n",
            "Iteration 150, loss = 0.60433897\n",
            "Iteration 151, loss = 0.60467576\n",
            "Iteration 152, loss = 0.60513808\n",
            "Iteration 153, loss = 0.60361065\n",
            "Iteration 154, loss = 0.60201933\n",
            "Iteration 155, loss = 0.60211149\n",
            "Iteration 156, loss = 0.60212050\n",
            "Iteration 157, loss = 0.60117374\n",
            "Iteration 158, loss = 0.60174894\n",
            "Iteration 159, loss = 0.60162875\n",
            "Iteration 160, loss = 0.60313107\n",
            "Iteration 161, loss = 0.60008383\n",
            "Iteration 162, loss = 0.60037519\n",
            "Iteration 163, loss = 0.59846730\n",
            "Iteration 164, loss = 0.60250804\n",
            "Iteration 165, loss = 0.60129733\n",
            "Iteration 166, loss = 0.59815985\n",
            "Iteration 167, loss = 0.59842966\n",
            "Iteration 168, loss = 0.59972914\n",
            "Iteration 169, loss = 0.60129659\n",
            "Iteration 170, loss = 0.59869579\n",
            "Iteration 171, loss = 0.59918438\n",
            "Iteration 172, loss = 0.59990284\n",
            "Iteration 173, loss = 0.59936554\n",
            "Iteration 174, loss = 0.60216337\n",
            "Iteration 175, loss = 0.60044189\n",
            "Iteration 176, loss = 0.60350300\n",
            "Iteration 177, loss = 0.59579704\n",
            "Iteration 178, loss = 0.60041369\n",
            "Iteration 179, loss = 0.59522832\n",
            "Iteration 180, loss = 0.59829643\n",
            "Iteration 181, loss = 0.59726741\n",
            "Iteration 182, loss = 0.59506861\n",
            "Iteration 183, loss = 0.59538251\n",
            "Iteration 184, loss = 0.59639329\n",
            "Iteration 185, loss = 0.59707952\n",
            "Iteration 186, loss = 0.59547849\n",
            "Iteration 187, loss = 0.59301357\n",
            "Iteration 188, loss = 0.59571351\n",
            "Iteration 189, loss = 0.59701173\n",
            "Iteration 190, loss = 0.59409690\n",
            "Iteration 191, loss = 0.58866466\n",
            "Iteration 192, loss = 0.59716826\n",
            "Iteration 193, loss = 0.59383973\n",
            "Iteration 194, loss = 0.58914956\n",
            "Iteration 195, loss = 0.59426377\n",
            "Iteration 196, loss = 0.59306225\n",
            "Iteration 197, loss = 0.58995918\n",
            "Iteration 198, loss = 0.59144010\n",
            "Iteration 199, loss = 0.59252248\n",
            "Iteration 200, loss = 0.58772957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.69809067\n",
            "Iteration 2, loss = 0.67350780\n",
            "Iteration 3, loss = 0.66837277\n",
            "Iteration 4, loss = 0.66926321\n",
            "Iteration 5, loss = 0.66345668\n",
            "Iteration 6, loss = 0.66113559\n",
            "Iteration 7, loss = 0.65793934\n",
            "Iteration 8, loss = 0.65524908\n",
            "Iteration 9, loss = 0.65143183\n",
            "Iteration 10, loss = 0.64877057\n",
            "Iteration 11, loss = 0.64702555\n",
            "Iteration 12, loss = 0.64383088\n",
            "Iteration 13, loss = 0.64277651\n",
            "Iteration 14, loss = 0.64060366\n",
            "Iteration 15, loss = 0.63920686\n",
            "Iteration 16, loss = 0.63569062\n",
            "Iteration 17, loss = 0.63464455\n",
            "Iteration 18, loss = 0.63408890\n",
            "Iteration 19, loss = 0.63064505\n",
            "Iteration 20, loss = 0.62836443\n",
            "Iteration 21, loss = 0.62772945\n",
            "Iteration 22, loss = 0.63005480\n",
            "Iteration 23, loss = 0.62545948\n",
            "Iteration 24, loss = 0.62464233\n",
            "Iteration 25, loss = 0.62362337\n",
            "Iteration 26, loss = 0.62531439\n",
            "Iteration 27, loss = 0.62378860\n",
            "Iteration 28, loss = 0.62160428\n",
            "Iteration 29, loss = 0.62133194\n",
            "Iteration 30, loss = 0.62087734\n",
            "Iteration 31, loss = 0.61993137\n",
            "Iteration 32, loss = 0.61998935\n",
            "Iteration 33, loss = 0.62348098\n",
            "Iteration 34, loss = 0.61945603\n",
            "Iteration 35, loss = 0.61942861\n",
            "Iteration 36, loss = 0.61993402\n",
            "Iteration 37, loss = 0.61838626\n",
            "Iteration 38, loss = 0.61990156\n",
            "Iteration 39, loss = 0.61841433\n",
            "Iteration 40, loss = 0.61840124\n",
            "Iteration 41, loss = 0.61958322\n",
            "Iteration 42, loss = 0.61834494\n",
            "Iteration 43, loss = 0.61839847\n",
            "Iteration 44, loss = 0.61809376\n",
            "Iteration 45, loss = 0.61883102\n",
            "Iteration 46, loss = 0.61810405\n",
            "Iteration 47, loss = 0.61790905\n",
            "Iteration 48, loss = 0.61776487\n",
            "Iteration 49, loss = 0.61801934\n",
            "Iteration 50, loss = 0.61693070\n",
            "Iteration 51, loss = 0.61917608\n",
            "Iteration 52, loss = 0.61675619\n",
            "Iteration 53, loss = 0.61637976\n",
            "Iteration 54, loss = 0.61713812\n",
            "Iteration 55, loss = 0.61667827\n",
            "Iteration 56, loss = 0.61635532\n",
            "Iteration 57, loss = 0.61654628\n",
            "Iteration 58, loss = 0.61677437\n",
            "Iteration 59, loss = 0.61499326\n",
            "Iteration 60, loss = 0.61562833\n",
            "Iteration 61, loss = 0.61639208\n",
            "Iteration 62, loss = 0.61462490\n",
            "Iteration 63, loss = 0.61567862\n",
            "Iteration 64, loss = 0.61604496\n",
            "Iteration 65, loss = 0.61609063\n",
            "Iteration 66, loss = 0.61617486\n",
            "Iteration 67, loss = 0.61450567\n",
            "Iteration 68, loss = 0.61512634\n",
            "Iteration 69, loss = 0.61422641\n",
            "Iteration 70, loss = 0.61377426\n",
            "Iteration 71, loss = 0.61704714\n",
            "Iteration 72, loss = 0.61465211\n",
            "Iteration 73, loss = 0.61567410\n",
            "Iteration 74, loss = 0.61401133\n",
            "Iteration 75, loss = 0.61389554\n",
            "Iteration 76, loss = 0.61388661\n",
            "Iteration 77, loss = 0.61545270\n",
            "Iteration 78, loss = 0.61407953\n",
            "Iteration 79, loss = 0.61316612\n",
            "Iteration 80, loss = 0.61338272\n",
            "Iteration 81, loss = 0.61642968\n",
            "Iteration 82, loss = 0.61335814\n",
            "Iteration 83, loss = 0.61250094\n",
            "Iteration 84, loss = 0.61236151\n",
            "Iteration 85, loss = 0.61509115\n",
            "Iteration 86, loss = 0.61290803\n",
            "Iteration 87, loss = 0.61162485\n",
            "Iteration 88, loss = 0.61266517\n",
            "Iteration 89, loss = 0.61153981\n",
            "Iteration 90, loss = 0.61298531\n",
            "Iteration 91, loss = 0.61102895\n",
            "Iteration 92, loss = 0.61188071\n",
            "Iteration 93, loss = 0.61242407\n",
            "Iteration 94, loss = 0.61252441\n",
            "Iteration 95, loss = 0.61075521\n",
            "Iteration 96, loss = 0.61200213\n",
            "Iteration 97, loss = 0.61236483\n",
            "Iteration 98, loss = 0.61295162\n",
            "Iteration 99, loss = 0.60959437\n",
            "Iteration 100, loss = 0.61151595\n",
            "Iteration 101, loss = 0.60963041\n",
            "Iteration 102, loss = 0.60851178\n",
            "Iteration 103, loss = 0.61224647\n",
            "Iteration 104, loss = 0.60986243\n",
            "Iteration 105, loss = 0.60953119\n",
            "Iteration 106, loss = 0.61012587\n",
            "Iteration 107, loss = 0.61073779\n",
            "Iteration 108, loss = 0.61368410\n",
            "Iteration 109, loss = 0.60877339\n",
            "Iteration 110, loss = 0.61228511\n",
            "Iteration 111, loss = 0.60884538\n",
            "Iteration 112, loss = 0.60745952\n",
            "Iteration 113, loss = 0.60913116\n",
            "Iteration 114, loss = 0.60844647\n",
            "Iteration 115, loss = 0.60738066\n",
            "Iteration 116, loss = 0.60722695\n",
            "Iteration 117, loss = 0.60909083\n",
            "Iteration 118, loss = 0.60654839\n",
            "Iteration 119, loss = 0.60846663\n",
            "Iteration 120, loss = 0.60954064\n",
            "Iteration 121, loss = 0.60734277\n",
            "Iteration 122, loss = 0.60677216\n",
            "Iteration 123, loss = 0.61144364\n",
            "Iteration 124, loss = 0.60638486\n",
            "Iteration 125, loss = 0.60719917\n",
            "Iteration 126, loss = 0.60635590\n",
            "Iteration 127, loss = 0.60938851\n",
            "Iteration 128, loss = 0.60560054\n",
            "Iteration 129, loss = 0.60451242\n",
            "Iteration 130, loss = 0.60580093\n",
            "Iteration 131, loss = 0.60846835\n",
            "Iteration 132, loss = 0.60379856\n",
            "Iteration 133, loss = 0.60444254\n",
            "Iteration 134, loss = 0.60531341\n",
            "Iteration 135, loss = 0.60367204\n",
            "Iteration 136, loss = 0.60414051\n",
            "Iteration 137, loss = 0.60315892\n",
            "Iteration 138, loss = 0.60287911\n",
            "Iteration 139, loss = 0.60281833\n",
            "Iteration 140, loss = 0.60247854\n",
            "Iteration 141, loss = 0.60393875\n",
            "Iteration 142, loss = 0.60342819\n",
            "Iteration 143, loss = 0.60287750\n",
            "Iteration 144, loss = 0.60665429\n",
            "Iteration 145, loss = 0.60372304\n",
            "Iteration 146, loss = 0.60466524\n",
            "Iteration 147, loss = 0.60299805\n",
            "Iteration 148, loss = 0.60256051\n",
            "Iteration 149, loss = 0.60155704\n",
            "Iteration 150, loss = 0.60197003\n",
            "Iteration 151, loss = 0.60219160\n",
            "Iteration 152, loss = 0.59991498\n",
            "Iteration 153, loss = 0.60240320\n",
            "Iteration 154, loss = 0.60220328\n",
            "Iteration 155, loss = 0.60506485\n",
            "Iteration 156, loss = 0.59990182\n",
            "Iteration 157, loss = 0.60206293\n",
            "Iteration 158, loss = 0.59970136\n",
            "Iteration 159, loss = 0.60057548\n",
            "Iteration 160, loss = 0.60107986\n",
            "Iteration 161, loss = 0.59929058\n",
            "Iteration 162, loss = 0.60107573\n",
            "Iteration 163, loss = 0.60319157\n",
            "Iteration 164, loss = 0.59926877\n",
            "Iteration 165, loss = 0.60033570\n",
            "Iteration 166, loss = 0.60006344\n",
            "Iteration 167, loss = 0.60021156\n",
            "Iteration 168, loss = 0.59787037\n",
            "Iteration 169, loss = 0.59674239\n",
            "Iteration 170, loss = 0.59760217\n",
            "Iteration 171, loss = 0.59592444\n",
            "Iteration 172, loss = 0.59666364\n",
            "Iteration 173, loss = 0.59795637\n",
            "Iteration 174, loss = 0.59560923\n",
            "Iteration 175, loss = 0.59640815\n",
            "Iteration 176, loss = 0.59659041\n",
            "Iteration 177, loss = 0.59593751\n",
            "Iteration 178, loss = 0.59577181\n",
            "Iteration 179, loss = 0.59661287\n",
            "Iteration 180, loss = 0.59525408\n",
            "Iteration 181, loss = 0.59547176\n",
            "Iteration 182, loss = 0.59297256\n",
            "Iteration 183, loss = 0.59435760\n",
            "Iteration 184, loss = 0.59666650\n",
            "Iteration 185, loss = 0.59292773\n",
            "Iteration 186, loss = 0.59380839\n",
            "Iteration 187, loss = 0.59329774\n",
            "Iteration 188, loss = 0.59217654\n",
            "Iteration 189, loss = 0.59319979\n",
            "Iteration 190, loss = 0.59257807\n",
            "Iteration 191, loss = 0.59277477\n",
            "Iteration 192, loss = 0.59213691\n",
            "Iteration 193, loss = 0.59269326\n",
            "Iteration 194, loss = 0.59613608\n",
            "Iteration 195, loss = 0.59166238\n",
            "Iteration 196, loss = 0.59219746\n",
            "Iteration 197, loss = 0.59105041\n",
            "Iteration 198, loss = 0.58912003\n",
            "Iteration 199, loss = 0.59124523\n",
            "Iteration 200, loss = 0.59098783\n",
            "Iteration 1, loss = 0.65814526\n",
            "Iteration 2, loss = 0.64553649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3, loss = 0.59540747\n",
            "Iteration 4, loss = 0.65325593\n",
            "Iteration 5, loss = 0.59174511\n",
            "Iteration 6, loss = 0.61647944\n",
            "Iteration 7, loss = 0.57801582\n",
            "Iteration 8, loss = 0.54583961\n",
            "Iteration 9, loss = 0.55819027\n",
            "Iteration 10, loss = 0.53614789\n",
            "Iteration 11, loss = 0.55464563\n",
            "Iteration 12, loss = 0.54283841\n",
            "Iteration 13, loss = 0.53456274\n",
            "Iteration 14, loss = 0.50013532\n",
            "Iteration 15, loss = 0.49677292\n",
            "Iteration 16, loss = 0.49418158\n",
            "Iteration 17, loss = 0.53724687\n",
            "Iteration 18, loss = 0.49246638\n",
            "Iteration 19, loss = 0.47703000\n",
            "Iteration 20, loss = 0.50720171\n",
            "Iteration 21, loss = 0.46601246\n",
            "Iteration 22, loss = 0.48171735\n",
            "Iteration 23, loss = 0.48009186\n",
            "Iteration 24, loss = 0.46459255\n",
            "Iteration 25, loss = 0.47368755\n",
            "Iteration 26, loss = 0.45877389\n",
            "Iteration 27, loss = 0.44389839\n",
            "Iteration 28, loss = 0.43973151\n",
            "Iteration 29, loss = 0.43804742\n",
            "Iteration 30, loss = 0.47767310\n",
            "Iteration 31, loss = 0.47449747\n",
            "Iteration 32, loss = 0.45365617\n",
            "Iteration 33, loss = 0.48934041\n",
            "Iteration 34, loss = 0.44752827\n",
            "Iteration 35, loss = 0.44018308\n",
            "Iteration 36, loss = 0.45148297\n",
            "Iteration 37, loss = 0.45171789\n",
            "Iteration 38, loss = 0.44637632\n",
            "Iteration 39, loss = 0.42970851\n",
            "Iteration 40, loss = 0.41225561\n",
            "Iteration 41, loss = 0.42321909\n",
            "Iteration 42, loss = 0.50823580\n",
            "Iteration 43, loss = 0.49700355\n",
            "Iteration 44, loss = 0.43402367\n",
            "Iteration 45, loss = 0.44060404\n",
            "Iteration 46, loss = 0.43032875\n",
            "Iteration 47, loss = 0.44816074\n",
            "Iteration 48, loss = 0.43045386\n",
            "Iteration 49, loss = 0.42836418\n",
            "Iteration 50, loss = 0.42277205\n",
            "Iteration 51, loss = 0.41205565\n",
            "Iteration 52, loss = 0.42830002\n",
            "Iteration 53, loss = 0.42806959\n",
            "Iteration 54, loss = 0.40693927\n",
            "Iteration 55, loss = 0.45477950\n",
            "Iteration 56, loss = 0.45347146\n",
            "Iteration 57, loss = 0.41835728\n",
            "Iteration 58, loss = 0.43776981\n",
            "Iteration 59, loss = 0.48209581\n",
            "Iteration 60, loss = 0.43625050\n",
            "Iteration 61, loss = 0.42321308\n",
            "Iteration 62, loss = 0.44572854\n",
            "Iteration 63, loss = 0.42674974\n",
            "Iteration 64, loss = 0.40780123\n",
            "Iteration 65, loss = 0.42029310\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.83251862\n",
            "Iteration 2, loss = 0.72692167\n",
            "Iteration 3, loss = 0.69294548\n",
            "Iteration 4, loss = 0.60971625\n",
            "Iteration 5, loss = 0.59688431\n",
            "Iteration 6, loss = 0.57857153\n",
            "Iteration 7, loss = 0.55690489\n",
            "Iteration 8, loss = 0.56945855\n",
            "Iteration 9, loss = 0.64034991\n",
            "Iteration 10, loss = 0.55913341\n",
            "Iteration 11, loss = 0.59226767\n",
            "Iteration 12, loss = 0.56221800\n",
            "Iteration 13, loss = 0.55658171\n",
            "Iteration 14, loss = 0.51863915\n",
            "Iteration 15, loss = 0.51945726\n",
            "Iteration 16, loss = 0.50022798\n",
            "Iteration 17, loss = 0.50823377\n",
            "Iteration 18, loss = 0.62036013\n",
            "Iteration 19, loss = 0.49179338\n",
            "Iteration 20, loss = 0.48298119\n",
            "Iteration 21, loss = 0.45646826\n",
            "Iteration 22, loss = 0.47802976\n",
            "Iteration 23, loss = 0.47554718\n",
            "Iteration 24, loss = 0.50805218\n",
            "Iteration 25, loss = 0.46288074\n",
            "Iteration 26, loss = 0.45609139\n",
            "Iteration 27, loss = 0.47580030\n",
            "Iteration 28, loss = 0.45426984\n",
            "Iteration 29, loss = 0.53169410\n",
            "Iteration 30, loss = 0.45491757\n",
            "Iteration 31, loss = 0.46187291\n",
            "Iteration 32, loss = 0.44588976\n",
            "Iteration 33, loss = 0.46260489\n",
            "Iteration 34, loss = 0.48789423\n",
            "Iteration 35, loss = 0.49799043\n",
            "Iteration 36, loss = 0.47708091\n",
            "Iteration 37, loss = 0.45977642\n",
            "Iteration 38, loss = 0.43451506\n",
            "Iteration 39, loss = 0.42334211\n",
            "Iteration 40, loss = 0.51388600\n",
            "Iteration 41, loss = 0.51717719\n",
            "Iteration 42, loss = 0.43594279\n",
            "Iteration 43, loss = 0.52644606\n",
            "Iteration 44, loss = 0.43618886\n",
            "Iteration 45, loss = 0.50453716\n",
            "Iteration 46, loss = 0.44751089\n",
            "Iteration 47, loss = 0.53358533\n",
            "Iteration 48, loss = 0.42370134\n",
            "Iteration 49, loss = 0.42356232\n",
            "Iteration 50, loss = 0.43459099\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.27353192\n",
            "Iteration 2, loss = 0.63660513\n",
            "Iteration 3, loss = 0.66287782\n",
            "Iteration 4, loss = 0.61482496\n",
            "Iteration 5, loss = 0.57544344\n",
            "Iteration 6, loss = 0.60708084\n",
            "Iteration 7, loss = 0.63415346\n",
            "Iteration 8, loss = 0.54356361\n",
            "Iteration 9, loss = 0.55383110\n",
            "Iteration 10, loss = 0.53273412\n",
            "Iteration 11, loss = 0.54778026\n",
            "Iteration 12, loss = 0.54028241\n",
            "Iteration 13, loss = 0.51538149\n",
            "Iteration 14, loss = 0.52523697\n",
            "Iteration 15, loss = 0.54036709\n",
            "Iteration 16, loss = 0.53934861\n",
            "Iteration 17, loss = 0.50108145\n",
            "Iteration 18, loss = 0.47359791\n",
            "Iteration 19, loss = 0.55614141\n",
            "Iteration 20, loss = 0.50234895\n",
            "Iteration 21, loss = 0.50647784\n",
            "Iteration 22, loss = 0.51201650\n",
            "Iteration 23, loss = 0.48205071\n",
            "Iteration 24, loss = 0.50298528\n",
            "Iteration 25, loss = 0.45866082\n",
            "Iteration 26, loss = 0.45703427\n",
            "Iteration 27, loss = 0.48203761\n",
            "Iteration 28, loss = 0.46764772\n",
            "Iteration 29, loss = 0.49265280\n",
            "Iteration 30, loss = 0.45228902\n",
            "Iteration 31, loss = 0.48448758\n",
            "Iteration 32, loss = 0.44533287\n",
            "Iteration 33, loss = 0.51900351\n",
            "Iteration 34, loss = 0.47428058\n",
            "Iteration 35, loss = 0.44756647\n",
            "Iteration 36, loss = 0.47839545\n",
            "Iteration 37, loss = 0.44432899\n",
            "Iteration 38, loss = 0.44177354\n",
            "Iteration 39, loss = 0.44956854\n",
            "Iteration 40, loss = 0.46640334\n",
            "Iteration 41, loss = 0.47966380\n",
            "Iteration 42, loss = 0.46149870\n",
            "Iteration 43, loss = 0.45973082\n",
            "Iteration 44, loss = 0.44772448\n",
            "Iteration 45, loss = 0.45144335\n",
            "Iteration 46, loss = 0.45574643\n",
            "Iteration 47, loss = 0.42425696\n",
            "Iteration 48, loss = 0.43829267\n",
            "Iteration 49, loss = 0.47256902\n",
            "Iteration 50, loss = 0.44971628\n",
            "Iteration 51, loss = 0.49097154\n",
            "Iteration 52, loss = 0.43577827\n",
            "Iteration 53, loss = 0.45451998\n",
            "Iteration 54, loss = 0.41173000\n",
            "Iteration 55, loss = 0.43342660\n",
            "Iteration 56, loss = 0.41671429\n",
            "Iteration 57, loss = 0.42383469\n",
            "Iteration 58, loss = 0.41466963\n",
            "Iteration 59, loss = 0.41626556\n",
            "Iteration 60, loss = 0.43178952\n",
            "Iteration 61, loss = 0.44125069\n",
            "Iteration 62, loss = 0.40770001\n",
            "Iteration 63, loss = 0.42483846\n",
            "Iteration 64, loss = 0.46063017\n",
            "Iteration 65, loss = 0.41883332\n",
            "Iteration 66, loss = 0.43015528\n",
            "Iteration 67, loss = 0.40699204\n",
            "Iteration 68, loss = 0.45545177\n",
            "Iteration 69, loss = 0.42738851\n",
            "Iteration 70, loss = 0.41930275\n",
            "Iteration 71, loss = 0.41409790\n",
            "Iteration 72, loss = 0.49372987\n",
            "Iteration 73, loss = 0.40730750\n",
            "Iteration 74, loss = 0.42434136\n",
            "Iteration 75, loss = 0.40191205\n",
            "Iteration 76, loss = 0.45973505\n",
            "Iteration 77, loss = 0.41351858\n",
            "Iteration 78, loss = 0.40002085\n",
            "Iteration 79, loss = 0.39270677\n",
            "Iteration 80, loss = 0.39820590\n",
            "Iteration 81, loss = 0.43184478\n",
            "Iteration 82, loss = 0.41475871\n",
            "Iteration 83, loss = 0.40790449\n",
            "Iteration 84, loss = 0.40805888\n",
            "Iteration 85, loss = 0.41776385\n",
            "Iteration 86, loss = 0.41484040\n",
            "Iteration 87, loss = 0.42155399\n",
            "Iteration 88, loss = 0.39795069\n",
            "Iteration 89, loss = 0.44281455\n",
            "Iteration 90, loss = 0.42242450\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.72193621\n",
            "Iteration 2, loss = 0.66904539\n",
            "Iteration 3, loss = 0.64516386\n",
            "Iteration 4, loss = 0.61546048\n",
            "Iteration 5, loss = 0.63411659\n",
            "Iteration 6, loss = 0.58762026\n",
            "Iteration 7, loss = 0.53882640\n",
            "Iteration 8, loss = 0.53947759\n",
            "Iteration 9, loss = 0.51749212\n",
            "Iteration 10, loss = 0.55772163\n",
            "Iteration 11, loss = 0.59991984\n",
            "Iteration 12, loss = 0.54430073\n",
            "Iteration 13, loss = 0.54004564\n",
            "Iteration 14, loss = 0.51542693\n",
            "Iteration 15, loss = 0.53013145\n",
            "Iteration 16, loss = 0.47453936\n",
            "Iteration 17, loss = 0.57115669\n",
            "Iteration 18, loss = 0.45724839\n",
            "Iteration 19, loss = 0.50064747\n",
            "Iteration 20, loss = 0.46639206\n",
            "Iteration 21, loss = 0.47925411\n",
            "Iteration 22, loss = 0.47990752\n",
            "Iteration 23, loss = 0.48239725\n",
            "Iteration 24, loss = 0.43877838\n",
            "Iteration 25, loss = 0.45316464\n",
            "Iteration 26, loss = 0.45325522\n",
            "Iteration 27, loss = 0.49563623\n",
            "Iteration 28, loss = 0.44024808\n",
            "Iteration 29, loss = 0.46519776\n",
            "Iteration 30, loss = 0.51247328\n",
            "Iteration 31, loss = 0.49897142\n",
            "Iteration 32, loss = 0.47536759\n",
            "Iteration 33, loss = 0.44492001\n",
            "Iteration 34, loss = 0.46514180\n",
            "Iteration 35, loss = 0.44981214\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.78833089\n",
            "Iteration 2, loss = 0.67246381\n",
            "Iteration 3, loss = 0.61160084\n",
            "Iteration 4, loss = 0.60453825\n",
            "Iteration 5, loss = 0.58996728\n",
            "Iteration 6, loss = 0.58777214\n",
            "Iteration 7, loss = 0.56734187\n",
            "Iteration 8, loss = 0.60812705\n",
            "Iteration 9, loss = 0.55571814\n",
            "Iteration 10, loss = 0.55808101\n",
            "Iteration 11, loss = 0.62425409\n",
            "Iteration 12, loss = 0.58766160\n",
            "Iteration 13, loss = 0.51680914\n",
            "Iteration 14, loss = 0.50536706\n",
            "Iteration 15, loss = 0.51635188\n",
            "Iteration 16, loss = 0.49263686\n",
            "Iteration 17, loss = 0.49301425\n",
            "Iteration 18, loss = 0.48546576\n",
            "Iteration 19, loss = 0.46855078\n",
            "Iteration 20, loss = 0.47104943\n",
            "Iteration 21, loss = 0.50982650\n",
            "Iteration 22, loss = 0.47410640\n",
            "Iteration 23, loss = 0.44741684\n",
            "Iteration 24, loss = 0.46259210\n",
            "Iteration 25, loss = 0.48905641\n",
            "Iteration 26, loss = 0.45407252\n",
            "Iteration 27, loss = 0.44349120\n",
            "Iteration 28, loss = 0.45661416\n",
            "Iteration 29, loss = 0.45827020\n",
            "Iteration 30, loss = 0.46520684\n",
            "Iteration 31, loss = 0.45504894\n",
            "Iteration 32, loss = 0.45228462\n",
            "Iteration 33, loss = 0.44860119\n",
            "Iteration 34, loss = 0.45029991\n",
            "Iteration 35, loss = 0.44386870\n",
            "Iteration 36, loss = 0.44439016\n",
            "Iteration 37, loss = 0.43564117\n",
            "Iteration 38, loss = 0.47202718\n",
            "Iteration 39, loss = 0.45939723\n",
            "Iteration 40, loss = 0.44502885\n",
            "Iteration 41, loss = 0.45635232\n",
            "Iteration 42, loss = 0.44138257\n",
            "Iteration 43, loss = 0.45140363\n",
            "Iteration 44, loss = 0.43699515\n",
            "Iteration 45, loss = 0.47706244\n",
            "Iteration 46, loss = 0.45253822\n",
            "Iteration 47, loss = 0.44101308\n",
            "Iteration 48, loss = 0.45228480\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.06960974\n",
            "Iteration 2, loss = 0.94424625\n",
            "Iteration 3, loss = 0.65388630\n",
            "Iteration 4, loss = 0.61470313\n",
            "Iteration 5, loss = 0.61647737\n",
            "Iteration 6, loss = 0.59863265\n",
            "Iteration 7, loss = 0.60049833\n",
            "Iteration 8, loss = 0.60009397\n",
            "Iteration 9, loss = 0.61045281\n",
            "Iteration 10, loss = 0.58821129\n",
            "Iteration 11, loss = 0.58060993\n",
            "Iteration 12, loss = 0.58233509\n",
            "Iteration 13, loss = 0.60289383\n",
            "Iteration 14, loss = 0.58167908\n",
            "Iteration 15, loss = 0.58994213\n",
            "Iteration 16, loss = 0.57516968\n",
            "Iteration 17, loss = 0.57224469\n",
            "Iteration 18, loss = 0.56916984\n",
            "Iteration 19, loss = 0.58025220\n",
            "Iteration 20, loss = 0.58467919\n",
            "Iteration 21, loss = 0.56897111\n",
            "Iteration 22, loss = 0.56299686\n",
            "Iteration 23, loss = 0.57612031\n",
            "Iteration 24, loss = 0.56812643\n",
            "Iteration 25, loss = 0.56623914\n",
            "Iteration 26, loss = 0.56127600\n",
            "Iteration 27, loss = 0.56773389\n",
            "Iteration 28, loss = 0.55117122\n",
            "Iteration 29, loss = 0.55667165\n",
            "Iteration 30, loss = 0.56060037\n",
            "Iteration 31, loss = 0.56240449\n",
            "Iteration 32, loss = 0.55566125\n",
            "Iteration 33, loss = 0.54685972\n",
            "Iteration 34, loss = 0.54379034\n",
            "Iteration 35, loss = 0.55933502\n",
            "Iteration 36, loss = 0.54693987\n",
            "Iteration 37, loss = 0.55182983\n",
            "Iteration 38, loss = 0.56802464\n",
            "Iteration 39, loss = 0.53917638\n",
            "Iteration 40, loss = 0.53946341\n",
            "Iteration 41, loss = 0.55231882\n",
            "Iteration 42, loss = 0.54095040\n",
            "Iteration 43, loss = 0.54691574\n",
            "Iteration 44, loss = 0.54437236\n",
            "Iteration 45, loss = 0.52621703\n",
            "Iteration 46, loss = 0.52334441\n",
            "Iteration 47, loss = 0.52944792\n",
            "Iteration 48, loss = 0.53836857\n",
            "Iteration 49, loss = 0.51784551\n",
            "Iteration 50, loss = 0.53340292\n",
            "Iteration 51, loss = 0.52441010\n",
            "Iteration 52, loss = 0.53921837\n",
            "Iteration 53, loss = 0.52245766\n",
            "Iteration 54, loss = 0.54528175\n",
            "Iteration 55, loss = 0.53006749\n",
            "Iteration 56, loss = 0.51993425\n",
            "Iteration 57, loss = 0.50517977\n",
            "Iteration 58, loss = 0.50803810\n",
            "Iteration 59, loss = 0.51264308\n",
            "Iteration 60, loss = 0.53004456\n",
            "Iteration 61, loss = 0.52075054\n",
            "Iteration 62, loss = 0.52424521\n",
            "Iteration 63, loss = 0.52515463\n",
            "Iteration 64, loss = 0.51919951\n",
            "Iteration 65, loss = 0.50823474\n",
            "Iteration 66, loss = 0.49403628\n",
            "Iteration 67, loss = 0.51526679\n",
            "Iteration 68, loss = 0.51074992\n",
            "Iteration 69, loss = 0.50420045\n",
            "Iteration 70, loss = 0.51723967\n",
            "Iteration 71, loss = 0.49995505\n",
            "Iteration 72, loss = 0.50875919\n",
            "Iteration 73, loss = 0.49900585\n",
            "Iteration 74, loss = 0.51110799\n",
            "Iteration 75, loss = 0.50557821\n",
            "Iteration 76, loss = 0.53039272\n",
            "Iteration 77, loss = 0.52356027\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.01876058\n",
            "Iteration 2, loss = 0.77388941\n",
            "Iteration 3, loss = 0.62485707\n",
            "Iteration 4, loss = 0.65428902\n",
            "Iteration 5, loss = 0.61516921\n",
            "Iteration 6, loss = 0.61041165\n",
            "Iteration 7, loss = 0.60269020\n",
            "Iteration 8, loss = 0.62753212\n",
            "Iteration 9, loss = 0.58884858\n",
            "Iteration 10, loss = 0.59652399\n",
            "Iteration 11, loss = 0.59984346\n",
            "Iteration 12, loss = 0.59075459\n",
            "Iteration 13, loss = 0.58887342\n",
            "Iteration 14, loss = 0.58141490\n",
            "Iteration 15, loss = 0.58162740\n",
            "Iteration 16, loss = 0.57635039\n",
            "Iteration 17, loss = 0.58695920\n",
            "Iteration 18, loss = 0.58008116\n",
            "Iteration 19, loss = 0.57624757\n",
            "Iteration 20, loss = 0.57283042\n",
            "Iteration 21, loss = 0.57067402\n",
            "Iteration 22, loss = 0.58087431\n",
            "Iteration 23, loss = 0.56732395\n",
            "Iteration 24, loss = 0.57225016\n",
            "Iteration 25, loss = 0.56582990\n",
            "Iteration 26, loss = 0.56810097\n",
            "Iteration 27, loss = 0.55905277\n",
            "Iteration 28, loss = 0.55798794\n",
            "Iteration 29, loss = 0.55307277\n",
            "Iteration 30, loss = 0.55906141\n",
            "Iteration 31, loss = 0.55975894\n",
            "Iteration 32, loss = 0.55247991\n",
            "Iteration 33, loss = 0.54778013\n",
            "Iteration 34, loss = 0.54865495\n",
            "Iteration 35, loss = 0.55188174\n",
            "Iteration 36, loss = 0.55373256\n",
            "Iteration 37, loss = 0.54164329\n",
            "Iteration 38, loss = 0.55260204\n",
            "Iteration 39, loss = 0.58856576\n",
            "Iteration 40, loss = 0.55650537\n",
            "Iteration 41, loss = 0.54585423\n",
            "Iteration 42, loss = 0.52752576\n",
            "Iteration 43, loss = 0.53874314\n",
            "Iteration 44, loss = 0.53442796\n",
            "Iteration 45, loss = 0.52869774\n",
            "Iteration 46, loss = 0.53672304\n",
            "Iteration 47, loss = 0.53928102\n",
            "Iteration 48, loss = 0.53446698\n",
            "Iteration 49, loss = 0.52314291\n",
            "Iteration 50, loss = 0.53098481\n",
            "Iteration 51, loss = 0.54956885\n",
            "Iteration 52, loss = 0.53251710\n",
            "Iteration 53, loss = 0.52293095\n",
            "Iteration 54, loss = 0.51559963\n",
            "Iteration 55, loss = 0.53927168\n",
            "Iteration 56, loss = 0.51473603\n",
            "Iteration 57, loss = 0.52566453\n",
            "Iteration 58, loss = 0.51531946\n",
            "Iteration 59, loss = 0.52055824\n",
            "Iteration 60, loss = 0.52302288\n",
            "Iteration 61, loss = 0.51188326\n",
            "Iteration 62, loss = 0.52023443\n",
            "Iteration 63, loss = 0.50097656\n",
            "Iteration 64, loss = 0.51595621\n",
            "Iteration 65, loss = 0.53043843\n",
            "Iteration 66, loss = 0.52282518\n",
            "Iteration 67, loss = 0.49015859\n",
            "Iteration 68, loss = 0.51942629\n",
            "Iteration 69, loss = 0.52053713\n",
            "Iteration 70, loss = 0.50285084\n",
            "Iteration 71, loss = 0.49765867\n",
            "Iteration 72, loss = 0.52590860\n",
            "Iteration 73, loss = 0.49369923\n",
            "Iteration 74, loss = 0.49989450\n",
            "Iteration 75, loss = 0.51515653\n",
            "Iteration 76, loss = 0.50343356\n",
            "Iteration 77, loss = 0.50790151\n",
            "Iteration 78, loss = 0.49829869\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.82095465\n",
            "Iteration 2, loss = 0.67219044\n",
            "Iteration 3, loss = 0.66071745\n",
            "Iteration 4, loss = 0.71610759\n",
            "Iteration 5, loss = 0.63972272\n",
            "Iteration 6, loss = 0.62910221\n",
            "Iteration 7, loss = 0.59901925\n",
            "Iteration 8, loss = 0.59063988\n",
            "Iteration 9, loss = 0.59832437\n",
            "Iteration 10, loss = 0.59037563\n",
            "Iteration 11, loss = 0.60240528\n",
            "Iteration 12, loss = 0.60714433\n",
            "Iteration 13, loss = 0.59088978\n",
            "Iteration 14, loss = 0.58114103\n",
            "Iteration 15, loss = 0.59076407\n",
            "Iteration 16, loss = 0.58458055\n",
            "Iteration 17, loss = 0.58329160\n",
            "Iteration 18, loss = 0.57611916\n",
            "Iteration 19, loss = 0.57411930\n",
            "Iteration 20, loss = 0.57144737\n",
            "Iteration 21, loss = 0.56609660\n",
            "Iteration 22, loss = 0.57845536\n",
            "Iteration 23, loss = 0.56553225\n",
            "Iteration 24, loss = 0.56829310\n",
            "Iteration 25, loss = 0.56471899\n",
            "Iteration 26, loss = 0.56741002\n",
            "Iteration 27, loss = 0.55921700\n",
            "Iteration 28, loss = 0.55830153\n",
            "Iteration 29, loss = 0.56293912\n",
            "Iteration 30, loss = 0.56573530\n",
            "Iteration 31, loss = 0.54380652\n",
            "Iteration 32, loss = 0.55710288\n",
            "Iteration 33, loss = 0.54642553\n",
            "Iteration 34, loss = 0.54027324\n",
            "Iteration 35, loss = 0.55162724\n",
            "Iteration 36, loss = 0.54167529\n",
            "Iteration 37, loss = 0.54026465\n",
            "Iteration 38, loss = 0.53338564\n",
            "Iteration 39, loss = 0.53826279\n",
            "Iteration 40, loss = 0.53085218\n",
            "Iteration 41, loss = 0.53863421\n",
            "Iteration 42, loss = 0.53699017\n",
            "Iteration 43, loss = 0.53378588\n",
            "Iteration 44, loss = 0.53925983\n",
            "Iteration 45, loss = 0.52497165\n",
            "Iteration 46, loss = 0.54837688\n",
            "Iteration 47, loss = 0.52250395\n",
            "Iteration 48, loss = 0.54098747\n",
            "Iteration 49, loss = 0.52399028\n",
            "Iteration 50, loss = 0.52266449\n",
            "Iteration 51, loss = 0.51353735\n",
            "Iteration 52, loss = 0.54596582\n",
            "Iteration 53, loss = 0.52155227\n",
            "Iteration 54, loss = 0.50991994\n",
            "Iteration 55, loss = 0.51725568\n",
            "Iteration 56, loss = 0.51438824\n",
            "Iteration 57, loss = 0.52277379\n",
            "Iteration 58, loss = 0.52392388\n",
            "Iteration 59, loss = 0.52093235\n",
            "Iteration 60, loss = 0.51377397\n",
            "Iteration 61, loss = 0.53358634\n",
            "Iteration 62, loss = 0.52322465\n",
            "Iteration 63, loss = 0.51504946\n",
            "Iteration 64, loss = 0.49914034\n",
            "Iteration 65, loss = 0.50260283\n",
            "Iteration 66, loss = 0.50809371\n",
            "Iteration 67, loss = 0.50322319\n",
            "Iteration 68, loss = 0.51481665\n",
            "Iteration 69, loss = 0.50591038\n",
            "Iteration 70, loss = 0.50444623\n",
            "Iteration 71, loss = 0.50566925\n",
            "Iteration 72, loss = 0.49972447\n",
            "Iteration 73, loss = 0.48180905\n",
            "Iteration 74, loss = 0.50840204\n",
            "Iteration 75, loss = 0.49906747\n",
            "Iteration 76, loss = 0.50034720\n",
            "Iteration 77, loss = 0.48475232\n",
            "Iteration 78, loss = 0.50613296\n",
            "Iteration 79, loss = 0.48697416\n",
            "Iteration 80, loss = 0.50570294\n",
            "Iteration 81, loss = 0.50790190\n",
            "Iteration 82, loss = 0.48403445\n",
            "Iteration 83, loss = 0.52102935\n",
            "Iteration 84, loss = 0.48096064\n",
            "Iteration 85, loss = 0.51807555\n",
            "Iteration 86, loss = 0.47974897\n",
            "Iteration 87, loss = 0.48768733\n",
            "Iteration 88, loss = 0.47756924\n",
            "Iteration 89, loss = 0.50863051\n",
            "Iteration 90, loss = 0.51132050\n",
            "Iteration 91, loss = 0.47966398\n",
            "Iteration 92, loss = 0.48819367\n",
            "Iteration 93, loss = 0.50027104\n",
            "Iteration 94, loss = 0.48452558\n",
            "Iteration 95, loss = 0.50212186\n",
            "Iteration 96, loss = 0.48231077\n",
            "Iteration 97, loss = 0.48193080\n",
            "Iteration 98, loss = 0.47698047\n",
            "Iteration 99, loss = 0.47355547\n",
            "Iteration 100, loss = 0.47413000\n",
            "Iteration 101, loss = 0.49159047\n",
            "Iteration 102, loss = 0.48817412\n",
            "Iteration 103, loss = 0.47296448\n",
            "Iteration 104, loss = 0.49428764\n",
            "Iteration 105, loss = 0.47716639\n",
            "Iteration 106, loss = 0.47313143\n",
            "Iteration 107, loss = 0.46958398\n",
            "Iteration 108, loss = 0.46935231\n",
            "Iteration 109, loss = 0.48542080\n",
            "Iteration 110, loss = 0.48690098\n",
            "Iteration 111, loss = 0.48763494\n",
            "Iteration 112, loss = 0.47901442\n",
            "Iteration 113, loss = 0.49889208\n",
            "Iteration 114, loss = 0.49832001\n",
            "Iteration 115, loss = 0.47737485\n",
            "Iteration 116, loss = 0.48108658\n",
            "Iteration 117, loss = 0.46953677\n",
            "Iteration 118, loss = 0.47720910\n",
            "Iteration 119, loss = 0.45844039\n",
            "Iteration 120, loss = 0.47411380\n",
            "Iteration 121, loss = 0.47991012\n",
            "Iteration 122, loss = 0.46522164\n",
            "Iteration 123, loss = 0.47231321\n",
            "Iteration 124, loss = 0.46948789\n",
            "Iteration 125, loss = 0.46448602\n",
            "Iteration 126, loss = 0.47761116\n",
            "Iteration 127, loss = 0.48018799\n",
            "Iteration 128, loss = 0.48180206\n",
            "Iteration 129, loss = 0.46393273\n",
            "Iteration 130, loss = 0.47851706\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.00162245\n",
            "Iteration 2, loss = 0.67954722\n",
            "Iteration 3, loss = 0.68083186\n",
            "Iteration 4, loss = 0.66946894\n",
            "Iteration 5, loss = 0.66137318\n",
            "Iteration 6, loss = 0.65350858\n",
            "Iteration 7, loss = 0.63580148\n",
            "Iteration 8, loss = 0.63561260\n",
            "Iteration 9, loss = 0.62975375\n",
            "Iteration 10, loss = 0.62227272\n",
            "Iteration 11, loss = 0.62063085\n",
            "Iteration 12, loss = 0.61627111\n",
            "Iteration 13, loss = 0.61850984\n",
            "Iteration 14, loss = 0.59763689\n",
            "Iteration 15, loss = 0.60141680\n",
            "Iteration 16, loss = 0.59843009\n",
            "Iteration 17, loss = 0.60595147\n",
            "Iteration 18, loss = 0.60832208\n",
            "Iteration 19, loss = 0.60966002\n",
            "Iteration 20, loss = 0.60236504\n",
            "Iteration 21, loss = 0.59767579\n",
            "Iteration 22, loss = 0.58510548\n",
            "Iteration 23, loss = 0.59250203\n",
            "Iteration 24, loss = 0.59046935\n",
            "Iteration 25, loss = 0.58120010\n",
            "Iteration 26, loss = 0.58737955\n",
            "Iteration 27, loss = 0.58396322\n",
            "Iteration 28, loss = 0.58949445\n",
            "Iteration 29, loss = 0.57218927\n",
            "Iteration 30, loss = 0.58035538\n",
            "Iteration 31, loss = 0.57664473\n",
            "Iteration 32, loss = 0.57331331\n",
            "Iteration 33, loss = 0.57577832\n",
            "Iteration 34, loss = 0.57401272\n",
            "Iteration 35, loss = 0.58505561\n",
            "Iteration 36, loss = 0.54845731\n",
            "Iteration 37, loss = 0.58590948\n",
            "Iteration 38, loss = 0.57177022\n",
            "Iteration 39, loss = 0.56455334\n",
            "Iteration 40, loss = 0.56835452\n",
            "Iteration 41, loss = 0.56120752\n",
            "Iteration 42, loss = 0.55063945\n",
            "Iteration 43, loss = 0.56874800\n",
            "Iteration 44, loss = 0.56493968\n",
            "Iteration 45, loss = 0.54784743\n",
            "Iteration 46, loss = 0.55147432\n",
            "Iteration 47, loss = 0.55829474\n",
            "Iteration 48, loss = 0.55236943\n",
            "Iteration 49, loss = 0.54810803\n",
            "Iteration 50, loss = 0.54462042\n",
            "Iteration 51, loss = 0.55508922\n",
            "Iteration 52, loss = 0.54013254\n",
            "Iteration 53, loss = 0.54661750\n",
            "Iteration 54, loss = 0.54118711\n",
            "Iteration 55, loss = 0.53789605\n",
            "Iteration 56, loss = 0.52731534\n",
            "Iteration 57, loss = 0.53121514\n",
            "Iteration 58, loss = 0.52773414\n",
            "Iteration 59, loss = 0.53030983\n",
            "Iteration 60, loss = 0.52721562\n",
            "Iteration 61, loss = 0.53241673\n",
            "Iteration 62, loss = 0.53164381\n",
            "Iteration 63, loss = 0.52521809\n",
            "Iteration 64, loss = 0.51833508\n",
            "Iteration 65, loss = 0.52913238\n",
            "Iteration 66, loss = 0.51322709\n",
            "Iteration 67, loss = 0.52527469\n",
            "Iteration 68, loss = 0.54160477\n",
            "Iteration 69, loss = 0.52609905\n",
            "Iteration 70, loss = 0.53665679\n",
            "Iteration 71, loss = 0.53512907\n",
            "Iteration 72, loss = 0.52327993\n",
            "Iteration 73, loss = 0.50232228\n",
            "Iteration 74, loss = 0.51573925\n",
            "Iteration 75, loss = 0.51092602\n",
            "Iteration 76, loss = 0.53528711\n",
            "Iteration 77, loss = 0.50109393\n",
            "Iteration 78, loss = 0.50420242\n",
            "Iteration 79, loss = 0.50051254\n",
            "Iteration 80, loss = 0.50929109\n",
            "Iteration 81, loss = 0.50049355\n",
            "Iteration 82, loss = 0.49814662\n",
            "Iteration 83, loss = 0.51083477\n",
            "Iteration 84, loss = 0.51737041\n",
            "Iteration 85, loss = 0.49566137\n",
            "Iteration 86, loss = 0.51361915\n",
            "Iteration 87, loss = 0.51960091\n",
            "Iteration 88, loss = 0.48960761\n",
            "Iteration 89, loss = 0.49770103\n",
            "Iteration 90, loss = 0.48000105\n",
            "Iteration 91, loss = 0.49569012\n",
            "Iteration 92, loss = 0.51560958\n",
            "Iteration 93, loss = 0.50576552\n",
            "Iteration 94, loss = 0.52200183\n",
            "Iteration 95, loss = 0.49022868\n",
            "Iteration 96, loss = 0.49925241\n",
            "Iteration 97, loss = 0.48797354\n",
            "Iteration 98, loss = 0.50236357\n",
            "Iteration 99, loss = 0.49695102\n",
            "Iteration 100, loss = 0.49898599\n",
            "Iteration 101, loss = 0.49827964\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.86353151\n",
            "Iteration 2, loss = 0.69506346\n",
            "Iteration 3, loss = 0.66981767\n",
            "Iteration 4, loss = 0.65672187\n",
            "Iteration 5, loss = 0.65002640\n",
            "Iteration 6, loss = 0.62880855\n",
            "Iteration 7, loss = 0.60910979\n",
            "Iteration 8, loss = 0.61337751\n",
            "Iteration 9, loss = 0.61824415\n",
            "Iteration 10, loss = 0.62285106\n",
            "Iteration 11, loss = 0.60070760\n",
            "Iteration 12, loss = 0.59549442\n",
            "Iteration 13, loss = 0.60015713\n",
            "Iteration 14, loss = 0.59315875\n",
            "Iteration 15, loss = 0.58880682\n",
            "Iteration 16, loss = 0.59560088\n",
            "Iteration 17, loss = 0.59318380\n",
            "Iteration 18, loss = 0.59821277\n",
            "Iteration 19, loss = 0.59355139\n",
            "Iteration 20, loss = 0.59757110\n",
            "Iteration 21, loss = 0.59583677\n",
            "Iteration 22, loss = 0.58788867\n",
            "Iteration 23, loss = 0.57437289\n",
            "Iteration 24, loss = 0.57476686\n",
            "Iteration 25, loss = 0.59156933\n",
            "Iteration 26, loss = 0.57657921\n",
            "Iteration 27, loss = 0.57987176\n",
            "Iteration 28, loss = 0.57103398\n",
            "Iteration 29, loss = 0.58010045\n",
            "Iteration 30, loss = 0.56967832\n",
            "Iteration 31, loss = 0.56683777\n",
            "Iteration 32, loss = 0.57233990\n",
            "Iteration 33, loss = 0.57316172\n",
            "Iteration 34, loss = 0.56549597\n",
            "Iteration 35, loss = 0.56099977\n",
            "Iteration 36, loss = 0.55282820\n",
            "Iteration 37, loss = 0.55466547\n",
            "Iteration 38, loss = 0.57804711\n",
            "Iteration 39, loss = 0.55744993\n",
            "Iteration 40, loss = 0.54902896\n",
            "Iteration 41, loss = 0.56065902\n",
            "Iteration 42, loss = 0.56148564\n",
            "Iteration 43, loss = 0.54635569\n",
            "Iteration 44, loss = 0.54554999\n",
            "Iteration 45, loss = 0.55120941\n",
            "Iteration 46, loss = 0.54630927\n",
            "Iteration 47, loss = 0.54610504\n",
            "Iteration 48, loss = 0.55348730\n",
            "Iteration 49, loss = 0.55675381\n",
            "Iteration 50, loss = 0.54354336\n",
            "Iteration 51, loss = 0.53955922\n",
            "Iteration 52, loss = 0.54381477\n",
            "Iteration 53, loss = 0.53055530\n",
            "Iteration 54, loss = 0.55008654\n",
            "Iteration 55, loss = 0.53692307\n",
            "Iteration 56, loss = 0.53107777\n",
            "Iteration 57, loss = 0.52440312\n",
            "Iteration 58, loss = 0.54757553\n",
            "Iteration 59, loss = 0.54496502\n",
            "Iteration 60, loss = 0.53571156\n",
            "Iteration 61, loss = 0.53782464\n",
            "Iteration 62, loss = 0.52178018\n",
            "Iteration 63, loss = 0.53267534\n",
            "Iteration 64, loss = 0.53903183\n",
            "Iteration 65, loss = 0.52452710\n",
            "Iteration 66, loss = 0.55102819\n",
            "Iteration 67, loss = 0.52593606\n",
            "Iteration 68, loss = 0.53784963\n",
            "Iteration 69, loss = 0.52664482\n",
            "Iteration 70, loss = 0.52462361\n",
            "Iteration 71, loss = 0.51532468\n",
            "Iteration 72, loss = 0.51577179\n",
            "Iteration 73, loss = 0.51747450\n",
            "Iteration 74, loss = 0.51832163\n",
            "Iteration 75, loss = 0.51573328\n",
            "Iteration 76, loss = 0.51487073\n",
            "Iteration 77, loss = 0.54431225\n",
            "Iteration 78, loss = 0.51946408\n",
            "Iteration 79, loss = 0.51759104\n",
            "Iteration 80, loss = 0.50375513\n",
            "Iteration 81, loss = 0.50652701\n",
            "Iteration 82, loss = 0.51864137\n",
            "Iteration 83, loss = 0.51132825\n",
            "Iteration 84, loss = 0.52764108\n",
            "Iteration 85, loss = 0.52709503\n",
            "Iteration 86, loss = 0.51102156\n",
            "Iteration 87, loss = 0.49928900\n",
            "Iteration 88, loss = 0.51554442\n",
            "Iteration 89, loss = 0.53312582\n",
            "Iteration 90, loss = 0.50113177\n",
            "Iteration 91, loss = 0.50904594\n",
            "Iteration 92, loss = 0.50557980\n",
            "Iteration 93, loss = 0.53404267\n",
            "Iteration 94, loss = 0.52696912\n",
            "Iteration 95, loss = 0.48414302\n",
            "Iteration 96, loss = 0.49571974\n",
            "Iteration 97, loss = 0.52213703\n",
            "Iteration 98, loss = 0.49998333\n",
            "Iteration 99, loss = 0.51490361\n",
            "Iteration 100, loss = 0.50093350\n",
            "Iteration 101, loss = 0.49866507\n",
            "Iteration 102, loss = 0.50122935\n",
            "Iteration 103, loss = 0.50087003\n",
            "Iteration 104, loss = 0.49457120\n",
            "Iteration 105, loss = 0.49881685\n",
            "Iteration 106, loss = 0.48827424\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.95840255\n",
            "Iteration 2, loss = 0.72555416\n",
            "Iteration 3, loss = 0.61915048\n",
            "Iteration 4, loss = 0.60328860\n",
            "Iteration 5, loss = 0.60291779\n",
            "Iteration 6, loss = 0.58861640\n",
            "Iteration 7, loss = 0.58606406\n",
            "Iteration 8, loss = 0.59115947\n",
            "Iteration 9, loss = 0.59084811\n",
            "Iteration 10, loss = 0.58263401\n",
            "Iteration 11, loss = 0.57634024\n",
            "Iteration 12, loss = 0.57069163\n",
            "Iteration 13, loss = 0.57314493\n",
            "Iteration 14, loss = 0.57109102\n",
            "Iteration 15, loss = 0.56904516\n",
            "Iteration 16, loss = 0.57164199\n",
            "Iteration 17, loss = 0.57323450\n",
            "Iteration 18, loss = 0.56282710\n",
            "Iteration 19, loss = 0.55764875\n",
            "Iteration 20, loss = 0.55886222\n",
            "Iteration 21, loss = 0.55819272\n",
            "Iteration 22, loss = 0.55831821\n",
            "Iteration 23, loss = 0.55346071\n",
            "Iteration 24, loss = 0.55523976\n",
            "Iteration 25, loss = 0.56263485\n",
            "Iteration 26, loss = 0.55312364\n",
            "Iteration 27, loss = 0.55409298\n",
            "Iteration 28, loss = 0.58784479\n",
            "Iteration 29, loss = 0.54526883\n",
            "Iteration 30, loss = 0.54293003\n",
            "Iteration 31, loss = 0.55522492\n",
            "Iteration 32, loss = 0.54226593\n",
            "Iteration 33, loss = 0.54897175\n",
            "Iteration 34, loss = 0.53734782\n",
            "Iteration 35, loss = 0.54448566\n",
            "Iteration 36, loss = 0.54368927\n",
            "Iteration 37, loss = 0.53500188\n",
            "Iteration 38, loss = 0.53304497\n",
            "Iteration 39, loss = 0.53949513\n",
            "Iteration 40, loss = 0.52943836\n",
            "Iteration 41, loss = 0.53599317\n",
            "Iteration 42, loss = 0.54250681\n",
            "Iteration 43, loss = 0.53915327\n",
            "Iteration 44, loss = 0.52721307\n",
            "Iteration 45, loss = 0.53177864\n",
            "Iteration 46, loss = 0.52774410\n",
            "Iteration 47, loss = 0.55256588\n",
            "Iteration 48, loss = 0.54557945\n",
            "Iteration 49, loss = 0.52389752\n",
            "Iteration 50, loss = 0.53152159\n",
            "Iteration 51, loss = 0.51937847\n",
            "Iteration 52, loss = 0.52374863\n",
            "Iteration 53, loss = 0.53675417\n",
            "Iteration 54, loss = 0.53181389\n",
            "Iteration 55, loss = 0.52021905\n",
            "Iteration 56, loss = 0.55153776\n",
            "Iteration 57, loss = 0.51847847\n",
            "Iteration 58, loss = 0.51823683\n",
            "Iteration 59, loss = 0.51888256\n",
            "Iteration 60, loss = 0.52365600\n",
            "Iteration 61, loss = 0.53269467\n",
            "Iteration 62, loss = 0.52091563\n",
            "Iteration 63, loss = 0.55344977\n",
            "Iteration 64, loss = 0.51471576\n",
            "Iteration 65, loss = 0.53240391\n",
            "Iteration 66, loss = 0.51655243\n",
            "Iteration 67, loss = 0.51322641\n",
            "Iteration 68, loss = 0.52462596\n",
            "Iteration 69, loss = 0.53587369\n",
            "Iteration 70, loss = 0.52006810\n",
            "Iteration 71, loss = 0.51736369\n",
            "Iteration 72, loss = 0.52146099\n",
            "Iteration 73, loss = 0.52634282\n",
            "Iteration 74, loss = 0.51232267\n",
            "Iteration 75, loss = 0.52705379\n",
            "Iteration 76, loss = 0.51629131\n",
            "Iteration 77, loss = 0.51016431\n",
            "Iteration 78, loss = 0.50157922\n",
            "Iteration 79, loss = 0.50761136\n",
            "Iteration 80, loss = 0.51709557\n",
            "Iteration 81, loss = 0.52322380\n",
            "Iteration 82, loss = 0.56414432\n",
            "Iteration 83, loss = 0.52968274\n",
            "Iteration 84, loss = 0.52114773\n",
            "Iteration 85, loss = 0.51174021\n",
            "Iteration 86, loss = 0.51806609\n",
            "Iteration 87, loss = 0.49977539\n",
            "Iteration 88, loss = 0.50387288\n",
            "Iteration 89, loss = 0.51797605\n",
            "Iteration 90, loss = 0.49652188\n",
            "Iteration 91, loss = 0.48984276\n",
            "Iteration 92, loss = 0.50509779\n",
            "Iteration 93, loss = 0.51001104\n",
            "Iteration 94, loss = 0.50444669\n",
            "Iteration 95, loss = 0.49552861\n",
            "Iteration 96, loss = 0.48332212\n",
            "Iteration 97, loss = 0.49841319\n",
            "Iteration 98, loss = 0.49462879\n",
            "Iteration 99, loss = 0.50124071\n",
            "Iteration 100, loss = 0.49613541\n",
            "Iteration 101, loss = 0.49481266\n",
            "Iteration 102, loss = 0.49186993\n",
            "Iteration 103, loss = 0.49267555\n",
            "Iteration 104, loss = 0.49238584\n",
            "Iteration 105, loss = 0.49741182\n",
            "Iteration 106, loss = 0.48499992\n",
            "Iteration 107, loss = 0.49641408\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 2.62980764\n",
            "Iteration 2, loss = 0.92727657\n",
            "Iteration 3, loss = 0.72628183\n",
            "Iteration 4, loss = 0.64193449\n",
            "Iteration 5, loss = 0.62611184\n",
            "Iteration 6, loss = 0.62234726\n",
            "Iteration 7, loss = 0.61931702\n",
            "Iteration 8, loss = 0.62435055\n",
            "Iteration 9, loss = 0.61871070\n",
            "Iteration 10, loss = 0.60651273\n",
            "Iteration 11, loss = 0.61773703\n",
            "Iteration 12, loss = 0.60679681\n",
            "Iteration 13, loss = 0.59711931\n",
            "Iteration 14, loss = 0.60184931\n",
            "Iteration 15, loss = 0.59564482\n",
            "Iteration 16, loss = 0.59164186\n",
            "Iteration 17, loss = 0.59096222\n",
            "Iteration 18, loss = 0.59251306\n",
            "Iteration 19, loss = 0.59191001\n",
            "Iteration 20, loss = 0.59584963\n",
            "Iteration 21, loss = 0.58769741\n",
            "Iteration 22, loss = 0.59027138\n",
            "Iteration 23, loss = 0.58708023\n",
            "Iteration 24, loss = 0.58339286\n",
            "Iteration 25, loss = 0.59014268\n",
            "Iteration 26, loss = 0.58819293\n",
            "Iteration 27, loss = 0.58612462\n",
            "Iteration 28, loss = 0.59176659\n",
            "Iteration 29, loss = 0.59333187\n",
            "Iteration 30, loss = 0.58328189\n",
            "Iteration 31, loss = 0.58716018\n",
            "Iteration 32, loss = 0.58351082\n",
            "Iteration 33, loss = 0.59953763\n",
            "Iteration 34, loss = 0.57958606\n",
            "Iteration 35, loss = 0.56783017\n",
            "Iteration 36, loss = 0.59044798\n",
            "Iteration 37, loss = 0.58026876\n",
            "Iteration 38, loss = 0.59014731\n",
            "Iteration 39, loss = 0.57066747\n",
            "Iteration 40, loss = 0.58602733\n",
            "Iteration 41, loss = 0.57143853\n",
            "Iteration 42, loss = 0.58439145\n",
            "Iteration 43, loss = 0.57806670\n",
            "Iteration 44, loss = 0.56888667\n",
            "Iteration 45, loss = 0.57079026\n",
            "Iteration 46, loss = 0.58675379\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.10254457\n",
            "Iteration 2, loss = 0.98346658\n",
            "Iteration 3, loss = 0.64991039\n",
            "Iteration 4, loss = 0.62500073\n",
            "Iteration 5, loss = 0.62752550\n",
            "Iteration 6, loss = 0.60541378\n",
            "Iteration 7, loss = 0.60824780\n",
            "Iteration 8, loss = 0.61070680\n",
            "Iteration 9, loss = 0.59239927\n",
            "Iteration 10, loss = 0.60381687\n",
            "Iteration 11, loss = 0.59850537\n",
            "Iteration 12, loss = 0.59439808\n",
            "Iteration 13, loss = 0.59445306\n",
            "Iteration 14, loss = 0.59544987\n",
            "Iteration 15, loss = 0.59066926\n",
            "Iteration 16, loss = 0.59434385\n",
            "Iteration 17, loss = 0.60508140\n",
            "Iteration 18, loss = 0.58996174\n",
            "Iteration 19, loss = 0.58714922\n",
            "Iteration 20, loss = 0.59315393\n",
            "Iteration 21, loss = 0.58221825\n",
            "Iteration 22, loss = 0.58309585\n",
            "Iteration 23, loss = 0.59455850\n",
            "Iteration 24, loss = 0.57818118\n",
            "Iteration 25, loss = 0.58759729\n",
            "Iteration 26, loss = 0.58025197\n",
            "Iteration 27, loss = 0.58016676\n",
            "Iteration 28, loss = 0.58972738\n",
            "Iteration 29, loss = 0.59915070\n",
            "Iteration 30, loss = 0.58739760\n",
            "Iteration 31, loss = 0.57200726\n",
            "Iteration 32, loss = 0.57587125\n",
            "Iteration 33, loss = 0.56798823\n",
            "Iteration 34, loss = 0.56987422\n",
            "Iteration 35, loss = 0.57418344\n",
            "Iteration 36, loss = 0.56595465\n",
            "Iteration 37, loss = 0.57010794\n",
            "Iteration 38, loss = 0.56057035\n",
            "Iteration 39, loss = 0.56696785\n",
            "Iteration 40, loss = 0.56173902\n",
            "Iteration 41, loss = 0.56660611\n",
            "Iteration 42, loss = 0.55364832\n",
            "Iteration 43, loss = 0.55773878\n",
            "Iteration 44, loss = 0.55193640\n",
            "Iteration 45, loss = 0.56224975\n",
            "Iteration 46, loss = 0.56104947\n",
            "Iteration 47, loss = 0.55910225\n",
            "Iteration 48, loss = 0.55952981\n",
            "Iteration 49, loss = 0.55317480\n",
            "Iteration 50, loss = 0.55655959\n",
            "Iteration 51, loss = 0.54396390\n",
            "Iteration 52, loss = 0.54860911\n",
            "Iteration 53, loss = 0.55297108\n",
            "Iteration 54, loss = 0.54958202\n",
            "Iteration 55, loss = 0.55651110\n",
            "Iteration 56, loss = 0.54807597\n",
            "Iteration 57, loss = 0.54664141\n",
            "Iteration 58, loss = 0.54982459\n",
            "Iteration 59, loss = 0.55461699\n",
            "Iteration 60, loss = 0.54662193\n",
            "Iteration 61, loss = 0.55635104\n",
            "Iteration 62, loss = 0.55305287\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.95165676\n",
            "Iteration 2, loss = 0.81416219\n",
            "Iteration 3, loss = 0.63732263\n",
            "Iteration 4, loss = 0.66018684\n",
            "Iteration 5, loss = 0.64691086\n",
            "Iteration 6, loss = 0.64526031\n",
            "Iteration 7, loss = 0.62789197\n",
            "Iteration 8, loss = 0.61580833\n",
            "Iteration 9, loss = 0.62984367\n",
            "Iteration 10, loss = 0.62265536\n",
            "Iteration 11, loss = 0.61640344\n",
            "Iteration 12, loss = 0.61279620\n",
            "Iteration 13, loss = 0.60516498\n",
            "Iteration 14, loss = 0.62073274\n",
            "Iteration 15, loss = 0.62126566\n",
            "Iteration 16, loss = 0.60167204\n",
            "Iteration 17, loss = 0.60390560\n",
            "Iteration 18, loss = 0.66071163\n",
            "Iteration 19, loss = 0.61210372\n",
            "Iteration 20, loss = 0.60482144\n",
            "Iteration 21, loss = 0.61034282\n",
            "Iteration 22, loss = 0.60404093\n",
            "Iteration 23, loss = 0.60411170\n",
            "Iteration 24, loss = 0.59806918\n",
            "Iteration 25, loss = 0.60424756\n",
            "Iteration 26, loss = 0.62211000\n",
            "Iteration 27, loss = 0.60000671\n",
            "Iteration 28, loss = 0.60378186\n",
            "Iteration 29, loss = 0.59805702\n",
            "Iteration 30, loss = 0.61401631\n",
            "Iteration 31, loss = 0.62457891\n",
            "Iteration 32, loss = 0.61791761\n",
            "Iteration 33, loss = 0.59622950\n",
            "Iteration 34, loss = 0.60950072\n",
            "Iteration 35, loss = 0.60339181\n",
            "Iteration 36, loss = 0.59512211\n",
            "Iteration 37, loss = 0.59154079\n",
            "Iteration 38, loss = 0.58712110\n",
            "Iteration 39, loss = 0.58857349\n",
            "Iteration 40, loss = 0.58896289\n",
            "Iteration 41, loss = 0.59373049\n",
            "Iteration 42, loss = 0.60065134\n",
            "Iteration 43, loss = 0.58665214\n",
            "Iteration 44, loss = 0.58566244\n",
            "Iteration 45, loss = 0.58585135\n",
            "Iteration 46, loss = 0.59543486\n",
            "Iteration 47, loss = 0.58889989\n",
            "Iteration 48, loss = 0.58478390\n",
            "Iteration 49, loss = 0.59631534\n",
            "Iteration 50, loss = 0.60325949\n",
            "Iteration 51, loss = 0.58091160\n",
            "Iteration 52, loss = 0.59067359\n",
            "Iteration 53, loss = 0.58260476\n",
            "Iteration 54, loss = 0.58256064\n",
            "Iteration 55, loss = 0.58464155\n",
            "Iteration 56, loss = 0.58074435\n",
            "Iteration 57, loss = 0.58389503\n",
            "Iteration 58, loss = 0.60646551\n",
            "Iteration 59, loss = 0.58229561\n",
            "Iteration 60, loss = 0.58201205\n",
            "Iteration 61, loss = 0.57277848\n",
            "Iteration 62, loss = 0.57772816\n",
            "Iteration 63, loss = 0.58112038\n",
            "Iteration 64, loss = 0.57302811\n",
            "Iteration 65, loss = 0.57255880\n",
            "Iteration 66, loss = 0.58149442\n",
            "Iteration 67, loss = 0.58157699\n",
            "Iteration 68, loss = 0.57282741\n",
            "Iteration 69, loss = 0.57129592\n",
            "Iteration 70, loss = 0.56820142\n",
            "Iteration 71, loss = 0.58144100\n",
            "Iteration 72, loss = 0.63389098\n",
            "Iteration 73, loss = 0.58712089\n",
            "Iteration 74, loss = 0.57433880\n",
            "Iteration 75, loss = 0.56680710\n",
            "Iteration 76, loss = 0.57539082\n",
            "Iteration 77, loss = 0.56496637\n",
            "Iteration 78, loss = 0.57167469\n",
            "Iteration 79, loss = 0.56557094\n",
            "Iteration 80, loss = 0.56843530\n",
            "Iteration 81, loss = 0.56675393\n",
            "Iteration 82, loss = 0.56493697\n",
            "Iteration 83, loss = 0.56413505\n",
            "Iteration 84, loss = 0.56089749\n",
            "Iteration 85, loss = 0.56400656\n",
            "Iteration 86, loss = 0.57393655\n",
            "Iteration 87, loss = 0.55393688\n",
            "Iteration 88, loss = 0.55776491\n",
            "Iteration 89, loss = 0.55953354\n",
            "Iteration 90, loss = 0.56293269\n",
            "Iteration 91, loss = 0.56217543\n",
            "Iteration 92, loss = 0.55545837\n",
            "Iteration 93, loss = 0.55786019\n",
            "Iteration 94, loss = 0.55167666\n",
            "Iteration 95, loss = 0.55667088\n",
            "Iteration 96, loss = 0.55469682\n",
            "Iteration 97, loss = 0.56062268\n",
            "Iteration 98, loss = 0.55904639\n",
            "Iteration 99, loss = 0.54942585\n",
            "Iteration 100, loss = 0.55534034\n",
            "Iteration 101, loss = 0.54450320\n",
            "Iteration 102, loss = 0.54267770\n",
            "Iteration 103, loss = 0.54789966\n",
            "Iteration 104, loss = 0.55294292\n",
            "Iteration 105, loss = 0.55818893\n",
            "Iteration 106, loss = 0.54032272\n",
            "Iteration 107, loss = 0.54749881\n",
            "Iteration 108, loss = 0.54407039\n",
            "Iteration 109, loss = 0.54185686\n",
            "Iteration 110, loss = 0.54737417\n",
            "Iteration 111, loss = 0.53489146\n",
            "Iteration 112, loss = 0.55747708\n",
            "Iteration 113, loss = 0.54298959\n",
            "Iteration 114, loss = 0.53767097\n",
            "Iteration 115, loss = 0.53512907\n",
            "Iteration 116, loss = 0.54456125\n",
            "Iteration 117, loss = 0.53848766\n",
            "Iteration 118, loss = 0.53469525\n",
            "Iteration 119, loss = 0.54378269\n",
            "Iteration 120, loss = 0.53073728\n",
            "Iteration 121, loss = 0.52636012\n",
            "Iteration 122, loss = 0.52638478\n",
            "Iteration 123, loss = 0.53010040\n",
            "Iteration 124, loss = 0.52315141\n",
            "Iteration 125, loss = 0.54082839\n",
            "Iteration 126, loss = 0.53341474\n",
            "Iteration 127, loss = 0.53149273\n",
            "Iteration 128, loss = 0.53353256\n",
            "Iteration 129, loss = 0.52641895\n",
            "Iteration 130, loss = 0.51734858\n",
            "Iteration 131, loss = 0.51816060\n",
            "Iteration 132, loss = 0.52785934\n",
            "Iteration 133, loss = 0.54120187\n",
            "Iteration 134, loss = 0.53582535\n",
            "Iteration 135, loss = 0.51640015\n",
            "Iteration 136, loss = 0.51067495\n",
            "Iteration 137, loss = 0.52362116\n",
            "Iteration 138, loss = 0.51622591\n",
            "Iteration 139, loss = 0.52124715\n",
            "Iteration 140, loss = 0.50724147\n",
            "Iteration 141, loss = 0.51312544\n",
            "Iteration 142, loss = 0.53782506\n",
            "Iteration 143, loss = 0.51811235\n",
            "Iteration 144, loss = 0.50634196\n",
            "Iteration 145, loss = 0.53214181\n",
            "Iteration 146, loss = 0.50312476\n",
            "Iteration 147, loss = 0.50155440\n",
            "Iteration 148, loss = 0.51554804\n",
            "Iteration 149, loss = 0.53084358\n",
            "Iteration 150, loss = 0.50816567\n",
            "Iteration 151, loss = 0.50864077\n",
            "Iteration 152, loss = 0.50981344\n",
            "Iteration 153, loss = 0.51070248\n",
            "Iteration 154, loss = 0.50183641\n",
            "Iteration 155, loss = 0.49563797\n",
            "Iteration 156, loss = 0.49131962\n",
            "Iteration 157, loss = 0.49961831\n",
            "Iteration 158, loss = 0.50447628\n",
            "Iteration 159, loss = 0.52374168\n",
            "Iteration 160, loss = 0.50019461\n",
            "Iteration 161, loss = 0.50054965\n",
            "Iteration 162, loss = 0.49013107\n",
            "Iteration 163, loss = 0.51066989\n",
            "Iteration 164, loss = 0.49121454\n",
            "Iteration 165, loss = 0.49878469\n",
            "Iteration 166, loss = 0.48755818\n",
            "Iteration 167, loss = 0.49010388\n",
            "Iteration 168, loss = 0.50501190\n",
            "Iteration 169, loss = 0.48262585\n",
            "Iteration 170, loss = 0.50511778\n",
            "Iteration 171, loss = 0.49249559\n",
            "Iteration 172, loss = 0.49286946\n",
            "Iteration 173, loss = 0.50800987\n",
            "Iteration 174, loss = 0.48149047\n",
            "Iteration 175, loss = 0.50119361\n",
            "Iteration 176, loss = 0.49559475\n",
            "Iteration 177, loss = 0.49476108\n",
            "Iteration 178, loss = 0.48324863\n",
            "Iteration 179, loss = 0.47826372\n",
            "Iteration 180, loss = 0.48189276\n",
            "Iteration 181, loss = 0.49734630\n",
            "Iteration 182, loss = 0.50290551\n",
            "Iteration 183, loss = 0.47957867\n",
            "Iteration 184, loss = 0.50232726\n",
            "Iteration 185, loss = 0.48414199\n",
            "Iteration 186, loss = 0.48875525\n",
            "Iteration 187, loss = 0.47398168\n",
            "Iteration 188, loss = 0.48221059\n",
            "Iteration 189, loss = 0.48964773\n",
            "Iteration 190, loss = 0.47742443\n",
            "Iteration 191, loss = 0.46956347\n",
            "Iteration 192, loss = 0.48673131\n",
            "Iteration 193, loss = 0.48825071\n",
            "Iteration 194, loss = 0.50421723\n",
            "Iteration 195, loss = 0.46652328\n",
            "Iteration 196, loss = 0.47207077\n",
            "Iteration 197, loss = 0.47345028\n",
            "Iteration 198, loss = 0.48402190\n",
            "Iteration 199, loss = 0.48232536\n",
            "Iteration 200, loss = 0.47219883\n",
            "Iteration 1, loss = 1.79909057\n",
            "Iteration 2, loss = 0.82597801\n",
            "Iteration 3, loss = 0.64100001\n",
            "Iteration 4, loss = 0.61172670\n",
            "Iteration 5, loss = 0.61511848\n",
            "Iteration 6, loss = 0.60808092\n",
            "Iteration 7, loss = 0.62150173\n",
            "Iteration 8, loss = 0.60603416\n",
            "Iteration 9, loss = 0.60189058\n",
            "Iteration 10, loss = 0.59903230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 11, loss = 0.59886115\n",
            "Iteration 12, loss = 0.60083662\n",
            "Iteration 13, loss = 0.60099060\n",
            "Iteration 14, loss = 0.60081263\n",
            "Iteration 15, loss = 0.59640893\n",
            "Iteration 16, loss = 0.59488522\n",
            "Iteration 17, loss = 0.60573191\n",
            "Iteration 18, loss = 0.59432272\n",
            "Iteration 19, loss = 0.59707163\n",
            "Iteration 20, loss = 0.59059773\n",
            "Iteration 21, loss = 0.59388931\n",
            "Iteration 22, loss = 0.59262559\n",
            "Iteration 23, loss = 0.59475025\n",
            "Iteration 24, loss = 0.63231566\n",
            "Iteration 25, loss = 0.59941140\n",
            "Iteration 26, loss = 0.59879843\n",
            "Iteration 27, loss = 0.59019983\n",
            "Iteration 28, loss = 0.58994991\n",
            "Iteration 29, loss = 0.58853932\n",
            "Iteration 30, loss = 0.59693476\n",
            "Iteration 31, loss = 0.59100869\n",
            "Iteration 32, loss = 0.58948798\n",
            "Iteration 33, loss = 0.58327063\n",
            "Iteration 34, loss = 0.58713604\n",
            "Iteration 35, loss = 0.59244343\n",
            "Iteration 36, loss = 0.58678055\n",
            "Iteration 37, loss = 0.58416661\n",
            "Iteration 38, loss = 0.58965640\n",
            "Iteration 39, loss = 0.58241948\n",
            "Iteration 40, loss = 0.58188549\n",
            "Iteration 41, loss = 0.58336959\n",
            "Iteration 42, loss = 0.58569687\n",
            "Iteration 43, loss = 0.57926486\n",
            "Iteration 44, loss = 0.57774500\n",
            "Iteration 45, loss = 0.58648081\n",
            "Iteration 46, loss = 0.58059919\n",
            "Iteration 47, loss = 0.58409005\n",
            "Iteration 48, loss = 0.59600488\n",
            "Iteration 49, loss = 0.58032397\n",
            "Iteration 50, loss = 0.57981332\n",
            "Iteration 51, loss = 0.57947065\n",
            "Iteration 52, loss = 0.58320330\n",
            "Iteration 53, loss = 0.57692904\n",
            "Iteration 54, loss = 0.58131224\n",
            "Iteration 55, loss = 0.57366946\n",
            "Iteration 56, loss = 0.57419238\n",
            "Iteration 57, loss = 0.57304321\n",
            "Iteration 58, loss = 0.57963574\n",
            "Iteration 59, loss = 0.57347913\n",
            "Iteration 60, loss = 0.57533435\n",
            "Iteration 61, loss = 0.59153453\n",
            "Iteration 62, loss = 0.59377119\n",
            "Iteration 63, loss = 0.57480328\n",
            "Iteration 64, loss = 0.57471478\n",
            "Iteration 65, loss = 0.57388655\n",
            "Iteration 66, loss = 0.56854608\n",
            "Iteration 67, loss = 0.57271921\n",
            "Iteration 68, loss = 0.57861124\n",
            "Iteration 69, loss = 0.56952873\n",
            "Iteration 70, loss = 0.56587702\n",
            "Iteration 71, loss = 0.56591033\n",
            "Iteration 72, loss = 0.57245409\n",
            "Iteration 73, loss = 0.56652916\n",
            "Iteration 74, loss = 0.56862251\n",
            "Iteration 75, loss = 0.56400565\n",
            "Iteration 76, loss = 0.57081701\n",
            "Iteration 77, loss = 0.56207203\n",
            "Iteration 78, loss = 0.56615404\n",
            "Iteration 79, loss = 0.56073229\n",
            "Iteration 80, loss = 0.57961953\n",
            "Iteration 81, loss = 0.57416185\n",
            "Iteration 82, loss = 0.56963046\n",
            "Iteration 83, loss = 0.57084666\n",
            "Iteration 84, loss = 0.56989409\n",
            "Iteration 85, loss = 0.56054826\n",
            "Iteration 86, loss = 0.56175384\n",
            "Iteration 87, loss = 0.56375375\n",
            "Iteration 88, loss = 0.58785749\n",
            "Iteration 89, loss = 0.55883068\n",
            "Iteration 90, loss = 0.57968474\n",
            "Iteration 91, loss = 0.56027016\n",
            "Iteration 92, loss = 0.55607812\n",
            "Iteration 93, loss = 0.55379094\n",
            "Iteration 94, loss = 0.55982893\n",
            "Iteration 95, loss = 0.56604409\n",
            "Iteration 96, loss = 0.57163033\n",
            "Iteration 97, loss = 0.56093608\n",
            "Iteration 98, loss = 0.55013596\n",
            "Iteration 99, loss = 0.55601128\n",
            "Iteration 100, loss = 0.54881806\n",
            "Iteration 101, loss = 0.55414311\n",
            "Iteration 102, loss = 0.55347216\n",
            "Iteration 103, loss = 0.54622946\n",
            "Iteration 104, loss = 0.56543165\n",
            "Iteration 105, loss = 0.55571741\n",
            "Iteration 106, loss = 0.55521582\n",
            "Iteration 107, loss = 0.54920879\n",
            "Iteration 108, loss = 0.54681582\n",
            "Iteration 109, loss = 0.55275258\n",
            "Iteration 110, loss = 0.54919067\n",
            "Iteration 111, loss = 0.55471348\n",
            "Iteration 112, loss = 0.54536582\n",
            "Iteration 113, loss = 0.54306109\n",
            "Iteration 114, loss = 0.54098566\n",
            "Iteration 115, loss = 0.54726543\n",
            "Iteration 116, loss = 0.53918605\n",
            "Iteration 117, loss = 0.54549109\n",
            "Iteration 118, loss = 0.54390638\n",
            "Iteration 119, loss = 0.54340486\n",
            "Iteration 120, loss = 0.53567150\n",
            "Iteration 121, loss = 0.55519700\n",
            "Iteration 122, loss = 0.54181932\n",
            "Iteration 123, loss = 0.53546353\n",
            "Iteration 124, loss = 0.53350713\n",
            "Iteration 125, loss = 0.54543190\n",
            "Iteration 126, loss = 0.53563907\n",
            "Iteration 127, loss = 0.53594047\n",
            "Iteration 128, loss = 0.55178492\n",
            "Iteration 129, loss = 0.53363370\n",
            "Iteration 130, loss = 0.54964226\n",
            "Iteration 131, loss = 0.54267908\n",
            "Iteration 132, loss = 0.54744442\n",
            "Iteration 133, loss = 0.53876364\n",
            "Iteration 134, loss = 0.56014782\n",
            "Iteration 135, loss = 0.53619089\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64924719\n",
            "Iteration 2, loss = 0.62392022\n",
            "Iteration 3, loss = 0.60281213\n",
            "Iteration 4, loss = 0.60234109\n",
            "Iteration 5, loss = 0.58710223\n",
            "Iteration 6, loss = 0.58419596\n",
            "Iteration 7, loss = 0.57751520\n",
            "Iteration 8, loss = 0.57155483\n",
            "Iteration 9, loss = 0.56282498\n",
            "Iteration 10, loss = 0.55143972\n",
            "Iteration 11, loss = 0.53781187\n",
            "Iteration 12, loss = 0.52789436\n",
            "Iteration 13, loss = 0.51002224\n",
            "Iteration 14, loss = 0.50323518\n",
            "Iteration 15, loss = 0.49085256\n",
            "Iteration 16, loss = 0.48594394\n",
            "Iteration 17, loss = 0.48860435\n",
            "Iteration 18, loss = 0.46966908\n",
            "Iteration 19, loss = 0.46275086\n",
            "Iteration 20, loss = 0.45773731\n",
            "Iteration 21, loss = 0.45308880\n",
            "Iteration 22, loss = 0.46013162\n",
            "Iteration 23, loss = 0.44593102\n",
            "Iteration 24, loss = 0.44659316\n",
            "Iteration 25, loss = 0.44808033\n",
            "Iteration 26, loss = 0.44973608\n",
            "Iteration 27, loss = 0.44605346\n",
            "Iteration 28, loss = 0.44058181\n",
            "Iteration 29, loss = 0.44529294\n",
            "Iteration 30, loss = 0.43711377\n",
            "Iteration 31, loss = 0.43258479\n",
            "Iteration 32, loss = 0.43516356\n",
            "Iteration 33, loss = 0.43086192\n",
            "Iteration 34, loss = 0.44123708\n",
            "Iteration 35, loss = 0.43014852\n",
            "Iteration 36, loss = 0.44062208\n",
            "Iteration 37, loss = 0.42536963\n",
            "Iteration 38, loss = 0.43881575\n",
            "Iteration 39, loss = 0.42630704\n",
            "Iteration 40, loss = 0.42994604\n",
            "Iteration 41, loss = 0.42912322\n",
            "Iteration 42, loss = 0.42176164\n",
            "Iteration 43, loss = 0.43036941\n",
            "Iteration 44, loss = 0.43195313\n",
            "Iteration 45, loss = 0.42380198\n",
            "Iteration 46, loss = 0.42559982\n",
            "Iteration 47, loss = 0.42683834\n",
            "Iteration 48, loss = 0.42909592\n",
            "Iteration 49, loss = 0.42258111\n",
            "Iteration 50, loss = 0.42393627\n",
            "Iteration 51, loss = 0.41776771\n",
            "Iteration 52, loss = 0.42395994\n",
            "Iteration 53, loss = 0.44041634\n",
            "Iteration 54, loss = 0.41472994\n",
            "Iteration 55, loss = 0.42532962\n",
            "Iteration 56, loss = 0.42247421\n",
            "Iteration 57, loss = 0.41471364\n",
            "Iteration 58, loss = 0.40742366\n",
            "Iteration 59, loss = 0.40293443\n",
            "Iteration 60, loss = 0.43176298\n",
            "Iteration 61, loss = 0.40953207\n",
            "Iteration 62, loss = 0.40098100\n",
            "Iteration 63, loss = 0.41563683\n",
            "Iteration 64, loss = 0.41575630\n",
            "Iteration 65, loss = 0.42992725\n",
            "Iteration 66, loss = 0.41810666\n",
            "Iteration 67, loss = 0.40586108\n",
            "Iteration 68, loss = 0.40562317\n",
            "Iteration 69, loss = 0.40690059\n",
            "Iteration 70, loss = 0.41474572\n",
            "Iteration 71, loss = 0.40389406\n",
            "Iteration 72, loss = 0.40779855\n",
            "Iteration 73, loss = 0.40471339\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66552311\n",
            "Iteration 2, loss = 0.61827992\n",
            "Iteration 3, loss = 0.60678915\n",
            "Iteration 4, loss = 0.59802818\n",
            "Iteration 5, loss = 0.59125550\n",
            "Iteration 6, loss = 0.58796449\n",
            "Iteration 7, loss = 0.58450363\n",
            "Iteration 8, loss = 0.57403884\n",
            "Iteration 9, loss = 0.57519344\n",
            "Iteration 10, loss = 0.55625518\n",
            "Iteration 11, loss = 0.54573969\n",
            "Iteration 12, loss = 0.53691427\n",
            "Iteration 13, loss = 0.52100072\n",
            "Iteration 14, loss = 0.50850114\n",
            "Iteration 15, loss = 0.49881188\n",
            "Iteration 16, loss = 0.48932388\n",
            "Iteration 17, loss = 0.48288359\n",
            "Iteration 18, loss = 0.47029848\n",
            "Iteration 19, loss = 0.46670633\n",
            "Iteration 20, loss = 0.46033824\n",
            "Iteration 21, loss = 0.44218724\n",
            "Iteration 22, loss = 0.44887365\n",
            "Iteration 23, loss = 0.43741331\n",
            "Iteration 24, loss = 0.44060868\n",
            "Iteration 25, loss = 0.44256768\n",
            "Iteration 26, loss = 0.44556907\n",
            "Iteration 27, loss = 0.43017872\n",
            "Iteration 28, loss = 0.43843768\n",
            "Iteration 29, loss = 0.42695625\n",
            "Iteration 30, loss = 0.42904771\n",
            "Iteration 31, loss = 0.43720537\n",
            "Iteration 32, loss = 0.43295487\n",
            "Iteration 33, loss = 0.41824610\n",
            "Iteration 34, loss = 0.42220947\n",
            "Iteration 35, loss = 0.41748824\n",
            "Iteration 36, loss = 0.41791741\n",
            "Iteration 37, loss = 0.41407400\n",
            "Iteration 38, loss = 0.42047837\n",
            "Iteration 39, loss = 0.41503408\n",
            "Iteration 40, loss = 0.41709789\n",
            "Iteration 41, loss = 0.40913220\n",
            "Iteration 42, loss = 0.42131891\n",
            "Iteration 43, loss = 0.41089626\n",
            "Iteration 44, loss = 0.44291082\n",
            "Iteration 45, loss = 0.41586382\n",
            "Iteration 46, loss = 0.41090655\n",
            "Iteration 47, loss = 0.40703263\n",
            "Iteration 48, loss = 0.41236637\n",
            "Iteration 49, loss = 0.41615762\n",
            "Iteration 50, loss = 0.40493456\n",
            "Iteration 51, loss = 0.40996378\n",
            "Iteration 52, loss = 0.40656276\n",
            "Iteration 53, loss = 0.40425946\n",
            "Iteration 54, loss = 0.41780412\n",
            "Iteration 55, loss = 0.39942041\n",
            "Iteration 56, loss = 0.41415330\n",
            "Iteration 57, loss = 0.41438152\n",
            "Iteration 58, loss = 0.39532293\n",
            "Iteration 59, loss = 0.40380746\n",
            "Iteration 60, loss = 0.40722838\n",
            "Iteration 61, loss = 0.40295565\n",
            "Iteration 62, loss = 0.40626320\n",
            "Iteration 63, loss = 0.39497962\n",
            "Iteration 64, loss = 0.39489739\n",
            "Iteration 65, loss = 0.39554617\n",
            "Iteration 66, loss = 0.39258462\n",
            "Iteration 67, loss = 0.39348205\n",
            "Iteration 68, loss = 0.39762712\n",
            "Iteration 69, loss = 0.39510178\n",
            "Iteration 70, loss = 0.40139114\n",
            "Iteration 71, loss = 0.39824162\n",
            "Iteration 72, loss = 0.38609047\n",
            "Iteration 73, loss = 0.39383621\n",
            "Iteration 74, loss = 0.39538085\n",
            "Iteration 75, loss = 0.39985445\n",
            "Iteration 76, loss = 0.38849413\n",
            "Iteration 77, loss = 0.39060636\n",
            "Iteration 78, loss = 0.39181268\n",
            "Iteration 79, loss = 0.38798035\n",
            "Iteration 80, loss = 0.38576628\n",
            "Iteration 81, loss = 0.38866199\n",
            "Iteration 82, loss = 0.39002321\n",
            "Iteration 83, loss = 0.39100934\n",
            "Iteration 84, loss = 0.39212207\n",
            "Iteration 85, loss = 0.38894852\n",
            "Iteration 86, loss = 0.38431407\n",
            "Iteration 87, loss = 0.39003893\n",
            "Iteration 88, loss = 0.38112361\n",
            "Iteration 89, loss = 0.37904365\n",
            "Iteration 90, loss = 0.38468192\n",
            "Iteration 91, loss = 0.38684361\n",
            "Iteration 92, loss = 0.37854346\n",
            "Iteration 93, loss = 0.38004472\n",
            "Iteration 94, loss = 0.38678514\n",
            "Iteration 95, loss = 0.37428223\n",
            "Iteration 96, loss = 0.38502528\n",
            "Iteration 97, loss = 0.38156392\n",
            "Iteration 98, loss = 0.38409843\n",
            "Iteration 99, loss = 0.39050599\n",
            "Iteration 100, loss = 0.37400931\n",
            "Iteration 101, loss = 0.37517004\n",
            "Iteration 102, loss = 0.37958006\n",
            "Iteration 103, loss = 0.38364483\n",
            "Iteration 104, loss = 0.38109811\n",
            "Iteration 105, loss = 0.37720044\n",
            "Iteration 106, loss = 0.37795917\n",
            "Iteration 107, loss = 0.37352941\n",
            "Iteration 108, loss = 0.37677702\n",
            "Iteration 109, loss = 0.37252526\n",
            "Iteration 110, loss = 0.38009498\n",
            "Iteration 111, loss = 0.37512826\n",
            "Iteration 112, loss = 0.37131651\n",
            "Iteration 113, loss = 0.37314602\n",
            "Iteration 114, loss = 0.37166472\n",
            "Iteration 115, loss = 0.37913458\n",
            "Iteration 116, loss = 0.37000470\n",
            "Iteration 117, loss = 0.37282188\n",
            "Iteration 118, loss = 0.37237543\n",
            "Iteration 119, loss = 0.38778094\n",
            "Iteration 120, loss = 0.37225567\n",
            "Iteration 121, loss = 0.36924605\n",
            "Iteration 122, loss = 0.36677208\n",
            "Iteration 123, loss = 0.37196001\n",
            "Iteration 124, loss = 0.37081868\n",
            "Iteration 125, loss = 0.37334743\n",
            "Iteration 126, loss = 0.37656173\n",
            "Iteration 127, loss = 0.37444932\n",
            "Iteration 128, loss = 0.37405137\n",
            "Iteration 129, loss = 0.37197075\n",
            "Iteration 130, loss = 0.38308833\n",
            "Iteration 131, loss = 0.37562222\n",
            "Iteration 132, loss = 0.36534935\n",
            "Iteration 133, loss = 0.37497905\n",
            "Iteration 134, loss = 0.36123950\n",
            "Iteration 135, loss = 0.37348840\n",
            "Iteration 136, loss = 0.36244858\n",
            "Iteration 137, loss = 0.36411410\n",
            "Iteration 138, loss = 0.36611315\n",
            "Iteration 139, loss = 0.37162921\n",
            "Iteration 140, loss = 0.37320752\n",
            "Iteration 141, loss = 0.37431275\n",
            "Iteration 142, loss = 0.37522990\n",
            "Iteration 143, loss = 0.36133870\n",
            "Iteration 144, loss = 0.36662469\n",
            "Iteration 145, loss = 0.37337610\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.65172226\n",
            "Iteration 2, loss = 0.60838526\n",
            "Iteration 3, loss = 0.59909210\n",
            "Iteration 4, loss = 0.59207717\n",
            "Iteration 5, loss = 0.58135969\n",
            "Iteration 6, loss = 0.58067153\n",
            "Iteration 7, loss = 0.56835690\n",
            "Iteration 8, loss = 0.55539859\n",
            "Iteration 9, loss = 0.54705345\n",
            "Iteration 10, loss = 0.53719679\n",
            "Iteration 11, loss = 0.51386208\n",
            "Iteration 12, loss = 0.49440276\n",
            "Iteration 13, loss = 0.48016467\n",
            "Iteration 14, loss = 0.46930022\n",
            "Iteration 15, loss = 0.47008298\n",
            "Iteration 16, loss = 0.45825604\n",
            "Iteration 17, loss = 0.44864049\n",
            "Iteration 18, loss = 0.45008116\n",
            "Iteration 19, loss = 0.43746568\n",
            "Iteration 20, loss = 0.45335500\n",
            "Iteration 21, loss = 0.43554359\n",
            "Iteration 22, loss = 0.43340413\n",
            "Iteration 23, loss = 0.43475300\n",
            "Iteration 24, loss = 0.42945864\n",
            "Iteration 25, loss = 0.41817426\n",
            "Iteration 26, loss = 0.45036340\n",
            "Iteration 27, loss = 0.42610745\n",
            "Iteration 28, loss = 0.44056672\n",
            "Iteration 29, loss = 0.42380830\n",
            "Iteration 30, loss = 0.41776784\n",
            "Iteration 31, loss = 0.42549037\n",
            "Iteration 32, loss = 0.41665177\n",
            "Iteration 33, loss = 0.42561104\n",
            "Iteration 34, loss = 0.41876596\n",
            "Iteration 35, loss = 0.41966754\n",
            "Iteration 36, loss = 0.41963928\n",
            "Iteration 37, loss = 0.42350937\n",
            "Iteration 38, loss = 0.41783772\n",
            "Iteration 39, loss = 0.41593080\n",
            "Iteration 40, loss = 0.41200512\n",
            "Iteration 41, loss = 0.41174926\n",
            "Iteration 42, loss = 0.42091215\n",
            "Iteration 43, loss = 0.41441624\n",
            "Iteration 44, loss = 0.40408311\n",
            "Iteration 45, loss = 0.40669142\n",
            "Iteration 46, loss = 0.40231750\n",
            "Iteration 47, loss = 0.40325224\n",
            "Iteration 48, loss = 0.41664336\n",
            "Iteration 49, loss = 0.40215744\n",
            "Iteration 50, loss = 0.39866015\n",
            "Iteration 51, loss = 0.40604754\n",
            "Iteration 52, loss = 0.39723444\n",
            "Iteration 53, loss = 0.40381330\n",
            "Iteration 54, loss = 0.39838568\n",
            "Iteration 55, loss = 0.40104568\n",
            "Iteration 56, loss = 0.39999890\n",
            "Iteration 57, loss = 0.39429385\n",
            "Iteration 58, loss = 0.39410152\n",
            "Iteration 59, loss = 0.38958663\n",
            "Iteration 60, loss = 0.39595264\n",
            "Iteration 61, loss = 0.38914609\n",
            "Iteration 62, loss = 0.40128547\n",
            "Iteration 63, loss = 0.38672851\n",
            "Iteration 64, loss = 0.39405422\n",
            "Iteration 65, loss = 0.38584973\n",
            "Iteration 66, loss = 0.38879544\n",
            "Iteration 67, loss = 0.40154696\n",
            "Iteration 68, loss = 0.38920699\n",
            "Iteration 69, loss = 0.38901150\n",
            "Iteration 70, loss = 0.37953604\n",
            "Iteration 71, loss = 0.38644367\n",
            "Iteration 72, loss = 0.38610300\n",
            "Iteration 73, loss = 0.38241986\n",
            "Iteration 74, loss = 0.38551392\n",
            "Iteration 75, loss = 0.38964013\n",
            "Iteration 76, loss = 0.38184541\n",
            "Iteration 77, loss = 0.39074665\n",
            "Iteration 78, loss = 0.39823094\n",
            "Iteration 79, loss = 0.39392106\n",
            "Iteration 80, loss = 0.38042391\n",
            "Iteration 81, loss = 0.38043265\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.65760233\n",
            "Iteration 2, loss = 0.62678421\n",
            "Iteration 3, loss = 0.61479877\n",
            "Iteration 4, loss = 0.61018109\n",
            "Iteration 5, loss = 0.60778445\n",
            "Iteration 6, loss = 0.60108915\n",
            "Iteration 7, loss = 0.59636718\n",
            "Iteration 8, loss = 0.58179100\n",
            "Iteration 9, loss = 0.57229299\n",
            "Iteration 10, loss = 0.56096040\n",
            "Iteration 11, loss = 0.54312093\n",
            "Iteration 12, loss = 0.52941614\n",
            "Iteration 13, loss = 0.50560078\n",
            "Iteration 14, loss = 0.49913072\n",
            "Iteration 15, loss = 0.48685675\n",
            "Iteration 16, loss = 0.47956076\n",
            "Iteration 17, loss = 0.47722039\n",
            "Iteration 18, loss = 0.46489106\n",
            "Iteration 19, loss = 0.46685670\n",
            "Iteration 20, loss = 0.48384991\n",
            "Iteration 21, loss = 0.46768089\n",
            "Iteration 22, loss = 0.45385627\n",
            "Iteration 23, loss = 0.45198189\n",
            "Iteration 24, loss = 0.45358843\n",
            "Iteration 25, loss = 0.44567244\n",
            "Iteration 26, loss = 0.43969037\n",
            "Iteration 27, loss = 0.45347401\n",
            "Iteration 28, loss = 0.44189228\n",
            "Iteration 29, loss = 0.44996513\n",
            "Iteration 30, loss = 0.44653198\n",
            "Iteration 31, loss = 0.43921493\n",
            "Iteration 32, loss = 0.43753030\n",
            "Iteration 33, loss = 0.43179316\n",
            "Iteration 34, loss = 0.44319838\n",
            "Iteration 35, loss = 0.43347988\n",
            "Iteration 36, loss = 0.43196141\n",
            "Iteration 37, loss = 0.44137227\n",
            "Iteration 38, loss = 0.44356524\n",
            "Iteration 39, loss = 0.43099013\n",
            "Iteration 40, loss = 0.42970431\n",
            "Iteration 41, loss = 0.43367164\n",
            "Iteration 42, loss = 0.42729484\n",
            "Iteration 43, loss = 0.42487057\n",
            "Iteration 44, loss = 0.42396722\n",
            "Iteration 45, loss = 0.43878124\n",
            "Iteration 46, loss = 0.42540819\n",
            "Iteration 47, loss = 0.42590952\n",
            "Iteration 48, loss = 0.43285895\n",
            "Iteration 49, loss = 0.43975704\n",
            "Iteration 50, loss = 0.42523423\n",
            "Iteration 51, loss = 0.41819693\n",
            "Iteration 52, loss = 0.41920185\n",
            "Iteration 53, loss = 0.41900632\n",
            "Iteration 54, loss = 0.43378385\n",
            "Iteration 55, loss = 0.43175513\n",
            "Iteration 56, loss = 0.41799048\n",
            "Iteration 57, loss = 0.41383880\n",
            "Iteration 58, loss = 0.41596770\n",
            "Iteration 59, loss = 0.42536877\n",
            "Iteration 60, loss = 0.41482556\n",
            "Iteration 61, loss = 0.40956486\n",
            "Iteration 62, loss = 0.41505778\n",
            "Iteration 63, loss = 0.41285431\n",
            "Iteration 64, loss = 0.42300955\n",
            "Iteration 65, loss = 0.42187112\n",
            "Iteration 66, loss = 0.40667441\n",
            "Iteration 67, loss = 0.41408596\n",
            "Iteration 68, loss = 0.41033842\n",
            "Iteration 69, loss = 0.41143904\n",
            "Iteration 70, loss = 0.41409226\n",
            "Iteration 71, loss = 0.40567477\n",
            "Iteration 72, loss = 0.40382212\n",
            "Iteration 73, loss = 0.41757896\n",
            "Iteration 74, loss = 0.40524092\n",
            "Iteration 75, loss = 0.42230000\n",
            "Iteration 76, loss = 0.40303210\n",
            "Iteration 77, loss = 0.40699730\n",
            "Iteration 78, loss = 0.40282619\n",
            "Iteration 79, loss = 0.41024249\n",
            "Iteration 80, loss = 0.40890120\n",
            "Iteration 81, loss = 0.40428732\n",
            "Iteration 82, loss = 0.42988431\n",
            "Iteration 83, loss = 0.41313999\n",
            "Iteration 84, loss = 0.40508177\n",
            "Iteration 85, loss = 0.41008038\n",
            "Iteration 86, loss = 0.40007399\n",
            "Iteration 87, loss = 0.40222430\n",
            "Iteration 88, loss = 0.41216821\n",
            "Iteration 89, loss = 0.40337958\n",
            "Iteration 90, loss = 0.39743366\n",
            "Iteration 91, loss = 0.40469485\n",
            "Iteration 92, loss = 0.43311102\n",
            "Iteration 93, loss = 0.40837484\n",
            "Iteration 94, loss = 0.39696111\n",
            "Iteration 95, loss = 0.40779943\n",
            "Iteration 96, loss = 0.39847411\n",
            "Iteration 97, loss = 0.39788373\n",
            "Iteration 98, loss = 0.40320074\n",
            "Iteration 99, loss = 0.39807751\n",
            "Iteration 100, loss = 0.39563747\n",
            "Iteration 101, loss = 0.40314166\n",
            "Iteration 102, loss = 0.40007960\n",
            "Iteration 103, loss = 0.41048870\n",
            "Iteration 104, loss = 0.40340512\n",
            "Iteration 105, loss = 0.39336164\n",
            "Iteration 106, loss = 0.39497476\n",
            "Iteration 107, loss = 0.40009138\n",
            "Iteration 108, loss = 0.39477022\n",
            "Iteration 109, loss = 0.39355845\n",
            "Iteration 110, loss = 0.39394243\n",
            "Iteration 111, loss = 0.40432221\n",
            "Iteration 112, loss = 0.41497597\n",
            "Iteration 113, loss = 0.40103168\n",
            "Iteration 114, loss = 0.39792725\n",
            "Iteration 115, loss = 0.39756900\n",
            "Iteration 116, loss = 0.40329905\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.65115340\n",
            "Iteration 2, loss = 0.63805830\n",
            "Iteration 3, loss = 0.61520097\n",
            "Iteration 4, loss = 0.61574432\n",
            "Iteration 5, loss = 0.60796214\n",
            "Iteration 6, loss = 0.60147495\n",
            "Iteration 7, loss = 0.59256055\n",
            "Iteration 8, loss = 0.58139358\n",
            "Iteration 9, loss = 0.57511610\n",
            "Iteration 10, loss = 0.56384859\n",
            "Iteration 11, loss = 0.54752943\n",
            "Iteration 12, loss = 0.54432072\n",
            "Iteration 13, loss = 0.52113798\n",
            "Iteration 14, loss = 0.51716434\n",
            "Iteration 15, loss = 0.49788871\n",
            "Iteration 16, loss = 0.49269942\n",
            "Iteration 17, loss = 0.48941034\n",
            "Iteration 18, loss = 0.49632532\n",
            "Iteration 19, loss = 0.49305144\n",
            "Iteration 20, loss = 0.48784561\n",
            "Iteration 21, loss = 0.47019661\n",
            "Iteration 22, loss = 0.47051366\n",
            "Iteration 23, loss = 0.47566789\n",
            "Iteration 24, loss = 0.46513545\n",
            "Iteration 25, loss = 0.45393357\n",
            "Iteration 26, loss = 0.45829202\n",
            "Iteration 27, loss = 0.45689848\n",
            "Iteration 28, loss = 0.45816210\n",
            "Iteration 29, loss = 0.45527719\n",
            "Iteration 30, loss = 0.45881239\n",
            "Iteration 31, loss = 0.46026329\n",
            "Iteration 32, loss = 0.46102380\n",
            "Iteration 33, loss = 0.44859112\n",
            "Iteration 34, loss = 0.44424900\n",
            "Iteration 35, loss = 0.44752194\n",
            "Iteration 36, loss = 0.45606488\n",
            "Iteration 37, loss = 0.44524980\n",
            "Iteration 38, loss = 0.44021077\n",
            "Iteration 39, loss = 0.43625247\n",
            "Iteration 40, loss = 0.45685795\n",
            "Iteration 41, loss = 0.44290242\n",
            "Iteration 42, loss = 0.44372557\n",
            "Iteration 43, loss = 0.43299694\n",
            "Iteration 44, loss = 0.43608797\n",
            "Iteration 45, loss = 0.44856373\n",
            "Iteration 46, loss = 0.44001776\n",
            "Iteration 47, loss = 0.43193439\n",
            "Iteration 48, loss = 0.43717795\n",
            "Iteration 49, loss = 0.43155669\n",
            "Iteration 50, loss = 0.44304009\n",
            "Iteration 51, loss = 0.43026629\n",
            "Iteration 52, loss = 0.44379367\n",
            "Iteration 53, loss = 0.42489344\n",
            "Iteration 54, loss = 0.42726146\n",
            "Iteration 55, loss = 0.43088270\n",
            "Iteration 56, loss = 0.42919773\n",
            "Iteration 57, loss = 0.42426646\n",
            "Iteration 58, loss = 0.42811574\n",
            "Iteration 59, loss = 0.42231161\n",
            "Iteration 60, loss = 0.42177251\n",
            "Iteration 61, loss = 0.42336175\n",
            "Iteration 62, loss = 0.42924607\n",
            "Iteration 63, loss = 0.41465631\n",
            "Iteration 64, loss = 0.42111261\n",
            "Iteration 65, loss = 0.42823282\n",
            "Iteration 66, loss = 0.43275373\n",
            "Iteration 67, loss = 0.41295122\n",
            "Iteration 68, loss = 0.41867452\n",
            "Iteration 69, loss = 0.41373603\n",
            "Iteration 70, loss = 0.41255124\n",
            "Iteration 71, loss = 0.41161117\n",
            "Iteration 72, loss = 0.40880546\n",
            "Iteration 73, loss = 0.42558667\n",
            "Iteration 74, loss = 0.41807990\n",
            "Iteration 75, loss = 0.41391162\n",
            "Iteration 76, loss = 0.41187166\n",
            "Iteration 77, loss = 0.41753563\n",
            "Iteration 78, loss = 0.41584055\n",
            "Iteration 79, loss = 0.40897602\n",
            "Iteration 80, loss = 0.41325816\n",
            "Iteration 81, loss = 0.40533557\n",
            "Iteration 82, loss = 0.40994948\n",
            "Iteration 83, loss = 0.40832165\n",
            "Iteration 84, loss = 0.40809204\n",
            "Iteration 85, loss = 0.40553009\n",
            "Iteration 86, loss = 0.41189157\n",
            "Iteration 87, loss = 0.40832239\n",
            "Iteration 88, loss = 0.40802845\n",
            "Iteration 89, loss = 0.40870905\n",
            "Iteration 90, loss = 0.40766837\n",
            "Iteration 91, loss = 0.40788365\n",
            "Iteration 92, loss = 0.41369452\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66446833\n",
            "Iteration 2, loss = 0.62371820\n",
            "Iteration 3, loss = 0.61117119\n",
            "Iteration 4, loss = 0.60237490\n",
            "Iteration 5, loss = 0.59421961\n",
            "Iteration 6, loss = 0.59002051\n",
            "Iteration 7, loss = 0.58757610\n",
            "Iteration 8, loss = 0.58117163\n",
            "Iteration 9, loss = 0.57772193\n",
            "Iteration 10, loss = 0.56964346\n",
            "Iteration 11, loss = 0.56213619\n",
            "Iteration 12, loss = 0.55519782\n",
            "Iteration 13, loss = 0.54659684\n",
            "Iteration 14, loss = 0.53749475\n",
            "Iteration 15, loss = 0.53096904\n",
            "Iteration 16, loss = 0.51510589\n",
            "Iteration 17, loss = 0.51419249\n",
            "Iteration 18, loss = 0.49948830\n",
            "Iteration 19, loss = 0.50172813\n",
            "Iteration 20, loss = 0.48283262\n",
            "Iteration 21, loss = 0.47710170\n",
            "Iteration 22, loss = 0.47344814\n",
            "Iteration 23, loss = 0.46508671\n",
            "Iteration 24, loss = 0.45962975\n",
            "Iteration 25, loss = 0.47252763\n",
            "Iteration 26, loss = 0.45304737\n",
            "Iteration 27, loss = 0.45803272\n",
            "Iteration 28, loss = 0.44574926\n",
            "Iteration 29, loss = 0.44425083\n",
            "Iteration 30, loss = 0.44207456\n",
            "Iteration 31, loss = 0.44690727\n",
            "Iteration 32, loss = 0.43758768\n",
            "Iteration 33, loss = 0.44287384\n",
            "Iteration 34, loss = 0.43808663\n",
            "Iteration 35, loss = 0.43940089\n",
            "Iteration 36, loss = 0.44031448\n",
            "Iteration 37, loss = 0.42883971\n",
            "Iteration 38, loss = 0.43957953\n",
            "Iteration 39, loss = 0.43565729\n",
            "Iteration 40, loss = 0.42853133\n",
            "Iteration 41, loss = 0.43584571\n",
            "Iteration 42, loss = 0.43465706\n",
            "Iteration 43, loss = 0.42555049\n",
            "Iteration 44, loss = 0.42871282\n",
            "Iteration 45, loss = 0.42327307\n",
            "Iteration 46, loss = 0.43049346\n",
            "Iteration 47, loss = 0.43341753\n",
            "Iteration 48, loss = 0.42569376\n",
            "Iteration 49, loss = 0.42818541\n",
            "Iteration 50, loss = 0.42263404\n",
            "Iteration 51, loss = 0.42446550\n",
            "Iteration 52, loss = 0.41958753\n",
            "Iteration 53, loss = 0.41887790\n",
            "Iteration 54, loss = 0.41888534\n",
            "Iteration 55, loss = 0.42243798\n",
            "Iteration 56, loss = 0.42298801\n",
            "Iteration 57, loss = 0.42963860\n",
            "Iteration 58, loss = 0.42204128\n",
            "Iteration 59, loss = 0.41593220\n",
            "Iteration 60, loss = 0.41750700\n",
            "Iteration 61, loss = 0.42074871\n",
            "Iteration 62, loss = 0.41240420\n",
            "Iteration 63, loss = 0.41566454\n",
            "Iteration 64, loss = 0.42695302\n",
            "Iteration 65, loss = 0.41421304\n",
            "Iteration 66, loss = 0.41532639\n",
            "Iteration 67, loss = 0.41489819\n",
            "Iteration 68, loss = 0.40767088\n",
            "Iteration 69, loss = 0.41239176\n",
            "Iteration 70, loss = 0.41015624\n",
            "Iteration 71, loss = 0.40662461\n",
            "Iteration 72, loss = 0.41418918\n",
            "Iteration 73, loss = 0.40816011\n",
            "Iteration 74, loss = 0.40537940\n",
            "Iteration 75, loss = 0.41204581\n",
            "Iteration 76, loss = 0.40525480\n",
            "Iteration 77, loss = 0.40869601\n",
            "Iteration 78, loss = 0.41079404\n",
            "Iteration 79, loss = 0.40581418\n",
            "Iteration 80, loss = 0.41177193\n",
            "Iteration 81, loss = 0.41476195\n",
            "Iteration 82, loss = 0.40228591\n",
            "Iteration 83, loss = 0.40797413\n",
            "Iteration 84, loss = 0.40346535\n",
            "Iteration 85, loss = 0.40694863\n",
            "Iteration 86, loss = 0.40121606\n",
            "Iteration 87, loss = 0.40550651\n",
            "Iteration 88, loss = 0.40465030\n",
            "Iteration 89, loss = 0.40820329\n",
            "Iteration 90, loss = 0.40270608\n",
            "Iteration 91, loss = 0.40042379\n",
            "Iteration 92, loss = 0.40771114\n",
            "Iteration 93, loss = 0.40258794\n",
            "Iteration 94, loss = 0.40160919\n",
            "Iteration 95, loss = 0.40548473\n",
            "Iteration 96, loss = 0.40011658\n",
            "Iteration 97, loss = 0.41023615\n",
            "Iteration 98, loss = 0.40358060\n",
            "Iteration 99, loss = 0.40281423\n",
            "Iteration 100, loss = 0.39851793\n",
            "Iteration 101, loss = 0.39464795\n",
            "Iteration 102, loss = 0.39841858\n",
            "Iteration 103, loss = 0.40865267\n",
            "Iteration 104, loss = 0.40202161\n",
            "Iteration 105, loss = 0.39478472\n",
            "Iteration 106, loss = 0.40265036\n",
            "Iteration 107, loss = 0.39949194\n",
            "Iteration 108, loss = 0.39445477\n",
            "Iteration 109, loss = 0.39987392\n",
            "Iteration 110, loss = 0.40721589\n",
            "Iteration 111, loss = 0.40131613\n",
            "Iteration 112, loss = 0.39509350\n",
            "Iteration 113, loss = 0.39134713\n",
            "Iteration 114, loss = 0.39464109\n",
            "Iteration 115, loss = 0.39128689\n",
            "Iteration 116, loss = 0.39543139\n",
            "Iteration 117, loss = 0.39259436\n",
            "Iteration 118, loss = 0.38954922\n",
            "Iteration 119, loss = 0.39504139\n",
            "Iteration 120, loss = 0.38879394\n",
            "Iteration 121, loss = 0.38678049\n",
            "Iteration 122, loss = 0.39448685\n",
            "Iteration 123, loss = 0.39963235\n",
            "Iteration 124, loss = 0.39477337\n",
            "Iteration 125, loss = 0.38920042\n",
            "Iteration 126, loss = 0.38777668\n",
            "Iteration 127, loss = 0.39550190\n",
            "Iteration 128, loss = 0.38957053\n",
            "Iteration 129, loss = 0.39537866\n",
            "Iteration 130, loss = 0.38787934\n",
            "Iteration 131, loss = 0.38739347\n",
            "Iteration 132, loss = 0.39876701\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66278351\n",
            "Iteration 2, loss = 0.62830159\n",
            "Iteration 3, loss = 0.60777479\n",
            "Iteration 4, loss = 0.60122511\n",
            "Iteration 5, loss = 0.59298252\n",
            "Iteration 6, loss = 0.59484828\n",
            "Iteration 7, loss = 0.58513092\n",
            "Iteration 8, loss = 0.58268295\n",
            "Iteration 9, loss = 0.57554816\n",
            "Iteration 10, loss = 0.56950745\n",
            "Iteration 11, loss = 0.56305699\n",
            "Iteration 12, loss = 0.55491318\n",
            "Iteration 13, loss = 0.54860330\n",
            "Iteration 14, loss = 0.53969991\n",
            "Iteration 15, loss = 0.52744414\n",
            "Iteration 16, loss = 0.52615091\n",
            "Iteration 17, loss = 0.51110755\n",
            "Iteration 18, loss = 0.49808655\n",
            "Iteration 19, loss = 0.50135078\n",
            "Iteration 20, loss = 0.48803311\n",
            "Iteration 21, loss = 0.47305525\n",
            "Iteration 22, loss = 0.46596805\n",
            "Iteration 23, loss = 0.45845809\n",
            "Iteration 24, loss = 0.47933622\n",
            "Iteration 25, loss = 0.44646121\n",
            "Iteration 26, loss = 0.44305014\n",
            "Iteration 27, loss = 0.44076079\n",
            "Iteration 28, loss = 0.43550035\n",
            "Iteration 29, loss = 0.43572595\n",
            "Iteration 30, loss = 0.42943695\n",
            "Iteration 31, loss = 0.43760797\n",
            "Iteration 32, loss = 0.43088202\n",
            "Iteration 33, loss = 0.42979767\n",
            "Iteration 34, loss = 0.42538011\n",
            "Iteration 35, loss = 0.42311976\n",
            "Iteration 36, loss = 0.44040245\n",
            "Iteration 37, loss = 0.43466585\n",
            "Iteration 38, loss = 0.42754051\n",
            "Iteration 39, loss = 0.41956023\n",
            "Iteration 40, loss = 0.42942914\n",
            "Iteration 41, loss = 0.41964130\n",
            "Iteration 42, loss = 0.42453191\n",
            "Iteration 43, loss = 0.42135001\n",
            "Iteration 44, loss = 0.41487223\n",
            "Iteration 45, loss = 0.40975291\n",
            "Iteration 46, loss = 0.42059076\n",
            "Iteration 47, loss = 0.40739930\n",
            "Iteration 48, loss = 0.41583952\n",
            "Iteration 49, loss = 0.41132326\n",
            "Iteration 50, loss = 0.40900301\n",
            "Iteration 51, loss = 0.40465358\n",
            "Iteration 52, loss = 0.42018274\n",
            "Iteration 53, loss = 0.40383231\n",
            "Iteration 54, loss = 0.40215326\n",
            "Iteration 55, loss = 0.40857648\n",
            "Iteration 56, loss = 0.40550791\n",
            "Iteration 57, loss = 0.40140003\n",
            "Iteration 58, loss = 0.40190728\n",
            "Iteration 59, loss = 0.39454261\n",
            "Iteration 60, loss = 0.40179407\n",
            "Iteration 61, loss = 0.40345836\n",
            "Iteration 62, loss = 0.39825739\n",
            "Iteration 63, loss = 0.39077683\n",
            "Iteration 64, loss = 0.39580877\n",
            "Iteration 65, loss = 0.40011357\n",
            "Iteration 66, loss = 0.40264120\n",
            "Iteration 67, loss = 0.39536338\n",
            "Iteration 68, loss = 0.39522527\n",
            "Iteration 69, loss = 0.39263591\n",
            "Iteration 70, loss = 0.39411239\n",
            "Iteration 71, loss = 0.40232218\n",
            "Iteration 72, loss = 0.40293402\n",
            "Iteration 73, loss = 0.39585154\n",
            "Iteration 74, loss = 0.40105716\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70260451\n",
            "Iteration 2, loss = 0.64099858\n",
            "Iteration 3, loss = 0.61841450\n",
            "Iteration 4, loss = 0.60776935\n",
            "Iteration 5, loss = 0.59743919\n",
            "Iteration 6, loss = 0.59052921\n",
            "Iteration 7, loss = 0.58312366\n",
            "Iteration 8, loss = 0.57782427\n",
            "Iteration 9, loss = 0.57016441\n",
            "Iteration 10, loss = 0.56288743\n",
            "Iteration 11, loss = 0.55736543\n",
            "Iteration 12, loss = 0.54856189\n",
            "Iteration 13, loss = 0.54014841\n",
            "Iteration 14, loss = 0.53669194\n",
            "Iteration 15, loss = 0.52296017\n",
            "Iteration 16, loss = 0.51765965\n",
            "Iteration 17, loss = 0.50166796\n",
            "Iteration 18, loss = 0.49239098\n",
            "Iteration 19, loss = 0.48620718\n",
            "Iteration 20, loss = 0.47550720\n",
            "Iteration 21, loss = 0.46913706\n",
            "Iteration 22, loss = 0.46519766\n",
            "Iteration 23, loss = 0.45853997\n",
            "Iteration 24, loss = 0.44775447\n",
            "Iteration 25, loss = 0.43894206\n",
            "Iteration 26, loss = 0.43616940\n",
            "Iteration 27, loss = 0.43439680\n",
            "Iteration 28, loss = 0.43526027\n",
            "Iteration 29, loss = 0.42613608\n",
            "Iteration 30, loss = 0.42800018\n",
            "Iteration 31, loss = 0.42655835\n",
            "Iteration 32, loss = 0.42446562\n",
            "Iteration 33, loss = 0.41945878\n",
            "Iteration 34, loss = 0.42042998\n",
            "Iteration 35, loss = 0.41187778\n",
            "Iteration 36, loss = 0.42203009\n",
            "Iteration 37, loss = 0.41340244\n",
            "Iteration 38, loss = 0.41108581\n",
            "Iteration 39, loss = 0.41075209\n",
            "Iteration 40, loss = 0.42132053\n",
            "Iteration 41, loss = 0.41074287\n",
            "Iteration 42, loss = 0.40384854\n",
            "Iteration 43, loss = 0.40697222\n",
            "Iteration 44, loss = 0.40960713\n",
            "Iteration 45, loss = 0.39960335\n",
            "Iteration 46, loss = 0.40881913\n",
            "Iteration 47, loss = 0.40603782\n",
            "Iteration 48, loss = 0.42437780\n",
            "Iteration 49, loss = 0.40708553\n",
            "Iteration 50, loss = 0.39826635\n",
            "Iteration 51, loss = 0.40409362\n",
            "Iteration 52, loss = 0.40970593\n",
            "Iteration 53, loss = 0.39546603\n",
            "Iteration 54, loss = 0.40165060\n",
            "Iteration 55, loss = 0.39752983\n",
            "Iteration 56, loss = 0.40225123\n",
            "Iteration 57, loss = 0.40180110\n",
            "Iteration 58, loss = 0.39673548\n",
            "Iteration 59, loss = 0.39775886\n",
            "Iteration 60, loss = 0.40001734\n",
            "Iteration 61, loss = 0.39423784\n",
            "Iteration 62, loss = 0.39274081\n",
            "Iteration 63, loss = 0.39490238\n",
            "Iteration 64, loss = 0.39291412\n",
            "Iteration 65, loss = 0.38887774\n",
            "Iteration 66, loss = 0.39339392\n",
            "Iteration 67, loss = 0.38887104\n",
            "Iteration 68, loss = 0.39000500\n",
            "Iteration 69, loss = 0.39814006\n",
            "Iteration 70, loss = 0.39611862\n",
            "Iteration 71, loss = 0.39521695\n",
            "Iteration 72, loss = 0.40786399\n",
            "Iteration 73, loss = 0.39166239\n",
            "Iteration 74, loss = 0.38672735\n",
            "Iteration 75, loss = 0.38743537\n",
            "Iteration 76, loss = 0.38739935\n",
            "Iteration 77, loss = 0.39000158\n",
            "Iteration 78, loss = 0.38875634\n",
            "Iteration 79, loss = 0.38715541\n",
            "Iteration 80, loss = 0.39139395\n",
            "Iteration 81, loss = 0.38706146\n",
            "Iteration 82, loss = 0.38521509\n",
            "Iteration 83, loss = 0.38301093\n",
            "Iteration 84, loss = 0.38330193\n",
            "Iteration 85, loss = 0.38016987\n",
            "Iteration 86, loss = 0.37932156\n",
            "Iteration 87, loss = 0.38705593\n",
            "Iteration 88, loss = 0.37992830\n",
            "Iteration 89, loss = 0.38637835\n",
            "Iteration 90, loss = 0.37828579\n",
            "Iteration 91, loss = 0.37661142\n",
            "Iteration 92, loss = 0.38533421\n",
            "Iteration 93, loss = 0.37962333\n",
            "Iteration 94, loss = 0.39120140\n",
            "Iteration 95, loss = 0.38492873\n",
            "Iteration 96, loss = 0.38043738\n",
            "Iteration 97, loss = 0.38154078\n",
            "Iteration 98, loss = 0.37781279\n",
            "Iteration 99, loss = 0.37906469\n",
            "Iteration 100, loss = 0.38167436\n",
            "Iteration 101, loss = 0.37968374\n",
            "Iteration 102, loss = 0.38128332\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66958252\n",
            "Iteration 2, loss = 0.64321503\n",
            "Iteration 3, loss = 0.62771793\n",
            "Iteration 4, loss = 0.61997760\n",
            "Iteration 5, loss = 0.61271583\n",
            "Iteration 6, loss = 0.61146732\n",
            "Iteration 7, loss = 0.60933956\n",
            "Iteration 8, loss = 0.59732376\n",
            "Iteration 9, loss = 0.58899535\n",
            "Iteration 10, loss = 0.58271889\n",
            "Iteration 11, loss = 0.57504073\n",
            "Iteration 12, loss = 0.56646769\n",
            "Iteration 13, loss = 0.55961103\n",
            "Iteration 14, loss = 0.54786264\n",
            "Iteration 15, loss = 0.53027879\n",
            "Iteration 16, loss = 0.52362325\n",
            "Iteration 17, loss = 0.51377178\n",
            "Iteration 18, loss = 0.50199467\n",
            "Iteration 19, loss = 0.49632244\n",
            "Iteration 20, loss = 0.48646606\n",
            "Iteration 21, loss = 0.47583022\n",
            "Iteration 22, loss = 0.47063317\n",
            "Iteration 23, loss = 0.46866109\n",
            "Iteration 24, loss = 0.46046566\n",
            "Iteration 25, loss = 0.46397797\n",
            "Iteration 26, loss = 0.46219786\n",
            "Iteration 27, loss = 0.46084077\n",
            "Iteration 28, loss = 0.45658938\n",
            "Iteration 29, loss = 0.45099591\n",
            "Iteration 30, loss = 0.45029410\n",
            "Iteration 31, loss = 0.45073929\n",
            "Iteration 32, loss = 0.45060285\n",
            "Iteration 33, loss = 0.44607388\n",
            "Iteration 34, loss = 0.44863702\n",
            "Iteration 35, loss = 0.44387240\n",
            "Iteration 36, loss = 0.44637653\n",
            "Iteration 37, loss = 0.43946796\n",
            "Iteration 38, loss = 0.44348946\n",
            "Iteration 39, loss = 0.44567264\n",
            "Iteration 40, loss = 0.44295811\n",
            "Iteration 41, loss = 0.43679148\n",
            "Iteration 42, loss = 0.44687174\n",
            "Iteration 43, loss = 0.44023578\n",
            "Iteration 44, loss = 0.43735498\n",
            "Iteration 45, loss = 0.43126449\n",
            "Iteration 46, loss = 0.43197075\n",
            "Iteration 47, loss = 0.42803562\n",
            "Iteration 48, loss = 0.43663651\n",
            "Iteration 49, loss = 0.42782225\n",
            "Iteration 50, loss = 0.43128220\n",
            "Iteration 51, loss = 0.42777521\n",
            "Iteration 52, loss = 0.42818338\n",
            "Iteration 53, loss = 0.42359027\n",
            "Iteration 54, loss = 0.43002316\n",
            "Iteration 55, loss = 0.42455271\n",
            "Iteration 56, loss = 0.42529472\n",
            "Iteration 57, loss = 0.42439132\n",
            "Iteration 58, loss = 0.41886783\n",
            "Iteration 59, loss = 0.43005847\n",
            "Iteration 60, loss = 0.42352020\n",
            "Iteration 61, loss = 0.42250477\n",
            "Iteration 62, loss = 0.42078500\n",
            "Iteration 63, loss = 0.41681883\n",
            "Iteration 64, loss = 0.42289439\n",
            "Iteration 65, loss = 0.41657946\n",
            "Iteration 66, loss = 0.42302241\n",
            "Iteration 67, loss = 0.41957719\n",
            "Iteration 68, loss = 0.41446148\n",
            "Iteration 69, loss = 0.41356177\n",
            "Iteration 70, loss = 0.41501347\n",
            "Iteration 71, loss = 0.41426572\n",
            "Iteration 72, loss = 0.40935714\n",
            "Iteration 73, loss = 0.40972267\n",
            "Iteration 74, loss = 0.41474095\n",
            "Iteration 75, loss = 0.40971723\n",
            "Iteration 76, loss = 0.40952498\n",
            "Iteration 77, loss = 0.40720997\n",
            "Iteration 78, loss = 0.42126872\n",
            "Iteration 79, loss = 0.41260116\n",
            "Iteration 80, loss = 0.41321103\n",
            "Iteration 81, loss = 0.42094593\n",
            "Iteration 82, loss = 0.40191744\n",
            "Iteration 83, loss = 0.41078428\n",
            "Iteration 84, loss = 0.41024873\n",
            "Iteration 85, loss = 0.40386468\n",
            "Iteration 86, loss = 0.40515551\n",
            "Iteration 87, loss = 0.41134180\n",
            "Iteration 88, loss = 0.40130845\n",
            "Iteration 89, loss = 0.40650680\n",
            "Iteration 90, loss = 0.40523097\n",
            "Iteration 91, loss = 0.39988475\n",
            "Iteration 92, loss = 0.40323174\n",
            "Iteration 93, loss = 0.41013777\n",
            "Iteration 94, loss = 0.40107422\n",
            "Iteration 95, loss = 0.40085345\n",
            "Iteration 96, loss = 0.40016730\n",
            "Iteration 97, loss = 0.39628732\n",
            "Iteration 98, loss = 0.39695146\n",
            "Iteration 99, loss = 0.39575060\n",
            "Iteration 100, loss = 0.40500174\n",
            "Iteration 101, loss = 0.40051024\n",
            "Iteration 102, loss = 0.39467370\n",
            "Iteration 103, loss = 0.40280484\n",
            "Iteration 104, loss = 0.39859856\n",
            "Iteration 105, loss = 0.40306291\n",
            "Iteration 106, loss = 0.39395926\n",
            "Iteration 107, loss = 0.39900249\n",
            "Iteration 108, loss = 0.39607373\n",
            "Iteration 109, loss = 0.40999464\n",
            "Iteration 110, loss = 0.39122031\n",
            "Iteration 111, loss = 0.39449400\n",
            "Iteration 112, loss = 0.39647344\n",
            "Iteration 113, loss = 0.39961883\n",
            "Iteration 114, loss = 0.39056991\n",
            "Iteration 115, loss = 0.39870801\n",
            "Iteration 116, loss = 0.39486205\n",
            "Iteration 117, loss = 0.39489923\n",
            "Iteration 118, loss = 0.39959558\n",
            "Iteration 119, loss = 0.39076720\n",
            "Iteration 120, loss = 0.39116329\n",
            "Iteration 121, loss = 0.39230141\n",
            "Iteration 122, loss = 0.39346586\n",
            "Iteration 123, loss = 0.38722815\n",
            "Iteration 124, loss = 0.39260565\n",
            "Iteration 125, loss = 0.38766182\n",
            "Iteration 126, loss = 0.39540335\n",
            "Iteration 127, loss = 0.38408516\n",
            "Iteration 128, loss = 0.40020170\n",
            "Iteration 129, loss = 0.38774042\n",
            "Iteration 130, loss = 0.39547416\n",
            "Iteration 131, loss = 0.38836658\n",
            "Iteration 132, loss = 0.38767112\n",
            "Iteration 133, loss = 0.38466922\n",
            "Iteration 134, loss = 0.39175148\n",
            "Iteration 135, loss = 0.39592709\n",
            "Iteration 136, loss = 0.39200939\n",
            "Iteration 137, loss = 0.39222058\n",
            "Iteration 138, loss = 0.38062128\n",
            "Iteration 139, loss = 0.39886317\n",
            "Iteration 140, loss = 0.38648878\n",
            "Iteration 141, loss = 0.40057394\n",
            "Iteration 142, loss = 0.38501295\n",
            "Iteration 143, loss = 0.38929055\n",
            "Iteration 144, loss = 0.38717662\n",
            "Iteration 145, loss = 0.38695292\n",
            "Iteration 146, loss = 0.38769319\n",
            "Iteration 147, loss = 0.38865087\n",
            "Iteration 148, loss = 0.38999247\n",
            "Iteration 149, loss = 0.38466164\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66390823\n",
            "Iteration 2, loss = 0.64053878\n",
            "Iteration 3, loss = 0.62607600\n",
            "Iteration 4, loss = 0.61481783\n",
            "Iteration 5, loss = 0.60979404\n",
            "Iteration 6, loss = 0.60284584\n",
            "Iteration 7, loss = 0.59746352\n",
            "Iteration 8, loss = 0.59222008\n",
            "Iteration 9, loss = 0.58465318\n",
            "Iteration 10, loss = 0.57599094\n",
            "Iteration 11, loss = 0.56685957\n",
            "Iteration 12, loss = 0.55920575\n",
            "Iteration 13, loss = 0.54921037\n",
            "Iteration 14, loss = 0.53942399\n",
            "Iteration 15, loss = 0.52916561\n",
            "Iteration 16, loss = 0.51637796\n",
            "Iteration 17, loss = 0.51731360\n",
            "Iteration 18, loss = 0.49757731\n",
            "Iteration 19, loss = 0.49181383\n",
            "Iteration 20, loss = 0.48754096\n",
            "Iteration 21, loss = 0.48063128\n",
            "Iteration 22, loss = 0.47372916\n",
            "Iteration 23, loss = 0.46221890\n",
            "Iteration 24, loss = 0.46966303\n",
            "Iteration 25, loss = 0.45814518\n",
            "Iteration 26, loss = 0.45577963\n",
            "Iteration 27, loss = 0.46089598\n",
            "Iteration 28, loss = 0.45902685\n",
            "Iteration 29, loss = 0.46601607\n",
            "Iteration 30, loss = 0.44945602\n",
            "Iteration 31, loss = 0.44741236\n",
            "Iteration 32, loss = 0.44356412\n",
            "Iteration 33, loss = 0.44161532\n",
            "Iteration 34, loss = 0.45813935\n",
            "Iteration 35, loss = 0.44418674\n",
            "Iteration 36, loss = 0.44576999\n",
            "Iteration 37, loss = 0.44686417\n",
            "Iteration 38, loss = 0.43698317\n",
            "Iteration 39, loss = 0.44031053\n",
            "Iteration 40, loss = 0.43983327\n",
            "Iteration 41, loss = 0.43858165\n",
            "Iteration 42, loss = 0.44414997\n",
            "Iteration 43, loss = 0.44066618\n",
            "Iteration 44, loss = 0.43335454\n",
            "Iteration 45, loss = 0.43263026\n",
            "Iteration 46, loss = 0.43464026\n",
            "Iteration 47, loss = 0.43452675\n",
            "Iteration 48, loss = 0.43503178\n",
            "Iteration 49, loss = 0.42582167\n",
            "Iteration 50, loss = 0.43264996\n",
            "Iteration 51, loss = 0.42959449\n",
            "Iteration 52, loss = 0.42916894\n",
            "Iteration 53, loss = 0.43930301\n",
            "Iteration 54, loss = 0.42785041\n",
            "Iteration 55, loss = 0.42961617\n",
            "Iteration 56, loss = 0.42311662\n",
            "Iteration 57, loss = 0.41996920\n",
            "Iteration 58, loss = 0.41895341\n",
            "Iteration 59, loss = 0.41935405\n",
            "Iteration 60, loss = 0.42620364\n",
            "Iteration 61, loss = 0.41904818\n",
            "Iteration 62, loss = 0.42340023\n",
            "Iteration 63, loss = 0.41952014\n",
            "Iteration 64, loss = 0.42294826\n",
            "Iteration 65, loss = 0.41661353\n",
            "Iteration 66, loss = 0.41791312\n",
            "Iteration 67, loss = 0.41536365\n",
            "Iteration 68, loss = 0.41186640\n",
            "Iteration 69, loss = 0.41574711\n",
            "Iteration 70, loss = 0.42005261\n",
            "Iteration 71, loss = 0.42229967\n",
            "Iteration 72, loss = 0.41513603\n",
            "Iteration 73, loss = 0.42629292\n",
            "Iteration 74, loss = 0.41164327\n",
            "Iteration 75, loss = 0.41093609\n",
            "Iteration 76, loss = 0.41815632\n",
            "Iteration 77, loss = 0.41154097\n",
            "Iteration 78, loss = 0.40791243\n",
            "Iteration 79, loss = 0.41204165\n",
            "Iteration 80, loss = 0.40697553\n",
            "Iteration 81, loss = 0.40384631\n",
            "Iteration 82, loss = 0.41373718\n",
            "Iteration 83, loss = 0.40840544\n",
            "Iteration 84, loss = 0.40847572\n",
            "Iteration 85, loss = 0.40640340\n",
            "Iteration 86, loss = 0.42024106\n",
            "Iteration 87, loss = 0.40554057\n",
            "Iteration 88, loss = 0.40719547\n",
            "Iteration 89, loss = 0.40660010\n",
            "Iteration 90, loss = 0.40647472\n",
            "Iteration 91, loss = 0.41080367\n",
            "Iteration 92, loss = 0.40508373\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.01218593\n",
            "Iteration 2, loss = 0.68374414\n",
            "Iteration 3, loss = 0.66727145\n",
            "Iteration 4, loss = 0.61273987\n",
            "Iteration 5, loss = 0.63325980\n",
            "Iteration 6, loss = 0.60316780\n",
            "Iteration 7, loss = 0.59281318\n",
            "Iteration 8, loss = 0.59022596\n",
            "Iteration 9, loss = 0.60598268\n",
            "Iteration 10, loss = 0.58167282\n",
            "Iteration 11, loss = 0.57882585\n",
            "Iteration 12, loss = 0.56945587\n",
            "Iteration 13, loss = 0.57685609\n",
            "Iteration 14, loss = 0.58423490\n",
            "Iteration 15, loss = 0.57939779\n",
            "Iteration 16, loss = 0.58065257\n",
            "Iteration 17, loss = 0.56771242\n",
            "Iteration 18, loss = 0.55939153\n",
            "Iteration 19, loss = 0.55523236\n",
            "Iteration 20, loss = 0.55279999\n",
            "Iteration 21, loss = 0.55575169\n",
            "Iteration 22, loss = 0.56324410\n",
            "Iteration 23, loss = 0.54939668\n",
            "Iteration 24, loss = 0.54942095\n",
            "Iteration 25, loss = 0.55422443\n",
            "Iteration 26, loss = 0.54557379\n",
            "Iteration 27, loss = 0.53581718\n",
            "Iteration 28, loss = 0.54702097\n",
            "Iteration 29, loss = 0.55155929\n",
            "Iteration 30, loss = 0.55515445\n",
            "Iteration 31, loss = 0.53954878\n",
            "Iteration 32, loss = 0.54105803\n",
            "Iteration 33, loss = 0.54900393\n",
            "Iteration 34, loss = 0.54443706\n",
            "Iteration 35, loss = 0.54716422\n",
            "Iteration 36, loss = 0.55226925\n",
            "Iteration 37, loss = 0.54395569\n",
            "Iteration 38, loss = 0.54197204\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.86729434\n",
            "Iteration 2, loss = 0.74518922\n",
            "Iteration 3, loss = 0.66527562\n",
            "Iteration 4, loss = 0.63376068\n",
            "Iteration 5, loss = 0.62925873\n",
            "Iteration 6, loss = 0.60990735\n",
            "Iteration 7, loss = 0.60450735\n",
            "Iteration 8, loss = 0.59475965\n",
            "Iteration 9, loss = 0.59645755\n",
            "Iteration 10, loss = 0.61442658\n",
            "Iteration 11, loss = 0.59615363\n",
            "Iteration 12, loss = 0.58866880\n",
            "Iteration 13, loss = 0.58891807\n",
            "Iteration 14, loss = 0.59340492\n",
            "Iteration 15, loss = 0.59115590\n",
            "Iteration 16, loss = 0.58414586\n",
            "Iteration 17, loss = 0.57855480\n",
            "Iteration 18, loss = 0.59212807\n",
            "Iteration 19, loss = 0.57844834\n",
            "Iteration 20, loss = 0.58624064\n",
            "Iteration 21, loss = 0.58174743\n",
            "Iteration 22, loss = 0.57574204\n",
            "Iteration 23, loss = 0.57767344\n",
            "Iteration 24, loss = 0.57440819\n",
            "Iteration 25, loss = 0.56691321\n",
            "Iteration 26, loss = 0.58103553\n",
            "Iteration 27, loss = 0.57007684\n",
            "Iteration 28, loss = 0.57470146\n",
            "Iteration 29, loss = 0.57850176\n",
            "Iteration 30, loss = 0.56137109\n",
            "Iteration 31, loss = 0.55497347\n",
            "Iteration 32, loss = 0.57310982\n",
            "Iteration 33, loss = 0.56701492\n",
            "Iteration 34, loss = 0.57992440\n",
            "Iteration 35, loss = 0.57199796\n",
            "Iteration 36, loss = 0.55734036\n",
            "Iteration 37, loss = 0.54732838\n",
            "Iteration 38, loss = 0.55586514\n",
            "Iteration 39, loss = 0.56018403\n",
            "Iteration 40, loss = 0.55346901\n",
            "Iteration 41, loss = 0.57084429\n",
            "Iteration 42, loss = 0.56238057\n",
            "Iteration 43, loss = 0.55873061\n",
            "Iteration 44, loss = 0.54267049\n",
            "Iteration 45, loss = 0.54857622\n",
            "Iteration 46, loss = 0.55741172\n",
            "Iteration 47, loss = 0.55251973\n",
            "Iteration 48, loss = 0.55249052\n",
            "Iteration 49, loss = 0.53501925\n",
            "Iteration 50, loss = 0.53837999\n",
            "Iteration 51, loss = 0.56138600\n",
            "Iteration 52, loss = 0.54978880\n",
            "Iteration 53, loss = 0.53751825\n",
            "Iteration 54, loss = 0.54129888\n",
            "Iteration 55, loss = 0.53597114\n",
            "Iteration 56, loss = 0.53717673\n",
            "Iteration 57, loss = 0.53521087\n",
            "Iteration 58, loss = 0.52880881\n",
            "Iteration 59, loss = 0.53970900\n",
            "Iteration 60, loss = 0.55073126\n",
            "Iteration 61, loss = 0.53277271\n",
            "Iteration 62, loss = 0.54387037\n",
            "Iteration 63, loss = 0.54015792\n",
            "Iteration 64, loss = 0.53981635\n",
            "Iteration 65, loss = 0.52943742\n",
            "Iteration 66, loss = 0.54307327\n",
            "Iteration 67, loss = 0.52338546\n",
            "Iteration 68, loss = 0.53585596\n",
            "Iteration 69, loss = 0.52930595\n",
            "Iteration 70, loss = 0.52121814\n",
            "Iteration 71, loss = 0.55933559\n",
            "Iteration 72, loss = 0.52740696\n",
            "Iteration 73, loss = 0.53413933\n",
            "Iteration 74, loss = 0.52229936\n",
            "Iteration 75, loss = 0.52435219\n",
            "Iteration 76, loss = 0.54172504\n",
            "Iteration 77, loss = 0.54560786\n",
            "Iteration 78, loss = 0.51401217\n",
            "Iteration 79, loss = 0.51361734\n",
            "Iteration 80, loss = 0.54660947\n",
            "Iteration 81, loss = 0.52742700\n",
            "Iteration 82, loss = 0.50164670\n",
            "Iteration 83, loss = 0.51637287\n",
            "Iteration 84, loss = 0.52104476\n",
            "Iteration 85, loss = 0.49859334\n",
            "Iteration 86, loss = 0.49598899\n",
            "Iteration 87, loss = 0.50573270\n",
            "Iteration 88, loss = 0.50801136\n",
            "Iteration 89, loss = 0.52528724\n",
            "Iteration 90, loss = 0.51412820\n",
            "Iteration 91, loss = 0.51782990\n",
            "Iteration 92, loss = 0.51691324\n",
            "Iteration 93, loss = 0.52822660\n",
            "Iteration 94, loss = 0.51666764\n",
            "Iteration 95, loss = 0.52200530\n",
            "Iteration 96, loss = 0.52079936\n",
            "Iteration 97, loss = 0.50850023\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.08863163\n",
            "Iteration 2, loss = 0.73103979\n",
            "Iteration 3, loss = 0.65405766\n",
            "Iteration 4, loss = 0.62752805\n",
            "Iteration 5, loss = 0.60897280\n",
            "Iteration 6, loss = 0.63257580\n",
            "Iteration 7, loss = 0.62155056\n",
            "Iteration 8, loss = 0.61678237\n",
            "Iteration 9, loss = 0.59196606\n",
            "Iteration 10, loss = 0.60446273\n",
            "Iteration 11, loss = 0.61251858\n",
            "Iteration 12, loss = 0.59988732\n",
            "Iteration 13, loss = 0.60245760\n",
            "Iteration 14, loss = 0.58402177\n",
            "Iteration 15, loss = 0.58151885\n",
            "Iteration 16, loss = 0.57054686\n",
            "Iteration 17, loss = 0.57377522\n",
            "Iteration 18, loss = 0.56998762\n",
            "Iteration 19, loss = 0.56423296\n",
            "Iteration 20, loss = 0.57121437\n",
            "Iteration 21, loss = 0.55732210\n",
            "Iteration 22, loss = 0.55682635\n",
            "Iteration 23, loss = 0.56091904\n",
            "Iteration 24, loss = 0.55792895\n",
            "Iteration 25, loss = 0.57989355\n",
            "Iteration 26, loss = 0.56067279\n",
            "Iteration 27, loss = 0.57387783\n",
            "Iteration 28, loss = 0.54723994\n",
            "Iteration 29, loss = 0.55527921\n",
            "Iteration 30, loss = 0.56395012\n",
            "Iteration 31, loss = 0.55261946\n",
            "Iteration 32, loss = 0.54075460\n",
            "Iteration 33, loss = 0.53082116\n",
            "Iteration 34, loss = 0.53679595\n",
            "Iteration 35, loss = 0.53870116\n",
            "Iteration 36, loss = 0.53828209\n",
            "Iteration 37, loss = 0.53447758\n",
            "Iteration 38, loss = 0.53157148\n",
            "Iteration 39, loss = 0.53836416\n",
            "Iteration 40, loss = 0.52508526\n",
            "Iteration 41, loss = 0.53490796\n",
            "Iteration 42, loss = 0.52984850\n",
            "Iteration 43, loss = 0.52604769\n",
            "Iteration 44, loss = 0.53132658\n",
            "Iteration 45, loss = 0.52235556\n",
            "Iteration 46, loss = 0.52867837\n",
            "Iteration 47, loss = 0.52952526\n",
            "Iteration 48, loss = 0.51176644\n",
            "Iteration 49, loss = 0.52036522\n",
            "Iteration 50, loss = 0.52701882\n",
            "Iteration 51, loss = 0.50688190\n",
            "Iteration 52, loss = 0.50402245\n",
            "Iteration 53, loss = 0.52072456\n",
            "Iteration 54, loss = 0.49955509\n",
            "Iteration 55, loss = 0.52220248\n",
            "Iteration 56, loss = 0.50879506\n",
            "Iteration 57, loss = 0.51248921\n",
            "Iteration 58, loss = 0.49675330\n",
            "Iteration 59, loss = 0.51778446\n",
            "Iteration 60, loss = 0.51174136\n",
            "Iteration 61, loss = 0.52477914\n",
            "Iteration 62, loss = 0.50993534\n",
            "Iteration 63, loss = 0.52547632\n",
            "Iteration 64, loss = 0.50240430\n",
            "Iteration 65, loss = 0.53041701\n",
            "Iteration 66, loss = 0.50403849\n",
            "Iteration 67, loss = 0.51455883\n",
            "Iteration 68, loss = 0.54329161\n",
            "Iteration 69, loss = 0.51469969\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.88068557\n",
            "Iteration 2, loss = 0.75690956\n",
            "Iteration 3, loss = 0.65002926\n",
            "Iteration 4, loss = 0.66497663\n",
            "Iteration 5, loss = 0.61796694\n",
            "Iteration 6, loss = 0.65912910\n",
            "Iteration 7, loss = 0.64984138\n",
            "Iteration 8, loss = 0.61535786\n",
            "Iteration 9, loss = 0.62132714\n",
            "Iteration 10, loss = 0.61336740\n",
            "Iteration 11, loss = 0.62268923\n",
            "Iteration 12, loss = 0.61580814\n",
            "Iteration 13, loss = 0.59786856\n",
            "Iteration 14, loss = 0.59239804\n",
            "Iteration 15, loss = 0.59797320\n",
            "Iteration 16, loss = 0.59132922\n",
            "Iteration 17, loss = 0.58415385\n",
            "Iteration 18, loss = 0.59206492\n",
            "Iteration 19, loss = 0.57929078\n",
            "Iteration 20, loss = 0.57845392\n",
            "Iteration 21, loss = 0.59535897\n",
            "Iteration 22, loss = 0.57066832\n",
            "Iteration 23, loss = 0.57412153\n",
            "Iteration 24, loss = 0.57529598\n",
            "Iteration 25, loss = 0.56764794\n",
            "Iteration 26, loss = 0.57925256\n",
            "Iteration 27, loss = 0.57435242\n",
            "Iteration 28, loss = 0.57371937\n",
            "Iteration 29, loss = 0.56970785\n",
            "Iteration 30, loss = 0.57559102\n",
            "Iteration 31, loss = 0.56160258\n",
            "Iteration 32, loss = 0.57639592\n",
            "Iteration 33, loss = 0.56095938\n",
            "Iteration 34, loss = 0.57208483\n",
            "Iteration 35, loss = 0.57925192\n",
            "Iteration 36, loss = 0.59521471\n",
            "Iteration 37, loss = 0.55796430\n",
            "Iteration 38, loss = 0.56454824\n",
            "Iteration 39, loss = 0.56372353\n",
            "Iteration 40, loss = 0.54520118\n",
            "Iteration 41, loss = 0.53865151\n",
            "Iteration 42, loss = 0.58532493\n",
            "Iteration 43, loss = 0.56952984\n",
            "Iteration 44, loss = 0.57891254\n",
            "Iteration 45, loss = 0.55480912\n",
            "Iteration 46, loss = 0.55426488\n",
            "Iteration 47, loss = 0.54759563\n",
            "Iteration 48, loss = 0.54482506\n",
            "Iteration 49, loss = 0.53586887\n",
            "Iteration 50, loss = 0.52485098\n",
            "Iteration 51, loss = 0.52377457\n",
            "Iteration 52, loss = 0.55656449\n",
            "Iteration 53, loss = 0.54454602\n",
            "Iteration 54, loss = 0.54042593\n",
            "Iteration 55, loss = 0.53429086\n",
            "Iteration 56, loss = 0.55054886\n",
            "Iteration 57, loss = 0.54023925\n",
            "Iteration 58, loss = 0.52907979\n",
            "Iteration 59, loss = 0.56466048\n",
            "Iteration 60, loss = 0.54293388\n",
            "Iteration 61, loss = 0.52795439\n",
            "Iteration 62, loss = 0.52192065\n",
            "Iteration 63, loss = 0.55999118\n",
            "Iteration 64, loss = 0.52506127\n",
            "Iteration 65, loss = 0.52145731\n",
            "Iteration 66, loss = 0.51496183\n",
            "Iteration 67, loss = 0.52585615\n",
            "Iteration 68, loss = 0.51712233\n",
            "Iteration 69, loss = 0.55197061\n",
            "Iteration 70, loss = 0.51141083\n",
            "Iteration 71, loss = 0.51435813\n",
            "Iteration 72, loss = 0.52070789\n",
            "Iteration 73, loss = 0.52038907\n",
            "Iteration 74, loss = 0.50670398\n",
            "Iteration 75, loss = 0.51362936\n",
            "Iteration 76, loss = 0.51637650\n",
            "Iteration 77, loss = 0.51662185\n",
            "Iteration 78, loss = 0.50850395\n",
            "Iteration 79, loss = 0.52934310\n",
            "Iteration 80, loss = 0.58056444\n",
            "Iteration 81, loss = 0.50567304\n",
            "Iteration 82, loss = 0.58665412\n",
            "Iteration 83, loss = 0.51993318\n",
            "Iteration 84, loss = 0.52448631\n",
            "Iteration 85, loss = 0.51476581\n",
            "Iteration 86, loss = 0.50922962\n",
            "Iteration 87, loss = 0.53991152\n",
            "Iteration 88, loss = 0.51797508\n",
            "Iteration 89, loss = 0.52870229\n",
            "Iteration 90, loss = 0.52583390\n",
            "Iteration 91, loss = 0.55094676\n",
            "Iteration 92, loss = 0.51447754\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.23153921\n",
            "Iteration 2, loss = 0.70191323\n",
            "Iteration 3, loss = 0.67181229\n",
            "Iteration 4, loss = 0.68126324\n",
            "Iteration 5, loss = 0.62779248\n",
            "Iteration 6, loss = 0.63094326\n",
            "Iteration 7, loss = 0.60711698\n",
            "Iteration 8, loss = 0.60753301\n",
            "Iteration 9, loss = 0.61660824\n",
            "Iteration 10, loss = 0.59267763\n",
            "Iteration 11, loss = 0.61283087\n",
            "Iteration 12, loss = 0.60052915\n",
            "Iteration 13, loss = 0.60523754\n",
            "Iteration 14, loss = 0.60465115\n",
            "Iteration 15, loss = 0.60341344\n",
            "Iteration 16, loss = 0.59021303\n",
            "Iteration 17, loss = 0.58644522\n",
            "Iteration 18, loss = 0.59455819\n",
            "Iteration 19, loss = 0.59281536\n",
            "Iteration 20, loss = 0.61250053\n",
            "Iteration 21, loss = 0.58918773\n",
            "Iteration 22, loss = 0.58164437\n",
            "Iteration 23, loss = 0.60714388\n",
            "Iteration 24, loss = 0.57556082\n",
            "Iteration 25, loss = 0.57854945\n",
            "Iteration 26, loss = 0.58829557\n",
            "Iteration 27, loss = 0.58130478\n",
            "Iteration 28, loss = 0.55934150\n",
            "Iteration 29, loss = 0.58851125\n",
            "Iteration 30, loss = 0.57394405\n",
            "Iteration 31, loss = 0.57999983\n",
            "Iteration 32, loss = 0.57679507\n",
            "Iteration 33, loss = 0.56970827\n",
            "Iteration 34, loss = 0.56562324\n",
            "Iteration 35, loss = 0.55952662\n",
            "Iteration 36, loss = 0.55869157\n",
            "Iteration 37, loss = 0.55916901\n",
            "Iteration 38, loss = 0.55894418\n",
            "Iteration 39, loss = 0.56875486\n",
            "Iteration 40, loss = 0.56353853\n",
            "Iteration 41, loss = 0.56525863\n",
            "Iteration 42, loss = 0.54874267\n",
            "Iteration 43, loss = 0.54176948\n",
            "Iteration 44, loss = 0.56228025\n",
            "Iteration 45, loss = 0.54655324\n",
            "Iteration 46, loss = 0.54321997\n",
            "Iteration 47, loss = 0.55661434\n",
            "Iteration 48, loss = 0.54348178\n",
            "Iteration 49, loss = 0.54995359\n",
            "Iteration 50, loss = 0.53350890\n",
            "Iteration 51, loss = 0.53509173\n",
            "Iteration 52, loss = 0.55603782\n",
            "Iteration 53, loss = 0.53840362\n",
            "Iteration 54, loss = 0.55255771\n",
            "Iteration 55, loss = 0.53476409\n",
            "Iteration 56, loss = 0.54427112\n",
            "Iteration 57, loss = 0.54417464\n",
            "Iteration 58, loss = 0.53930290\n",
            "Iteration 59, loss = 0.52415389\n",
            "Iteration 60, loss = 0.55609519\n",
            "Iteration 61, loss = 0.52468674\n",
            "Iteration 62, loss = 0.52295451\n",
            "Iteration 63, loss = 0.52716106\n",
            "Iteration 64, loss = 0.54173798\n",
            "Iteration 65, loss = 0.52599029\n",
            "Iteration 66, loss = 0.54185217\n",
            "Iteration 67, loss = 0.51191387\n",
            "Iteration 68, loss = 0.53296103\n",
            "Iteration 69, loss = 0.53331183\n",
            "Iteration 70, loss = 0.54289912\n",
            "Iteration 71, loss = 0.53336287\n",
            "Iteration 72, loss = 0.53612762\n",
            "Iteration 73, loss = 0.51151548\n",
            "Iteration 74, loss = 0.51113156\n",
            "Iteration 75, loss = 0.51936078\n",
            "Iteration 76, loss = 0.52417314\n",
            "Iteration 77, loss = 0.51213989\n",
            "Iteration 78, loss = 0.51134930\n",
            "Iteration 79, loss = 0.51489425\n",
            "Iteration 80, loss = 0.50901825\n",
            "Iteration 81, loss = 0.51683806\n",
            "Iteration 82, loss = 0.52200038\n",
            "Iteration 83, loss = 0.51822592\n",
            "Iteration 84, loss = 0.50769510\n",
            "Iteration 85, loss = 0.51212034\n",
            "Iteration 86, loss = 0.51227776\n",
            "Iteration 87, loss = 0.52910974\n",
            "Iteration 88, loss = 0.52531584\n",
            "Iteration 89, loss = 0.50988186\n",
            "Iteration 90, loss = 0.50911822\n",
            "Iteration 91, loss = 0.48989654\n",
            "Iteration 92, loss = 0.52908022\n",
            "Iteration 93, loss = 0.50695412\n",
            "Iteration 94, loss = 0.51468427\n",
            "Iteration 95, loss = 0.50643636\n",
            "Iteration 96, loss = 0.50182883\n",
            "Iteration 97, loss = 0.49177689\n",
            "Iteration 98, loss = 0.49919862\n",
            "Iteration 99, loss = 0.50999126\n",
            "Iteration 100, loss = 0.52382640\n",
            "Iteration 101, loss = 0.49558416\n",
            "Iteration 102, loss = 0.50304732\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.64832732\n",
            "Iteration 2, loss = 0.60812025\n",
            "Iteration 3, loss = 0.60903971\n",
            "Iteration 4, loss = 0.59405476\n",
            "Iteration 5, loss = 0.58781534\n",
            "Iteration 6, loss = 0.57480367\n",
            "Iteration 7, loss = 0.56904178\n",
            "Iteration 8, loss = 0.55307129\n",
            "Iteration 9, loss = 0.53483530\n",
            "Iteration 10, loss = 0.51888129\n",
            "Iteration 11, loss = 0.50282745\n",
            "Iteration 12, loss = 0.49891611\n",
            "Iteration 13, loss = 0.49831377\n",
            "Iteration 14, loss = 0.46791010\n",
            "Iteration 15, loss = 0.46196746\n",
            "Iteration 16, loss = 0.45323046\n",
            "Iteration 17, loss = 0.45148457\n",
            "Iteration 18, loss = 0.44891892\n",
            "Iteration 19, loss = 0.44434983\n",
            "Iteration 20, loss = 0.46222132\n",
            "Iteration 21, loss = 0.44781224\n",
            "Iteration 22, loss = 0.43961896\n",
            "Iteration 23, loss = 0.44115619\n",
            "Iteration 24, loss = 0.43769330\n",
            "Iteration 25, loss = 0.45410379\n",
            "Iteration 26, loss = 0.43996880\n",
            "Iteration 27, loss = 0.43439481\n",
            "Iteration 28, loss = 0.43641112\n",
            "Iteration 29, loss = 0.43641746\n",
            "Iteration 30, loss = 0.43433813\n",
            "Iteration 31, loss = 0.42915413\n",
            "Iteration 32, loss = 0.43274725\n",
            "Iteration 33, loss = 0.42625491\n",
            "Iteration 34, loss = 0.43643986\n",
            "Iteration 35, loss = 0.42666131\n",
            "Iteration 36, loss = 0.42805822\n",
            "Iteration 37, loss = 0.43564619\n",
            "Iteration 38, loss = 0.42155687\n",
            "Iteration 39, loss = 0.43028388\n",
            "Iteration 40, loss = 0.42146617\n",
            "Iteration 41, loss = 0.41730903\n",
            "Iteration 42, loss = 0.42019003\n",
            "Iteration 43, loss = 0.41595877\n",
            "Iteration 44, loss = 0.41491469\n",
            "Iteration 45, loss = 0.41535747\n",
            "Iteration 46, loss = 0.42773104\n",
            "Iteration 47, loss = 0.42087416\n",
            "Iteration 48, loss = 0.41197609\n",
            "Iteration 49, loss = 0.41208839\n",
            "Iteration 50, loss = 0.40583977\n",
            "Iteration 51, loss = 0.40771614\n",
            "Iteration 52, loss = 0.40928550\n",
            "Iteration 53, loss = 0.40837505\n",
            "Iteration 54, loss = 0.40728446\n",
            "Iteration 55, loss = 0.40943837\n",
            "Iteration 56, loss = 0.40839283\n",
            "Iteration 57, loss = 0.41937073\n",
            "Iteration 58, loss = 0.42066436\n",
            "Iteration 59, loss = 0.40010180\n",
            "Iteration 60, loss = 0.40040266\n",
            "Iteration 61, loss = 0.40712940\n",
            "Iteration 62, loss = 0.41127008\n",
            "Iteration 63, loss = 0.40570840\n",
            "Iteration 64, loss = 0.40741232\n",
            "Iteration 65, loss = 0.40468925\n",
            "Iteration 66, loss = 0.41270980\n",
            "Iteration 67, loss = 0.40417364\n",
            "Iteration 68, loss = 0.40062018\n",
            "Iteration 69, loss = 0.40032961\n",
            "Iteration 70, loss = 0.40419234\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "RandomizedSearchCV took 841.04 seconds for 1000 candidates parameter settings \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "start = time()\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
        "      \" parameter settings \\n\\n\" % ((time() - start), 1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ScIsAryGjFEw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "ec751da3-d682-4597-f231-dc3aa1cee277"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='logistic', batch_size=10,\n",
              "              hidden_layer_sizes=(100, 100), max_iter=400, verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, batch_size=10,\n",
              "              hidden_layer_sizes=(100, 100), max_iter=400, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, batch_size=10,\n",
              "              hidden_layer_sizes=(100, 100), max_iter=400, verbose=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "random_search.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = random_search.best_estimator_"
      ],
      "metadata": {
        "id": "KDQrJ-3AeUNG"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnwrifU-egWe",
        "outputId": "f860e21b-f099-46dd-91a2-bfa516195885"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.839563862928349"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_best_model = random_search.best_estimator_.predict(X_test)"
      ],
      "metadata": {
        "id": "ScJkNdvSeku9"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test, y_pred_best_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueaybKyrfmZm",
        "outputId": "69fbf640-9328-4c8a-d60b-388271dec001"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7777777777777778"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnf_matrix_best_score = confusion_matrix(y_test, y_pred_best_model)"
      ],
      "metadata": {
        "id": "tEILBleAfuT5"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnf_matrix_best_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaZehLsCf9vW",
        "outputId": "20f61cb9-864d-4b53-bc67-429937e123aa"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[37,  7],\n",
              "       [ 9, 19]])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnf_table = pd.DataFrame(data=cnf_matrix_best_score, index=['Survived', 'Died'], columns=['Survived', 'Died'])"
      ],
      "metadata": {
        "id": "VInRx4jBgAkR"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cnf_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYm1Kptxgj6-",
        "outputId": "e027763b-cfeb-480b-9d5a-0e78ef6e739c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Survived  Died\n",
            "Survived        37     7\n",
            "Died             9    19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred_best_model, target_names=['Survived', 'Died']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTIlLPFPgvaH",
        "outputId": "34eee258-d5a3-4543-9e07-0dd8b8ee62a6"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Survived       0.80      0.84      0.82        44\n",
            "        Died       0.73      0.68      0.70        28\n",
            "\n",
            "    accuracy                           0.78        72\n",
            "   macro avg       0.77      0.76      0.76        72\n",
            "weighted avg       0.78      0.78      0.78        72\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kdi4LkDbg95d"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}